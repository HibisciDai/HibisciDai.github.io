<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HibisciDai</title>
  
  <subtitle>Waiting/Patience/Trusting/Times All Takes</subtitle>
  <link href="http://hibiscidai.com/atom.xml" rel="self"/>
  
  <link href="http://hibiscidai.com/"/>
  <updated>2024-08-24T12:46:28.000Z</updated>
  <id>http://hibiscidai.com/</id>
  
  <author>
    <name>HibisciDai</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>内网穿透</title>
    <link href="http://hibiscidai.com/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    <id>http://hibiscidai.com/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/</id>
    <published>2024-10-22T10:48:12.620Z</published>
    <updated>2024-08-24T12:46:28.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F.png" class="" title="内网穿透"><p>内网穿透</p><span id="more"></span><h1 id="内网穿透"><a href="#内网穿透" class="headerlink" title="内网穿透"></a>内网穿透</h1><p>内网穿透，简单地说就是内网的数据让外网可以获取，可以映射到公共网络上，这样就可以在公共网络上访问内网的数据。 内网是不能被外网直接访问的，只能通过一些中转技术，如DingTalk Design CLI、花生壳、Natap 等工具，让内网“假装”成外网，就是内网穿透。</p><h1 id="花生壳"><a href="#花生壳" class="headerlink" title="花生壳"></a>花生壳</h1><p>国内老牌内网映射品牌，有较成熟的<br>可以做域名映射等。</p><blockquote><p>工具使用简单，流量烧钱，成熟品牌，价格较贵。</p></blockquote><p><a href="https://hsk.oray.com/">花生壳官网</a><br><a href="https://console.hsk.oray.com/home">花生壳控制台</a></p><h1 id="FRP"><a href="#FRP" class="headerlink" title="FRP"></a>FRP</h1><p>frp 是一款高性能的反向代理应用，专注于内网穿透。它支持多种协议，包括 TCP、UDP、HTTP、HTTPS 等，并且具备 P2P 通信功能。使用 frp，您可以安全、便捷地将内网服务暴露到公网，通过拥有公网 IP 的节点进行中转。</p><p><a href="https://github.com/fatedier/frp">github.com/fatedier/frp</a><br><a href="https://github.com/fatedier/frp/releases">下载地址</a><br><a href="https://gofrp.org/zh-cn/">FRP说明文档</a></p><h1 id="樱花内网穿透-自用"><a href="#樱花内网穿透-自用" class="headerlink" title="樱花内网穿透-自用"></a>樱花内网穿透-自用</h1><p><a href="https://www.natfrp.com/">SAKURA FRP</a></p><p>目前发现比较便宜的二次开发FRP</p><h1 id="Padavan固件路由器内网穿透"><a href="#Padavan固件路由器内网穿透" class="headerlink" title="Padavan固件路由器内网穿透"></a>Padavan固件路由器内网穿透</h1><h2 id="使用花生壳内网版"><a href="#使用花生壳内网版" class="headerlink" title="使用花生壳内网版"></a>使用花生壳内网版</h2><img src="/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/%E8%8A%B1%E7%94%9F%E5%A3%B3%E5%86%85%E7%BD%91%E7%89%88-1.png" class="" title="花生壳内网版-1"><p>点击开，需要等一会才可以读取到目前设备的SN。</p><p>然后去花生壳官网。</p><img src="/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/%E8%8A%B1%E7%94%9F%E5%A3%B3%E5%86%85%E7%BD%91%E7%89%88-2.png" class="" title="花生壳内网版-2"><p>然后购买服务就额可以了。</p><p>有6块的新人券，但是速度和流量进行了限制，可以用作偶尔的登录页面管理，可以采用。</p><p>博主到此为止没有尝试（太贵了）。</p><h2 id="使用FRP"><a href="#使用FRP" class="headerlink" title="使用FRP"></a>使用FRP</h2><img src="/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/FRP-1.png" class="" title="FRP-1"><p>目前发现版本BUG，指定版本不太好用。</p><p>可以用来当做服务器，但是使用第三方服务的时候不通。</p><p>默认参数文件位置：<code>/etc/storage/frp_script.sh</code></p><p>贴默认启动参数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">export PATH=&#x27;/etc/storage/bin:/tmp/script:/etc/storage/script:/opt/usr/sbin:/opt/usr/bin:/opt/sbin:/opt/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/sbin:/bin&#x27;</span><br><span class="line">export LD_LIBRARY_PATH=/lib:/opt/lib</span><br><span class="line">killall frpc frps</span><br><span class="line">rm -f /dev/null ; mknod /dev/null c 1 3 ; chmod 666 /dev/null;</span><br><span class="line">mkdir -p /tmp/frp</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动frp功能后会运行以下脚本</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">frp项目地址教程: https://github.com/fatedier/frp/blob/master/README_zh.md</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">请自行修改 token 用于对客户端连接进行身份验证</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">IP查询： http://119.29.29.29/d?dn=github.com</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat &gt; &quot;/tmp/frp/myfrpc.ini&quot; &lt;&lt;-\EOF</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">==========客户端配置：==========</span></span><br><span class="line">[common]</span><br><span class="line">server_addr = xxx</span><br><span class="line">server_port = xxx</span><br><span class="line">token = xxx</span><br><span class="line">tls_enable = true</span><br><span class="line">pool_count = 1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">log_file = /dev/null</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">log_level = info</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">log_max_days = 3</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">[web]</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">remote_port = 6000</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">服务端开放的端口</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">remote_port = 6000</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">type</span> = tcp</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">local_ip = xxx</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">local_port = 80</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">subdomain = <span class="built_in">test</span></span></span><br><span class="line"></span><br><span class="line">[***]</span><br><span class="line">type = http</span><br><span class="line">local_ip = xxx</span><br><span class="line">local_port = 80</span><br><span class="line">subdomain = test</span><br><span class="line">use_compression = true</span><br><span class="line"></span><br><span class="line">[nas] 我的nas</span><br><span class="line">type = tcp</span><br><span class="line">local_ip = xxx</span><br><span class="line">local_port = 5000</span><br><span class="line">remote_port = 5000</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">host_header_rewrite = 实际你内网访问的域名，可以供公网的域名不一致，如果一致可以不写</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">====================</span></span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">请手动配置【外部网络 (WAN) - 端口转发 (UPnP)】开启 WAN 外网端口</span></span><br><span class="line">cat &gt; &quot;/tmp/frp/myfrps.ini&quot; &lt;&lt;-\EOF</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">==========服务端配置：==========</span></span><br><span class="line">[common]</span><br><span class="line">bind_port = 7000</span><br><span class="line">dashboard_port = 7500</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">dashboard 用户名密码，默认都为 admin</span></span><br><span class="line">dashboard_user = admin</span><br><span class="line">dashboard_pwd = admin</span><br><span class="line">vhost_http_port = 88</span><br><span class="line">token =</span><br><span class="line">subdomain_host = frps.com</span><br><span class="line">max_pool_count = 50</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">log_file = /dev/null</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">log_level = info</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">log_max_days = 3</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">====================</span></span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动：</span></span><br><span class="line">frpc_enable=`nvram get frpc_enable`</span><br><span class="line">frpc_enable=$&#123;frpc_enable:-&quot;0&quot;&#125;</span><br><span class="line">frps_enable=`nvram get frps_enable`</span><br><span class="line">frps_enable=$&#123;frps_enable:-&quot;0&quot;&#125;</span><br><span class="line">if [ &quot;$frpc_enable&quot; = &quot;1&quot; ] ; then</span><br><span class="line">    frpc -c /tmp/frp/myfrpc.ini 2&gt;&amp;1 &amp;</span><br><span class="line">fi</span><br><span class="line">if [ &quot;$frps_enable&quot; = &quot;1&quot; ] ; then</span><br><span class="line">    frps -c /tmp/frp/myfrps.ini 2&gt;&amp;1 &amp;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="使用第三方FRP-樱花内网为例"><a href="#使用第三方FRP-樱花内网为例" class="headerlink" title="使用第三方FRP-樱花内网为例"></a>使用第三方FRP-樱花内网为例</h2><h3 id="开启路由器ssh登录"><a href="#开启路由器ssh登录" class="headerlink" title="开启路由器ssh登录"></a>开启路由器ssh登录</h3><img src="/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/%E5%BC%80%E5%90%AF%E8%B7%AF%E7%94%B1%E5%99%A8ssh%E7%99%BB%E5%BD%95-1.png" class="" title="开启路由器ssh登录-1"><p>注意开启ssh后，设置路由器端口转发设置。</p><p>外部网络（WAN）→端口转发，注意打开端口。</p><h3 id="检查路由器处理器"><a href="#检查路由器处理器" class="headerlink" title="检查路由器处理器"></a>检查路由器处理器</h3><p>使用ssh登录到路由器后台</p><p>确认处理器架构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -m</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:center">输出</th><th style="text-align:center">架构</th></tr></thead><tbody><tr><td style="text-align:center">i386,i686</td><td style="text-align:center">i386</td></tr><tr><td style="text-align:center">x86_64</td><td style="text-align:center">amd64</td></tr><tr><td style="text-align:center">arm,armel</td><td style="text-align:center">arm_garbage</td></tr><tr><td style="text-align:center">armv71,armhf</td><td style="text-align:center">armv7</td></tr><tr><td style="text-align:center">aarch64,armv81</td><td style="text-align:center">arm64</td></tr><tr><td style="text-align:center">mips</td><td style="text-align:center">mips</td></tr><tr><td style="text-align:center">mips64</td><td style="text-align:center">mips64</td></tr></tbody></table></div><p>小米3路由器→mips</p><p>确认处理器字节序：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">一般来说只需要使用这条命令:</span></span><br><span class="line">echo -n I | hexdump -o | awk &#x27;&#123;print substr($2,6,1); exit&#125;&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果上面的命令报错，请尝试这条:</span></span><br><span class="line">echo -n I | od -to2 | awk &#x27;&#123;print substr($2,6,1); exit&#125;&#x27;</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">| 输出 | 架构 |</span><br><span class="line">| :-: | :-: |</span><br><span class="line">| 0 | mips/mips64 |</span><br><span class="line">| 1 | mipsle/mips64le |</span><br><span class="line"></span><br><span class="line">这一步指的是小端还是大端，可以直接查询路由器制造商或者百度。</span><br><span class="line"></span><br><span class="line">小米3路由器→小端</span><br><span class="line"></span><br><span class="line">所以最终确定的是`mipsle`</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">## 下载及上传适合系统的frp包</span></span></span><br><span class="line"></span><br><span class="line">前往[natfrp-download](https://www.natfrp.com/tunnel/download)下载</span><br><span class="line"></span><br><span class="line">{% asset_img 下载及上传适合系统的frp包-1.png 下载及上传适合系统的frp包-1 %}</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">注意Padavan系统不同于常规的linux，不能使用weget方式下载软件包，还需要打开文件传输软件进行上传。</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">Padavan的系统（或者说目前刷路由器，非软路由，可用的系统里）大部分目录都是重启即丢失的。</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">frp添加到/etc/storage目录下，并且烧录，使其重启可用。</span></span><br><span class="line"></span><br><span class="line">更改下载后的文件名字变为`frpc`</span><br><span class="line"></span><br><span class="line">上传至`/etc/storge/bin`中</span><br><span class="line"></span><br><span class="line">注意修改文件权限为755</span><br><span class="line"></span><br><span class="line">在该文件夹下运行</span><br><span class="line"></span><br><span class="line">```shell</span><br><span class="line">frpc -v</span><br></pre></td></tr></table></figure><p>查看软件版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.51.0-sakura-8</span><br></pre></td></tr></table></figure><h3 id="FRP服务器配置"><a href="#FRP服务器配置" class="headerlink" title="FRP服务器配置"></a>FRP服务器配置</h3><img src="/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/FRP%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE-1.png" class="" title="FRP服务器配置-1"><p>本地ip填写要转发的设备，我选的是NAS做登录页面测试转发。</p><p>访问密码随机生成。</p><img src="/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/FRP%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE-2.png" class="" title="FRP服务器配置-2"><p>对于FRP的配置，如果是上游原生的FRP软件，会根据对应的版本进行生成配置。生成的配置需要手动去调配，就比如Padavan系统内置的FRP，需要手动配置。</p><p>樱花第三方的包有一键配置功能，例如点击某个隧道的配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">frpc -f wdnmdtoken666666:12345 &amp;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> &amp;放在后台运行</span></span><br></pre></td></tr></table></figure><p>使用 启动参数 启动 frpc，只需要在启动参数中加上半角逗号 , 分隔的其他隧道 ID 即可，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">frpc -f &lt;访问密钥&gt;:&lt;隧道ID 1&gt;[,隧道ID 2[,隧道ID 3...]]</span><br><span class="line">frpc -f wdnmdtoken666666:114514,114515,114516</span><br></pre></td></tr></table></figure><p>此时回到路由器的ssh，执行命令，开启frpc。</p><p>然后会提示隧道启动成功，这时候可以通过提供的域名端口进行访问测试。</p><blockquote><p>注意路由器报打开端口转发。</p></blockquote><p>对于已经建立并且成功的隧道，官网前方灰色按钮会变绿。</p><p>对于http链接，第一次访问还需要认证。</p><img src="/2024/10/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/FRP%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE-3.png" class="" title="FRP服务器配置-3"><p>可以授权固定ip访问该隧道。</p><blockquote><p>到这一步已经跑通了所有流程。</p></blockquote><p>还可以运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frpc -w</span><br></pre></td></tr></table></figure><p>使用秘钥登录后可以直接进行</p><h3 id="路由配置"><a href="#路由配置" class="headerlink" title="路由配置"></a>路由配置</h3><h4 id="第三方配置"><a href="#第三方配置" class="headerlink" title="第三方配置"></a>第三方配置</h4><p>由于Padavan系统的特殊性，关键目录以外的目录均为 tmpfs ，可以理解为是把数据暂存在内存上。因此在/etc/storage目录里所做的修改，如果没有执行保存脚本，就并没有真正的写入 Rom 里，重启之后文件还会丢失。</p><p>完成上述测试后，确定没有问题，可以进行烧录。</p><p>选择“保存/etc/storage/内容到闪存”的提交。</p><p>重启路由器，检查文件是否还在</p><p>设置开机自启</p><p>高级设置→自定义设置→脚本→在路由器启动后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动樱花FRP</span></span><br><span class="line">/etc/storage/frpc -f xxx:xxx &amp;</span><br></pre></td></tr></table></figure><p>查询所有服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl list-units --all &quot;frpc@*&quot;</span><br></pre></td></tr></table></figure><blockquote><p>开机自启没有采用</p></blockquote><h4 id="使用自带FRP"><a href="#使用自带FRP" class="headerlink" title="使用自带FRP"></a>使用自带FRP</h4><p>检查版本 贴配置就行</p><p>最终Padavan0.6.0</p><h3 id="FRP客户端配置"><a href="#FRP客户端配置" class="headerlink" title="FRP客户端配置"></a>FRP客户端配置</h3><p>查看隧道日志：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">journalctl -u &lt;Unit名称&gt;</span><br><span class="line">journalctl -u frpc@wdnmdtoken666666:12345</span><br></pre></td></tr></table></figure><p>如果当前窗口无法显示所有日志，可以用 ↑、↓ 方向键滚动，输入大写的 G 跳转动到日志底部，输入 q 退出日志查看。</p><p>参考链接</p><p><a href="https://www.right.com.cn/forum/thread-5560332-1-1.html">如何正确配置Padavan老毛子frp的frp_script客户端</a><br><a href="https://sixdian.com/post/openwrt-padavan-frp/">OpenWrt、Padavan 下 frp 服务和客户端的配置</a><br><a href="https://sparkle.im/post/padavan%E8%87%AA%E5%B7%B1%E5%A2%9E%E5%8A%A0frp%E5%B9%B6%E6%8E%92%E9%99%A4%E6%95%85%E9%9A%9C">Padavan自己增加frp并排除故障</a></p><h1 id="Unbutu内网穿透"><a href="#Unbutu内网穿透" class="headerlink" title="Unbutu内网穿透"></a>Unbutu内网穿透</h1><h2 id="安装FRP客户端"><a href="#安装FRP客户端" class="headerlink" title="安装FRP客户端"></a>安装FRP客户端</h2><p>宝塔面板内置</p><h2 id="设置开机自启动"><a href="#设置开机自启动" class="headerlink" title="设置开机自启动"></a>设置开机自启动</h2><p>设置宝塔开机启动</p><p>设置开机自启：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">systemctl &lt;enable|disable&gt; &lt;Unit名称&gt;</span><br><span class="line">systemctl status &lt;Unit名称&gt;</span><br><span class="line"></span><br><span class="line">systemctl enable frpc@wdnmdtoken666666:12345</span><br><span class="line">systemctl status frpc@wdnmdtoken666666:12345</span><br><span class="line"></span><br><span class="line">systemctl enable frpc@tf7be1xpwaxpugip67ddu59eifs433fn:17294766</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>同上</p><h1 id="宝塔内网穿透"><a href="#宝塔内网穿透" class="headerlink" title="宝塔内网穿透"></a>宝塔内网穿透</h1><p>使用第三方插件安装</p><p>安装FRPs</p><p>后台7500可以看到连接数</p><p>然后在别的机器上使用FRPc即可</p><p>也有win端，可以实现本地端口的公网映射</p><p>如果使用的是阿里云或者腾讯云的服务器</p><p>流量包月模式可以不收费</p><p>第三方的穿透服务会收费</p><h1 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h1><p>ONVIF转发因为涉及到http所以只能发请求，但是没有回应就不行。</p><p>对于视频流可以利用rtsp承载，带账号密码的输入，就可以持续的穿透转发。</p>]]></content>
    
    
    <summary type="html">内网穿透</summary>
    
    
    
    <category term="计算机网络" scheme="http://hibiscidai.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="路由器" scheme="http://hibiscidai.com/tags/%E8%B7%AF%E7%94%B1%E5%99%A8/"/>
    
    <category term="Linux" scheme="http://hibiscidai.com/tags/Linux/"/>
    
    <category term="内网穿透" scheme="http://hibiscidai.com/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    
  </entry>
  
  <entry>
    <title>AVIZO二值化数据导出</title>
    <link href="http://hibiscidai.com/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/"/>
    <id>http://hibiscidai.com/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/</id>
    <published>2024-10-22T10:48:12.618Z</published>
    <updated>2024-08-30T06:49:10.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA.png" class="" title="AVIZO二值化数据导出"><p>AVIZO二值化数据导出</p><span id="more"></span><h1 id="AVIZO二值化数据导出"><a href="#AVIZO二值化数据导出" class="headerlink" title="AVIZO二值化数据导出"></a>AVIZO二值化数据导出</h1><p>万老师b站的视频教程：</p><p><a href="https://www.bilibili.com/video/BV1Le4y197rh/?spm_id_from=333.999.0.0&amp;vd_source=e29d2e4b12ca5d1c1091b9265d56c53d">Avizo中如何导出可直接浏览的二值化数据结果</a></p><p>样例数据：</p><p>11SANDSTONS，十一块贝瑞砂岩数字岩心数据。</p><p>数据说明<code>Berea_2d25um_binary.raw</code></p><p>导入格式：</p><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-0.png" class="" title="AVIZO二值化数据导出-0"><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-1.png" class="" title="AVIZO二值化数据导出-1"><p>骨架1，孔隙0。</p><p>导出为tif</p><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-2.png" class="" title="AVIZO二值化数据导出-2"><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-3.png" class="" title="AVIZO二值化数据导出-3"><p>系统会自动导出每一个切片为一个个单张的二值化图像，后缀为tif</p><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-4.png" class="" title="AVIZO二值化数据导出-4"><p>但是用本地图片查看器打开之后是一片黑</p><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-5.png" class="" title="AVIZO二值化数据导出-5"><p>查看图片属性</p><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-6.png" class="" title="AVIZO二值化数据导出-6"><p>原因是图片查看器是0-255显示，但是0-1二值图没有办法显示。</p><p>这种图可以用于训练，或者导入专用的图片查看器可以查看。</p><p>但是不方便认为查看。</p><p>解决思路：</p><p>骨架1，孔隙0。→骨架255，孔隙0。将数值放大。</p><p>操作方法：</p><p>原始数据→Arithmetic→输入A×255→输出保存即可</p><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-7.png" class="" title="AVIZO二值化数据导出-7"><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-8.png" class="" title="AVIZO二值化数据导出-8"><img src="/2024/10/22/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/AVIZO%E4%BA%8C%E5%80%BC%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA-9.png" class="" title="AVIZO二值化数据导出-9"><p>重新导出后，可以在本地看到数据，方便人预览。</p><p>对于porespy要求的数据格式：</p><p>骨架0，孔隙1。</p><p>需要进行的操作</p><p>骨架1，孔隙0。→骨架0，孔隙1。</p><p>步骤：</p><p>孔隙0，骨架1<br>A × 255<br>孔隙0，骨架255<br>255 - A<br>孔隙255，骨架0<br>A / 255<br>孔隙1，骨架0</p><p>经过多次运算后实现孔隙骨架数值取反。</p><p>直接使用NOT会出现问题，区间会变成254-255。</p>]]></content>
    
    
    <summary type="html">AVIZO二值化数据导出</summary>
    
    
    
    <category term="AvizoUsersGuide" scheme="http://hibiscidai.com/categories/AvizoUsersGuide/"/>
    
    
    <category term="Avizo" scheme="http://hibiscidai.com/tags/Avizo/"/>
    
    <category term="石油地质" scheme="http://hibiscidai.com/tags/%E7%9F%B3%E6%B2%B9%E5%9C%B0%E8%B4%A8/"/>
    
    <category term="数字岩心" scheme="http://hibiscidai.com/tags/%E6%95%B0%E5%AD%97%E5%B2%A9%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>AVIZO曲率计算</title>
    <link href="http://hibiscidai.com/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/"/>
    <id>http://hibiscidai.com/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/</id>
    <published>2024-10-22T10:48:12.616Z</published>
    <updated>2024-09-24T09:05:28.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97.png" class="" title="AVIZO曲率计算"><p>AVIZO曲率计算</p><span id="more"></span><h1 id="AVIZO曲率计算"><a href="#AVIZO曲率计算" class="headerlink" title="AVIZO曲率计算"></a>AVIZO曲率计算</h1><p>参考视频教程：</p><p><a href="https://www.bilibili.com/video/BV1aY4y1E7dE/?spm_id_from=333.880.my_history.page.click&amp;vd_source=e29d2e4b12ca5d1c1091b9265d56c53d"> 通过分形维数和表面曲率衡量样品的粗糙度</a></p><p><a href="https://www.bilibili.com/video/BV1nW4y1q7E7/?spm_id_from=333.999.0.0&amp;vd_source=e29d2e4b12ca5d1c1091b9265d56c53d">Avizo中利用auto refresh功能半自动化进行REV（体元表征）</a></p><h1 id="计算流程"><a href="#计算流程" class="headerlink" title="计算流程"></a>计算流程</h1><p>原始数据<br>↓<br>数据体分割提取<br>↓<br>分割后感兴趣的区域（孔隙）<br>↓<br>生成表面网格<br>↓<br>曲率计算</p><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97_1.png" class="" title="AVIZO曲率计算_1"><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97_2.png" class="" title="AVIZO曲率计算_2"><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97_3.png" class="" title="AVIZO曲率计算_3"><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97_4.png" class="" title="AVIZO曲率计算_4"><p>如果要实现批量计算需要写脚本代码实现</p><h1 id="Image-Curvature-3D-原理"><a href="#Image-Curvature-3D-原理" class="headerlink" title="Image Curvature 3D-原理"></a>Image Curvature 3D-原理</h1><p>3维会生成两个曲率：<br>第一个默认是高斯<br>第二个默认是平均</p><p>2维生成一个曲率→高斯</p><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97_6.png" class="" title="AVIZO曲率计算_6"><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97_7.png" class="" title="AVIZO曲率计算_7"><h2 id="Description"><a href="#Description" class="headerlink" title="Description:"></a>Description:</h2><p>This module computes for each voxel of an input image local texture curvature. Computed curvature(s) takes voxel size into account.<br>More informations on these algorithms can be found in:</p><p>该模块计算输入图像中每个体素的局部纹理曲率。计算出的曲率考虑了体素的大小。<br>有关这些算法的更多信息，请参见：</p><p>该模块接收两个或多个输入数据对象（例如两个表面或两个四面体网格），并通过线性插值计算输出对象。插值过程可以在线性插值顶点位置的同时插值附加到表面或网格上的数据字段。</p><ul><li>On Curvature Estimation of ISO Surfaces in 3D Gray-Value Images and the Computation of Shape Descriptors, Bernd Rieger, Frederik J. Timmermans, Lucas J. van Vliet and Piet W. Verbeek, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 26, NO. 8, AUGUST 2004.</li></ul><p>This module compute local gray level 2D curvature in image. For each pixel, the curvature parameter K is linked to radius of curvature R by formula:<br>该模块计算图像中的局部灰度二维曲率。对于每个像素，曲率参数 K 与曲率半径 R 的关系如下：</p><script type="math/tex; mode=display">k = \frac{1}{R}</script><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97_8.png" class="" title="AVIZO曲率计算_8"><p>This module computes local gray level 3D curvatures in image. Each voxel is then associated with two principal curvatures:  Kmax and Kmin. The Kmax stands for maximal local curvature whereas  stands for minimal curvature.</p><p>此模块计算图像中的局部灰度 3D 曲率。每个体素随后与两个主曲率相关联：Kmax 和 Kmin。Kmax 代表最大局部曲率，而 Kmin 代表最小曲率。</p><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97_5.png" class="" title="AVIZO曲率计算_5"><p>Two important indicators can be extracted from these values:<br>从这些数值中可以提取两个重要指标：</p><p>Gaussian curvature: </p><script type="math/tex; mode=display">K = K_{max} × K_{min}</script><p>Mean curvature:</p><script type="math/tex; mode=display">K = \frac{1}{2} (K_{max} + K_{min})</script><p>Gaussian curvature can be used to characterize local surface comportment:</p><ul><li>If Gaussian curvature is positive, both principal curvatures have same sign and local surface will be dome like (which mean that it is locally lying on one side of its tangent plane. In this case, a positive curvature stand for convex surface whereas a negative value stand for a concave surface.</li><li>If Gaussian curvature is negative, principal curvatures have opposite signs and local surface will be saddle shaped (see Figure 1).</li><li>If Gaussian curvature is null, surface will be locally parabolic like. If Kmax = 0, surface profil will locally be valley like. If Kmin = 0, surface profil will locally be crest like.<br>See also: Flow Inpainting.</li></ul><p>高斯曲率可用于表征局部表面行为：</p><ul><li>如果高斯曲率为正，则两个主曲率具有相同的符号，局部表面将呈圆顶状（这意味着它局部位于其切平面的一侧。在这种情况下，正曲率代表凸面，而负值代表凹面。</li><li>如果高斯曲率为负，则主曲率具有相反的符号，局部表面将呈马鞍形（见上图 ）。</li><li>如果高斯曲率为零，则表面将局部呈抛物线状。如果Kmax = 0 ，表面轮廓将局部呈山谷状。如果 Kmin = 0 ，表面轮廓将局部呈波峰状。<br>另请参阅：流修复。</li></ul><h2 id="Connections"><a href="#Connections" class="headerlink" title="Connections:"></a>Connections:</h2><h3 id="Input-Image-required-输入图像-必需"><a href="#Input-Image-required-输入图像-必需" class="headerlink" title="Input Image [required] 输入图像 [必需]"></a>Input Image [required] 输入图像 [必需]</h3><p>The image to be analyzed. Supported types include: grayscale image (Uniform Scalar Field), binary (Uniform Label Field with 2 labels) and label (Uniform Label Field) images.</p><p>要分析的图像。支持的类型包括：灰度图像（统一标量场）、二进制（具有 2 个标签的统一标签场）和标签（统一标签场）图像。</p><h3 id="Input-Image-Mask-optional-输入图像蒙版-可选"><a href="#Input-Image-Mask-optional-输入图像蒙版-可选" class="headerlink" title="Input Image Mask [optional] 输入图像蒙版 [可选]"></a>Input Image Mask [optional] 输入图像蒙版 [可选]</h3><p>The masking image. Supported types include: binary images (Uniform Label Field with 2 labels).</p><p>蒙版图像。支持的类型包括：二进制图像（具有 2 个标签的统一标签场）。</p><h2 id="Ports"><a href="#Ports" class="headerlink" title="Ports:"></a>Ports:</h2><h3 id="Interpretation-解释"><a href="#Interpretation-解释" class="headerlink" title="Interpretation 解释"></a>Interpretation 解释</h3><p>This port specifies whether the input will be interpreted as a 3D volume or a stack of 2D images for processing.<br>“3D”: the module configuration is set to 3D. The image will be processed as a whole in 3D.<br>“XY planes”: the module configuration is set to 2D. The image will be processed slice per slice.</p><p>此端口指定输入是否将被解释为 3D 体积或 2D 图像堆栈以供处理。<br>“3D”：模块配置设置为 3D。图像将以 3D 形式整体处理。<br>“XY 平面”：模块配置设置为 2D。图像将逐片处理。</p><h3 id="Standard-Deviation"><a href="#Standard-Deviation" class="headerlink" title="Standard Deviation"></a>Standard Deviation</h3><p>This port defines the standard deviation used for Gaussian smoothing of image Tensor.<br>该端口定义用于图像张量高斯平滑的标准偏差。</p><h1 id="Curvature-Integrals-原理"><a href="#Curvature-Integrals-原理" class="headerlink" title="Curvature Integrals -原理"></a>Curvature Integrals -原理</h1><h2 id="Description-1"><a href="#Description-1" class="headerlink" title="Description:"></a>Description:</h2><p>For an introduction, see section Analysis.<br>有关介绍，请参阅分析部分。</p><p>This module computes the integral of mean curvature and integral of total curvature of objects in a binary image. Intuitively, “curvature” is the amount by which a geometric object deviates from being “flat”.<br>此模块计算二值图像中物体的平均曲率积分和总曲率积分。直观地说，“曲率”是几何物体偏离“平面”的量。</p><p>This module computes a local measure. It is obtained as the sum of measures in local 2x2x2 neighborhoods (a cube), for 13 planes associated with different normal directions and hitting three or four vertices of the cells (in the cubical lattice).<br>此模块计算局部测量值。它是局部 2x2x2 邻域（立方体）中测量值的总和，针对与不同法线方向相关的 13 个平面，并击中立方体格子中的三个或四个顶点。</p><p>In the case of very elongated objects (needles or fibers) the integral of mean curvature M can be used to measure the length L of the object: L= M / π<br>对于非常细长的物体（针或纤维），平均曲率积分 M 可用于测量物体的长度 L：L= M / π</p><p>For convex object, the integral of mean curvature M is (up to a constant) equivalent to the mean diameter, i.e.<br>对于凸物体，平均曲率积分 M（最多一个常数）等于平均直径，即</p><script type="math/tex; mode=display">M = 2 \pi d where d = \frac{1}{13} (\sum^1_{i=0} 3 d_i)</script><p>The Euler number and the Integral of Total Curvature carry the same information about the object. They differ by the constant factor 4π. If we consider a set X of the 3-dimensional space and X（X） being its Euler number then the integral total curvature of X will be: K(X) = 4πX(X)<br>欧拉数和总曲率积分都包含相同的物体信息，它们相差一个常数因子 4π。如果我们考虑三维空间中的集合 X，X（X）是它的欧拉数，那么 X 的积分总曲率将是：K(X) = 4πX(X)</p><p>See Also: Euler Number</p><p>For more information on Integral Curvatures you can refer to</p><ul><li>C .Lang, J. Ohser, R.Hilfer (1999) On the Analysis of Spatial Binary Images</li></ul><p>See also: Area 3D, Euler Number 3D, Fractal Dimension, Moments of Inertia 3D, Volume Fraction.</p><h2 id="Connections-1"><a href="#Connections-1" class="headerlink" title="Connections:"></a>Connections:</h2><p>Input Image [required]<br>The image to be analyzed. Supported types include: binary images (Uniform Label Field with 2 labels).<br>需要分析的图像。支持的类型包括：二值图像（具有 2 个标签的统一标签字段）。</p><h2 id="Ports-1"><a href="#Ports-1" class="headerlink" title="Ports:"></a>Ports:</h2><p>Interpretation </p><p>This port specifies whether the input will be interpreted as a 3D volume or a stack of 2D images for processing. The port is grayed out if alternate interpretation is not available.<br>此端口指定输入是否将被解释为 3D 体积或 2D 图像堆栈以供处理。如果没有其他解释，则端口将变灰。</p><h1 id="curvature-原理"><a href="#curvature-原理" class="headerlink" title="curvature-原理"></a>curvature-原理</h1><p>需要生成表面</p><h2 id="Description-2"><a href="#Description-2" class="headerlink" title="Description:"></a>Description:</h2><p>This module computes curvature information for a discrete triangular surface of type Surface.<br>该模块计算表面类型的离散三角表面的曲率信息。</p><p>Either the maximum principal curvature value, the reciprocal curvature value, or the direction of maximum principal curvature can be computed. The algorithm works by approximating the surface locally by a quadric form. The eigenvalues and eigenvectors of the quadric form correspond to the principal curvature values and to the directions of principal curvature. Note that the algorithm does not produce meaningful results near locations where the input surface is not topologically flat, i.e., where it has non-manifold structure.</p><p>可以计算最大主曲率值、倒数曲率值或最大主曲率方向。该算法通过用二次型局部近似表面来工作。二次型的特征值和特征向量对应于主曲率值和主曲率方向。请注意，该算法在输入表面不是拓扑平坦的位置附近（即具有非流形结构的位置）不会产生有意义的结果。</p><p>Press the Apply button to start the computation.</p><p>按下“应用”按钮开始计算。</p><h2 id="Connections-2"><a href="#Connections-2" class="headerlink" title="Connections:"></a>Connections:</h2><p>Data [required]<br>The surface for which curvature information should be computed.</p><p>数据 [必需]<br>应计算曲率信息的表面。</p><h2 id="Ports-2"><a href="#Ports-2" class="headerlink" title="Ports:"></a>Ports:</h2><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86_1.png" class="" title="曲率计算原理_1"><p>Radio box allowing the user to select between two different computational algorithms. Choice on triangles produces a surface field with curvature values or curvature vectors being defined on the surface’s triangles. Alternatively, by selecting on vertices a surface field with data being defined on the vertices can be generated.</p><p>单选框允许用户在两种不同的计算算法之间进行选择。选择三角形<code>on triangles</code>会产生一个表面场，其曲率值或曲率向量在表面的三角形上定义。或者，选择顶点<code>on vertices</code>可以生成一个表面场，其数据在顶点上定义。</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86_2.png" class="" title="曲率计算原理_2"><p>The first input, denoted nLayers, determines which triangles are considered to be neighbors of a given triangle and which points are considered to be neighbors of a given point. If the value of this input is 1, then only triangles sharing a common edge with a given triangle are considered to be neighbors of this triangle and only points directly connected to a given point are considered to be neighbors of this point. For larger values of nLayers successively larger neighborhoods are taken into account.</p><p>第一个输入表示为 nLayers，它确定哪些三角形被视为给定三角形的邻居，哪些点被视为给定点的邻居。如果此输入的值为 1，则只有与给定三角形共享公共边的三角形才被视为此三角形的邻居，只有直接连接到给定点的点才被视为此点的邻居。对于较大的 nLayers 值，会依次考虑较大的邻域。</p><p>The second input, denoted nAverage, determines how many times the initial curvature values computed for a triangle or for a point are being averaged with the curvature values of direct (first-order) neighbor triangles or points. The larger the value of nAverage the smoother the curvature data being obtained. Note that averaging only applies to the scalar curvature values, not to the directional curvature vectors which are computed when port output is set to max direction.</p><p>第二个输入表示为 nAverage，它决定了为三角形或点计算的初始曲率值与直接（一阶）相邻三角形或点的曲率值取平均值的次数。nAverage 的值越大，获得的曲率数据越平滑。请注意，平均仅适用于标量曲率值，而不适用于当端口输出设置为最大方向时计算的方向曲率向量。</p><h3 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h3><img src="/2024/10/22/AVIZO%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97/%E6%9B%B2%E7%8E%87%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86_3.png" class="" title="曲率计算原理_3"><p>This menu controls the output of the curvature module.<br>此菜单控制曲率模块的输出。</p><h4 id="Max-curvature"><a href="#Max-curvature" class="headerlink" title="Max curvature"></a><code>Max curvature</code></h4><p>a surface scalar field is generated containing the maximal principal curvature of a triangle or of a point.<br>生成一个包含三角形或点的最大主曲率的表面标量场。</p><h4 id="1-Max-curvature"><a href="#1-Max-curvature" class="headerlink" title="1/Max curvature"></a><code>1/Max curvature</code></h4><p>the curvature values are inverted. In this case, the output values have the dimension of a length, indicating the radius of a sphere locally fitting the surface.</p><p>曲率值被反转。在这种情况下，输出值具有长度维度，表示局部拟合表面的球体的半径。</p><h4 id="Mean-curvature"><a href="#Mean-curvature" class="headerlink" title="Mean curvature"></a><code>Mean curvature</code></h4><p>is similar to Max curvature. Here, however, the mean value of the two principal curvature values is computed. This quantity will be negative in strictly concave regions and positive in strictly convex regions. It can be zero in regions where a positive and negative principal curvature cancel each other.<br>与最大曲率类似。不过，这里计算的是两个主曲率值的平均值。这个量在严格凹陷区域为负，在严格凸陷区域为正。在正负主曲率相互抵消的区域，它可以为零。</p><h4 id="1-Mean-curvature"><a href="#1-Mean-curvature" class="headerlink" title="1/Mean curvature"></a><code>1/Mean curvature</code></h4><p>the mean curvature values are inverted.<br>平均曲率值被反转。</p><h4 id="Gauss-curvature"><a href="#Gauss-curvature" class="headerlink" title="Gauss curvature"></a><code>Gauss curvature</code></h4><p>is the product of the two principal curvatures. It is negative in surface areas with hyperbolic geometry (convex-concave, like near saddle points) and positive in areas with elliptic geometry (strictly convex or strictly concave).<br>是两个主曲率的乘积。在具有双曲几何（凸凹，如鞍点附近）的表面区域，它是负值；在具有椭圆几何（严格凸或严格凹）的区域，它是正值。</p><h4 id="1-Gauss-curvature"><a href="#1-Gauss-curvature" class="headerlink" title="1/Gauss curvature"></a><code>1/Gauss curvature</code></h4><p>Gauss curvature values are inverted.<br>高斯曲率值被反转。</p><h4 id="Both-curvature-values"><a href="#Both-curvature-values" class="headerlink" title="Both curvature values"></a><code>Both curvature values</code></h4><p>returns the two principal curvature values as a surface complex scalar field that should be interpreted as a field of pairs of scalar values (use Arithmetic to retrieve each value).<br>将两个主曲率值作为表面复标量场返回，该标量场应解释为标量值对的场（使用算术来检索每个值）。</p><h4 id="Both-1-curvature-values"><a href="#Both-1-curvature-values" class="headerlink" title="Both 1/curvature values"></a><code>Both 1/curvature values</code></h4><p>returns the inverted two principal curvature values.<br>返回两个主曲率值的倒数。</p><h4 id="Max-curvature-vector"><a href="#Max-curvature-vector" class="headerlink" title="Max curvature vector"></a><code>Max curvature vector</code></h4><p>a surface vector field is computed indicating the direction of maximum principal curvature. The length of a directional vector is equal to the corresponding curvature value.<br>计算一个表面矢量场来指示最大主曲率的方向。方向矢量的长度等于相应的曲率值。</p><h4 id="Both-curvature-vector"><a href="#Both-curvature-vector" class="headerlink" title="Both curvature vector"></a><code>Both curvature vector</code></h4><p>a surface complex vector field (that should be interpreted as a field of pairs of vectors) is computed indicating the direction of the two principal curvatures. The length of a directional vector is equal to the corresponding curvature value.<br>计算表面复矢量场（应理解为矢量对的场），指示两个主曲率的方向。方向矢量的长度等于相应的曲率值。</p><h4 id="Shape-index"><a href="#Shape-index" class="headerlink" title="Shape index"></a><code>Shape index</code></h4><p>computes the surface scalar field which values are equal to<br>计算表面标量场，其值等于</p><script type="math/tex; mode=display">\frac{2}{\pi} atan \frac{C_1 + C_2}{C_1 - C_2}</script><p>where <script type="math/tex">C_1</script> and <script type="math/tex">C_2</script> are the two principal curvatures.<br>其中 <script type="math/tex">C_1</script> 和 <script type="math/tex">C_2</script> 是两个主曲率。</p><h4 id="Curvedness"><a href="#Curvedness" class="headerlink" title="Curvedness"></a><code>Curvedness</code></h4><p>computes the surface scalar field which values are equal to<br>计算表面标量场，其值等于</p><script type="math/tex; mode=display">\frac{1}{2} \sqrt{C_1^2 + C_2^2}</script><p>where <script type="math/tex">C_1</script> and <script type="math/tex">C_2</script> are the two principal curvatures.<br>其中 <script type="math/tex">C_1</script> 和 <script type="math/tex">C_2</script> 是两个主曲率。</p>]]></content>
    
    
    <summary type="html">AVIZO曲率计算</summary>
    
    
    
    <category term="AvizoUsersGuide" scheme="http://hibiscidai.com/categories/AvizoUsersGuide/"/>
    
    
    <category term="Avizo" scheme="http://hibiscidai.com/tags/Avizo/"/>
    
    <category term="石油地质" scheme="http://hibiscidai.com/tags/%E7%9F%B3%E6%B2%B9%E5%9C%B0%E8%B4%A8/"/>
    
    <category term="数字岩心" scheme="http://hibiscidai.com/tags/%E6%95%B0%E5%AD%97%E5%B2%A9%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>AVIZO自动化</title>
    <link href="http://hibiscidai.com/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    <id>http://hibiscidai.com/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/</id>
    <published>2024-10-22T10:48:12.614Z</published>
    <updated>2024-09-26T01:34:06.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96.png" class="" title="AVIZO自动化"><p>AVIZO自动化</p><span id="more"></span><h1 id="AVIZO自动化"><a href="#AVIZO自动化" class="headerlink" title="AVIZO自动化"></a>AVIZO自动化</h1><p>第9章《自动化、自定义和扩展》主要讲解了如何使用Avizo的自动化、自定义和扩展功能。以下是详细介绍：</p><p>9.1 模板项目</p><p>模板项目用于简化对相似数据集的重复任务处理。用户可以将原始项目保存为模板，以便在相同类型的数据上重复使用。模板项目可以通过右键菜单或“项目 &gt; 创建对象”子菜单加载和执行。模板项目为自动化重复操作提供了便捷的途径。</p><p>9.2 Avizo启动<br>这一节描述了Avizo的启动选项，包括：</p><p>命令行选项：可用于配置Avizo启动时的行为，例如日志记录、禁用图形界面等。<br>环境变量：用户可以通过环境变量设置临时目录、启用或禁用特定功能（例如3D立体视图）。<br>用户定义的启动脚本：可以通过编写用户定义的启动脚本（如Avizo.init）定制Avizo的行为，包含注册文件格式、模块和编辑器等。</p><p>9.3 脚本编写</p><p>这一节介绍了如何使用脚本来控制Avizo，实现自动化和自定义任务。Avizo的脚本基于Tcl语言，用户可以通过编写Tcl脚本来操控Avizo的大部分功能，包括：</p><p>命令替换：使用括号[]执行命令并返回结果。<br>控制结构：支持if-else条件判断、for循环、while循环等控制结构。<br>用户自定义过程：使用proc定义新函数或过程，支持灵活参数列表和局部变量。<br>列表和字符串操作：Tcl中的所有内容都是通过列表构建的，提供了多个操作列表的命令。</p><p>9.4 使用MATLAB与Avizo集成</p><p>Avizo提供了与MATLAB的集成模块，允许用户从Avizo传递数据到MATLAB进行复杂计算，并将结果返回到Avizo中。通过这种方式，用户可以在Avizo中执行MATLAB脚本、调用用户自定义的MATLAB函数、以及使用字段结构等。</p><p>总的来说，第9章提供了Avizo的高级功能，允许用户通过模板、脚本编写和MATLAB集成来自动化和扩展工作流程 。</p><h2 id="9-1-模板项目"><a href="#9-1-模板项目" class="headerlink" title="9.1 模板项目"></a>9.1 模板项目</h2><p>这一节描述了如何使用模板项目。</p><h3 id="9-1-1-模板项目说明"><a href="#9-1-1-模板项目说明" class="headerlink" title="9.1.1 模板项目说明"></a>9.1.1 模板项目说明</h3><p>模板项目可以用于简化对一组相似数据的重复任务处理。模板项目是原始项目的副本，可以在相同类型的其他数据上重新应用。</p><h4 id="9-1-1-1-如何保存模板项目"><a href="#9-1-1-1-如何保存模板项目" class="headerlink" title="9.1.1.1 如何保存模板项目"></a>9.1.1.1 如何保存模板项目</h4><p>要创建模板项目，请从文件菜单中选择“另存项目为模板”（Save Project As Template）。这时会弹出一个输入选择对话框，列出所有可能的模板输入（当前的所有数据对象）。模板输入代表在执行模板时必须提供的数据集。您可以更改每个所选模板输入的标签。由于该标签会在模板执行期间显示出来，因此标签应当通用且有意义。默认标签是原始数据对象的名称。注意：未使用的数据对象会默认被过滤，但您可以通过选择“包含未使用的数据”（Include unused data）选项将其包含在模板项目中。</p><p>如果模板只包含一个输入，对话框会询问您是否希望将模板与该类型的数据关联。如果点击“确定”，则该模板会在右键菜单中的模板子菜单下（Templates submenu）对所有相同数据类型的对象可用。</p><p>最后，将弹出文件对话框以命名输出文件。文件名也是模板的名称，即将显示在模板菜单中的名称。内置的模板项目存储在“share/templates”文件夹中，但您可能没有足够的权限在该目录中创建新文件。</p><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/Figure9.1.png" class="" title="Figure 9.1: The template project save dialog."><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/Figure9.2.png" class="" title="Figure 9.2: Data Type association is possible if template has only one input."><p>您可以将自定义模板保存在任何目录中。每次启动 Avizo 时，它们都会自动重新加载。</p><h4 id="9-1-1-2-如何使用模板项目"><a href="#9-1-1-2-如何使用模板项目" class="headerlink" title="9.1.1.2 如何使用模板项目"></a>9.1.1.2 如何使用模板项目</h4><p>内置的模板项目和已知的用户自定义模板项目会在Avizo启动时自动加载。加载模板并不意味着实例化模板项目。模板项目仅在用户请求时创建，例如通过“项目 &gt; 创建对象…”菜单。一个例外是：如果用户通过“打开数据”对话框加载模板文件，则模板资源会被加载并执行。</p><p>如果模板与某种数据类型相关联，您可以使用该类型的数据对象的右键菜单创建实例。在这种情况下，将立即使用所选数据对象创建模板。</p><p>对于其他模板，您可以从“项目 &gt; 创建对象…”菜单的模板子菜单中创建实例。模板也可能出现在宏按钮列表中。在这种情况下，模板执行时将出现以下对话框：</p><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/Figure9.3.png" class="" title="Figure 9.3: The template project run dialog."><p>每个模板输入都会显示其模板输入名称和一个组合框，用于选择将用于该输入的数据集。每个组合框中的候选数据根据其数据类型进行过滤。您可以通过取消选中“检查输入类型”选项来禁用此过滤并显示项目视图中的所有数据。如果没有适合的数据对象，组合框将为空。您还可以随时选择“&lt;加载文件…&gt;”项，显示文件打开对话框并选择数据文件。</p><p>对于色彩映射的特殊处理：默认情况下，项目视图中已存在的色彩映射会按原样重新使用。例如，模板项目中的对象可能会受到范围变化的影响。您还可以选择不与现有对象共享色彩映射，方法是选择“独立色彩映射”选项。</p><h2 id="9-2-Avizo-启动"><a href="#9-2-Avizo-启动" class="headerlink" title="9.2 Avizo 启动"></a>9.2 Avizo 启动</h2><p>这一节描述了一些与 Avizo 启动相关的选项，包括：</p><ul><li>命令行选项</li><li>环境变量</li><li>Avizo 启动脚本</li></ul><h3 id="9-2-1-命令行选项"><a href="#9-2-1-命令行选项" class="headerlink" title="9.2.1 命令行选项"></a>9.2.1 命令行选项</h3><p>本节介绍 Avizo 支持的命令行选项。通常，在 Unix 系统上，Avizo 是通过位于 bin 子目录中的启动脚本启动的。这个脚本通常链接到 /usr/local/bin/Avizo 或类似位置。用户也可以定义一个指向 bin/start 的 Avizo 别名。</p><p>在 Windows 系统上，Avizo 通常通过开始菜单或桌面图标启动。不过，用户也可以直接调用 bin/arch-Win64VC10-Optimize/Avizo.exe 来启动。在这种情况下，支持与 Unix 系统相同的命令行选项。</p><p>Avizo 的语法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Avizo [options] [files ...]</span><br></pre></td></tr></table></figure><p>命令行中指定的数据文件将自动加载。除了数据文件外，还可以指定脚本文件。这些脚本将在程序启动时执行。</p><p>支持的选项如下：</p><ul><li>help </li></ul><p>打印命令行选项的简要说明。</p><ul><li>version</li></ul><p>打印 Avizo 的版本信息。</p><ul><li>no stencils</li></ul><p>指示 Avizo 不在其 3D 图形窗口中请求模板缓冲区。此选项可用于在某些低端 PC 图形板上利用硬件加速。</p><ul><li>no overlays</li></ul><p>指示 Avizo 不在其 3D 图形窗口中使用叠加平面。如果在远程显示上重定向 Avizo 时遇到问题，可以使用此选项。</p><ul><li>no gui</li></ul><p>启动 Avizo 而不打开任何窗口。此选项对于批处理模式下执行脚本很有用。</p><ul><li>logfile filename</li></ul><p>将控制台窗口中打印的所有消息也写入指定的日志文件。此选项尤其适用于与 -no gui 选项结合使用。</p><ul><li>depth number</li></ul><p>此选项仅在 Linux 系统上支持。它指定首选的深度缓冲区深度。Linux 系统上的默认值为 16 位。</p><ul><li>style={windows | motif | cde} </li></ul><p>此选项设置 Avizo 的 Qt 用户界面的显示样式。</p><ul><li>debug</li></ul><p>此选项仅适用于开发者版本。它会导致本地包以调试版本执行。默认情况下将使用优化代码。</p><ul><li>cmd command [  - host hostname ] [ - port port] </li></ul><p>向正在运行的 Avizo 应用程序发送 Tcl 命令。可以选择指定主机名和端口号。在 Avizo 控制台窗口中键入 app -listen 后，才能接收命令。</p><ul><li>clusterdaemon</li></ul><p>以 VR 守护程序模式启动，适用于集群从节点（Avizo XScreen 扩展）。这可能会被服务替代。有关更多信息，请参阅在线文档。</p><ul><li>tclcmd command</li></ul><p>在启动应用程序时执行 Tcl 命令。</p><ul><li>edition { LiteEdition | AvizoEdition }</li></ul><p>在特定版本下启动 Avizo。</p><h3 id="9-2-2-环境变量"><a href="#9-2-2-环境变量" class="headerlink" title="9.2.2 环境变量"></a>9.2.2 环境变量</h3><p>执行 Avizo 不需要特别的环境设置。在 Unix 系统上，某些环境变量（如共享库路径或 AVIZO_ROOT 目录）会由 Avizo 启动脚本自动设置。用户还可以设置其他环境变量来控制某些功能，这些变量列举如下。在 Unix 系统上，可以使用 setenv（适用于 csh 或 tcsh）或 export（适用于 sh、bash 或 ksh）来设置环境变量。在 Windows 上，可以在系统属性对话框中定义环境变量（Microsoft Windows）。</p><ul><li>AVIZO_DATADIR</li></ul><p>一个数据目录路径。此目录将用作文件对话框的默认目录。请注意，为了快速访问多个目录，您可以使用操作系统功能（例如在文件对话框中添加收藏夹位置列表），或者使用包含快捷方式或指向其他目录的链接的目录。</p><ul><li>AVIZO_TEXMEM</li></ul><p>指定以兆字节为单位的纹理内存量。如果未设置此变量，将应用一些启发式算法来确定系统上可用的纹理内存量。然而，这些启发式算法可能并不总是得出正确的值。在这种情况下，可以使用此变量来提高体积渲染模块的性能。</p><ul><li>AVIZO_MULTISAMPLE</li></ul><p>在高端图形系统上，默认使用多采样视觉。这种方式可以实现高效的场景抗锯齿。如果您希望禁用此功能，请将环境变量 AVIZO_MULTISAMPLE 设置为 0。请注意，在其他系统上，特别是在 PC 上，抗锯齿无法通过应用程序控制，而是必须直接在图形驱动程序中激活。</p><ul><li>AVIZO_NO_LICENSE_MESSAGE</li></ul><p>默认情况下，当您的 Avizo 许可证即将到期时，Avizo 会向控制台发出警告消息。这使您可以及时采取行动，防止在许可证到期时意外中断 Avizo 的使用。要禁用这些消息，请将此变量设置为 1。</p><ul><li>AVIZO_NO_OVERLAYS</li></ul><p>如果设置此变量，Avizo 将不会在其 3D 图形窗口中使用叠加平面。通过 -no overlays 命令行选项可以获得相同的效果。如果在远程显示上重定向 Avizo 时遇到问题，或者您的 X 服务器不支持叠加视觉，可以关闭叠加。</p><ul><li>AVIZO_NO_SPLASH_SCREEN</li></ul><p>如果设置此变量，Avizo 在初始化时不会显示启动画面。</p><ul><li>AVIZO_LOCAL</li></ul><p>指定包含用户定义模块的本地 Avizo 目录。此目录中的 IO 例程或模块将替代主 Avizo 目录中定义的例程或模块。此环境变量将覆盖开发向导中设置的本地 Avizo 目录（有关详细信息，请参阅 Avizo 程序员指南）。</p><ul><li>AVIZO_SMALLFONT</li></ul><p>仅适用于 Unix 系统。如果设置了此变量，即使屏幕分辨率为 1280x1024 或更高，所有在属性区中显示的端口也将使用小字体。默认情况下，小字体只会在较低分辨率的情况下使用。</p><ul><li>AVIZO_XSHM</li></ul><p>仅适用于 Unix 系统。如果您希望在 Avizo 的分割编辑器中禁止使用 X 共享内存扩展，请将此变量设置为 0。</p><ul><li>AVIZO_SPACEMOUSE</li></ul><p>如果 Avizo 发现连接了 Spacemouse（详见 <a href="http://www.3dconnexion.com">http://www.3dconnexion.com</a> ），Spacemouse 支持将自动启用。如果驱动程序已安装，则控制台窗口中会打印一条消息。通过 Spacemouse，您可以在 3D 查看器窗口中导航。支持两种模式：旋转模式和飞行模式。可以通过按下 Spacemouse 按钮 1 或 2 来切换两种模式。更多配置选项可能在 Avizo.init 文件中可用。<br>3Dconnexion Spacemouse 限制：</p><ul><li>Mac OS X 不支持 Spacemouse。</li><li>Avizo 和 AvizoClue 应用程序会识别 Spacemouse。</li><li>尚未完全支持六自由度运动。</li><li>Spacemouse 只能控制第一个查看器。</li><li>在旋转模式下，无法平移相机或上下移动。</li><li>在飞行模式下，无法绕物体旋转或上下移动。</li><li><p>默认情况下，按钮 1 用于打开菜单，必须重新配置为“按钮 1”功能。</p></li><li><p>AVIZO STEREO ON DEFAULT</p></li></ul><p>如果设置了此变量，则默认情况下 3D 查看器将以 OpenGL 原始立体模式打开。<br>这样，可以避免从单声道切换到立体模式时出现的屏幕闪烁。目前，该变量仅在 Unix 系统上受支持。</p><ul><li>TMPDIR</li></ul><p>此变量指定应将临时数据存储在哪个目录中。如果未设置，则此类数据将在 /tmp 下创建。此外，此变量由 Avizo 的作业队列解释。</p><h3 id="9-2-3-用户自定义启动脚本"><a href="#9-2-3-用户自定义启动脚本" class="headerlink" title="9.2.3 用户自定义启动脚本"></a>9.2.3 用户自定义启动脚本</h3><p>Avizo 可以通过提供用户自定义的启动脚本进行某些定制。默认的启动脚本名为 Avizo.init，位于 Avizo 安装目录的 share/resources/Avizo 子目录中。每次程序启动时，都会读取此脚本。此启动脚本的功能包括注册文件格式、模块和编辑器，以及加载默认的颜色映射。</p><p>如果在当前工作目录中找到名为 Avizo.init 的文件，则会读取该文件而不是默认的启动脚本。如果没有找到该文件，在 Unix 系统上会检查用户的主目录中是否存在名为 .Avizo 的启动脚本。下面是一个用户定义启动脚本的示例：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行默认启动脚本</span></span><br><span class="line"><span class="keyword">source</span> <span class="variable">$AVIZO_ROOT</span>/share/resources/Avizo/Avizo.init <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置均匀的黑色背景</span></span><br><span class="line">viewer <span class="number">0</span> setBackgroundMode <span class="number">0</span></span><br><span class="line">viewer <span class="number">0</span> setBackgroundColor black</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为帮助浏览器选择非默认字体大小</span></span><br><span class="line">help setFontSize <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过按 [F3] 键恢复摄像机设置</span></span><br><span class="line"><span class="keyword">proc</span><span class="title"> onKeyF3</span> &#123; &#125; &#123;</span><br><span class="line">    viewer setCameraOrientation <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">3.14159</span></span><br><span class="line">    viewer setCameraPosition <span class="number">0</span> <span class="number">0</span> <span class="number">-2.50585</span></span><br><span class="line">    viewer setCameraFocalDistance <span class="number">2.50585</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在此示例中，首先执行系统的默认启动脚本，以确保所有 Avizo 对象被正确注册。接着进行一些特定的设置，最后为功能键 [F3] 定义了一个快捷键过程。您也可以为其他功能键定义此类过程。此外，还可以定义类似 onKeyShiftF3 或 onKeyCtrlF3 的过程。这些过程在按下功能键同时按住 [SHIFT] 或 [CTRL] 键时执行。</p><h2 id="9-3-脚本编写"><a href="#9-3-脚本编写" class="headerlink" title="9.3 脚本编写"></a>9.3 脚本编写</h2><p>本节描述如何在 Avizo 中使用脚本功能。</p><h3 id="9-3-1-脚本编写介绍"><a href="#9-3-1-脚本编写介绍" class="headerlink" title="9.3.1 脚本编写介绍"></a>9.3.1 脚本编写介绍</h3><p>本章专为高级 Avizo 用户而写。如果您不知道什么是脚本编写，那么很可能不需要本章中描述的功能。</p><p>除了通过图形用户界面的交互控制之外，大多数 Avizo 功能也可以通过特定的命令访问。这允许您自动化某些流程，创建用于管理日常任务或演示的脚本。Avizo 的脚本命令基于 Tcl（工具命令语言）。这意味着您可以使用 Tcl 编写带有 Avizo 特定扩展的命令脚本。</p><p>可以在 Avizo 的控制台窗口中键入命令，如第 8.1.13 节中所述。直接在控制台窗口中输入的命令将立即执行。或者，也可以将命令写入文本文件，然后作为整体执行。</p><p>本章内容如下：</p><p>9.3.2 Tcl 简介 提供了 Tcl 脚本语言的简短介绍。本节与 Avizo 相关性不大。</p><p>9.3.3 Avizo 脚本接口 解释了与脚本编写相关的 Avizo 特定命令和概念。包括全局命令参考。</p><p>9.3.5 Avizo 脚本文件 解释了编写和执行脚本文件的不同方法，包括脚本对象、资源文件和功能键绑定的 Tcl 过程的参考。</p><p>9.3.6 配置弹出菜单 描述了如何使用脚本命令配置对象的弹出菜单，并如何创建执行脚本的新条目。</p><p>9.3.7 注册拾取回调 描述了如何将脚本回调附加到对象或查看器中，并在用户拾取事件时调用它们。</p><p>9.3.8 Tcl 文件读取器 解释了如何注册用 Tcl 实现的自定义文件读取器。</p><h3 id="9-3-2-Tcl简介"><a href="#9-3-2-Tcl简介" class="headerlink" title="9.3.2 Tcl简介"></a>9.3.2 Tcl简介</h3><p>本章简要介绍Tcl脚本语言。如果你已经熟悉Tcl，你可以跳过本节。然而，请注意，输出到Avizo控制台时，应该使用echo命令，而不是puts命令。</p><p>本章并不打算涵盖Tcl语言的所有细节。要了解Tcl语言的完整文档或参考手册，请参阅John K. Ousterhout撰写的《Tcl and the Tk Toolkit》。许多其他关于Tcl的书籍也涵盖了Tk GUI工具包，但请注意，Tk在Avizo中并未使用。</p><p>你也可以在互联网上找到Tcl文档和参考手册，例如在<a href="http://www.scriptics.com">http://www.scriptics.com</a> ，或者通过搜索引擎查找”Tcl教程”或”Tcl文档”。</p><p>当你在Avizo控制台中键入Tcl命令时，命令会在按下回车键后立即执行。使用Avizo控制台提供的自动补全和历史功能，参见第8.1.13节（控制台窗口）。</p><h4 id="9-3-2-1-Tcl列表、命令和注释"><a href="#9-3-2-1-Tcl列表、命令和注释" class="headerlink" title="9.3.2.1 Tcl列表、命令和注释"></a>9.3.2.1 Tcl列表、命令和注释</h4><p>首先，请注意Tcl是区分大小写的：set和Set并不相同。</p><p>Tcl命令是由空格分隔的单词列表。第一个单词代表命令名称，后续的单词被视为该命令的参数。例如，尝试输入Avizo特定的命令echo，该命令将所有参数打印到Avizo控制台。尝试键入：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo Hello World</span><br></pre></td></tr></table></figure><p>这将输出字符串”Hello World”。注意，Tcl命令可以用分号（;）或换行符分隔。如果你想连续执行两个echo命令，可以这样：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo Hello World ; echo Hello World2</span><br></pre></td></tr></table></figure><p>或者这样：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo Hello World</span><br><span class="line">echo Hello World2</span><br></pre></td></tr></table></figure><p>除了命令，你还可以在Tcl代码中插入注释。注释以 # 字符开头，并以换行符结束：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是一个注释</span></span><br><span class="line">echo Hello World</span><br></pre></td></tr></table></figure><h4 id="9-3-2-2-Tcl变量"><a href="#9-3-2-2-Tcl变量" class="headerlink" title="9.3.2.2 Tcl变量"></a>9.3.2.2 Tcl变量</h4><p>Tcl中可以使用变量。变量表示某个特定的状态或值。使用Tcl代码，可以查询、定义和修改占位符的值。要定义一个变量，可以使用命令：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> name value</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> i <span class="number">1</span></span><br><span class="line"><span class="keyword">set</span> myVar foobar</span><br></pre></td></tr></table></figure><p>请注意，在Tcl中，内部所有变量都是字符串类型。因为set命令要求一个作为变量值的参数，所以你必须对包含空格的值加引号：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> Output <span class="string">&quot;Hello World&quot;</span></span><br></pre></td></tr></table></figure><p>或者：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> Output &#123;Hello World&#125;</span><br></pre></td></tr></table></figure><p>要替换变量名为varname的值，需要在变量名前加上$符号。表达式$varname将被替换为变量的值。在上面的定义之后：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="variable">$Output</span></span><br></pre></td></tr></table></figure><p>将会在控制台窗口中打印：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello World</span><br></pre></td></tr></table></figure><p>同时，</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="string">&quot;$i.) $Output&quot;</span></span><br></pre></td></tr></table></figure><p>将输出<code>1.) Hello World</code>。注意，对于使用双引号 “ 括起的字符串，会执行变量替换，但对于使用大括号{}括起的字符串，则不会进行替换。大括号内甚至允许换行符。然而，在Avizo控制台中不可能输入多行命令。</p><h4 id="9-3-2-3-Tcl命令替换"><a href="#9-3-2-3-Tcl命令替换" class="headerlink" title="9.3.2.3 Tcl命令替换"></a>9.3.2.3 Tcl命令替换</h4><p>在Tcl中进行数学计算时，可以使用<code>expr</code>命令，该命令将评估其参数并返回表达式的值。例如：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">expr</span> <span class="number">5</span> / ( <span class="number">7</span> + <span class="number">3</span>)</span><br><span class="line"><span class="keyword">expr</span> <span class="variable">$i</span> + <span class="number">1</span></span><br></pre></td></tr></table></figure><p>为了将像 expr 这样的命令的结果用于后续命令，必须使用 Tcl 中一个重要的机制：命令替换。命令替换由方括号 [] 表示。任何用方括号括起来的列表都会首先作为一个单独的命令执行，[…] 构造将被命令的结果替换。这类似于 Unix 命令 shell 中的反引号构造 …。例如，为了将变量 i 的值增加 1，可以使用：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> i [<span class="keyword">expr</span> <span class="variable">$i</span> + <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>当然，命令表达式可以任意嵌套。执行顺序总是从最内层的括号对到最外层的括号对：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo [<span class="keyword">expr</span> <span class="number">5</span> * [<span class="keyword">expr</span> <span class="number">7</span> + [<span class="keyword">expr</span> <span class="number">3</span> + <span class="number">3</span>]]]</span><br></pre></td></tr></table></figure><h4 id="9-3-2-4-Tcl-控制结构"><a href="#9-3-2-4-Tcl-控制结构" class="headerlink" title="9.3.2.4 Tcl 控制结构"></a>9.3.2.4 Tcl 控制结构</h4><p>另一个重要的语言元素是 if-else 结构、for 循环和 while 循环。这些结构通常是多行结构，因此不适合在 Avizo 控制台中方便地输入。如果您想尝试下面的示例，可以使用您选择的文本编辑器将它们写入文件，如 C:\test.txt，然后通过输入以下命令执行该文件：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> C:\test.txt</span><br></pre></td></tr></table></figure><p>首先介绍 if-then 机制。它用于在某个表达式求值为 “true” 时执行一些代码（意思是值不为 0）：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> a <span class="number">7</span></span><br><span class="line"><span class="keyword">set</span> b <span class="number">8</span></span><br><span class="line"><span class="keyword">if</span> &#123;<span class="variable">$a</span> &lt; <span class="variable">$b</span>&#125; &#123;</span><br><span class="line">    echo <span class="string">&quot;$a is smaller than $b&quot;</span></span><br><span class="line">&#125; elseif &#123;<span class="variable">$a</span> == <span class="variable">$b</span>&#125; &#123;</span><br><span class="line">    echo <span class="string">&quot;$a equals $b&quot;</span></span><br><span class="line">&#125; else &#123;</span><br><span class="line">    echo <span class="string">&quot;$a is greater than $b&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>elseif 和 else 部分是可选的。可以使用多个 elseif 部分，但只能有一个 if 和一个 else 部分。</p><p>另一个重要结构是条件循环。像 if 命令一样，它基于检查条件表达式。与 if 不同的是，条件代码会多次执行，只要表达式求值为 true：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;<span class="keyword">set</span> i <span class="number">1</span>&#125; &#123;<span class="variable">$i</span> &lt; <span class="number">100</span>&#125; &#123;<span class="keyword">set</span> i [<span class="keyword">expr</span> <span class="variable">$i</span>*<span class="number">2</span>]&#125; &#123;</span><br><span class="line">    echo <span class="variable">$i</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实际上，这段代码等同于：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> i <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> &#123;<span class="variable">$i</span> &lt; <span class="number">100</span>&#125; &#123;</span><br><span class="line">    echo <span class="variable">$i</span></span><br><span class="line">    <span class="keyword">set</span> i [<span class="keyword">expr</span> <span class="variable">$i</span> * <span class="number">2</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>两个循环都会产生输出 1、2、4、8、16、32、64。<br>如果要对列表的所有元素执行循环，还有另一个非常方便的命令：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">foreach</span> x &#123;<span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">8</span> <span class="number">16</span> <span class="number">32</span> <span class="number">64</span>&#125; &#123;</span><br><span class="line">echo <span class="variable">$x</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这将生成与上一个示例相同的输出。请注意，括号中的表达式是以空格分隔的单词列表。</p><h4 id="9-3-2-5-用户自定义的Tcl过程"><a href="#9-3-2-5-用户自定义的Tcl过程" class="headerlink" title="9.3.2.5 用户自定义的Tcl过程"></a>9.3.2.5 用户自定义的Tcl过程</h4><p>在Tcl中，使用 proc 命令来定义一个新的函数或过程。proc 需要两个参数：一个参数名称列表和要执行的Tcl代码。一旦定义了一个过程，它就可以像任何其他Tcl命令一样使用：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">proc</span><span class="title"> computeAverageA</span> &#123;a b&#125; &#123;</span><br><span class="line">    <span class="keyword">return</span> [<span class="keyword">expr</span> (<span class="variable">$a</span>+<span class="variable">$b</span>)/<span class="number">2.0</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">proc</span><span class="title"> computeAverageB</span> &#123;a b c&#125; &#123;</span><br><span class="line">    <span class="keyword">return</span> [<span class="keyword">expr</span> (<span class="variable">$a</span>+<span class="variable">$b</span>+<span class="variable">$c</span>)/<span class="number">3.0</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo <span class="string">&quot;average of 2 and 3: [computeAverageA 2 3]&quot;</span></span><br><span class="line">echo <span class="string">&quot;average of 2,3,4: [computeAverageB 2 3 4]&quot;</span></span><br></pre></td></tr></table></figure><p>在这个例子中，参数列表定义了可以在过程体内使用的局部变量的名称（例如 $a）。return 命令用于定义过程的结果。这个结果是通过方括号 [] 进行命令替换时使用的值。</p><p>如果想定义一个具有可变数量参数的过程，必须使用特殊参数名称 args。如果参数列表中只包含这个词，新的命令将接受任意数量的参数，并且这些参数会作为一个列表传递给名为 args 的变量：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">proc</span><span class="title"> computeAverage</span> args &#123;</span><br><span class="line">    <span class="keyword">set</span> result <span class="number">0</span></span><br><span class="line">    <span class="keyword">foreach</span> x <span class="variable">$args</span> &#123;</span><br><span class="line">        <span class="keyword">set</span> result [<span class="keyword">expr</span> <span class="variable">$result</span> + <span class="variable">$x</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> [<span class="keyword">expr</span> <span class="variable">$result</span> / [<span class="keyword">llength</span> <span class="variable">$args</span>]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个例子中，llength 命令返回 args 列表中包含的元素数量。</p><p>注意，过程内定义的变量 result 具有局部作用域，这意味着它在过程体外是未知的。此外，全局变量的值在过程内也是未知的，除非使用 global 关键字声明该全局变量：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> x <span class="number">3</span></span><br><span class="line"><span class="keyword">proc</span><span class="title"> printX</span> &#123;&#125; &#123;</span><br><span class="line">    <span class="keyword">global</span> x</span><br><span class="line">    echo <span class="string">&quot;x的值是 $x&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于过程还有很多内容，例如参数传递、在过程外部上下文中执行命令等。请参阅Tcl参考书以获取这些高级主题。</p><h4 id="9-3-2-6-列表和字符串操作"><a href="#9-3-2-6-列表和字符串操作" class="headerlink" title="9.3.2.6 列表和字符串操作"></a>9.3.2.6 列表和字符串操作</h4><p>最后，在这简短的Tcl介绍结束时，我们回到列表的概念。基本上Tcl中的一切都是通过列表构造的，因此了解最重要的列表操作命令以及理解一些微妙的细节是非常重要的。</p><p>下面是一个示例，展示了如何获取一个输入的数字列表并构造一个输出列表，其中每个元素的值是输入列表中相应元素的两倍：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> input [<span class="keyword">list</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]</span><br><span class="line"><span class="keyword">set</span> output [<span class="keyword">list</span>]</span><br><span class="line"><span class="keyword">foreach</span> element <span class="variable">$input</span> &#123;</span><br><span class="line">    <span class="keyword">lappend</span> output [<span class="keyword">expr</span> <span class="variable">$element</span> * <span class="number">2</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你可以将列表看作是用空格分隔列表元素的简单字符串。这意味着可以不用列表命令来实现与前面例子相同的结果：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> input <span class="string">&quot;1 2 3 4 5&quot;</span></span><br><span class="line"><span class="keyword">set</span> output <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">foreach</span> element <span class="variable">$input</span> &#123;</span><br><span class="line">    <span class="keyword">append</span> output [<span class="keyword">expr</span> <span class="variable">$element</span> * <span class="number">2</span>] <span class="string">&quot; &quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>append命令类似于lappend，但它只是在现有字符串的末尾添加字符串。当你开始嵌套列表时，列表操作变得更加复杂。嵌套列表用嵌套的大括号表示，例如：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> input &#123;<span class="number">1</span> <span class="number">2</span> &#123;<span class="number">3</span> <span class="number">4</span> <span class="number">5</span> &#123;<span class="number">6</span> <span class="number">7</span>&#125; <span class="number">8</span> &#125; <span class="number">9</span>&#125;</span><br><span class="line"><span class="keyword">foreach</span> x <span class="variable">$input</span> &#123;</span><br><span class="line">    echo <span class="variable">$x</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此命令的结果将是：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span> <span class="number">4</span> <span class="number">5</span> &#123;<span class="number">6</span> <span class="number">7</span>&#125; <span class="number">8</span></span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure><p>请注意，当构造列表时，Tcl会自动引用不是单个词的字符串。以下是一个示例：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> i [<span class="keyword">list</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"><span class="keyword">lappend</span> i <span class="string">&quot;4 5 6&quot;</span></span><br><span class="line">echo <span class="variable">$i</span></span><br></pre></td></tr></table></figure><p>将输出：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> &#123;<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>&#125;</span><br></pre></td></tr></table></figure><p>你可以使用lindex命令来访问列表的单个元素。lindex接受两个参数：列表和所需元素的索引号，索引从0开始：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> i [<span class="keyword">list</span> a b c d e]</span><br><span class="line">echo [<span class="keyword">lindex</span> <span class="variable">$i</span> <span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>将输出结果为 c：</p><h3 id="9-3-3-Avizo-脚本接口"><a href="#9-3-3-Avizo-脚本接口" class="headerlink" title="9.3.3 Avizo 脚本接口"></a>9.3.3 Avizo 脚本接口</h3><p>尽管 Tcl 语言本身并不是面向对象的，Avizo 的脚本接口是面向对象的。Avizo 项目视图中的每个对象都有一个与之关联的命令。此外，还有几个与 Avizo 中的全局对象（如查看器或 Avizo 主窗口）相关的全局命令。</p><p>与项目视图中的对象关联的命令（例如，“Ortho Slice”模块或“Isosurface”模块）仅在该对象存在时才存在。这些命令与项目视图中显示的对象名称相同。通常，特定对象的脚本接口包含许多不同的功能。Avizo 对象相关命令的通用语法是：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;object-name&gt; &lt;command-word&gt; &lt;optional-arguments&gt; ...</span><br></pre></td></tr></table></figure><p>例如，如果存在一个名为“Global Axes”的对象（从 Avizo 菜单中选择 View/Axis），那么你可以使用类似以下的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;GlobalAxes&quot; deselect</span><br><span class="line">&quot;GlobalAxes&quot; select</span><br><span class="line">&quot;GlobalAxes&quot; setIconPosition 100 100</span><br></pre></td></tr></table></figure><p>注意：模块名称使用驼峰命名法。命令中可能出现这三个词：“Global Axes”、“GlobalAxes”或 GlobalAxes。</p><p>请记住使用 Avizo 控制台提供的自动补全和历史功能（参见第 8.1.13 节“控制台窗口”），以节省打字时间。</p><p>如果你已经使用过 Avizo，你可能注意到 Avizo 模块的参数和行为是通过其端口控制的。选择模块时，端口提供了用户界面来更改其值。所有端口也可以通过命令接口控制。其通用语法是：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;object-name&gt; &lt;port-name&gt; &lt;port-command&gt; &lt;optional-arguments&gt; ...</span><br></pre></td></tr></table></figure><p>例如，对于“Global Axes”，你可以输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;GlobalAxes&quot; options setValue 1 1</span><br><span class="line">&quot;GlobalAxes&quot; thickness setValue 1.5</span><br><span class="line">&quot;GlobalAxes&quot; fire</span><br></pre></td></tr></table></figure><p>当你输入这些命令时，你会注意到用户界面中的值会立即更改。然而，模块的计算方法在显式调用 fire 命令之前不会被调用。这允许你先为多个端口设置值，而无需在每个命令之后重新计算。然而请注意，一些模块在连接新输入对象时会自动重置其某些端口。在这种情况下，你可能需要在为每个端口设置值后调用 fire。</p><p>通常，端口名称与图形用户界面中显示的文本标签相同，除了删除了空格并且命令名称以小写字母开头。要找出特定模块的所有端口名称，请使用以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;object-name&gt; allPorts</span><br></pre></td></tr></table></figure><p>几乎所有端口都提供 setValue 和 getValue 命令。当然，参数数量和语法取决于端口。</p><p>对象相关命令类型<code>&lt;object-name&gt; &lt;port-name&gt; setValue ...</code>构成了典型 Avizo 脚本的 90% 以上。然而，除了端口命令外，许多 Avizo 对象还提供其他特定命令。特定模块的命令接口在用户参考指南中有详细描述。当选择模块时，单击属性区域中的“?”按钮可以快速找到相应页面。</p><p>作为快速帮助，输入对象名称而不加任何选项将显示该对象可用的所有命令。请注意，这也会显示未记录、未发布和实验性的命令。为了获取有关特定模块或端口命令的更多信息，你可以将其输入控制台窗口而不加任何参数，然后按 F1 键。这将打开帮助浏览器，显示命令的描述。</p><p>Avizo 对象是类层次结构的一部分。与 C++ 编程接口类似，脚本命令也可以由派生类从其基类继承。这意味着像轴对象这样的特定对象，除了其自己的特定命令外，还提供其基类中的所有命令。模块文档中提供了指向基类命令的链接。</p><h4 id="9-3-3-1-预定义变量"><a href="#9-3-3-1-预定义变量" class="headerlink" title="9.3.3.1 预定义变量"></a>9.3.3.1 预定义变量</h4><p>在Avizo Tcl中存在一些预定义变量，它们具有特殊的含义。这些变量包括：</p><ul><li><p>AVIZO ROOT: Avizo安装目录。</p></li><li><p>AVIZO LOCAL: 个人Avizo开发目录（仅限Avizo XPand扩展）。</p></li><li><p>SCRIPTFILE: 当前正在执行的Tcl脚本文件。</p></li><li><p>SCRIPTDIR: 当前正在执行的脚本所在的目录。</p></li><li><p>hideNewModules: 如果设置为1，初始创建的新模块图标将被隐藏。请谨慎设置此变量，并在严格必要时使用。为了避免意外地持续隐藏创建的模块（例如在脚本中断的情况下），应在使用后立即恢复此变量。</p></li></ul><h4 id="9-3-3-2-对象命令"><a href="#9-3-3-2-对象命令" class="headerlink" title="9.3.3.2 对象命令"></a>9.3.3.2 对象命令</h4><p>Avizo模块和数据对象的基本命令接口在用户指南的参考部分的”对象”章节中进行了描述。对象命令的基本语法如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;object&gt; &lt;command&gt; &lt;arguments&gt; ...</span><br></pre></td></tr></table></figure><p>其中，<code>&lt;object&gt;</code> 是指对象的名称，<code>&lt;command&gt;</code> 表示要执行的命令。每个模块或数据对象可能定义自己的一组命令，除了其基类定义的命令之外。在”对象”章节中描述的命令由所有模块和数据对象提供。</p><p>全局命令将在下一节中介绍。</p><h3 id="9-3-4-全局命令"><a href="#9-3-4-全局命令" class="headerlink" title="9.3.4 全局命令"></a>9.3.4 全局命令</h3><p>本节列出了Avizo特定的全局Tcl命令。这些命令中的一些与Avizo中的某些全局对象相关联，如控制台窗口、主窗口或查看器窗口。其他命令如load或echo则不属于此类。以下命令分为不同的子部分：</p><p>• viewer command options (viewer)<br>• main window command options (theMain)<br>• console command options (theMsg)<br>• common commands for top-level windows<br>• progress bar command options (workArea)<br>• application command options (app)<br>• other global commands</p><ul><li>查看器命令选项（viewer）</li><li>主窗口命令选项（theMain）</li><li>控制台命令选项（theMsg）</li><li>顶级窗口的通用命令</li><li>进度条命令选项（workArea）</li><li>应用程序命令选项（app）</li><li>其他全局命令</li></ul><h4 id="9-3-4-1-视图命令选项"><a href="#9-3-4-1-视图命令选项" class="headerlink" title="9.3.4.1 视图命令选项"></a>9.3.4.1 视图命令选项</h4><p>可以在控制台窗口中输入对视图的命令。语法为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">viewer [&lt;number&gt;] <span class="built_in">command</span></span><br></pre></td></tr></table></figure><p>其中，<code>&lt;number&gt;</code> 指定要操作的视图。数值 0 表示主视图，可省略以方便操作。</p><h5 id="命令："><a href="#命令：" class="headerlink" title="命令："></a>命令：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">viewer [&lt;number&gt;] snapshot [-offscreen [&lt;width&gt; &lt;height&gt;]] [-stereo] [-alpha] [-tiled &lt;nx&gt; &lt;ny&gt;] &lt;filename&gt; [filename2]</span><br></pre></td></tr></table></figure><p>此命令截取当前场景的快照，并将其保存为指定的文件名。图像格式由文件名的扩展名自动决定。支持的格式列表包括：TIFF (.tif, .tiff)、SGI-RGB (.rgb, .sgi, .bw)、JPEG (.jpg, .jpeg)、PNM (.pgm, .ppm)、BMP (.bmp)、PNG (.png) 和 EPS (.eps)。如果没有提供视图编号，将从所有视图中截取快照，前提是从“视图”菜单中选择了2个或4个视图布局。</p><p>如果指定了 <code>-offscreen</code> 选项，将使用最大尺寸为 2048x2048 的屏幕外渲染。在这种情况下，即使是视图 0，也需要指定视图编号。如果没有明确指定宽度和高度，图像的尺寸将为当前视图的大小。</p><p><strong>注意</strong>： 如果视图中有多个可见的透明对象，并且想使用屏幕外渲染，请将透明度模式设置为“Blend Delayed”，并在拍摄快照之前检查所有对象是否正确渲染。</p><p>如果使用了 <code>-stereo</code> 选项，将创建立体模式图像。在这种情况下，filename2 文件可用于指定保存立体图像第二个视图的文件。</p><p>如果使用了 <code>-alpha</code> 选项，将创建具有透明背景的快照图像。</p><p>如果使用了 <code>-tiled nx ny</code> 选项，将使用 nx 和 ny 指定的拼贴渲染进行渲染，在水平和垂直方向上各渲染nx和ny个拼贴。</p><p><code>viewer [&lt;number&gt;] setPosition &lt;x&gt; &lt;y&gt;</code><br>（仅限顶级模式）设置视图窗口相对于屏幕左上角的位置。如果在同一窗口中显示了多个视图，则设置顶级窗口的位置。</p><p><code>viewer [&lt;number&gt;] getPosition</code><br>返回视图窗口的位置。如果同一窗口中显示了多个视图，则返回顶级窗口的位置。</p><p><code>viewer [&lt;number&gt;] setSize &lt;width&gt; &lt;height&gt;</code><br>（仅限顶级模式）设置视图窗口的大小。宽度和高度指定的是实际的图形区域大小。窗口的大小可能会稍大一些，因为还包括视图装饰和窗口框架。</p><p><strong>注意</strong>：当视图未按请求的大小调整时，控制台会打印警告消息。设置新大小时，可能出现以下情况导致视图未能按请求大小调整：</p><ul><li>视图在顶级窗口中，且给定的大小太小（例如：10x10）。</li><li>视图不在顶级窗口中，且主窗口无法调整为更小的尺寸（例如，Mac上的统一标题和工具栏或具有最小宽度的停靠窗口阻止了主窗口调整大小）。</li></ul><p><code>viewer [&lt;number&gt;] getSize</code><br>返回不带装饰和窗口框架的视图窗口大小。</p><p><code>viewer [&lt;number&gt;] setCamera &lt;camera-string&gt;</code><br>恢复所有相机设置。相机字符串应为 getCamera 命令的输出。</p><p><code>viewer [&lt;number&gt;] getCamera</code><br>此命令返回当前的相机设置，即位置、方向、焦距、类型和高度角（用于透视相机）或高度（用于正交相机）。这些值以 Avizo 命令的形式返回，可以执行这些命令以恢复相机设置。完整的命令字符串也可以传递给 setCamera 一次执行。</p><p><code>viewer [&lt;number&gt;] setCameraPosition &lt;x&gt; &lt;y&gt; &lt;z&gt;</code><br>定义相机在世界坐标中的位置。</p><p><code>viewer [&lt;number&gt;] getCameraPosition</code><br>返回相机在世界坐标中的位置。</p><p><code>viewer [&lt;number&gt;] setCameraOrientation &lt;x&gt; &lt;y&gt; &lt;z&gt; &lt;a&gt;</code><br>定义相机的方向。默认情况下，相机朝负z方向看，y轴向上。任何其他方向可以作为相对于默认方向的旋转来指定。旋转由一个旋转轴 x y z 以及旋转角度 a（以弧度为单位）指定。</p><p><code>viewer [&lt;number&gt;] getCameraOrientation</code><br>以与 setCameraOrientation 相同的格式返回当前相机的方向。</p><p><code>viewer [&lt;number&gt;] setCameraFocalDistance &lt;value&gt;</code><br>定义相机的焦距。焦距用于计算在交互查看模式下场景旋转的中心。</p><p><code>viewer [&lt;number&gt;] getCameraFocalDistance</code><br>返回当前相机的焦距。</p><p><code>viewer [&lt;number&gt;] setCameraHeightAngle &lt;degrees&gt;</code><br>以角度设置透视相机的高度角。减小角度会使视野变小，从而“放大”效果，类似于远摄镜头。如果不专门想改变相机的视野，通常更好的方法是将相机移得更近来放大对象。这条命令对正交相机无效。</p><p><code>viewer [&lt;number&gt;] getCameraHeightAngle</code><br>返回透视相机的高度角。</p><p><code>viewer [&lt;number&gt;] setCameraHeight &lt;height&gt;</code><br>设置正交相机视图体积的高度。此命令对透视相机无效。</p><p><code>viewer [&lt;number&gt;] getCameraHeight</code><br>返回正交相机的高度。</p><p><code>viewer [&lt;number&gt;] setCameraType &lt;perspective|orthographic&gt;</code><br>设置相机的类型，参数为透视（perspective）或正交（orthographic）。</p><p><code>viewer [&lt;number&gt;] getCameraType</code><br>返回当前相机的类型。</p><p><code>viewer [&lt;number&gt;] setTransparencyType &lt;type&gt;</code><br>此命令定义透明对象渲染策略。类型参数是介于0到8之间的数字，分别对应以下透明度模式：Screen Door、Add、Add Delayed、Add Sorted、Blend、Blend Delayed、Blend Sorted、Sorted Layers 和 Sorted Layers Delayed。建议使用模式8以获得最准确的结果，默认使用模式6。</p><p><code>viewer [&lt;number&gt;] getTransparencyType</code><br>返回当前使用的透明度模式。</p><p><code>viewer [&lt;number&gt;] setSortedLayersNumPasses &lt;value&gt;</code><br>当透明度模式为Sorted Layers或Sorted Layers Delayed时，此命令设置渲染的通过次数。通常，4次通过（默认值）能给出良好的效果。</p><p><code>viewer [&lt;number&gt;] getSortedLayersNumPasses</code><br>返回使用的渲染通过次数。</p><p><code>viewer [&lt;number&gt;] setBackgroundColor &lt;r&gt; &lt;g&gt; &lt;b&gt;</code><br>设置背景颜色，颜色值可以用RGB整数值（0到255）或小数（0.0到1.0）表示，也可以直接使用文本（如”white”）。</p><p><code>viewer [&lt;number&gt;] getBackgroundColor</code><br>返回主背景颜色，以RGB三元组（0到1之间的值）表示。</p><p><code>viewer [&lt;number&gt;] setBackgroundColor2 &lt;r&gt; &lt;g&gt; &lt;b&gt;</code><br>设置次背景颜色，次背景颜色用于非均匀背景模式。</p><p><code>viewer [&lt;number&gt;] getBackgroundColor2</code><br>返回次背景颜色，以RGB三元组（0到1之间的值）表示。</p><p><code>viewer setBackgroundMode &lt;mode&gt;</code><br>允许指定不同的背景模式。如果 mode 设置为 0，将显示均匀的背景。mode 1 表示渐变背景。mode 2 会显示棋盘格模式，这有助于理解透明对象的形状。最后，mode 3 在背景中绘制通过 setBackgroundImage 预定义的图像。</p><p><code>viewer getBackgroundMode</code><br>返回当前的背景模式。</p><p><code>viewer setBackgroundImage &lt;imagefile&gt; [&lt;imagefile2&gt;] [-stereo]</code><br>此命令允许在查看器背景中放置任意光栅图像。图像不得大于查看器窗口，否则会被裁剪。将通过文件扩展名自动检测图像文件格式，支持的格式与 snapshot 命令相同，除了 EPS 格式外。如果指定了第二个图像文件，并且使用的是立体渲染模式，则此文件将用作右眼图像。如果指定了 -stereo 选项且只有一个图像文件，则假设该文件包含左右眼的组合视图，并将自动分离这些视图。</p><p><code>viewer getBackgroundImage</code><br>返回通过 setBackgroundImage 定义的最后一个背景图像文件的文件名。如果指定了一对立体图像，则返回两个文件名。如果 setBackgroundImage 使用了 -stereo 选项，则也会返回该选项。</p><p><code>viewer [&lt;number&gt;] setAutoRedraw &lt;state&gt;</code><br>如果 state 为 0，则自动重绘模式关闭。在这种情况下，查看器窗口中的图像不会更新，除非发送 redraw 命令。如果 state 为 1，自动重绘模式再次开启。在脚本中，临时禁用自动重绘模式可能会很有用。</p><p><code>viewer [&lt;number&gt;] isAutoRedraw</code><br>返回自动重绘模式是否开启的状态。</p><p><code>viewer [&lt;number&gt;] redraw</code><br>此命令强制当前场景重新绘制。如果自动重绘模式已禁用，则只需要进行明确的重绘。</p><p><code>viewer [&lt;number&gt;] rotate &lt;degrees&gt; [x|y|z|m|u|v]</code><br>绕轴旋转相机。通过第二个参数指定轴，以下选项可用：x、y、z、m、u、v。</p><p>• x: the x-axis (1,0,0)<br>• y: the y-axis (0,1,0<br>• z: the z-axis (0,0,1)<br>• m: the most vertical axis of x, y, or z<br>• u: the viewer’s up direction<br>• v: the view direction</p><p>最后一个选项的作用与用户界面的旋转按钮相同。在大多数情况下，m 选项最合适。为了向后兼容，默认值为 u。</p><p><code>viewer [&lt;number&gt;] setDecoration &lt;state&gt;</code><br>此命令已废弃。</p><p><code>viewer [&lt;number&gt;] saveScene [-b] [-r] [-z] &lt;filename&gt;</code><br>将查看器中显示的所有几何体保存为 Open Inventor 3D 图形格式文件。请注意：由于许多 Avizo 模块使用了自定义的 Open Inventor 节点，场景通常无法在外部程序（如 ivview）中正确显示。可用的选项如下：</p><p>-b: 以二进制格式保存 Open Inventor 文件。<br>-r: 保存查看器中显示的几何体以及其他属性。<br>-z: 将 Open Inventor 文件保存为压缩格式（使用 zip 压缩）。</p><p><code>viewer [&lt;number&gt;] viewAll</code><br>重置相机，使整个场景可见。此方法会自动在查看器中显示第一个对象时调用。</p><p><code>viewer [&lt;number&gt;] show</code><br>打开指定的查看器，并确保查看器窗口位于屏幕上的所有其他窗口之上。</p><p><code>viewer [&lt;number&gt;] hide</code><br>关闭指定的查看器。</p><p><code>viewer [&lt;number&gt;] isVisible</code><br>此命令显示指定的查看器是否可见。</p><p><code>viewer [&lt;number&gt;] fogRange &lt;min&gt; &lt;max&gt;</code><br>设置雾效的衰减范围，该范围可通过视图菜单引入查看器场景。默认范围为 [0, 1]。范围内的值对应于场景点距相机的距离，其中离相机最近的点的值为 0，最远的点的值为 1。限制衰减范围意味着衰减将在指定最小值达到时开始，并在指定最大值达到时达到最大。由于雾效的最大衰减与不可见性相同，因此所有超过最大值的点将显示为背景。</p><p><code>viewer [&lt;number&gt;] setVideoFormat pal|ntsc</code><br>设置视图窗口的尺寸为 PAL 601 或 NTSC 601 分辨率，即 720x576 像素或 720x486 像素。当前装饰设置将被考虑在内。</p><p><code>viewer [&lt;number&gt;] setVideoFrame &lt;state&gt;</code><br>如果 state 为 1，则在查看器的覆盖平面中显示一个帧。这一帧显示了视频播放器上安全显示图像的区域。设置为 0 则关闭此帧。注意：在覆盖平面中显示的对象不会通过 snapshot 命令保存到文件中。</p><p><code>viewer [&lt;number&gt;] getViewerSpinAnimation</code><br>如果查看器旋转动画已启用，返回 1，否则返回 0。</p><p><code>viewer [&lt;number&gt;] setViewerSpinAnimation &lt;state&gt;</code><br>如果 state 为 1，启用查看器旋转动画。否则，传递 0 作为状态将关闭查看器旋转动画。注意：查看器旋转动画的状态会保存为首选项，因此重启 Avizo 后仍保持不变。</p><p><code>viewer [&lt;number&gt;] setViewing &lt;state&gt;</code><br>开启或关闭查看器的查看状态。如果 state 为 1，查看器进入查看模式；如果为 0，关闭查看模式。</p><p>设置查看器的查看状态。如果 state 为 0，查看器切换到交互模式；如果为 1，则切换到查看模式。</p><p><code>viewer [&lt;number&gt;] getViewing</code><br>返回当前查看状态：0 表示交互模式，1 表示查看模式。</p><p><code>viewer [&lt;number&gt;] linkViewers [&lt;ID&gt;...]</code><br>此命令用于将多个查看器链接在一起。它的操作与相应的 GUI 操作一致。</p><p><code>viewer [&lt;number&gt;] unlinkViewers [&lt;ID&gt;...] [all]</code><br>此命令用于取消已链接的查看器。</p><h4 id="9-3-4-2-主窗口命令选项"><a href="#9-3-4-2-主窗口命令选项" class="headerlink" title="9.3.4.2 主窗口命令选项"></a>9.3.4.2 主窗口命令选项</h4><p>命令 theMain 允许你访问并控制 Avizo 的主窗口。除了下列列出的特定命令选项外，所有在 9.3.4.4 节（顶层窗口的通用命令）中列出的子命令也可以使用。</p><h5 id="命令：-1"><a href="#命令：-1" class="headerlink" title="命令："></a>命令：</h5><p><code>theMain snapshot filename</code><br>创建并保存主窗口的快照图像。图像文件的格式由文件名扩展名决定。任何 Avizo 支持的标准图像文件格式都可以使用，例如 .jpg、.tif、.png 或 .bmp。</p><p><code>theMain setViewerTogglesOnIcons &#123;0|1&#125;</code><br>启用或禁用 Avizo 项目视图中对象图标上的橙色查看器切换。</p><p><code>theMain ignoreShow [0|1]</code><br>启用或禁用特殊用途的 “no show” 标志。如果设置了该标志，后续的 mainWindow show 命令将被忽略。这对于在 Avizo XScreen 扩展环境中运行标准 Avizo 脚本非常有用。如果不带参数调用该命令，则仅返回标志的当前值。</p><p><code>theMain showConsole [0|1]</code><br>启用或禁用 Avizo 中控制台窗口的显示。</p><h5 id="9-3-4-3-控制台命令选项"><a href="#9-3-4-3-控制台命令选项" class="headerlink" title="9.3.4.3 控制台命令选项"></a>9.3.4.3 控制台命令选项</h5><p>命令 theMsg 允许您访问和控制 Avizo 控制台窗口。除了下列的特定命令选项外，9.3.4.4 节（顶层窗口的通用命令）中列出的所有子命令也可以使用。</p><p>命令：</p><p><code>theMsg error &lt;message&gt; [&lt;btn0-text&gt;] [&lt;btn1-text&gt;] [&lt;btn2-text&gt;]</code><br>弹出一个带有指定消息的错误对话框。该对话框可以配置多达三个不同的按钮。命令会阻塞，直到用户按下按钮。返回按下按钮的 ID。</p><p><code>theMsg warning &lt;message&gt; [&lt;btn0-text&gt;] [&lt;btn1-text&gt;] [&lt;btn2-text&gt;]</code><br>弹出一个带有指定消息的警告对话框。对话框可以配置多达三个不同的按钮。命令会阻塞，直到用户按下按钮。返回按下按钮的 ID。</p><p><code>theMsg question &lt;message&gt; [&lt;btn0-text&gt;] [&lt;btn1-text&gt;] [&lt;btn2-text&gt;]</code><br>弹出一个带有指定消息的问题对话框。该对话框可以配置多达三个不同的按钮。命令会阻塞，直到用户按下按钮。返回按下按钮的 ID。</p><p><code>theMsg overwrite &lt;filename&gt;</code><br>弹出一个对话框，询问用户是否可以覆盖指定的文件。如果用户点击“确定”，则返回 1，否则返回 0。</p><h4 id="9-3-4-4-顶层窗口的通用命令"><a href="#9-3-4-4-顶层窗口的通用命令" class="headerlink" title="9.3.4.4 顶层窗口的通用命令"></a>9.3.4.4 顶层窗口的通用命令</h4><p>这些命令适用于所有打开独立顶层窗口的 Avizo 对象，特别是 Avizo 主窗口（theMain）、控制台窗口（theMsg）和查看器窗口（viewer 0）。例如，您可以使用相应的全局命令和 setPosition 或 getPosition 来设置或获取这些窗口的位置。</p><h5 id="命令：-2"><a href="#命令：-2" class="headerlink" title="命令："></a>命令：</h5><ul><li><p><code>getFrameGeometry</code><br>返回窗口的位置和大小，包括窗口框架。共返回四个数字。前两个数字表示窗口框架左上角相对于桌面左上角的位置，后两个数字表示窗口的大小（像素）。</p></li><li><p><code>getGeometry</code><br>返回窗口的位置和大小，不包括窗口框架。共返回四个数字。前两个数字表示窗口左上角相对于桌面左上角的位置，后两个数字表示窗口的大小（像素）。</p></li><li><p><code>getPosition</code><br>返回窗口左上角的位置，包括窗口框架。此结果与 getFrameGeometry 返回的前两个数字相同。</p></li><li><p><code>getRelativeGeometry</code><br>返回窗口的位置和大小，包括窗口框架，以相对坐标表示。桌面的大小为 (1,1)。窗口的位置和大小以 0 到 1 之间的小数表示。</p></li><li><p><code>getSize</code><br>返回窗口的大小，不包括窗口框架。此结果与 getGeometry 返回的后两个数字相同。</p></li><li><p><code>hide</code><br>隐藏窗口。</p></li><li><p><code>setCaption &lt;text&gt;</code><br>设置显示在窗口框架中的窗口标题。</p></li><li><p><code>setFrameGeometry &lt;x y width height&gt;</code><br>设置窗口的位置和大小，包括窗口框架。需要指定四个数字，即 x 和 y 位置、窗口宽度和高度。</p></li><li><p><code>setGeometry &lt;x y width height&gt;</code><br>设置窗口的位置和大小，不包括窗口框架。需要指定四个数字，即 x 和 y 位置、窗口宽度和高度。</p></li><li><p><code>setPosition &lt;x y&gt;</code><br>设置窗口的左上角位置，包括窗口框架。需要指定两个数字，即 x 和 y 坐标。</p></li><li><p><code>setRelativeGeometry &lt;x y width height&gt;</code><br>以相对桌面大小的比例设置窗口的位置和大小，包括窗口框架。输入值应该在 0 到 1 之间。</p></li><li><p><code>setSize &lt;width height&gt;</code><br>设置窗口的大小，不包括窗口框架。需要指定两个数字，即窗口的宽度和高度。</p></li><li><p><code>show</code><br>显示窗口。</p></li><li><p><code>showMinimized</code><br>使窗口在图标状态下可见。</p></li><li><p><code>showMaximized</code><br>使窗口在最大化状态下可见</p></li></ul><h4 id="9-3-4-5-进度条命令选项"><a href="#9-3-4-5-进度条命令选项" class="headerlink" title="9.3.4.5 进度条命令选项"></a>9.3.4.5 进度条命令选项</h4><p>workArea 命令允许你访问位于Avizo主窗口底部的进度条。你可以打印信息或检查是否按下了停止按钮。</p><h5 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h5><ul><li><p><code>workArea setProgressInfo &lt;text&gt;</code><br>设置要显示在进度条中的信息文本。该文本可用于描述某些计算期间的状态。</p></li><li><p><code>workArea setProgressValue &lt;value&gt;</code><br>设置进度条的值。该参数必须是0到1之间的浮点数。例如，值0.8表示当前任务已经完成了80%。</p></li><li><p><code>workArea startWorking [&lt;message&gt;]</code><br>激活停止按钮。调用此命令后，Avizo的停止按钮变为活动状态。在脚本中，可以通过调用workArea wasInterrupted来检查是否按下了停止按钮。当停止按钮处于活动状态时，除非在脚本中调用workArea stopWorking，否则无法与其他任何小部件进行交互。因此，不应直接在控制台窗口中输入此命令，而应仅在脚本文件或Tcl过程（procedure）中使用它。</p></li><li><p><code>workArea stopWorking</code><br>停用停止按钮。当通过workArea startWorking启动的计算任务完成或用户按下停止按钮时，调用此命令。该命令还会恢复调用startWorking之前显示的进度信息文本。</p></li><li><p><code>workArea wasInterrupted</code><br>检查用户是否按下了停止按钮。此命令应仅在workArea startWorking和workArea stopWorking之间使用。如果有多个嵌套计算任务且用户按下了停止按钮，那么在到达第一级之前，后续所有对wasInterrupted的调用都将返回true。</p></li></ul><h4 id="9-3-4-6-应用程序命令选项"><a href="#9-3-4-6-应用程序命令选项" class="headerlink" title="9.3.4.6 应用程序命令选项"></a>9.3.4.6 应用程序命令选项</h4><p>app 命令提供了与Avizo本身相关的几个选项，而不是特定于某个对象或组件。</p><h5 id="命令-1"><a href="#命令-1" class="headerlink" title="命令"></a>命令</h5><ul><li><p><code>app version</code><br>返回当前的Avizo版本。</p></li><li><p><code>app uname</code><br>返回简化的操作系统名称。</p></li><li><p><code>app arch</code><br>返回Avizo的架构字符串，例如arch-Win32VC8-Optimize、arch-LinuxAMD64-Optimize。</p></li><li><p><code>app hostid</code><br>返回创建Avizo许可证密钥所需的主机ID。</p></li><li><p><code>app listen [port]</code><br>打开一个套接字，Tcl命令可以通过该套接字发送。可以选择性地指定TCP/IP端口。<br>警告： 这可能会创建安全漏洞。除非在防火墙后且您明确知道自己在做什么，否则请勿使用此功能。</p></li><li><p><code>app close</code><br>关闭Avizo Tcl端口。</p></li><li><p><code>app port</code><br>返回Avizo Tcl端口号。如果套接字未打开，则返回-1。</p></li><li><p><code>app send &lt;command&gt; [&lt;host&gt; [&lt;port&gt;]]</code><br>将Tcl命令发送到监听的Avizo实例。如果未指定主机或端口，则Avizo实例会将命令发送给自己。</p></li><li><p><code>app opengl</code><br>检索有关所用 OpenGL 驱动程序的信息，包括版本号和支持的扩展。如果报告渲染问题，这些信息可以发送到热线。</p></li><li><p><code>app cluster</code><br>返回当前节点状态，如果某个集群模式处于活动状态，则该状态可以是“主”或“从”，否则，则简单地为“单个”。</p></li><li><p><code>app memTotal [-k | -m | -g]</code><br>返回系统的物理内存（以字节为单位）。可选开关 -k、-m、-g 分别将输出转换为千字节、兆字节或千兆字节。</p></li><li><p><code>app memAvail [-k | -m | -g]</code><br>返回系统的可用内存（以字节为单位）。可选开关 -k、-m、-g 分别将输出转换为千字节、兆字节或千兆字节。请注意，根据操作系统的不同，输出可能与其他工具报告的输出不同。</p></li></ul><h4 id="9-3-4-7-其他全局命令"><a href="#9-3-4-7-其他全局命令" class="headerlink" title="9.3.4.7 其他全局命令"></a>9.3.4.7 其他全局命令</h4><h5 id="命令-2"><a href="#命令-2" class="headerlink" title="命令"></a>命令</h5><ul><li><p><code>addTimeout msec procedure [arg]</code><br>安排在 msec 毫秒后调用 Tcl 过程。如果指定了 arg，它将作为参数传递给该过程。指定的过程只会被调用一次。如果需要，您可以在超时过程中再次安排它。例如：addTimeout 10000 echo {10 seconds are over.}</p></li><li><p><code>all [-selected | -visible | -hidden] [type]</code><br>返回当前在项目视图中的所有 Avizo 对象的列表。如果指定了 type，则只返回具有该 C++ 类类型（或派生对象）的对象。搜索可以分别限于已选择的、可见的或隐藏的对象。例如：all -hidden HxColormap.</p></li><li><p><code>aminfo [-a outfile|-b outfile] Avizo-File</code><br>如果只使用文件名作为参数，此命令将打开必须是 Avizo 数据格式的文件并打印头信息。如果使用 -a 或 -b 选项，则指定为参数的输出文件 outfile 将分别以 ASCII (-a) 或二进制 (-b) 格式写入。因此，aminfo 可用于将二进制 Avizo 数据转换为 ASCII，反之亦然。</p></li><li><p><code>clear</code><br>清除控制台窗口。</p></li><li><p><code>create class name [instance name]</code><br>创建 Avizo 对象（如模块或数据对象）的实例。返回实例名称。请注意，数据对象通常不是通过这种方式创建的，而是通过从文件加载它们。例如：create HxOrthoSlice MySlice.</p></li><li><p><code>dso options</code><br>控制动态库（“动态共享对象”）的加载。提供以下选项：</p></li></ul><p><code>addPath path ...</code>: 添加路径到加载动态库时搜索的目录列表中。<br><code>verbose &#123;0|1&#125;</code>: 开启或关闭与动态库相关的调试信息。<br><code>open &lt;package&gt;</code>: 尝试加载指定的动态库。只需指定包名，例如 hxfield。此名称将自动转换为平台相关的名称，例如 Linux 上的 libhxfield.so 或 Windows 上的 hxfield.dll。<br><code>unloadPackage &lt;package&gt;</code>: 卸载（如果可能）指定的动态库。<br><code>execute &lt;package&gt; &lt;function&gt;</code>: 执行在指定动态库中定义的函数。</p><ul><li><p><code>echo args</code><br>将其参数打印到 Avizo 控制台。请使用此命令，而不是原生的 Tcl 命令 puts，后者会打印到标准输出。</p></li><li><p><code>help arguments</code><br>不带参数时，打开 Avizo 帮助浏览器。</p></li><li><p><code>httpd [port]</code><br>启动内置的HTTP服务器。HTTP服务器将交付任何请求的文档。如果请求的文档以“.hx”结尾，Avizo将不会交付它，而是将其作为Tcl脚本执行。这可以用于从网页浏览器控制Avizo。<strong>警告：</strong>该命令可能会产生安全漏洞，除非你在防火墙后且清楚自己在做什么，否则不要使用此功能。</p></li><li><p><code>limit &#123;datasize | stacksize | coredumpsize&#125; size</code><br>更改进程限制。仅适用于Unix平台。使用“unlimited”作为无限制的大小。大小必须以字节为单位指定。或者，你可以使用1000k表示1000千字节或1m表示1兆字节。</p></li><li><p><code>load [fileformat] options files</code><br>从一个或多个文件中加载数据。可以选择指定文件格式以覆盖Avizo的自动文件格式识别。文件格式使用与在Avizo文件对话框中的文件格式组合框中显示的标签相同。使用全局命令fileFormats可以获得Avizo支持的所有文件格式列表。可以使用FTP或HTTP协议读取远程文件。</p></li></ul><p>其他选项包括：</p><ul><li><code>browse</code>：显示打开数据窗口。</li><li><code>avizoscript</code>：打开Avizo脚本文件。</li><li><code>avizo</code>：Avizo的本地通用文件格式。用于加载许多不同的数据对象，如在规则或四面体网格上定义的字段、分割结果、颜色映射或顶点集（如地标）。</li><li><code>dataOnly</code>：阻止导入器创建显示模块，适用于hx文件。</li></ul><p>继续翻译如下：</p><ul><li><code>load [fileformat] options files</code></li></ul><p>从一个或多个文件中加载数据。可以选择指定文件格式以覆盖Avizo的自动文件格式识别。文件格式使用与在Avizo文件对话框中的文件格式组合框中显示的标签相同。使用全局命令fileFormats可以获得Avizo支持的所有文件格式列表。可以使用FTP或HTTP协议读取远程文件。</p><p>其他选项包括：</p><ul><li>browse：显示打开数据窗口。</li><li>avizoscript：打开Avizo脚本文件。</li><li>avizo：Avizo的本地通用文件格式。用于加载许多不同的数据对象，如在规则或四面体网格上定义的字段、分割结果、颜色映射或顶点集（如地标）。</li><li>dataOnly：阻止导入器创建显示模块，适用于hx文件。</li></ul><p>Options for raw data:<br>load -raw FileName Endianess IndexOrder<br>DataType nDataVar dimX dimY dimZ<br>xMin xMax yMin yMax zMin zMax</p><p>Options for DICOM data:</p><ul><li><p>nodialog: option prevent the Dicom dialog box from being shown.</p></li><li><p><code>mem</code><br>Prints out some memory statistics.</p></li><li><p><code>quit</code><br>Immediately quits Avizo.</p></li><li><p><code>remove fobjectname | -all | -selectedg</code><br>Removes objects from Project View.<br>• objectname: the specified Avizo object.<br>• -all: all objects.<br>• -selected: selected objects.</p></li></ul><p>从项目视图中删除对象。<br>• objectname：指定的 Avizo 对象。<br>• -all：所有对象。<br>• -selected：选定对象。</p><ul><li><p><code>removeTimeout procedure [arg]</code><br>Unschedules a Tcl procedure previously scheduled with addTimeout.<br>取消先前使用 addTimeout 安排的 Tcl 过程。</p></li><li><p><code>rename objectname newname</code><br>Changes instance name of an object. Identical to objectname setLabel newname, except that it<br>returns 1 if successful, and nothing if unsuccessful.<br>更改对象的实例名称。与 objectname setLabel newname 相同，不同之处在于如果成功则返回 1，如果不成功则不返回任何内容。</p></li><li><p><code>sleep sec</code><br>Waits for sec seconds. Avizo will not process events in that time.<br>等待 sec 秒。Avizo 将不会在这段时间内处理事件。</p></li><li><p><code>source filename</code><br>Loads and executes Tcl commands from the specified file. If the script file contains the extension<br>.hx, the load command may be used as well.<br>从指定文件加载并执行 Tcl 命令。如果脚本文件包含扩展名 .hx，也可以使用加载命令。</p></li><li><p><code>system command</code><br>Executes an external program. Do not use this unless you know what you are doing.<br>执行外部程序。除非您知道自己在做什么，否则不要使用此功能。</p></li><li><p><code>saveProject</code><br>Saves current project. If the project is not previously saved, then the project will be saved in the<br>Avizo root dir as Untitled.hx.<br>保存当前项目。如果之前未保存该项目，则该项目将作为 Untitled.hx 保存在 Avizo 根目录中。</p></li><li><p><code>saveProjectAs [-forceAutoSave | -packAndGo] arg</code><br>A copy of the current project will be saved as arg in Avizo root dir.(e.g., saveProjectAs<br>myProject). When using a path, a full path needs to be specified and a .hx extension needs to be<br>added on the project name (e.g., saveProjectAs c:/work/myProject.hx). Optionally, a<br>forceAutoSave parameter can be specified to force auto saving of modified data without displaying<br>a warning dialog. If parameter packAndGo is specified, in the same folder where the project file<br>is saved, a new folder will be created and it will contain all data necessary for loading the saved<br>project. Note: If a file exists it will not be overwritten.<br>当前项目的副本将作为 arg 保存在 Avizo 根目录中。（例如，saveProjectAs<br>myProject）。使用路径时，需要指定完整路径，并在项目名称上添加 .hx 扩展名（例如，saveProjectAs c:/work/myProject.hx）。或者，可以指定 forceAutoSave 参数以强制自动保存修改后的数据而不显示警告对话框。如果指定了参数 packAndGo，则在保存项目文件的同一文件夹中，将创建一个新文件夹，其中包含加载已保存项目所需的所有数据。注意：如果文件存在，则不会覆盖它。</p></li><li><p><code>theObjectPool setSelectionOrder ffirst objectg fsecond objectg...</code><br>This command reorders the selection so that it matches the given object order. Selected objects not contained inside this list will be moved at the end of the selection (their relative order will not be<br>changed though).<br>此命令将重新排列所选内容，使其与给定的对象顺序相匹配。不包含在此列表中的所选对象将移动到所选内容的末尾（但它们的相对顺序不会改变）。</p></li><li><p><code>thePreferences [save | load] filename</code><br>This command saves or loads preferences to/from the file specified as filename.<br>此命令将首选项保存到指定为文件名的文件中，或从指定为文件名的文件中加载首选项。</p></li><li><p><code>theProperties [show | hide]</code><br>This command shows or hides the Properties panel in Avizo.<br>此命令显示或隐藏 Avizo 中的属性面板。</p></li><li><p><code>theProjectView [show | hide]</code><br>This command shows or hides the Project View panel in Avizo.<br>此命令显示或隐藏 Avizo 中的项目视图面板。</p></li><li><p><code>fileFormats</code><br>Shows all file formats which can be used in Avizo.<br>显示可在 Avizo 中使用的所有文件格式。</p></li></ul><h3 id="9-3-5-Avizo-脚本文件"><a href="#9-3-5-Avizo-脚本文件" class="headerlink" title="9.3.5 Avizo 脚本文件"></a>9.3.5 Avizo 脚本文件</h3><p>值得注意的是，一个 Avizo 项目本质上就是一个 Tcl 脚本，它能够重新生成当前的 Avizo 状态。因此，通常可以通过交互式地创建一个 Avizo 项目，将其保存为“保存项目”，并使用它作为脚本编写的起点。</p><p>在 Avizo 中执行 Tcl 命令的最简单方式是在 Avizo 控制台窗口中输入命令。然而，对于像循环或过程这样的多行结构，使用这种方式并不实用。在这种情况下，建议将 Tcl 代码写入文件，并通过 source 文件名 命令执行该文件。你还可以在一个文件中使用 source 命令以包含另一个文件的内容。</p><p>另一种方法是使用 load 文件名 命令或通过文件菜单中的“打开项目…”选项并使用文件浏览器加载文件。不过，要让 Avizo 识别该文件格式，文件名必须以 .hx 结尾，或者文件内容必须以以下标题行开头：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Avizo Script</span></span><br></pre></td></tr></table></figure><p>在 Avizo 启动时，有一些 Tcl 文件会被自动加载。启动时，程序会查找当前目录或主目录中的 .Avizo 文件（详见 9.2.3 节，启动脚本）。如果没有找到这样的用户自定义启动脚本，默认的初始化脚本 Avizo.init 将从 $AVIZO_LOCAL/share/resources/Avizo 或 $AVIZO_ROOT/share/resources/Avizo 目录加载。该脚本随后会读取所有 .rc 文件，这些文件用于注册模块和数据类型。因此，用户可以通过简单地向该目录添加新的 .rc 文件或修改 Avizo.init 文件来自定义 Avizo 的启动行为。</p><p>另一种执行 Tcl 代码的方式是定义与功能键相关联的过程。如果存在名为 onKeyF2、onKeyF3、…、onKeyShiftF2、…、onKeyCtrlF2、… 等的预定义过程，当相应的键在 Avizo 主窗口、控制台窗口或查看器窗口中按下时，这些过程将被自动调用。要定义这些过程，可以将其写入一个文件并使用 source 加载，或者将其写入 Avizo.init 或某个 .rc 文件中。例如：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">proc</span><span class="title"> onKeyF2</span> &#123; &#125; &#123;</span><br><span class="line">  echo <span class="string">&quot;Key F2 was hit&quot;</span></span><br><span class="line">  viewer <span class="number">0</span> viewAll</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：<br>某些功能键已保留给 Avizo 特定的操作。例如，[F1] 永远用于帮助，[F2] 在项目视图或树视图中用于对象重命名。</p><p>最后，Tcl 脚本也可以在 GUI 中表示并与用户界面结合使用。在 Avizo 中，这被称为脚本模块。</p><h3 id="9-3-6-配置弹出菜单"><a href="#9-3-6-配置弹出菜单" class="headerlink" title="9.3.6 配置弹出菜单"></a>9.3.6 配置弹出菜单</h3><p>在 Avizo 中，所有可以附加到数据对象的模块都会列在对象的弹出菜单中，该菜单通过右键单击对象图标激活。对于某些应用来说，在创建新模块后使用 Tcl 命令自定义这些模块是有意义的。有时还需要在对象的弹出菜单中添加新条目，以执行特定的脚本。本节将介绍如何通过修改 Avizo 资源文件或创建新文件来实现这些目标。</p><p>Avizo 的资源文件位于目录 $AVIZO_ROOT/share/resources 中，其中 $AVIZO_ROOT 表示 Avizo 的安装目录。资源文件实际上是普通的脚本文件，只是使用 .rc 作为后缀。当 Avizo 启动时，资源目录中的所有资源文件都会被读取。在资源文件中，可以使用特殊的 Tcl 命令注册模块、编辑器和 IO 例程。注册一个模块意味着指定它在弹出菜单中的名称，它可以附加的对象类型，模块所在的共享库或 DLL 的名称，等等。例如，Multi-Thresholding 模块通过以下命令在文件 hxlattice.rc 中注册：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">module -name &quot;Multi-Thresholding&quot; \</span><br><span class="line">-check &#123; ![$PRIMARY hasInterface HxLabelLattice3] &amp;&amp;</span><br><span class="line"><span class="meta prompt_">([$</span><span class="language-bash">PRIMARY primType]&lt;3 || [<span class="variable">$PRIMARY</span> primType]==7 || [<span class="variable">$PRIMARY</span> primType]==8) &#125; \</span></span><br><span class="line"><span class="language-bash">-primary <span class="string">&quot;HxUniformScalarField3 HxStackedScalarField3&quot;</span> \</span></span><br><span class="line"><span class="language-bash">-category <span class="string">&quot;&#123;Image Segmentation&#125;&quot;</span> \</span></span><br><span class="line"><span class="language-bash">-class <span class="string">&quot;HxLabelVoxel&quot;</span> \</span></span><br><span class="line"><span class="language-bash">-dso <span class="string">&quot;libhxlattice.so&quot;</span></span></span><br></pre></td></tr></table></figure><p>该命令的不同选项具有以下含义：</p><ul><li>选项 -name 指定模块的名称或标签，它将显示在弹出菜单中。</li><li>选项 -primary 表示该模块可以附加到类型为 HxUniformScalarField3 或 HxStackedScalarField3 的数据对象上。这意味着 Multi-Thresholding 只会包含在这些对象的弹出菜单中。</li><li>选项 -check 指定一个附加的 Tcl 表达式，该表达式在菜单弹出之前会被执行。如果表达式失败，模块将从菜单中删除。对于 Multi-Thresholding 模块，它会检查输入对象是否提供了 HxLabelLattice3 接口，即输入是否是一个标签场。尽管标签图像可以被视为 3D 图像，但对其进行阈值分割是没有意义的。因此，Multi-Thresholding 仅对原始 3D 图像提供，而不适用于标签场。还会检查输入的原始数据类型（有符号/无符号整数、浮点、短整型等）。这里，Multi-Thresholding 模块不支持浮点或双精度标签图像输入。</li><li>选项 -category 表示 Multi-Thresholding 应该出现在主弹出菜单的 “Image Segmentation” 子菜单中。如果模块不应出现在子菜单中，而是直接在弹出菜单中显示，则应使用类别 Main。</li><li>选项 -class 指定模块的内部类名称。对象的内部类名称可以使用命令 getTypeId 检索。正是这个类名称必须用于上述的 -primary 选项，而不是通过 -name 定义的对象标签。</li><li>最后，选项 -package 指定定义该模块的包（共享库或 DLL）。</li></ul><p>除了这些标准选项，还可以使用额外的 -proc 选项指定在创建模块后执行的额外 Tcl 命令。例如，假设你在一个医疗项目中，需要识别头部 CT 图像中的立体定位标记。此时，可以在弹出菜单中添加一个 Multi-Thresholding 模块的自定义版本，并预定义适当的材质名称和阈值。可以通过在 $AVIZO_ROOT/share/resources 目录中添加一个新资源文件，或者直接在 hxlattice.rc 文件中添加以下命令来实现：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">module -name &quot;Stereotaxy&quot; \</span><br><span class="line">-primary &quot;HxUniformScalarField3 HxStackedScalarField3&quot; \</span><br><span class="line">-check &#123; ![$PRIMARY hasInterface HxLabelLattice3] &#125; \</span><br><span class="line">-category &quot;&#123;Image Segmentation&#125;&quot; \</span><br><span class="line">-class &quot;HxLabelVoxel&quot; \</span><br><span class="line">-package &quot;hxlattice&quot; \</span><br><span class="line">-proc &#123; $this regions setValue &quot;Exterior Bone Markers&quot;; \</span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">this fire; \</span></span><br><span class="line"><span class="language-bash"><span class="variable">$this</span> boundary01 setValue 150; \</span></span><br><span class="line"><span class="language-bash"><span class="variable">$this</span> boundary12 setvalue 300 &#125;</span></span><br></pre></td></tr></table></figure><p>上述 Tcl 代码中的变量 $this 指的是新创建的模块，即 Multi-Thresholding 模块。注意，这些命令是在模块连接到弹出菜单的源对象之前执行的。某些模块在连接到新的输入对象时会进行一些特殊的初始化，这些初始化可能会覆盖使用自定义 -proc 选项通过 Tcl 命令设置的值。在这种情况下，可以通过以下命令显式连接模块到输入对象：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">this data connect <span class="variable">$PRIMARY</span>;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">this fire;</span></span><br></pre></td></tr></table></figure><p>其中 Tcl 变量 $PRIMARY 指的是输入对象。同样，该变量也用于上述 -check 选项中定义的 Tcl 表达式。</p><p>除了基于现有模块创建自定义弹出菜单条目外，还可以定义完全新的条目，这些条目只执行 Tcl 命令。例如，我们可以在每个 Avizo 对象的弹出菜单中添加一个新的子菜单“编辑”，并在此处放入通常包含在 Avizo 主窗口的“编辑”菜单中的“隐藏”、“删除”和“复制”命令。这可以通过以下方式实现：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">module -name &quot;Remove&quot; \</span><br><span class="line">    -primary &quot;HxObject&quot; \</span><br><span class="line">    -proc &#123; remove $PRIMARY &#125; \</span><br><span class="line">    -category &quot;Edit&quot;</span><br><span class="line">module -name &quot;Hide&quot; \</span><br><span class="line">    -primary &quot;HxObject&quot; \</span><br><span class="line">    -proc &#123; $PRIMARY hideIcon &#125; \</span><br><span class="line">    -category &quot;Edit&quot;</span><br><span class="line">module -name &quot;Duplicate&quot; \</span><br><span class="line">    -primary &quot;HxData&quot; \</span><br><span class="line">    -proc &#123; $PRIMARY duplicate &#125; \</span><br><span class="line">    -category &quot;Edit</span><br></pre></td></tr></table></figure><p>当然，也可以使用 <code>-proc</code> 命令执行普通的 Avizo 脚本，甚至 Avizo 脚本对象。</p><h3 id="9-3-7-注册点击回调函数"><a href="#9-3-7-注册点击回调函数" class="headerlink" title="9.3.7 注册点击回调函数"></a>9.3.7 注册点击回调函数</h3><p>点击回调函数是附加到模块或查看器上的 Tcl 过程。当点击事件发生在目标上时，会调用该回调函数。可以通过在模块或查看器上使用 Tcl 命令 setPickCallback 来注册此类回调：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;module&gt; setPickCallback &lt;<span class="keyword">proc</span>&gt; [&lt;EventType&gt;]<span class="title"></span></span><br><span class="line"><span class="title">viewer</span> &lt;n&gt;<span class="title"> setPickCallback</span> &lt;<span class="keyword">proc</span>&gt; [&lt;EventType&gt;]</span><br></pre></td></tr></table></figure><p>每个模块或查看器只能附加一个回调。为了分离回调，只需调用注册命令而不带参数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;module&gt; setPickCallback</span><br><span class="line">viewer &lt;n&gt; setPickCallback</span><br></pre></td></tr></table></figure><p>可选的 <code>&lt;EventType&gt;</code> 参数用于指定触发回调的事件类型。其他事件将被忽略。该参数可以取以下值：</p><ul><li>MouseButtonPress, MouseButtonRelease（任意鼠标按钮）</li><li>VRButtonPress, VRButtonRelease（任意3D按钮）</li><li>MouseButton1Press, MouseButton1Release 等（特定的鼠标按钮）</li><li>VRButton0Press, VRButton0Release 等（特定的3D按钮）</li></ul><p>默认值为 MouseButton1Press。</p><p>实际的回调过程 <code>&lt;proc&gt;</code> 预期接收一个参数，该参数被解释为一个关联数组，并编码所有点击信息。定义的元素如下：</p><ul><li>object：被点击几何体所属的 Avizo 对象的名称</li><li>x：被点击点的 x 坐标</li><li>y：被点击点的 y 坐标</li><li>z：被点击点的 z 坐标</li><li>idx：被点击元素的索引</li><li>stateBefore：事件发生之前的修饰符状态</li><li>stateAfter：事件发生之后的修饰符状态</li></ul><p>回调过程应返回 0，如果未处理点击事件，这样其他回调过程可以被调用。以下是一个示例：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">proc</span><span class="title"> pickCallback</span> arg &#123;</span><br><span class="line">    <span class="keyword">array</span> <span class="keyword">set</span> a <span class="variable">$arg</span></span><br><span class="line">    echo <span class="string">&quot;$a(object) : picked element $a(idx)&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>请注意，任何模块都可以向该参数数组添加特定信息。所有元素可以通过以下方式显示：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">proc</span><span class="title"> pickCallback</span> arg &#123;</span><br><span class="line">    echo <span class="string">&quot;arg = &#123; $arg &#125;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此，一些 Avizo 模块会追加额外的数据：</p><ul><li>Vertex View：idx 是被点击的点索引</li><li>Point Cloud View：idx 是被点击的点索引</li><li>Line Set View：idx 是被点击的线索引，pt0 和 pt1 是被点击段的两个点</li><li>Surface View：idx 是被点击的三角形索引</li><li>Hexa/Tetra Grid View：idx 是被点击的三角形索引，tetra0 和 tetra1 是相邻的四面体</li><li>Grid Boundary：idx 是被点击的三角形索引，originalIdx 是网格中的索引，tetra0 和 tetra1 是相邻的四面体</li></ul><h3 id="9-3-8-文件读取器在Tcl中"><a href="#9-3-8-文件读取器在Tcl中" class="headerlink" title="9.3.8 文件读取器在Tcl中"></a>9.3.8 文件读取器在Tcl中</h3><p>本节介绍如何注册一个由Tcl实现的自定义文件读取器。</p><p>首先，需要在全局作用域中声明Tcl读取器函数。该函数必须接受一个文件列表作为输入参数，并返回成功读取的文件数量。</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">proc</span><span class="title"> myReaderInTcl</span> &#123;args&#125; &#123;</span><br><span class="line">    echo <span class="string">&quot;myReaderInTcl $args&quot;</span></span><br><span class="line"><span class="comment">    # 其他相关处理</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接着，必须使用以下模板将Tcl读取器函数注册到目标文件格式声明中：</p><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataFile -name <span class="string">&quot;MyFormat&quot;</span> ... -<span class="keyword">package</span> hxcore -<span class="keyword">load</span> hxReadByTcl -loadArgs <span class="string">&quot;-cmd myReaderInTcl&quot;</span></span><br></pre></td></tr></table></figure><p>使用 -loadArgs 来指定自定义的Tcl读取器函数。参数 -package hxcore -load hxReadByTcl 需按原样填写，不做修改。这些参数用于设置内部包装器，该包装器将调用Tcl解释器。</p><p>可以在Tcl脚本中声明自定义读取器，或将其包含在一个资源文件中，在应用程序启动时加载。</p><h2 id="9-4-使用-MATLAB-与-Avizo"><a href="#9-4-使用-MATLAB-与-Avizo" class="headerlink" title="9.4 使用 MATLAB 与 Avizo"></a>9.4 使用 MATLAB 与 Avizo</h2><p>本节介绍如何在 Avizo 中使用 MATLAB 脚本。</p><h3 id="9-4-1-使用-MATLAB-脚本"><a href="#9-4-1-使用-MATLAB-脚本" class="headerlink" title="9.4.1 使用 MATLAB 脚本"></a>9.4.1 使用 MATLAB 脚本</h3><p>在本教程中，您将学习如何通过使用 MATLAB 计算模块（The MathWorks, Inc.）将复杂的计算集成到 Avizo 中。</p><p>为了使用 MATLAB 计算模块，MATLAB 必须正确安装在您的计算机上。此外，为了使该模块能够与 MATLAB 计算引擎建立连接，您可能需要注册 MATLAB 引擎（在 Windows 上），并根据您的系统设置环境变量以包含 MATLAB 库或程序在搜索路径中。请参阅 MATLAB 计算模块的文档以了解安装细节和限制。</p><p>本教程，作为在线文档提供，涵盖了以下主题，通过各种示例进行讲解：</p><ul><li>加载和执行 MATLAB 脚本。</li><li>在 Avizo 和 MATLAB 之间传递各种数据类型，并将数据导出回 Avizo。</li><li>使用字段结构。</li><li>使用时间滑块控制脚本变量。</li><li>从脚本中调用用户自定义的 MATLAB 函数。</li><li>你可以通过这些步骤来学习如何将 MATLAB 的复杂计算功能与 Avizo 集成，从而执行更加精确和复杂的数据处理任务。</li></ul><h1 id="Python-Scripting"><a href="#Python-Scripting" class="headerlink" title="Python Scripting"></a>Python Scripting</h1><p><a href="https://www.thermofisher.cn/cn/zh/home/electron-microscopy/products/software-em-3d-vis/3d-visualization-analysis-software/python-scripting.html">Amira-Avizo Software and PerGeos Software Python Integration</a></p><p><a href="https://www.youtube.com/watch?v=tUlZFJj-aYs">Amira-Avizo Software | Introduction to Python scripting</a></p><p><a href="D:\Program Files\Avizo-2019.1\share\doc\python\html\index.html">Thermo Fisher Python API documentation</a></p><h2 id="11-6-Python-脚本编写"><a href="#11-6-Python-脚本编写" class="headerlink" title="11.6 Python 脚本编写"></a>11.6 Python 脚本编写</h2><p>本节描述了如何在Avizo中使用Python脚本。</p><h3 id="11-6-1-Python-文档"><a href="#11-6-1-Python-文档" class="headerlink" title="11.6.1 Python 文档"></a>11.6.1 Python 文档</h3><p>本章按如下方式组织：</p><p>11.6.1.1 Python 简介<br>11.6.1.2 内嵌 Python 的使用<br>11.6.1.3 常见的全局命令<br>11.6.1.4 模块管理<br>11.6.1.5 脚本对象<br>11.6.1.6 Python 环境和包管理器<br>11.6.1.7 Python 包列表<br>Avizo Python API 文档可以在此处查看。</p><blockquote><p>安装文件’\share\doc\python\html\index.html’</p></blockquote><h4 id="11-6-1-1-Python-简介"><a href="#11-6-1-1-Python-简介" class="headerlink" title="11.6.1.1 Python 简介"></a>11.6.1.1 Python 简介</h4><p>什么是Python</p><p>Python是一种高级的、面向对象的、解释型语言，首次实现于1989年。<br>(<a href="https://www.python.org/dev/peps/pep-0020/">https://www.python.org/dev/peps/pep-0020/</a>)</p><p>• Beautiful is better than ugly<br>• Explicit is better than implicit<br>• Simple is better than complex<br>• Complex is better than complicated<br>• Readability counts</p><ul><li>美比丑好</li><li>明确比隐含好</li><li>简单比复杂好</li><li>复杂比难懂好</li><li>可读性很重要</li><li>包含的内容</li></ul><p>Avizo 使用 Python 3.5.2。关于如何使用Python 3.X的详细信息可以在以下链接找到：<br><a href="https://docs.python.org/3.5/tutorial/index.html">https://docs.python.org/3.5/tutorial/index.html</a> 。许多包都已包含在Python的安装中（参见此处的Python包列表）。Numpy 和 Scipy 是此安装中包含的两个最流行的Python包。</p><p>Numpy 是用于处理多维数组的扩展，它允许元素操作、比较、逻辑操作、统计等。Numpy数组在C中实现，计算速度更快。更多信息请参见：<a href="http://www.numpy.org/">http://www.numpy.org/</a> 。</p><p>Scipy 是一个提供科学计算工具的扩展，例如插值、积分、图像处理、线性代数、信号处理和统计。更多信息请参见：<a href="http://www.scipy.org/">http://www.scipy.org/</a> 。</p><p>从 Python 2 迁移到 Python 3</p><p>Python 2 和 Python 3 之间存在一些兼容性问题。关于迁移到Python 3的官方文档可以在这里找到：<br><a href="https://docs.python.org/3.5/howto/pyporting.html">https://docs.python.org/3.5/howto/pyporting.html</a> 。</p><h5 id="11-6-1-1-1-使用-Python"><a href="#11-6-1-1-1-使用-Python" class="headerlink" title="11.6.1.1.1 使用 Python"></a>11.6.1.1.1 使用 Python</h5><p>本节并不旨在涵盖Python语言的所有应用和细节。要了解更多关于Python、Scipy和Numpy的信息，请参阅前一节中的链接。要了解Python如何与Avizo交互，请继续阅读第11.6.1.2节《内嵌Python的使用》。</p><h6 id="Python控制台"><a href="#Python控制台" class="headerlink" title="Python控制台"></a>Python控制台</h6><p>可以通过 Windows &gt; Consoles 访问控制台。控制台面板具有带有多个不同工具的标签界面。控制台充当Python解释器，因此任何编写的命令在按Enter键后将立即执行。执行后将显示赋值或返回值。</p><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/Figure11.4.png" class="" title="Figure 11.4: Accessing the Consoles"><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/Figure11.5.png" class="" title="Figure 11.5: Main Python Console Interface"><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/Figure11.6.png" class="" title="Figure 11.6: Python Script Object Console Interface"><p>从左到右图标：</p><ul><li>删除此内容：删除此解释器中显示的所有内容</li><li>删除 Pythonic 对象：这会从 Avizo 的工作区中删除控制台内创建的所有 Python 对象</li><li>重定向所有输出：这会将所有解释器的所有输出推送到主控制台</li><li>重定向此输出：这会将所有当前控制台的输出推送到主控制台</li></ul><p>控制台充当 Python 解释器，因此按下 Enter 后将立即执行任何书面命令。执行后将显示赋值或返回值。</p><h6 id="常用快捷键和命令"><a href="#常用快捷键和命令" class="headerlink" title="常用快捷键和命令"></a>常用快捷键和命令</h6><ul><li><p><code>TAB</code><br>在控制台没有编写任何内容时，按下TAB键会自动补全以下命令 hx_project.get(module)，该命令将为对象创建一个Python句柄。当控制台中有文本时，TAB键将尝试从可用方法、属性和模块列表中自动补全当前字符串。选中的项目会被补全。</p></li><li><p><code>UP</code> or <code>DOWN</code><br>这些按钮可以循环浏览您最近的历史记录。UP按钮可以检索上一个命令，而DOWN按钮可以检索后续命令。</p></li></ul><h5 id="11-6-1-1-2-关于Python的注意事项"><a href="#11-6-1-1-2-关于Python的注意事项" class="headerlink" title="11.6.1.1.2 关于Python的注意事项"></a>11.6.1.1.2 关于Python的注意事项</h5><h6 id="软件包"><a href="#软件包" class="headerlink" title="软件包"></a>软件包</h6><p>Python是一种面向对象的语言，这意味着代码通常会被分解为类，在实例化时变量和方法可以被对象继承。</p><p>这种方式可以避免重复编写经常使用的功能。关于Python面向对象编程（OOP）的更多信息，可以在以下链接中找到：<a href="http://www.tutorialspoint.com/python/python_classes_objects.htm">Python类和对象教程</a>。</p><p>软件包是类、方法和变量的集合，可以被导入到Python的命名空间中。例如，如果用户想要计算sin(π/2)，首先需要将Numpy软件包导入命名空间：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> numpy</span><br></pre></td></tr></table></figure><p>Numpy软件包包含一个sin(x)方法，可以用于计算sin(π/2)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;numpy.sin(<span class="number">3.1415</span>/<span class="number">2</span>)</span><br><span class="line"><span class="number">0.99999999892691405</span></span><br></pre></td></tr></table></figure><p>Numpy已经包含了一个全局变量π，可以通过以下方式访问：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;numpy.pi</span><br><span class="line"><span class="number">3.141592653589793</span></span><br><span class="line">&gt;&gt;&gt;numpy.sin(numpy.pi/<span class="number">2</span>)</span><br><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure><p>导入软件包时，可以将其赋值给变量以简化代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt;&gt;np.sin(np.pi/<span class="number">2</span>)</span><br><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure><h6 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h6><p>Python支持多种不同的对象类型：</p><div class="table-container"><table><thead><tr><th style="text-align:center">类型</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th></tr></thead><tbody><tr><td style="text-align:center">Number</td><td style="text-align:center">Integers, floats, complex, booleans</td><td style="text-align:center">1, 1.05, 3j+2, 3&gt;2 is True</td></tr><tr><td style="text-align:center">String List</td><td style="text-align:center">Sequence of characters Container to group items that can be changed</td><td style="text-align:center">“String of charaters” [1, 5, “Dragon”, 948.5]</td></tr><tr><td style="text-align:center">Tuple</td><td style="text-align:center">Container to group items that cannot be changed</td><td style="text-align:center">(948.5, “Dragon”, 5, 1)</td></tr><tr><td style="text-align:center">Dictionary</td><td style="text-align:center">Associated arrays with unique keys</td><td style="text-align:center">{‘a’:99, ’b’:’red’, ’c’:’balloons’}</td></tr><tr><td style="text-align:center">Array</td><td style="text-align:center">Vectorized numeric array optimized for C</td><td style="text-align:center">numpy.ones ((10,5))</td></tr></tbody></table></div><p>有些类型通过特定的字符标识。例如，单引号或双引号用于创建字符串：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;a = <span class="string">&#x27;这是一个字符串。&#x27;</span></span><br><span class="line">&gt;&gt;&gt;b = <span class="string">&quot;这也是一个字符串。&quot;</span></span><br></pre></td></tr></table></figure><p>列表通过方括号进行赋值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;c = [<span class="number">1</span>, <span class="number">5</span>, <span class="string">&quot;Dragon&quot;</span>, <span class="number">948.5</span>]</span><br></pre></td></tr></table></figure><p>对象之间可以使用标准语法进行操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+  加法        ==  相等</span><br><span class="line">-  减法        !=  不相等</span><br><span class="line">*  乘法        &gt;  大于</span><br><span class="line">/  除法        &lt;  小于</span><br><span class="line">** 指数运算    &lt;= 小于或等于</span><br><span class="line"><span class="meta prompt_">% </span><span class="language-bash"> 取余数      &gt;= 大于或等于</span></span><br></pre></td></tr></table></figure><p>某些关键词是保留用于全局变量或执行特定功能的。应尽量避免将这些关键词作为变量使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">and</span>, <span class="keyword">as</span>, <span class="keyword">assert</span>, <span class="keyword">break</span>, <span class="keyword">class</span>, <span class="keyword">continue</span>, <span class="keyword">def</span>, <span class="keyword">del</span>, <span class="keyword">elif</span>, <span class="keyword">else</span>, <span class="keyword">except</span>, <span class="built_in">exec</span>, <span class="keyword">finally</span>, <span class="keyword">for</span>, <span class="keyword">from</span>, <span class="keyword">global</span>, <span class="keyword">if</span>, <span class="keyword">import</span>, <span class="keyword">in</span>, <span class="keyword">is</span>, <span class="keyword">lambda</span>, <span class="keyword">not</span>, <span class="keyword">or</span>, <span class="keyword">pass</span>, <span class="built_in">print</span>, <span class="keyword">raise</span>, <span class="keyword">return</span>, <span class="keyword">try</span>, <span class="keyword">while</span>, <span class="keyword">with</span>, <span class="keyword">yield</span></span><br></pre></td></tr></table></figure><p>关于Python语法和对象类型的更深入的讨论可以在官方教程中找到：<a href="https://docs.python.org/3.5/tutorial/index.html">Python官方教程</a>。</p><h4 id="11-6-1-2-嵌入式Python的使用"><a href="#11-6-1-2-嵌入式Python的使用" class="headerlink" title="11.6.1.2 嵌入式Python的使用"></a>11.6.1.2 嵌入式Python的使用</h4><h5 id="11-6-1-2-1-概述"><a href="#11-6-1-2-1-概述" class="headerlink" title="11.6.1.2.1 概述"></a>11.6.1.2.1 概述</h5><p>与TCL类似，Python已通过Pythonic API在Avizo中实现。Avizo的特定命令允许你访问模块中包含的信息和功能。通过Python与Avizo进行交互有两种主要方式。第一种方式是通过Python控制台，它与TCL控制台分离，这是一个解释器。控制台的基本功能，如Tab自动补全，已经在第11.6.1.1节“Python简介”中描述。</p><p>第二种方式是通过脚本模块在Avizo中使用Python。Python脚本模块允许你继承预定义的PyScriptObject类的属性，然后覆盖这些属性，以创建与Avizo集成的扩展。脚本模块的行为类似于Avizo中的常规模块，并可以通过资源文件伴随出现在对象弹出菜单中。资源文件仍然必须用TCL编写。PyScriptObject类中包含以下四种方法，供你方便使用：</p><ul><li><code>__init__()</code>：构造函数方法，可以包含在项目视图中创建脚本对象时运行的代码。</li><li><code>update()</code>：可以调用update方法来更新脚本对象在属性面板中的GUI。</li><li><code>compute()</code>：通常包含主要代码的compute方法应在需要进行计算时调用。</li><li><code>__del__()</code>：这个析构函数方法可以包含帮助在删除模块时清理命名空间的代码。</li></ul><h5 id="11-6-1-2-2-示例：与模块交互"><a href="#11-6-1-2-2-示例：与模块交互" class="headerlink" title="11.6.1.2.2 示例：与模块交互"></a>11.6.1.2.2 示例：与模块交互</h5><p>以下是如何通过控制台与项目视图中的模块交互的一个示例，通过创建一个正交切片并更改其属性：</p><p>1、确保在工作区显示Python控制台。</p><p>2、打开 <code>$AVIZO_ROOT/data/tutorials/chocolate-bar.am</code>。</p><ul><li>如果启用了自动视图，将会自动创建一个正交切片。删除它。</li></ul><p>3、要创建正交切片，请在<code>HxProject</code>类中访问<code>create()</code>方法。<code>create()</code>方法要求我们传递我们要创建的对象的类型ID作为字符串参数。</p><ul><li>如果你不确定对象的类型ID，可以在GUI中创建对象，然后使用TCL控制台中的<code>&lt; module &gt;</code> getTypeId命令来了解其类型。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_project.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)</span><br></pre></td></tr></table></figure><p>4、现在我们需要将正交切片连接到 <code>chocolate-bar.am</code>，但首先我们会使用 get() 方法将正交切片分配给变量，以便以后更容易访问。我们也会对 <code>chocolate-bar.am</code> 执行相同操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;ortho = hx_project.get(<span class="string">&#x27;Ortho Slice&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt;input_data = hx_project.get(<span class="string">&#x27;chocolate-bar.am&#x27;</span>)</span><br></pre></td></tr></table></figure><p>5、为了将正交切片连接到 <code>chocolate-bar.am</code>，我们需要查看如何访问正交切片的 Data 端口。通过在控制台中打印 <code>portnames</code> 命令的结果，展示可供交互的端口列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;ortho.portnames</span><br><span class="line">[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;origin&#x27;</span>, <span class="string">&#x27;normal&#x27;</span>, <span class="string">&#x27;frameSettings&#x27;</span>, ...]</span><br></pre></td></tr></table></figure><p>6、从 <code>portnames</code> 命令可以看到 <code>data</code> 端口可能对应于正交切片属性中显示的”Data数据”连接。使用 <code>ports</code> 层级中的 <code>connect()</code> 方法将 <code>chocolate-bar.am</code> 连接到这个端口，然后使用 <code>fire()</code> 方法应用更改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;ortho.ports.data.connect(input_data)</span><br><span class="line">&gt;&gt;&gt;ortho.fire()</span><br></pre></td></tr></table></figure><p>7、我们还可以通过设置 sliceNumber 端口的值来更改切片位置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;ortho.ports.sliceNumber.value = <span class="number">100</span></span><br><span class="line">&gt;&gt;&gt;ortho.fire()</span><br></pre></td></tr></table></figure><p>8、尝试正交切片中的其他端口，查看是否可以更改切片方向和色彩图。要访问这些端口的帮助，可以将 access 命令传递给 help() 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="built_in">help</span>(ortho.ports.sliceOrientation)</span><br></pre></td></tr></table></figure><p>9、通过将它们传递给 help() 方法，进一步了解可供操作的方法和类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="built_in">help</span>(ortho.fire)</span><br></pre></td></tr></table></figure><h5 id="11-6-1-2-3-示例：开发脚本"><a href="#11-6-1-2-3-示例：开发脚本" class="headerlink" title="11.6.1.2.3 示例：开发脚本"></a>11.6.1.2.3 示例：开发脚本</h5><p>在本示例中，我们将编写一个简单的脚本来计算数据集边界框的总体积。该脚本可以直接在Avizo的Python控制台中输入，也可以在文本编辑器中输入命令，然后将其复制到Python控制台执行。</p><p>1、加载 <code>$AVIZO_ROOT/data/tutorials/chocolate-bar.am</code>。</p><p>2、将 <code>chocolate-bar.am</code> 分配给一个变量，以便在后续代码中快速引用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;data = hx_project.get(<span class="string">&#x27;chocolate-bar.am&#x27;</span>)</span><br></pre></td></tr></table></figure><p>3、创建一个边界框并将其附加到 <code>chocolate-bar.am</code> 上。</p><ul><li>你可以在创建时直接将边界框分配给一个变量，而不是在单独的命令中创建它后再使用 get() 方法获取。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;bbox = hx_project.create(<span class="string">&#x27;HxBoundingBox&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt;bbox.ports.data.connect(data)</span><br><span class="line">&gt;&gt;&gt;bbox.fire()</span><br></pre></td></tr></table></figure><p>4、检索边界框在 X、Y 和 Z 方向的范围，以计算其体积。已经存在一个可以使用的边界框命令，该命令以元组的形式存储这些信息，定义为 <code>((xmin, ymin, zmin), (xmax, ymax, zmax))</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="built_in">type</span>(data.bounding_box)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;tuple&#x27;</span>&gt;</span><br><span class="line">&gt;&gt;&gt;data.bounding_box</span><br><span class="line">((<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>),</span><br><span class="line">(<span class="number">0.02807999961078167</span>,</span><br><span class="line"><span class="number">0.020880000665783882</span>,</span><br><span class="line"><span class="number">0.035280000418424606</span>))</span><br></pre></td></tr></table></figure><p>5、Python还可以很容易地从这个两列表元组中提取两个变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;min_bounds, max_bounds = data.bounding_box</span><br></pre></td></tr></table></figure><p>6、使用方括号 <code>[ ]</code> 访问最小和最大边界列表中的每个索引，并将这些信息插入公式中，以计算框的体积。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;x_extent = max_bounds[<span class="number">0</span>] - min_bounds[<span class="number">0</span>]</span><br><span class="line">&gt;&gt;&gt;y_extent = max_bounds[<span class="number">1</span>] - min_bounds[<span class="number">1</span>]</span><br><span class="line">&gt;&gt;&gt;z_extent = max_bounds[<span class="number">2</span>] - min_bounds[<span class="number">2</span>]</span><br><span class="line">&gt;&gt;&gt;volume = x_extent * y_extent * z_extent</span><br></pre></td></tr></table></figure><p>7、最后，以精确的格式将信息打印到控制台。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="built_in">print</span>(<span class="string">&#x27;The volume of %s is %.g.&#x27;</span> % (data.name, volume))</span><br></pre></td></tr></table></figure><h5 id="11-6-1-2-4-示例：创建一个函数"><a href="#11-6-1-2-4-示例：创建一个函数" class="headerlink" title="11.6.1.2.4 示例：创建一个函数"></a>11.6.1.2.4 示例：创建一个函数</h5><p>在本例中，我们将边界框体积计算器封装到一个函数中，该函数接受输入数据作为参数，并将答案返回到控制台。关于函数和Python语法的一些注意事项：</p><ul><li>如果你在Avizo控制台中直接执行，请按<code>SHIFT+ENTER</code>键以输入一个换行符，而不执行代码。</li><li>Python要求在函数体内保持一致的缩进。最佳实践是将代码体缩进四个空格（而不是Tab键）。</li><li>Avizo中的Python控制台不会为用户自动缩进行，用户必须自行控制缩进。</li></ul><p>1、加载 <code>$AVIZO_ROOT/data/tutorials/chocolate-bar.am</code><br>2、使用<code>def</code>关键字定义一个名为<code>bbVol</code>的函数，该函数接受单个输入数据集作为参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">bbVol</span>(<span class="params">data_arg</span>):  <span class="comment"># 记住这里要按SHIFT+ENTER键！</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>3、解释器中会显示省略号，表示它正在等待更多代码输入。在此时手动输入四个空格，然后像之前一样，将最小和最大边界框列表赋给变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">... </span>min_bounds, max_bounds = data_arg.bounding_box</span><br></pre></td></tr></table></figure><p>4、按下<code>SHIFT+ENTER</code>键以创建新行，再次输入四个空格，然后继续执行体积计算器的其余部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">... </span>x_extent = max_bounds[<span class="number">0</span>] - min_bounds[<span class="number">0</span>]</span><br><span class="line"><span class="meta">... </span>y_extent = max_bounds[<span class="number">1</span>] - min_bounds[<span class="number">1</span>]</span><br><span class="line"><span class="meta">... </span>z_extent = max_bounds[<span class="number">2</span>] - min_bounds[<span class="number">2</span>]</span><br><span class="line"><span class="meta">... </span>volume = x_extent * y_extent * z_extent</span><br></pre></td></tr></table></figure><p>5、最后，添加一个<code>return</code>语句将<code>volume</code>变量返回到控制台。这允许你将计算结果设置为一个变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">... </span><span class="keyword">return</span> volume</span><br></pre></td></tr></table></figure><p>6、通过按<code>ENTER</code>键执行函数定义，然后使用<code>chocolate-bar.am</code>测试代码。测试将计算结果分配给变量的能力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>bbVol(hx_project.get(<span class="string">&#x27;chocolate-bar.am&#x27;</span>))</span><br><span class="line">The volume of chocolate-bar.am is2e-05。</span><br><span class="line"><span class="number">2e-05</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>chocolatebar_volume = bbVol(hx_project.get(<span class="string">&#x27;chocolate-bar.am&#x27;</span>))</span><br></pre></td></tr></table></figure><h4 id="11-6-1-3-常用全局命令"><a href="#11-6-1-3-常用全局命令" class="headerlink" title="11.6.1.3 常用全局命令"></a>11.6.1.3 常用全局命令</h4><p>在 Avizo 的 Python 环境中，有两个主要的函数可以帮助用户探索 Python 的结构。<br><code>dir()</code> 函数允许用户查看给定对象可用的属性和方法的列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;ortho = hx_project.get(<span class="string">&#x27;Ortho Slice&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt;<span class="built_in">dir</span>(ortho)</span><br><span class="line">[<span class="string">&#x27;__class__&#x27;</span>, <span class="string">&#x27;__delattr__&#x27;</span>, <span class="string">&#x27;__dir__&#x27;</span>, <span class="string">&#x27;__doc__&#x27;</span>, <span class="string">&#x27;__eq__&#x27;</span>, <span class="string">&#x27;__format__&#x27;</span>, <span class="string">&#x27;__ge__&#x27;</span>, <span class="string">&#x27;__getattribute__&#x27;</span>, <span class="string">&#x27;__gt__&#x27;</span>, <span class="string">&#x27;__hash__&#x27;</span>, <span class="string">&#x27;__init__&#x27;</span>, <span class="string">&#x27;__le__&#x27;</span>, <span class="string">&#x27;__lt__&#x27;</span>, <span class="string">&#x27;__ne__&#x27;</span>, <span class="string">&#x27;__new__&#x27;</span>, <span class="string">&#x27;__pyx_vtable__&#x27;</span>, <span class="string">&#x27;__reduce__&#x27;</span>, <span class="string">&#x27;__reduce_ex__&#x27;</span>, <span class="string">&#x27;__repr__&#x27;</span>, <span class="string">&#x27;__setattr__&#x27;</span>, <span class="string">&#x27;__sizeof__&#x27;</span>, <span class="string">&#x27;__str__&#x27;</span>, <span class="string">&#x27;__subclasshook__&#x27;</span>, <span class="string">&#x27;_cpp_classname&#x27;</span>, <span class="string">&#x27;_cpp_package&#x27;</span>, <span class="string">&#x27;_tcl_interp&#x27;</span>, <span class="string">&#x27;all_interfaces&#x27;</span>, <span class="string">&#x27;can_be_renamed&#x27;</span>, <span class="string">&#x27;clip_geometry&#x27;</span>, <span class="string">&#x27;compute&#x27;</span>, <span class="string">&#x27;create_doc_file&#x27;</span>, <span class="string">&#x27;create_port_snaps&#x27;</span>, <span class="string">&#x27;downstream_connections&#x27;</span>, <span class="string">&#x27;duplicate&#x27;</span>, <span class="string">&#x27;execute&#x27;</span>, <span class="string">&#x27;fire&#x27;</span>, <span class="string">&#x27;get_all_interface_names&#x27;</span>, <span class="string">&#x27;icon_color&#x27;</span>, <span class="string">&#x27;icon_position&#x27;</span>, <span class="string">&#x27;icon_visible&#x27;</span>, <span class="string">&#x27;is_geometry_clipped&#x27;</span>, <span class="string">&#x27;is_same_object&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;need_saving&#x27;</span>, <span class="string">&#x27;portnames&#x27;</span>, <span class="string">&#x27;ports&#x27;</span>, <span class="string">&#x27;removable&#x27;</span>, <span class="string">&#x27;selected&#x27;</span>, <span class="string">&#x27;unclip_geometry&#x27;</span>, <span class="string">&#x27;update&#x27;</span>, <span class="string">&#x27;viewer_mask&#x27;</span>]</span><br></pre></td></tr></table></figure><p>这些信息也可以在你开始输入命令时通过下拉列表显示。</p><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/Figure11.7.png" class="" title="Figure 11.7: Pulldown List of Commands"><p><code>help()</code> 函数提供了有关属性和方法的详细信息及其使用的示例。<br><code>help()</code> 的输出还包含有关相关父类的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="built_in">help</span>(ortho)</span><br><span class="line">Help on HxPlanarModBase <span class="built_in">object</span>:</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HxPlanarModBase</span>(<span class="title class_ inherited__">HxModule</span>)</span><br><span class="line">| This <span class="keyword">is</span> the first <span class="keyword">class</span> <span class="title class_">of</span> HxBase hierarchy which represents items that can be added to the project view.</span><br><span class="line">|</span><br><span class="line">| Method resolution order:</span><br><span class="line">| HxPlanarModBase</span><br><span class="line">| HxModule</span><br><span class="line">| HxObject</span><br><span class="line">| HxBase</span><br><span class="line">| McInterface</span><br><span class="line">| builtins.<span class="built_in">object</span></span><br><span class="line">|</span><br><span class="line">| Methods defined here:</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">| all_interfaces</span><br><span class="line">| Attribute that contains <span class="built_in">all</span> admissible interfaces <span class="keyword">as</span> sub-members.</span><br><span class="line">|</span><br><span class="line">| Examples</span><br><span class="line">| --------</span><br><span class="line">|</span><br><span class="line">| Retrieve an ‘HxBase‘ interface on an orthoslice:</span><br><span class="line">|</span><br><span class="line">| &gt;&gt;&gt; ortho = hx_project.create(’HxOrthoSlice’)</span><br><span class="line">| &gt;&gt;&gt; base = ortho.all_interfaces.HxBase</span><br></pre></td></tr></table></figure><p>下方列出了一些更具体的全局命令：</p><ul><li><code>print()</code> 是 Python 的原生命令，用于在 Avizo 中的 Python 控制台输出结果。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;x = <span class="number">3</span></span><br><span class="line">&gt;&gt;&gt;y = <span class="number">2</span></span><br><span class="line">&gt;&gt;&gt;<span class="built_in">print</span>(<span class="string">&#x27;The sum of %i + %i is %i.&#x27;</span> % (x,y,x+y))</span><br><span class="line">The <span class="built_in">sum</span> of <span class="number">3</span> + <span class="number">2</span> <span class="keyword">is</span> <span class="number">5.</span></span><br></pre></td></tr></table></figure><ul><li><code>import</code> 是 Python 的原生命令，用于通过加载额外的包来扩展 Python 的功能。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;x = <span class="number">3</span></span><br><span class="line">&gt;&gt;&gt;y = <span class="number">2</span></span><br><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> numpy</span><br><span class="line">&gt;&gt;&gt;numpy.add(x,y)</span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure><p>一些用于与项目视图交互的有用全局函数包含在 <code>hx_project</code> 方法中。</p><div class="table-container"><table><thead><tr><th style="text-align:center">方法</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">hx_project.create()</td><td style="text-align:center">创建某个 Avizo 类的实例并将其添加到项目视图中</td></tr><tr><td style="text-align:center">hx_project.remove()</td><td style="text-align:center">从项目视图中删除对象</td></tr><tr><td style="text-align:center">hx_project.add()</td><td style="text-align:center">将对象添加到项目视图中</td></tr><tr><td style="text-align:center">hx_project.load()</td><td style="text-align:center">当指定了文件名时加载定义格式的数据</td></tr></tbody></table></div><p><code>hx_paths</code> 方法还提供了包含目录路径的变量。</p><div class="table-container"><table><thead><tr><th style="text-align:center">变量</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">hx_paths.install.dir</td><td style="text-align:center">安装目录，也称为 &lt;$AVIZO_ROOT&gt;</td></tr><tr><td style="text-align:center">hx_paths.tutorials.dir</td><td style="text-align:center">教程数据目录</td></tr><tr><td style="text-align:center">hx_paths.python_modules.dir</td><td style="text-align:center">包含额外包的 Python 模块目录</td></tr><tr><td style="text-align:center">hx_paths.python_script_objects.dir</td><td style="text-align:center">包含用户创建的自定义 Python 脚本的 Python 脚本对象目录</td></tr><tr><td style="text-align:center">hx_paths.executable_dir</td><td style="text-align:center">包含 Avizo.exe 的目录</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;hx_paths.install_dir</span><br><span class="line"><span class="string">&#x27;C:/Program Files/&lt;INSTALL DIR&gt;&#x27;</span></span><br><span class="line">&gt;&gt;&gt;hx_paths.tutorials_dir</span><br><span class="line"><span class="string">&#x27;C:/Program Files/&lt;INSTALL DIR&gt;/data/tutorials&#x27;</span></span><br><span class="line">&gt;&gt;&gt;hx_paths.python_modules_dir</span><br><span class="line"><span class="string">&#x27;C:/Program Files/&lt;INSTALL DIR&gt;/share/python_modules&#x27;</span></span><br><span class="line">&gt;&gt;&gt;hx_paths.python_script_objects_dir</span><br><span class="line"><span class="string">&#x27;C:/Program Files/&lt;INSTALL DIR&gt;/share/python_script_objects&#x27;</span></span><br><span class="line">&gt;&gt;&gt;hx_paths.executable_dir</span><br><span class="line"><span class="string">&#x27;C:\\Program Files\\&lt;INSTALL DIR&gt;\\bin\\arch-Win64VC12-Optimize\\Avizo.exe&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="11-6-1-4-模块管理"><a href="#11-6-1-4-模块管理" class="headerlink" title="11.6.1.4 模块管理"></a>11.6.1.4 模块管理</h4><p>一个包含许多代码片段的完整参考手册，解释了如何配置所有端口和模块，可以从Python参考帮助菜单项中访问。</p><h5 id="11-6-1-4-1-模块属性"><a href="#11-6-1-4-1-模块属性" class="headerlink" title="11.6.1.4.1 模块属性"></a>11.6.1.4.1 模块属性</h5><h6 id="什么是属性"><a href="#什么是属性" class="headerlink" title="什么是属性"></a>什么是属性</h6><p>属性是类中包含的数据字段。有些属性可能是只读的。</p><h6 id="常见属性列表"><a href="#常见属性列表" class="headerlink" title="常见属性列表"></a>常见属性列表</h6><p>这里的变量 “a” 指的是你的对象的 Python 句柄。</p><p><code>a.name</code>:<br>这是一个字符串属性，指的是你的 Avizo 对象的名称。可以分配一个字符串来更改对象的显示名称。</p><p><code>a.portnames</code>:<br>这是一个只读列表，包含属于 Avizo 对象的所有端口名称。</p><p><code>a.viewer_mask</code>:<br>这是一个整数属性，触发视图中的可见性。有16个配置，可以从 [0, 15] 中设置。如果使用了该范围之外的数字，该数字将在执行模16操作后评估。</p><p><code>a.bounding_box</code>:<br>这存储了一对元组，描述对象的空间维度。可以以 ((xmin, ymin, zmin), (xmax, ymax, zmax)) 的格式分配新的维度。</p><p><code>a.downstream_connections[x]</code>:<br>这存储了一个只读序列，显示引用它的所有对象。每个连接都分配了一个整数索引 “x”。要从指针中找到对象的名称，可以使用命令 a.downstream_connections[x].get_owner().name</p><p><code>a.range</code>:<br>这存储了一个只读的元组，显示数据的强度范围，格式为 (min, max)。</p><p><code>a.transform</code>:<br>这存储了一个显示 4x4 转换矩阵的元组。可以分配一个新的转换矩阵。</p><h6 id="11-6-1-4-2-模块方法"><a href="#11-6-1-4-2-模块方法" class="headerlink" title="11.6.1.4.2 模块方法"></a>11.6.1.4.2 模块方法</h6><h6 id="什么是方法"><a href="#什么是方法" class="headerlink" title="什么是方法"></a>什么是方法</h6><p>方法是类中包含的函数。许多方法不需要参数。</p><h6 id="常见方法列表"><a href="#常见方法列表" class="headerlink" title="常见方法列表"></a>常见方法列表</h6><p>这里的变量 “a” 指的是你的对象的 Python 句柄。</p><p><code>a.compute()</code>:<br>如果 a.ports.doIt.was_hit = True，则此方法执行对象的计算。这模拟了在条件允许时单击应用按钮。</p><p><code>a.update()</code>:<br>此方法更新对象 GUI 的属性窗口。</p><p><code>a.fire()</code>:<br>此方法调用 a.update() 和 a.compute()。</p><p><code>a.execute()</code>:<br>此方法结合了上述所有方法，模拟了单击应用按钮并刷新 GUI。</p><p><code>a.get_array()</code>:<br>此方法访问对象的 NumPy 数组。访问该数组将阻止对象删除。</p><p><code>a.set_array(...)</code>:<br>此方法将 NumPy 数组分配给对象。数组的所有权不会传递，因此未来对数组的更改不会传播，除非重新分配。</p><h4 id="11-6-1-5-脚本对象"><a href="#11-6-1-5-脚本对象" class="headerlink" title="11.6.1.5 脚本对象"></a>11.6.1.5 脚本对象</h4><h5 id="11-6-1-5-1-什么是-Python-脚本对象"><a href="#11-6-1-5-1-什么是-Python-脚本对象" class="headerlink" title="11.6.1.5.1 什么是 Python 脚本对象"></a>11.6.1.5.1 什么是 Python 脚本对象</h5><p>Python 脚本对象是一个计算模块，其行为在继承自 PyScriptObject 的 Python 类中进行硬编码。Python 脚本对象对于创建自定义工具非常有用，其功能可通过与 Avizo 计算模块相同的方式访问。</p><h6 id="脚本结构"><a href="#脚本结构" class="headerlink" title="脚本结构"></a>脚本结构</h6><p>当你将 Python 脚本对象定义为类时，以下类方法非常有用，但并不要求必须这样定义。任何脚本都会像在控制台中输入的那样运行。变量 self 指代其类的标识。</p><p><code>def init(self)</code>:<br>此方法在对象创建时调用。在此处定义脚本结构并设置 GUI 是非常有用的。</p><p><code>def del(self)</code>:<br>此方法在对象重新启动或删除时调用。这是清理连接的好地方。</p><p><code>def update(self)</code>:<br>当 GUI 需要刷新时调用此方法。</p><p><code>def compute(self)</code>:<br>当单击“应用”时调用此方法。</p><p>注意：Python 脚本对象的执行是在单独的 Python 解释器上完成的，并且有自己的 Python 控制台。但是，请注意，在某些特殊情况下，例如来自 PyQt 插槽的消息打印，流式传输可能会重定向到主 Python 解释器。</p><h6 id="创建端口"><a href="#创建端口" class="headerlink" title="创建端口"></a>创建端口</h6><p>用户通过端口与对象进行交互。端口可以通过简单的分配进行初始化，它们属于 <code>HxPort</code> 类。以下是一些有用的端口：</p><p><code>HxConnection</code><br>此端口类允许用户连接模块。模块的类型也可以限制。</p><p><code>HxPortFilename</code><br>此端口类允许用户加载或保存文件。端口的功能由 mode 属性定义。文件名可以以文本形式输入或通过文件浏览器访问。</p><p><code>HxPortIntSlider</code><br>此端口类包含一个可以通过滑动比例访问的整数值范围。此端口用于定义“Ortho Slice”模块的切片编号。</p><p><code>HxPortDoIt</code><br>此端口类控制自动刷新框，并处理应用按钮。</p><p><code>HxPortInfo</code><br>此端口类用于为用户提供说明、备注或警告。文本可以在模块属性中找到。</p><h6 id="Bounding-Box-脚本示例"><a href="#Bounding-Box-脚本示例" class="headerlink" title="Bounding Box 脚本示例"></a>Bounding Box 脚本示例</h6><p>在本例中，我们将 Bounding Box 体积计算器（在“嵌入式 Python 使用”一章中描述）封装成一个脚本模块，可以通过对象弹出菜单加载到 Avizo GUI 中。</p><p>1、打开文本编辑器并创建一个新文件，结构如下。将对象命名为 <code>BoundingBoxVolume</code>。</p><ul><li>你也可以从 <code>$AVIZO_ROOT/share/python_script_objects/PythonScriptObjectTemplate.pyscro</code> 模板开始。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BoundingBoxVolume</span>(<span class="title class_ inherited__">PyScriptObject</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 这里是初始化代码</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 这里是更新代码</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 这里是计算代码</span></span><br></pre></td></tr></table></figure><p>2、脚本对象需要能够连接到数据对象，以便知道需要计算体积的 Bounding Box。默认情况下，Python 脚本对象从 <code>PyScriptObject</code> 类继承数据连接。确保它在<code>__init__()</code> 方法中可见。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    self.data.visible = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>3、在此示例中，使用 pass 关键字跳过 update() 方法中的 GUI 更新，因为没有需要更新的端口。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>4、在 <code>compute()</code> 方法中，我们希望在没有数据对象连接到脚本模块时停止体积计算。添加一个 <code>if</code> 语句检查数据连接是否为空。如果数据连接为空，使用 <code>return</code> 语句退出 <code>compute()</code> 方法。</p><ul><li>Python 使用 None 关键字来简化此操作（即：<code>if &lt;expression&gt; is None</code>）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 检查是否连接了输入数据</span></span><br><span class="line">    <span class="keyword">if</span> self.<span class="built_in">input</span>.source() <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>5、最后，如果逻辑检查失败，因为已连接数据对象，请使用 <code>bbVol</code> 脚本计算体积。使用 <code>source()</code> 方法获取已连接数据对象的名称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 检查是否连接了输入数据</span></span><br><span class="line">    <span class="keyword">if</span> self.<span class="built_in">input</span>.source() <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    data = self.<span class="built_in">input</span>.source()</span><br><span class="line">    min_bounds, max_bounds = data.bounding_box</span><br><span class="line">    x_extent = max_bounds[<span class="number">0</span>] - min_bounds[<span class="number">0</span>]</span><br><span class="line">    y_extent = max_bounds[<span class="number">1</span>] - min_bounds[<span class="number">1</span>]</span><br><span class="line">    z_extent = max_bounds[<span class="number">2</span>] - min_bounds[<span class="number">2</span>]</span><br><span class="line">    volume = x_extent * y_extent * z_extent</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;The volume of %s is %.g.&#x27;</span> % (data.name, volume))</span><br></pre></td></tr></table></figure><p>6、完成模块后，在 TCL 中创建一个资源文件，以便在对象弹出菜单中将此模块显示为所有数据对象的选项。</p><ul><li>有关创建资源文件的更多信息，请参考配置弹出菜单部分。</li><li>资源文件可在 <code>$AVIZO_ROOT/share/resources/PythonBoundingBoxVolume.rc</code> 中找到。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">module -name <span class="string">&quot;Bounding Box Volume&quot;</span> \</span><br><span class="line">-primary <span class="string">&quot;HxUniformScalarField3&quot;</span> \</span><br><span class="line">-package <span class="string">&quot;py_core&quot;</span> \</span><br><span class="line">-category <span class="string">&quot;&#123;Measure And Analyze&#125;&quot;</span> \</span><br><span class="line">-proc &#123;</span><br><span class="line">    <span class="built_in">set</span> this [[create HxPythonScriptObject] \</span><br><span class="line">    setLabel <span class="string">&quot;Bounding Box Volume&quot;</span>]</span><br><span class="line">    <span class="string">&quot;$this&quot;</span> startStop hideMaskIncrease</span><br><span class="line">    <span class="string">&quot;$this&quot;</span> filename hideMaskIncrease</span><br><span class="line">    <span class="string">&quot;$this&quot;</span> filename setValue \</span><br><span class="line">    &lt;PRODUCT_PATH&gt;/share/python_script_objects/PythonBoundingBoxVolume.pyscro</span><br><span class="line">    <span class="string">&quot;$this&quot;</span> startStop hit <span class="number">0</span></span><br><span class="line">    <span class="string">&quot;$this&quot;</span> fire</span><br><span class="line">    <span class="keyword">if</span> &#123; [exists $PRIMARY] &#125; &#123;</span><br><span class="line">        $this data connect $PRIMARY</span><br><span class="line">        $this fire</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="11-6-1-5-2-资源文件"><a href="#11-6-1-5-2-资源文件" class="headerlink" title="11.6.1.5.2 资源文件"></a>11.6.1.5.2 资源文件</h5><p>资源文件是一个在 Avizo 启动过程中读取和执行的 TCL 脚本。你可以配置一个资源文件，使你的 Python 脚本对象出现在下拉菜单中或作为宏按钮。资源文件位于 <code>$AVIZO_ROOT/share/resources</code> 目录中。</p><h6 id="结构化下拉菜单资源文件"><a href="#结构化下拉菜单资源文件" class="headerlink" title="结构化下拉菜单资源文件"></a>结构化下拉菜单资源文件</h6><p>脚本以 TCL 命令 <code>&lt;module&gt;</code> 开始，后跟一个简单案例的以下标志。在下例中，<code>$PRIMARY</code> 是指你最初右键单击的对象。</p><p><code>-name</code><br>这是菜单中模块的名称。</p><p><code>-package</code><br>这定义了对象的包。</p><p><code>-primary</code><br>这限制了脚本出现时所需的数据类型。</p><p><code>-category</code><br>这定义了脚本在菜单中出现的文件夹。</p><p><code>-proc</code><br>这是资源文件的主体部分，指定 .pyscro 文件的加载位置以及如何将脚本连接到你最初右键单击的对象。</p><h6 id="结构化宏资源文件"><a href="#结构化宏资源文件" class="headerlink" title="结构化宏资源文件"></a>结构化宏资源文件</h6><p>脚本以 TCL 命令 <code>&lt;macroButton&gt;</code> 开始，后跟一个简单案例的以下标志。</p><p><code>-add</code><br>这为按钮创建了一个名称。</p><p><code>-color</code><br>这控制按钮的颜色。</p><p><code>-proc</code><br>这是资源文件的主体部分，指定 <code>.pyscro</code> 文件的加载位置或过程的运行。</p><h4 id="11-6-1-6-Python-环境和包管理器"><a href="#11-6-1-6-Python-环境和包管理器" class="headerlink" title="11.6.1.6 Python 环境和包管理器"></a>11.6.1.6 Python 环境和包管理器</h4><h5 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h5><p>本节解释了如何在 Windows 的命令提示符或 Linux/Mac 的终端中列出、安装或更新 Avizo 的新 Python 包。包管理器允许用户创建多个自包含的 Python 环境，每个环境都有自己的 Python 可执行文件（例如 Windows 上的 python.exe）和一组包。每个自包含环境都可以在 Avizo 中使用。</p><h5 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h5><p><code>EDM(Enthought Deployment Manager)</code>包管理器可用于安装、删除或升级在两个存储库中提供的 Python 包：</p><p>• official ThermoScientific/3dSoftware<br>• enthought/free</p><p>此工具应用于查找可用的 Python 包并安装新包。它允许查看、更新和删除已安装的包。此外，它还允许恢复到以前的状态，并恢复到 Avizo 的原始 Python 环境。</p><h5 id="如何安装和配置-EDM"><a href="#如何安装和配置-EDM" class="headerlink" title="如何安装和配置 EDM"></a>如何安装和配置 EDM</h5><p>注意：EDM 安装程序可能需要重新启动你的计算机。</p><p>EDM 安装程序可以从 Enthought 网站获取：<a href="https://www.enthought.com/product/enthought-deployment-manager/">https://www.enthought.com/product/enthought-deployment-manager/</a></p><p>安装程序会将 EDM（例如，Windows 上的 edm.bat）提取到默认文件夹中（例如，Windows 上的 C:\Enthought\edm）。</p><p>要创建一个名为 hxEnv 的新 Python 环境，在 Windows 的命令提示符（转到 EDM 安装目录）和 Linux/Mac 的终端中执行以下命令行（使用与正确架构对应的 .json 文件）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edm envs <span class="keyword">import</span> -f $AVIZO_ROOT/python/bundles/3dSoftware_win64.json hxEnv</span><br></pre></td></tr></table></figure><p>如果此命令失败（例如，出现以下错误消息：“The packages repository ‘ThermoScientific/3dSoftware’ does not exist or is not available under your subscription. Check your repositories settings”），则配置文件 $HOME/.edm.yaml 可能不完整或缺失。打开 $HOME 文件夹（$HOME 环境变量包含用户主目录的绝对路径）并搜索 .edm.yaml 文件。如果它存在，请删除并重新启动 Avizo，它应会重新创建此文件。如果文件仍然缺失，请联系支持人员。</p><p>新创建的 Python 环境存储在 <code>$HOME\.edm\envs\hxEnv</code> 中。</p><p>要在 Avizo 中使用名为 hxEnv 的新 Python 环境，你需要将环境变量 <code>HX_FORCE_PYTHON_PATH</code> 设置为 <code>$HOME/.edm/envs/hxEnv</code>。</p><p>可以通过以下命令获取可用环境的列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edm environments <span class="built_in">list</span></span><br></pre></td></tr></table></figure><h5 id="如何搜索和安装包"><a href="#如何搜索和安装包" class="headerlink" title="如何搜索和安装包"></a>如何搜索和安装包</h5><p>首先，按如下方式搜索所需的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edm search &lt;package_name&gt; -e hxEnv</span><br></pre></td></tr></table></figure><p>如果该包可用，按如下方式安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edm install &lt;package_name&gt; -e hxEnv</span><br></pre></td></tr></table></figure><h5 id="如何列出当前安装的包"><a href="#如何列出当前安装的包" class="headerlink" title="如何列出当前安装的包"></a>如何列出当前安装的包</h5><p>要列出 Avizo 中当前安装的所有包，请输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edm <span class="built_in">list</span> -e hxEnv</span><br></pre></td></tr></table></figure><h5 id="如何重新初始化-Avizo-包"><a href="#如何重新初始化-Avizo-包" class="headerlink" title="如何重新初始化 Avizo 包"></a>如何重新初始化 Avizo 包</h5><p>要将 Python 包重新初始化到其原始状态，请使用 .json 文件创建一个新环境，或取消设置 <code>HX_FORCE_PYTHON_PATH</code> 环境变量。取消设置该变量将使 Avizo 使用其嵌入版本。</p><p>如果 Python 分发版因在 Avizo 中安装了不受支持的包而变得无法使用，这可能非常有用。</p><p>要显示所有可用选项，请使用以下命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edm <span class="built_in">help</span></span><br></pre></td></tr></table></figure><h4 id="11-6-1-7-Python-包列表"><a href="#11-6-1-7-Python-包列表" class="headerlink" title="11.6.1.7 Python 包列表"></a>11.6.1.7 Python 包列表</h4><p>以下是 Thermo Scientific Python 中已经包含的包列表：</p><p>alabaster 0.7.10-1<br>appdirs 1.4.3-1<br>babel 2.4.0-2<br>backports.abc 0.5-1<br>backports abc remove 0.4-2<br>certifi 2017.7.27.1-1<br>chardet 3.0.4-1<br>colorama 0.3.7-1<br>configobj 5.0.6-2<br>cycler 0.10.0-1<br>cython 0.25.2-1<br>decorator 4.1.2-1<br>distribute remove 1.0.0-4<br>docutils 0.13.1-1<br>h5py 2.7.0-2<br>idna 2.5-1<br>imagesize 0.7.1-1<br>intel runtime 15.0.6.285-2<br>jdcal 1.2-1<br>jinja2 2.9.6-1<br>lxml 3.7.3-2<br>markupsafe 0.23-2<br>matplotlib 2.0.0-5<br>mkl 2017.0.3-1<br>networkx 1.11-7<br>nose 1.3.7-3<br>numexpr 2.6.2-3<br>numpy 1.13.3-3<br>numpydoc 0.6.0-4<br>opencv 3.2.0-3<br>openpyxl 2.4.1-2<br>packaging 16.8-2<br>pandas 0.20.3-3<br>patsy 0.4.1-4<br>pillow 4.0.0-1<br>pip 10.0.1-1<br>py 1.4.34-1<br>pydicom 0.9.9-1<br>pygments 2.2.0-1<br>pyparsing 2.2.0-1<br>pyqt5 5.8.2-3<br>pytables 3.3.0-5<br>pytest 3.1.2-1<br>python dateutil 2.6.0-1<br>pytz 2017.3-1<br>pywavelets 0.5.2-2<br>requests 2.18.4-1<br>scikit learn 0.19.1-2<br>scikits.image 0.13.0-5<br>scipy 1.0.0-2<br>seaborn 0.8.1-2<br>setuptools 38.2.5-1<br>singledispatch 3.4.0.3-1<br>sip 4.19.2-2<br>six 1.10.0-1<br>snowballstemmer 1.2.1-1<br>sphinx 1.5.5-5<br>sphinx rtd theme 0.2.4-1<br>ssl match hostname 3.5.0.1-1<br>statsmodels 0.8.0-4<br>tornado 4.4.2-3<br>urllib3 1.22-1<br>xlwt 1.2.0-1</p><h3 id="11-6-2-Python-教程"><a href="#11-6-2-Python-教程" class="headerlink" title="11.6.2 Python 教程"></a>11.6.2 Python 教程</h3><h4 id="11-6-2-1-Python-教程-在-Avizo-中使用-Python-生态系统中的工具"><a href="#11-6-2-1-Python-教程-在-Avizo-中使用-Python-生态系统中的工具" class="headerlink" title="11.6.2.1 Python 教程 - 在 Avizo 中使用 Python 生态系统中的工具"></a>11.6.2.1 Python 教程 - 在 Avizo 中使用 Python 生态系统中的工具</h4><p>将 Python 集成到 Avizo 中的一个优点是，可以使用 Python 工具扩展 Avizo 的功能。这种扩展允许编写封装 Python 函数的脚本对象，以使它们在 Avizo 的图形用户界面中作为模块可用。这些 Python 工具随后可以通过标准的 Avizo 端口进行控制。</p><p>本教程演示如何将 Scipy 中的快速傅里叶变换 (FFT) 作为 Avizo 中使用的 FFT 的替代方案进行集成。您可以按照逐步说明操作，或者查看 <code>$AVIZO_ROOT/share/python_script_objects</code> 目录中的相关文件（ScipyFFT.pyscro 和 ScipyFFT.rc）。</p><p>从 <code>$AVIZO_ROOT/share/python_script_objects</code> 目录中复制 <code>PythonScriptObjectTemplate.pyscro</code> 文件到您选择的位置，并将其重命名为 <code>ScipyFFT.pyscro</code>。</p><p>1、用文本编辑器打开该文件，并通过更改第一行给新模块命名：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScipyFFT</span>(<span class="title class_ inherited__">PyScriptObject</span>):</span><br></pre></td></tr></table></figure><p>2、在初始化函数中，将使用默认数据输入端口进行模块的数据连接，并定义允许的连接类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.data.valid_types = [<span class="string">&#x27;HxUniformScalarField3&#x27;</span>]</span><br></pre></td></tr></table></figure><p>3、最终的初始化函数定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    self.data.valid_types = [<span class="string">&#x27;HxUniformScalarField3&#x27;</span>]</span><br><span class="line">    <span class="comment"># 创建一个“应用”按钮。</span></span><br><span class="line">    self.do_it = HxPortDoIt(self, <span class="string">&#x27;apply&#x27;</span>, <span class="string">&#x27;Apply&#x27;</span>)</span><br></pre></td></tr></table></figure><p>4、保持 update 函数不变：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>5、FFT 的计算将在 compute 函数中完成。检查是否点击了<code>Apply</code>按钮以及是否选择了输入数据集后，从 Python 中导入几个包。在本例中，我们需要从 scipy 导入 fftpack、从 numpy 导入一些数学函数，以及 time 包来测量执行时间：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> fftpack</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><p>6、计算的第一步是创建一个变量来存储 FFT 计算的结果作为 3D 均匀标量场。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = hx_project.create(<span class="string">&#x27;HxUniformScalarField3&#x27;</span>)</span><br></pre></td></tr></table></figure><p>7、要测量执行时间，首先需要在计算开始时获取时间戳：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start_time = time.time()</span><br></pre></td></tr></table></figure><p>8、为输入数据创建一个 Python 变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = self.data.source()</span><br></pre></td></tr></table></figure><p>9、要计算输入数据的 FFT 绝对值，执行以下三条命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算离散傅里叶变换</span></span><br><span class="line">F1 = fftpack.fftn(<span class="built_in">input</span>.get_array())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将零频率分量移至频谱中心</span></span><br><span class="line">F2 = fftpack.fftshift(F1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有系数的幅值</span></span><br><span class="line">F3 = numpy.<span class="built_in">abs</span>(F2)</span><br></pre></td></tr></table></figure><p>10、计算完成后，将结果数组分配给您之前创建的 result 变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result.set_array(F3)</span><br></pre></td></tr></table></figure><p>11、输出 FFT 的总执行时间：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- %s seconds ---&quot;</span> % (time.time() - start_time))</span><br></pre></td></tr></table></figure><p>12、整个 compute 函数应如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 检查是否已触碰模块的应用按钮</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.do_it.was_hit:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查输入数据是否连接到有效对象</span></span><br><span class="line">    <span class="keyword">if</span> self.data.source() <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 导入用于执行 FFT 的 scipy 包</span></span><br><span class="line">    <span class="keyword">from</span> scipy <span class="keyword">import</span> fftpack</span><br><span class="line">    <span class="keyword">import</span> numpy</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建输出字段</span></span><br><span class="line">    result = hx_project.create(<span class="string">&#x27;HxUniformScalarField3&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检索输入数据</span></span><br><span class="line">    <span class="built_in">input</span> = self.data.source()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算离散傅里叶变换</span></span><br><span class="line">    F1 = fftpack.fftn(<span class="built_in">input</span>.get_array())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将零频率分量移至频谱中心</span></span><br><span class="line">    F2 = fftpack.fftshift(F1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有系数的幅值</span></span><br><span class="line">    F3 = numpy.<span class="built_in">abs</span>(F2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将结果 numpy 数组赋值给输出标量场</span></span><br><span class="line">    result.set_array(F3)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在控制台显示计算时间</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--- %s seconds ---&quot;</span> % (time.time() - start_time))</span><br></pre></td></tr></table></figure><p>13、为了让此模块在 Avizo 的图形用户界面中可用，您还需要编写一个资源文件。用文本编辑器创建一个新文件，并将其保存为 ScipyFFT.rc。资源文件通常以注释行开头：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#############################################################</span></span><br><span class="line"><span class="comment"># .rc for pyscro Scipy FFT</span></span><br><span class="line"><span class="comment">#############################################################</span></span><br></pre></td></tr></table></figure><p>14、为模块命名：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">module -name <span class="string">&quot;Scipy FFT&quot;</span> \</span><br></pre></td></tr></table></figure><p>15、指定希望将其附加到的数据类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-primary <span class="string">&quot;HxUniformScalarField3&quot;</span> \</span><br></pre></td></tr></table></figure><p>16、将其声明为 Python 脚本对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-package <span class="string">&quot;py_core&quot;</span> \</span><br></pre></td></tr></table></figure><p>17、下一行定义了它将出现在 Avizo 对象弹出菜单中的位置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-category <span class="string">&quot;&#123;Python Scripts&#125;&quot;</span> \</span><br></pre></td></tr></table></figure><p>18、运行几个 TCL 命令以在 Avizo 的图形用户界面中初始化模块。第一个命令将创建脚本对象并为模块设置标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-proc &#123;</span><br><span class="line"><span class="built_in">set</span> this [[create HxPythonScriptObject] setLabel <span class="string">&quot;Python FFT&quot;</span>]</span><br></pre></td></tr></table></figure><p>19、设置文件名以找到该模块的 Python 脚本位置，其中 <code>&lt;PRODUCT_PATH&gt;</code> 是 <code>$AVIZO_ROOT</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;$this&quot;</span> startStop hideMaskIncrease</span><br><span class="line"><span class="string">&quot;$this&quot;</span> filename hideMaskIncrease</span><br><span class="line"><span class="string">&quot;$this&quot;</span> filename setValue \</span><br><span class="line">&lt;PRODUCT_PATH&gt;/share/python_script_objects/ScipyFFT.pyscro</span><br></pre></td></tr></table></figure><p>20、脚本将运行以使更改生效：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;$this&quot;</span> startStop hit <span class="number">0</span></span><br><span class="line"><span class="string">&quot;$this&quot;</span> fire</span><br></pre></td></tr></table></figure><p>21、将数据集连接到您右键单击以创建模块的默认输入数据端口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> &#123; [exists $PRIMARY] &#125; &#123;</span><br><span class="line">    $this data connect $PRIMARY</span><br><span class="line">    $this fire</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>22、完整的资源文件应如下所示，其中 <code>&lt;PRODUCT_PATH&gt;</code> 是 <code>$AVIZO_ROOT</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#############################################################</span></span><br><span class="line"><span class="comment"># .rc for pyscro Scipy FFT</span></span><br><span class="line"><span class="comment">#############################################################</span></span><br><span class="line">module -name <span class="string">&quot;Scipy FFT&quot;</span> \</span><br><span class="line">-primary <span class="string">&quot;HxUniformScalarField3&quot;</span> \</span><br><span class="line">-package <span class="string">&quot;py_core&quot;</span> \</span><br><span class="line">-category <span class="string">&quot;&#123;Python Scripts&#125;&quot;</span> \</span><br><span class="line">-proc &#123;</span><br><span class="line">    <span class="built_in">set</span> this [[create HxPythonScriptObject] setLabel <span class="string">&quot;Python FFT&quot;</span>]</span><br><span class="line">    <span class="string">&quot;$this&quot;</span> startStop hideMaskIncrease</span><br><span class="line">    <span class="string">&quot;$this&quot;</span> filename hideMaskIncrease</span><br><span class="line">    <span class="string">&quot;$this&quot;</span> filename setValue \</span><br><span class="line">    &lt;PRODUCT_PATH&gt;/share/python_script_objects/ScipyFFT.pyscro</span><br><span class="line">    <span class="string">&quot;$this&quot;</span> startStop hit <span class="number">0</span></span><br><span class="line">    <span class="string">&quot;$this&quot;</span> fire</span><br><span class="line">    <span class="keyword">if</span> &#123; [exists $PRIMARY] &#125; &#123;</span><br><span class="line">        $this data connect $PRIMARY</span><br><span class="line">        $this fire</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>23、要使此 Python 脚本对象在 Avizo 中可用，需要将这两个文件（覆盖现有文件）复制到 <code>$AVIZO_ROOT/share/python_script_objects/</code>，并在修改资源文件后重新启动 Avizo。 如果要重复使用内嵌的 Python FFT 示例，请编辑 <code>$AVIZO_ROOT/share/python_script_objects/ScipyFFT.rc</code> 文件，并将 -category 值从 “None” 改为 “{Python Scripts}”。</p><p>24、要测试此模块，请执行以下操作：</p><p>（1）启动 Avizo。</p><p>（2）加载 <code>$AVIZO_ROOT/data/tutorials/chocolate-bar.am</code>。</p><p>（3）右键单击数据对象，并从对象弹出菜单中选择 <code>Python Scripts/Scipy</code> FFT。</p><p>（4）单击<code>Apply</code>，并且具有结果 FFT 的新数据对象将在项目视图中显示。</p><p>11.7 使用 MATLAB 与 Avizo</p><p>本节描述了如何在 Avizo 中使用 MATLAB 脚本。</p><h3 id="11-7-1-使用-MATLAB-脚本"><a href="#11-7-1-使用-MATLAB-脚本" class="headerlink" title="11.7.1 使用 MATLAB 脚本"></a>11.7.1 使用 MATLAB 脚本</h3><p>在本教程中，您将学习如何通过 Calculus MATLAB 模块在 Avizo 中使用 MATLAB（The MathWorks, Inc.）进行复杂计算。</p><p>为了使用 Calculus MATLAB 模块，必须在您的计算机上正确安装 MATLAB。此外，为了允许此模块与 MATLAB 计算引擎建立连接，您可能需要注册 MATLAB 引擎（在 Windows 上），并根据您的系统设置环境变量以包括 MATLAB 库或程序的搜索路径。请参阅 Calculus MATLAB 模块的文档以了解安装详情和限制。</p><p>本教程在在线文档中提供，通过各种示例涵盖以下主题：</p><ul><li>加载和执行 MATLAB 脚本。</li><li>将各种数据类型从 Avizo 传递给 MATLAB 并导出它们。</li><li>使用字段结构。</li><li>使用时间滑块控制脚本变量。</li><li>从脚本调用用户自定义的 MATLAB 函数。</li></ul><h1 id="Thermo-Fisher-Python-API-documentation"><a href="#Thermo-Fisher-Python-API-documentation" class="headerlink" title="Thermo Fisher Python API documentation"></a>Thermo Fisher Python API documentation</h1><h2 id="Object-handling-classes"><a href="#Object-handling-classes" class="headerlink" title="Object handling classes"></a>Object handling classes</h2><h3 id="hx-core-HxObjectFactory"><a href="#hx-core-HxObjectFactory" class="headerlink" title="hx.core.HxObjectFactory"></a>hx.core.HxObjectFactory</h3><p>这是一个类，用于在Python中实例化所有模块并加载所有数据。该类的所有实例是可互换的，已经实例化的一个实例被命名为 <code>hx_object_factory</code>。</p><h4 id="create-classname"><a href="#create-classname" class="headerlink" title="create(classname)"></a>create(classname)</h4><p>此函数用于创建一个模块。</p><h5 id="参数"><a href="#参数" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>classname</code> : str<br>字符串，表示要实例化的类名。</li></ul><h5 id="返回"><a href="#返回" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>HxBase</code><br>返回最接近派生自 <code>HxBase</code> 的实例。</li></ul><h5 id="示例"><a href="#示例" class="headerlink" title="示例:"></a>示例:</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>my_var = hx_object_factory.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="get-obj-name"><a href="#get-obj-name" class="headerlink" title="get(obj_name)"></a>get(obj_name)</h4><p>此函数尝试通过名称找到一个模块或数据。</p><h5 id="参数-1"><a href="#参数-1" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>obj_name</code> : str<br>表示要查找的模块或数据的名称属性（它对应于 <code>HxBase.name</code> 属性）。</li></ul><h5 id="返回-1"><a href="#返回-1" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>HxBase</code><br>返回与名称匹配的 <code>HxBase</code> 派生实例。</li></ul><h5 id="Raises"><a href="#Raises" class="headerlink" title="Raises:"></a>Raises:</h5><ul><li><code>KeyError</code><br>如果找不到名为 <code>obj_name</code> 的模块，则引发此异常。</li></ul><h5 id="示例-1"><a href="#示例-1" class="headerlink" title="示例:"></a>示例:</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>my_var = hx_object_factory.get(<span class="string">&#x27;Ortho Slice&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="load-filename"><a href="#load-filename" class="headerlink" title="load(filename)"></a>load(filename)</h4><p>此函数用于加载数据。</p><h5 id="参数-2"><a href="#参数-2" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>filename</code> : str<br>表示要加载的文件。</li></ul><h5 id="返回-2"><a href="#返回-2" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>HxBase 或 list[HxBase]</code><br>如果文件名对应的模块为一个，返回 <code>HxData</code> 的派生实例；如果文件名对应多个模块，返回一个 <code>HxBase</code> 的列表。</li></ul><h5 id="Raises-1"><a href="#Raises-1" class="headerlink" title="Raises:"></a>Raises:</h5><ul><li><code>KeyError</code></li><li><code>RuntimeError</code></li></ul><h5 id="示例-2"><a href="#示例-2" class="headerlink" title="示例:"></a>示例:</h5><p>此示例说明如何加载文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>my_var = hx_object_factory.load(hx_paths.tutorials_dir + <span class="string">&#x27;/chocolate-bar.am&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="hx-core-HxProject"><a href="#hx-core-HxProject" class="headerlink" title="hx.core.HxProject"></a>hx.core.HxProject</h3><p>这是一个类，用于通过添加或删除对象与项目视图（也称为对象池）进行交互。该类的所有实例是可互换的，实例之一被称为 <code>hx_project</code>。</p><h4 id="add-obj"><a href="#add-obj" class="headerlink" title="add(obj)"></a>add(obj)</h4><p>将 <code>obj</code> 添加到项目视图中。</p><h5 id="参数-3"><a href="#参数-3" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>obj</code> : HxObject<br>必须添加到项目视图的对象。</li></ul><h5 id="示例-3"><a href="#示例-3" class="headerlink" title="示例:"></a>示例:</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ortho = hx_object_factory.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_project.add(ortho)<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="create-classname-1"><a href="#create-classname-1" class="headerlink" title="create(classname)"></a>create(classname)</h4><p>创建一个 <code>classname</code> 类型的对象并将其添加到项目视图中。</p><h5 id="参数-4"><a href="#参数-4" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>classname</code> : str<br>表示要实例化的类的字符串。</li></ul><h5 id="返回-3"><a href="#返回-3" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>HxBase</code><br>返回最接近派生自 <code>HxBase</code> 的实例。</li></ul><h5 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h5><p>The following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_obj = hx_project.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)</span><br></pre></td></tr></table></figure><p>is equivalent to:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_obj = hx_object_factory.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_project.add(some_obj)</span><br></pre></td></tr></table></figure><h5 id="示例-4"><a href="#示例-4" class="headerlink" title="示例:"></a>示例:</h5><p>创建正交切片并将其添加到项目视图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_obj = hx_project.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="get-obj-name-1"><a href="#get-obj-name-1" class="headerlink" title="get(obj_name)"></a>get(obj_name)</h4><p>根据名称从项目视图中检索对象。</p><h5 id="参数-5"><a href="#参数-5" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>obj_name</code> : str<br>要检索的对象名称。</li></ul><h5 id="Raises-2"><a href="#Raises-2" class="headerlink" title="Raises:"></a>Raises:</h5><ul><li><code>KeyError</code><br>如果在项目视图中找不到名为 <code>obj_name</code> 的对象，则引发此异常。</li></ul><h5 id="示例-5"><a href="#示例-5" class="headerlink" title="示例:"></a>示例:</h5><p>我们创建一个 Ortho Slice，给它命名，将它添加到项目视图并通过其名称进行查询：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ortho = hx_object_factory.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)  </span><br><span class="line">ortho.name = <span class="string">&quot;MyNewName&quot;</span>  </span><br><span class="line">hx_project.add(ortho)  </span><br><span class="line">ortho2 = hx_project.get(<span class="string">&quot;MyNewName&quot;</span>)<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><p>请注意，在这种情况下，ortho 和 ortho2 代表应用程序中的同一个对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(ortho.is_same_object(ortho2))</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><h4 id="load-filename-1"><a href="#load-filename-1" class="headerlink" title="load(filename)"></a>load(filename)</h4><p>加载指定文件并将其添加到项目视图中。</p><h5 id="参数-6"><a href="#参数-6" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>filename</code> : str<br>表示要加载的文件。</li></ul><h5 id="返回-4"><a href="#返回-4" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>HxBase</code><br>返回最接近派生自 <code>HxData</code> 的实例。</li></ul><h5 id="Notes-1"><a href="#Notes-1" class="headerlink" title="Notes"></a>Notes</h5><p>The following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_obj = hx_project.load(hx_paths.tutorials_dir + <span class="string">&#x27;/chocolate-bar.am&#x27;</span>)</span><br></pre></td></tr></table></figure><p>is equivalent to:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_obj = hx_object_factory.load(hx_paths.tutorials_dir + <span class="string">&#x27;/chocolate-bar.am&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_project.add(some_obj)</span><br></pre></td></tr></table></figure><h5 id="示例-6"><a href="#示例-6" class="headerlink" title="示例:"></a>示例:</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_obj = hx_project.load(hx_paths.tutorials_dir + <span class="string">&#x27;/chocolate-bar.am&#x27;</span>)<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="remove-obj"><a href="#remove-obj" class="headerlink" title="remove(obj)"></a>remove(obj)</h4><p>从项目视图中移除 <code>obj</code>。</p><h5 id="参数-7"><a href="#参数-7" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>obj</code> : HxObject<br>要从项目视图中删除的对象。</li></ul><h5 id="Raises-3"><a href="#Raises-3" class="headerlink" title="Raises:"></a>Raises:</h5><ul><li><code>KeyError</code><br>如果 <code>obj</code> 不在项目视图中，则引发此异常。</li></ul><h5 id="示例-7"><a href="#示例-7" class="headerlink" title="示例:"></a>示例:</h5><p>我们使用object_factory创建一个正交切片，将其添加到项目视图，然后将其从项目视图中删除：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ortho = hx_object_factory.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_project.add(ortho)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_project.remove(ortho)<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="remove-all"><a href="#remove-all" class="headerlink" title="remove_all()"></a>remove_all()</h4><p>从项目视图中移除所有对象（仅移除可移除的对象）。</p><h5 id="示例-8"><a href="#示例-8" class="headerlink" title="示例:"></a>示例:</h5><p>从项目视图中删除所有对象。（只有可移动的对象才会被删除。）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_project.remove_all()<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="Path-management"><a href="#Path-management" class="headerlink" title="Path management"></a>Path management</h2><h3 id="hx-core-HxPaths"><a href="#hx-core-HxPaths" class="headerlink" title="hx.core.HxPaths"></a>hx.core.HxPaths</h3><p>这是一个类，用于检索与产品路径相关的信息。该类的一个实例称为 <code>hx_paths</code>。</p><h4 id="appdata-dir"><a href="#appdata-dir" class="headerlink" title="appdata_dir"></a>appdata_dir</h4><p>与产品的 <code>appdata</code> 目录绑定的只读属性。</p><h4 id="executable-dir"><a href="#executable-dir" class="headerlink" title="executable_dir"></a>executable_dir</h4><p>与产品的可执行目录绑定的只读属性。</p><h4 id="home-dir"><a href="#home-dir" class="headerlink" title="home_dir"></a>home_dir</h4><p>与产品的主目录绑定的只读属性。</p><h4 id="install-data-dir"><a href="#install-data-dir" class="headerlink" title="install_data_dir"></a>install_data_dir</h4><p>与产品的数据目录绑定的只读属性。</p><h4 id="install-dir"><a href="#install-dir" class="headerlink" title="install_dir"></a>install_dir</h4><p>与产品的安装目录绑定的只读属性。</p><h4 id="python-modules-dir"><a href="#python-modules-dir" class="headerlink" title="python_modules_dir"></a>python_modules_dir</h4><p>与产品的 Python 模块目录绑定的只读属性。</p><h4 id="python-script-objects-dir"><a href="#python-script-objects-dir" class="headerlink" title="python_script_objects_dir"></a>python_script_objects_dir</h4><p>与产品的 Python 脚本对象目录绑定的只读属性。</p><h4 id="python-share-dir"><a href="#python-share-dir" class="headerlink" title="python_share_dir"></a>python_share_dir</h4><p>与产品的 <code>share</code> 目录绑定的只读属性。</p><h4 id="tutorials-dir"><a href="#tutorials-dir" class="headerlink" title="tutorials_dir"></a>tutorials_dir</h4><p>与产品的教程目录绑定的只读属性。</p><h2 id="Messaging-utilities"><a href="#Messaging-utilities" class="headerlink" title="Messaging utilities"></a>Messaging utilities</h2><h3 id="hx-core-HxMessage"><a href="#hx-core-HxMessage" class="headerlink" title="hx.core.HxMessage"></a>hx.core.HxMessage</h3><p>此界面用于弹出模式对话框。（例如错误、警告、信息、文件覆盖、问题）。</p><p>这些简单对话框（消息框）已实现“不再显示此消息”功能。使用此功能，用户可以禁用显示某些消息。此外，用户还可以通过“首选项”面板恢复已禁用的消息框。在消息框中使用此功能时，将创建消息框的唯一密钥，并将该密钥与用户单击的按钮索引一起保存到设置中。默认情况下，所有消息框均禁用此功能，如果我们想启用此功能，则必须传递其他参数。</p><p>该类的一个实例称为 <code>hx_message</code>。</p><h4 id="confirmations-message-button0-text-button1-text"><a href="#confirmations-message-button0-text-button1-text" class="headerlink" title="confirmations(message, button0_text, button1_text)"></a>confirmations(message, button0_text, button1_text)</h4><p>确认对话框，使用方法与 <code>question()</code> 相同。</p><h5 id="参数-8"><a href="#参数-8" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>message</code> : str<br>要在消息框中显示的消息。</li><li><code>button0_text</code> : str<br>按钮0的标签。</li><li><code>button1_text</code> : str<br>按钮1的标签。</li><li><code>do_not_show_again_key</code> : str, optional<br>如果此参数不为空，将添加“不要再显示此消息”的复选框，用户勾选后可防止再次显示该消息。该字符串值将作为识别消息的键。</li></ul><h5 id="返回-5"><a href="#返回-5" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>int</code><br>返回值指示用户按下了哪个按钮。</li></ul><h5 id="示例-9"><a href="#示例-9" class="headerlink" title="示例:"></a>示例:</h5><p>显示一个简单的确认消息框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;ret = hx_message.confirmations(&quot;Computation may be very long. Do you want to continue ?&quot;, &quot;Yes&quot;, &quot;No&quot;)  </span></span><br><span class="line"><span class="string">&gt;&gt;&gt; if ret == 0:  </span></span><br><span class="line"><span class="string">&gt;&gt;&gt;     print(&quot;Execute your computation code.&quot;)&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="error-message-button0-text-””-button1-text-””-button2-text-””-default-button-index-1-escape-button-index-1"><a href="#error-message-button0-text-””-button1-text-””-button2-text-””-default-button-index-1-escape-button-index-1" class="headerlink" title="error(message, button0_text=””, button1_text=””, button2_text=””, default_button_index=-1, escape_button_index=-1)"></a>error(message, button0_text=””, button1_text=””, button2_text=””, default_button_index=-1, escape_button_index=-1)</h4><p>弹出一个模态对话框，显示用户定义的错误消息。</p><h5 id="参数-9"><a href="#参数-9" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>message</code> : str<br>要在消息框中显示的消息。</li><li><code>button0_text</code> : str, optional<br>按钮0的标签。如果为空，则显示“关闭”。</li><li><code>button1_text</code> : str, optional<br>按钮1的标签。</li><li><code>button2_text</code> : str, optional<br>按钮2的标签。</li><li><code>default_button_index</code> : int, optional<br>默认按钮的索引（0-2）。</li><li><code>escape_button_index</code> : int, optional<br>逃生按钮的索引。</li></ul><h5 id="返回-6"><a href="#返回-6" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>int</code><br>返回值指示用户按下了哪个按钮。</li></ul><h5 id="示例-10"><a href="#示例-10" class="headerlink" title="示例:"></a>示例:</h5><p>显示一个简单的消息框错误：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_message.error(<span class="string">&quot;Could not locate specified file.&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="info-message-do-not-show-again-key-””"><a href="#info-message-do-not-show-again-key-””" class="headerlink" title="info(message, do_not_show_again_key=””)"></a>info(message, do_not_show_again_key=””)</h4><p>与错误、警告和问题对话框不同，信息对话框始终只有一个按钮，标有“关闭”。</p><h5 id="参数-10"><a href="#参数-10" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>message</code> : str<br>要在消息框中显示的消息。</li><li><code>do_not_show_again_key</code> : str, optional<br>如果此参数不为空，将添加“不要再显示此消息”的复选框。</li></ul><h5 id="返回-7"><a href="#返回-7" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>int</code><br>返回0。</li></ul><h5 id="示例-11"><a href="#示例-11" class="headerlink" title="示例:"></a>示例:</h5><p>显示一个简单的信息消息框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_message.info(<span class="string">&quot;Computation was a success!&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="overwrite-filename"><a href="#overwrite-filename" class="headerlink" title="overwrite(filename)"></a>overwrite(filename)</h4><p>将会弹出一个模式对话框，提示指定的文件已经存在。用户可以选择覆盖或取消。如果选择覆盖，该方法将返回 True。否则，该方法将返回 False。不会检查指定的文件是否确实已经存在。</p><h5 id="参数-11"><a href="#参数-11" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>filename</code> : str<br>已存在的文件名。</li></ul><h5 id="返回-8"><a href="#返回-8" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>bool</code><br>如果用户选择覆盖返回 <code>True</code>，否则返回 <code>False</code>。</li></ul><h4 id="question-message-button0-text-button1-text-button2-text-””-default-button-index-1-escape-button-index-1-do-not-show-again-key-””"><a href="#question-message-button0-text-button1-text-button2-text-””-default-button-index-1-escape-button-index-1-do-not-show-again-key-””" class="headerlink" title="question(message, button0_text, button1_text, button2_text=””, default_button_index=-1, escape_button_index=-1, do_not_show_again_key=””)"></a>question(message, button0_text, button1_text, button2_text=””, default_button_index=-1, escape_button_index=-1, do_not_show_again_key=””)</h4><p>问题对话框，使用方法与 <code>error()</code> 相同。</p><h5 id="参数-12"><a href="#参数-12" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>message</code> : str<br>要在消息框中显示的问题。</li><li><code>button0_text</code> : str<br>按钮0的标签。</li><li><code>button1_text</code> : str<br>按钮1的标签。</li><li><code>button2_text</code> : str, optional<br>按钮2的标签。</li><li><code>default_button_index</code> : int, optional<br>默认按钮的索引（0-2）。</li><li><code>escape_button_index</code> : int, optional<br>逃生按钮的索引。</li><li><code>do_not_show_again_key</code> : str, optional<br>如果此参数不为空，将添加“不要再显示此消息”的复选框。</li></ul><h5 id="返回-9"><a href="#返回-9" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>int</code><br>返回值指示用户按下了哪个按钮。</li></ul><h5 id="示例-12"><a href="#示例-12" class="headerlink" title="示例:"></a>示例:</h5><p>显示一个简单的消息框问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ret = hx_message.question(<span class="string">&quot;Specified file seems corrupted. Do you want to load the data anyway ?&quot;</span>, <span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> ret == <span class="number">0</span>:  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="built_in">print</span>(<span class="string">&quot;Abort image loading...&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="warning-message-button0-text-””-button1-text-””-button2-text-””-default-button-index-1-escape-button-index-1-do-not-show-again-key-””"><a href="#warning-message-button0-text-””-button1-text-””-button2-text-””-default-button-index-1-escape-button-index-1-do-not-show-again-key-””" class="headerlink" title="warning(message, button0_text=””, button1_text=””, button2_text=””, default_button_index=-1, escape_button_index=-1, do_not_show_again_key=””)"></a>warning(message, button0_text=””, button1_text=””, button2_text=””, default_button_index=-1, escape_button_index=-1, do_not_show_again_key=””)</h4><p>警告对话框，使用方法与 <code>error()</code> 相同。</p><h5 id="参数-13"><a href="#参数-13" class="headerlink" title="参数:"></a>参数:</h5><ul><li><code>message</code> : str<br>要在消息框中显示的警告消息。</li><li><code>button0_text</code> : str, optional<br>按钮0的标签。</li><li><code>button1_text</code> : str, optional<br>按钮1的标签。</li><li><code>button2_text</code> : str, optional<br>按钮2的标签。</li><li><code>default_button_index</code> : int, optional<br>默认按钮的索引（0-2）。</li><li><code>escape_button_index</code> : int, optional<br>逃生按钮的索引。</li><li><code>do_not_show_again_key</code> : str, optional<br>如果此参数不为空，将添加“不要再显示此消息”的复选框。</li></ul><h5 id="返回-10"><a href="#返回-10" class="headerlink" title="返回:"></a>返回:</h5><ul><li><code>int</code><br>返回值指示用户按下了哪个按钮。</li></ul><h5 id="示例-13"><a href="#示例-13" class="headerlink" title="示例:"></a>示例:</h5><p>显示一个简单的消息框警告：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>hx_message.warning(<span class="string">&quot;Imported file does not have the expected size.&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="Hierarchy-of-objects"><a href="#Hierarchy-of-objects" class="headerlink" title="Hierarchy of objects"></a>Hierarchy of objects</h2><h3 id="hx-core-McInterface"><a href="#hx-core-McInterface" class="headerlink" title="hx.core.McInterface"></a>hx.core.McInterface</h3><p>McInterface 是所有接口的基类，包括 HxBase 层次结构。此类是抽象基类。</p><h4 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h4><h5 id="all-interfaces"><a href="#all-interfaces" class="headerlink" title="all_interfaces"></a>all_interfaces</h5><p>包含所有允许的接口作为子成员的属性。</p><h5 id="示例-14"><a href="#示例-14" class="headerlink" title="示例"></a>示例</h5><p>检索正交切片 (HxOrthoSlice) 的 HxBase 接口：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ortho = hx_project.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>base = ortho.all_interfaces.HxBase</span><br></pre></td></tr></table></figure></p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><h5 id="get-all-interface-names"><a href="#get-all-interface-names" class="headerlink" title="get_all_interface_names()"></a>get_all_interface_names()</h5><p>返回支持的所有接口名称的列表。</p><h5 id="返回-11"><a href="#返回-11" class="headerlink" title="返回:"></a>返回:</h5><ul><li>list of strings<br>返回对象支持的所有接口名称的列表。</li></ul><h5 id="示例-15"><a href="#示例-15" class="headerlink" title="示例"></a>示例</h5><p>打印正交切片支持的所有接口：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ortho = hx_project.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(ortho.get_all_interface_names())</span><br><span class="line">[<span class="string">&#x27;HxPlanarModBase&#x27;</span>, <span class="string">&#x27;HxModule&#x27;</span>, <span class="string">&#x27;HxObject&#x27;</span>, <span class="string">&#x27;HxBase&#x27;</span>]</span><br></pre></td></tr></table></figure></p><h3 id="hx-core-HxBase"><a href="#hx-core-HxBase" class="headerlink" title="hx.core.HxBase"></a>hx.core.HxBase</h3><p>文档太长，暂不翻译</p><h1 id="python-avizo代码测试"><a href="#python-avizo代码测试" class="headerlink" title="python+avizo代码测试"></a>python+avizo代码测试</h1><h2 id="代码创建方法"><a href="#代码创建方法" class="headerlink" title="代码创建方法"></a>代码创建方法</h2><p>在python控制台中调用 <code>Segmentation Editor</code> 很难实现，对于pyhon接口，只可以在project中实现的算法，可以采用API去调用。</p><p>对于某个具体的方法，可以鼠标放在方法栏上，然后悬停后显示方法名。</p><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/%E9%98%88%E5%80%BC%E5%88%86%E5%89%B2-1.png" class="" title="阈值分割-1"><p>例如<code>Image Curvature</code>方法，在API中对应的接口就是<code>curvature2d/curvature3d</code></p><p>对于<code>curvature</code>方法不熟悉，可以在控制台直接输出。</p><ul><li>curvature3d</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hx_object_factory.create(<span class="string">&#x27;curvature3d&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Python handle of type: &#x27;HxCompModule&#x27;</span><br><span class="line">Kernel item name     : &#x27;Image Curvature 3D&#x27;</span><br><span class="line">Kernel item type     : &#x27;HxQuant2GenericModule&#x27;</span><br><span class="line">All ports            :</span><br><span class="line">    &#x27;interpretation&#x27;: &#x27;HxPortRadioBox&#x27;</span><br><span class="line">    &#x27;outputLocation&#x27;: &#x27;HxPortMultiMenu&#x27;</span><br><span class="line">    &#x27;doIt&#x27;: &#x27;HxPortDoIt&#x27;</span><br><span class="line">    &#x27;inputImage&#x27;: &#x27;HxConnection&#x27;</span><br><span class="line">    &#x27;inputImageMask&#x27;: &#x27;HxConnection&#x27;</span><br><span class="line">    &#x27;standardDeviation&#x27;: &#x27;HxPortFloatTextN&#x27;</span><br></pre></td></tr></table></figure><ul><li>curvature2d</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hx_object_factory.create(<span class="string">&#x27;curvature2d&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Python handle of type: &#x27;HxCompModule&#x27;</span><br><span class="line">Kernel item name     : &#x27;Image Curvature 2&#x27;</span><br><span class="line">Kernel item type     : &#x27;HxQuant2GenericModule&#x27;</span><br><span class="line">All ports            :</span><br><span class="line">    &#x27;interpretation&#x27;: &#x27;HxPortRadioBox&#x27;</span><br><span class="line">    &#x27;outputLocation&#x27;: &#x27;HxPortMultiMenu&#x27;</span><br><span class="line">    &#x27;doIt&#x27;: &#x27;HxPortDoIt&#x27;</span><br><span class="line">    &#x27;inputImage&#x27;: &#x27;HxConnection&#x27;</span><br><span class="line">    &#x27;inputImageMask&#x27;: &#x27;HxConnection&#x27;</span><br><span class="line">    &#x27;standardDeviation&#x27;: &#x27;HxPortFloatTextN&#x27;</span><br></pre></td></tr></table></figure><ul><li>图形化界面可以看到，使用以下创建方式</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hx_project.create(<span class="string">&#x27;curvature3d&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="helloworld"><a href="#helloworld" class="headerlink" title="helloworld"></a>helloworld</h2><p>在Avizo的Main Python Console调用并执行脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exec</span>(<span class="built_in">open</span>(<span class="string">&#x27;F://PYCharmWorkSpace//DigitalCoreFeatureExtraction//avizo//avizo_helloworld.py&#x27;</span>).read())</span><br></pre></td></tr></table></figure><h2 id="读取文件并进行切片显示"><a href="#读取文件并进行切片显示" class="headerlink" title="读取文件并进行切片显示"></a>读取文件并进行切片显示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataLoaded = hx_project.load(<span class="string">&#x27;E:/digitalrock/eleven_sandstones_dataset/1_Berea/Berea_2d25um_binary_1-0_test.tif&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ortho_slice = hx_project.create(<span class="string">&#x27;HxOrthoSlice&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ortho_slice.ports.data.connect(dataLoaded)</span><br><span class="line"></span><br><span class="line">ortho_slice.fire()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exec</span>(<span class="built_in">open</span>(<span class="string">&#x27;F://PYCharmWorkSpace//DigitalCoreFeatureExtraction//avizo//avizo_read_test.py&#x27;</span>).read())</span><br></pre></td></tr></table></figure><h2 id="阈值分割"><a href="#阈值分割" class="headerlink" title="阈值分割"></a>阈值分割</h2><h3 id="avizo中Image-Segmentation方法"><a href="#avizo中Image-Segmentation方法" class="headerlink" title="avizo中Image Segmentation方法"></a>avizo中Image Segmentation方法</h3><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/ImageSegmentation-1.png" class="" title="ImageSegmentation-1"><ul><li>2D-Histogram Segmentation</li></ul><p>使用2D直方图分割脚本模块，您可以以半自动的方式对由两个或多个阶段组成的CT或MR灰度图像数据进行分割。</p><p>考虑多相材料，如教程数据 chocolate-bar.am，即气缸盖样品的 CT 扫描。有许多离散相，但直方图显示不同相的峰值重叠。在这种情况下，基于强度对图像进行阈值处理将失败，因为任何阈值都会导致某些像素被错误分配。<br>这里使用的方法称为 2D 直方图分割，依赖于梯度幅度与图像强度直方图。<br>此分割过程包括两个主要步骤：将一些体素初步分类为两个或多个相，然后是扩展步骤，在该步骤中扩展该分类，以便所有体素都被标记。<br>分类：对于此例程，体素的初始分类是基于这些体素的强度及其梯度幅度执行的。此例程的输入是 3D 体积的强度图。从该输入计算梯度幅度。计算梯度幅度后，将向用户显示显示梯度幅度与图像强度的 2D 散点图。用户将使用此图来确定初始分类（这在使用直方图初始化分类中进行了描述）。<br>扩展：扩展由标记种子分水岭变换计算（参见分水岭算法原理）。分水岭变换需要两个输入，一组种子标记和一个景观函数。种子标记将由分类确定（参见解释直方图），梯度幅度将用于景观函数。<br>2D 直方图分割的 6 个步骤：<br>步骤 1 - 计算梯度幅度以开始 - 请参阅计算梯度幅度中的详细信息：<br>选择精确计算梯度幅度或更快近似梯度幅度。<br>点击下一步：计算梯度幅度以继续。<br>步骤 2 - 绘制直方图，请参阅解释直方图中的详细信息：<br>点击下一步：绘制直方图以继续。<br>步骤 3 - 绘制窗口 - 请参阅使用直方图初始化分类中的详细信息：<br>如果您使用鼠标拖动绘图窗口，并且希望将其重置为原始大小和位置，则只需拖动属性面板上的伽马校正滑块即可。<br>使用伽马校正在 2D 直方图上可视化峰值。使用绘图窗口顶部的绘图工具直接在绘图上绘制窗口。通常，您需要选择梯度幅度非常低的区域（峰值靠近 x 轴）。<br>点击下一步：计算种子以继续下一步。<br>步骤 4 - 查看种子标签：<br>使用权重因子更新视图以显示灰度数据、种子标签图像或两个数据集的加权融合。<br>使用隐藏/显示直方图按钮切换直方图的显示，以便更轻松地在查看器中查看结果。使用切片编号调整正在显示的切片索引平面。如果您对从所选窗口计算出的种子不满意，请点击“删除最后一个窗口”以逐个删除窗口（按添加时的相反顺序）。然后您可以绘制新窗口。绘制新窗口后，请点击“重新计算种子”。您可以反复进行，直到对种子满意为止。<br>点击“下一步：应用分水岭”继续（请参阅“使用分水岭扩展”中的详细信息）<br>步骤 5 - 确认分水岭结果：<br>如果结果不令人满意，请绘制新区域，然后点击“上一步：重新计算种子”重新应用分水岭。<br>点击“下一步”继续执行最后一步，这是可选的。<br>步骤 6 - [可选] 删除通道：<br>如果分配的标签之一对应于空白空间，您可以选择从标签图像中删除该标签。<br>完成后，您应该删除此模块。</p><ul><li>Watershed Segmentation</li></ul><p>此模块通过对高梯度幅值应用分水岭来对不同相进行精确分割。<br>指定要分割的相数后，单击“跳过”按钮并按照说明进行操作。<br>要执行的操作包括：<br>指定相数。如果仅指定一个相，则将从指定相的侵蚀负片内部计算假标签。因此，分水岭计算期间指定相的扩散受到侵蚀的限制。<br>计算 3D 梯度幅值。内部使用 Avizo gradient_canny3d。或者，可以通过将梯度幅值标量场连接到 portGradient 来设置梯度幅值。<br>阈值梯度蒙版。使用滑块对梯度蒙版进行阈值设置，即定义无法设置标记的区域。<br>为相指定每个标记。可以使用最小和最大阈值滑块设置每个标记，也可以通过 portPhase 将相标记指定为二进制图像来设置每个标记。<br>应用分水岭计算。Avizo 分水岭算法根据先前设置的标记执行，以梯度幅度作为高度图像。<br>可视化最终阶段。颜色清洗模块连接到切片，同时提供阶段分割和原始数据可视化。</p><ul><li>Adaptive Thresholding</li></ul><p>该模块通过应用相对于滑动窗口的平均强度自动适应的阈值来执行二值化。<br>对于每个像素，根据其局部平均强度计算局部阈值。然后使用可以乘法或加法的模型应用此阈值。<br>保留的像素是那些根据比较标准“大于或等于”或“小于或等于”阈值的像素。</p><ul><li>Auto Thresholding</li></ul><p>自动阈值高是模块自动阈值的配置之一。有关其他配置，请参阅端口类型。<br>此模块计算灰度图像上的自动阈值，即将图像分成 2 个像素类。有四种分类方法：熵、因式分解、矩和 IsoData。计算出的阈值显示在表格面板中。</p><ul><li>Feature Adaptive Thresholding</li></ul><p>二值化将灰度图像转换为二进制图像。当灰度图像中的相关信息对应于特定的灰度间隔时，使用此方法。在二进制图像中，感兴趣的像素设置为 1，其他所有像素（背景）设置为 0。<br>此模块计算灰度图像的阈值，给定与原始图像的预分割相对应的标签图像。用户不是为阈值提供两个固定值，而是选择两个代表性度量（例如，直方图的第 10 和第 90 个百分位数）。每个标签的最小和最大阈值都是动态计算的，并对它们进行单独的阈值处理。</p><ul><li>Hysteresis Thresholding</li></ul><p>二值化将灰度图像转换为二进制图像。当灰度图像中的相关信息对应于特定的灰度间隔时，使用此方法。在二进制图像中，感兴趣的像素设置为 1，其他所有像素（背景）设置为 0。<br>滞后阈值使用滞后环来提供更连接的阈值结果。</p><ul><li>Interactive Thresholding</li></ul><p>此工具允许以交互方式选择阈值。当前选择显示为所连接正交视图的每个视图上的叠加层。<br>按“应用”按钮创建二进制图像。将创建一个新字段，该字段对于阈值间隔内的每个值均为 1，对于所有其他字段值均为 0。</p><ul><li>Interactive Top-Hat</li></ul><p>Top-Hat 分割从给定图像中提取小元素和细节。它检测对应于谷值或窄峰的暗区或白区。</p><p>有两种类型的 Top-Hat 变换：</p><p>黑色 Top-Hat：它被定义为使用给定大小的内核闭合的立方体与输入图像之间的差异。内核越小，Top-Hat 图像中的元素越小。阈值允许选择 Top-Hat 结果中较暗的元素，即所选谷值的深度。参见图 1。</p><p>白色 Top-Hat：它被定义为输入图像与其开口立方体之间的差异（使用给定大小的内核）。阈值允许选择 Top-Hat 结果中较亮的元素。</p><ul><li>Local Thresholding</li></ul><p>该模块提供算法，将图像堆栈二分分割为前景和背景对象。模块的输出是标签图像。如果需要在缓慢变化的背景之前分割多个小对象，则该算法效果最佳。<br>一些阈值算法需要大量主内存才能运行（Niblack、Oberlaender、Mardia-Hainsworth）。浮点分辨率缓冲区将根据输入图像大小进行分配。</p><ul><li>Threshold by Criterion</li></ul><p>按标准阈值对图像进行阈值处理。如果计算结果为真，则输出像素将设置为 1，否则设置为 0。</p><h3 id="是否需要阈值分割"><a href="#是否需要阈值分割" class="headerlink" title="是否需要阈值分割"></a>是否需要阈值分割</h3><img src="/2024/10/22/AVIZO%E8%87%AA%E5%8A%A8%E5%8C%96/ImageSegmentation-2.png" class="" title="ImageSegmentation-2"><p>相同的数据体，计算曲率的过程相同<br>上面执行材料划分<br>下面不执行材料划分<br>数据执行结果相同<br>因为孔隙为1，骨架为0。</p><h2 id="曲率计算及分析"><a href="#曲率计算及分析" class="headerlink" title="曲率计算及分析"></a>曲率计算及分析</h2><h3 id="Image-Curvature"><a href="#Image-Curvature" class="headerlink" title="Image Curvature"></a>Image Curvature</h3><p>从本地加载一张tif图片，然后调用curvature方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = hx_project.load(<span class="string">&#x27;E:/digitalrock/eleven_sandstones_dataset/1_Berea/Berea_2d25um_binary_1-0_test.tif&#x27;</span>)</span><br><span class="line"></span><br><span class="line">curvature3d = hx_project.create(<span class="string">&#x27;curvature3d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">curvature3d.ports.inputImage.connect(data)</span><br><span class="line"></span><br><span class="line">curvature3d.ports.doIt.was_hit = <span class="literal">True</span></span><br><span class="line">curvature3d.compute()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(curvature3d.results[<span class="number">0</span>].name)</span><br></pre></td></tr></table></figure><p><code>fire()</code> → 如果这是一个计算模块并且其端口 doIt 已被触碰或者自动刷新已被激活，则此方法将触发对对象上的 update() 的调用，并最终触发 compute() 的调用。<br><code>execute()</code> → 此方法将模拟点击此对象的属性区域下的绿色应用按钮，并对此对象执行<code>fire()</code>。<br><code>compute()</code> → 执行计算。</p><p>现在我们通过触发端口doIt来调用计算：<code>curvature3d.ports.doIt.was_hit = True</code></p><p>执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exec</span>(<span class="built_in">open</span>(<span class="string">&#x27;F://PYCharmWorkSpace//DigitalCoreFeatureExtraction//avizo//avizo_test.py&#x27;</span>).read())</span><br></pre></td></tr></table></figure><h3 id="Image-Statistics"><a href="#Image-Statistics" class="headerlink" title="Image Statistics"></a>Image Statistics</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hx_project.create(<span class="string">&#x27;statistics&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Python handle of type: &#x27;HxCompModule&#x27;</span><br><span class="line">Kernel item name     : &#x27;Image Statistics&#x27;</span><br><span class="line">Kernel item type     : &#x27;HxQuant2GenericModule&#x27;</span><br><span class="line">All ports            :</span><br><span class="line">    &#x27;Type&#x27;: &#x27;HxPortModuleSwitch&#x27;</span><br><span class="line">    &#x27;interpretation&#x27;: &#x27;HxPortRadioBox&#x27;</span><br><span class="line">    &#x27;outputLocation&#x27;: &#x27;HxPortMultiMenu&#x27;</span><br><span class="line">    &#x27;doIt&#x27;: &#x27;HxPortDoIt&#x27;</span><br><span class="line">    &#x27;inputImage&#x27;: &#x27;HxConnection&#x27;</span><br><span class="line">    &#x27;rangeMode&#x27;: &#x27;HxPortMultiMenu&#x27;</span><br><span class="line">    &#x27;inputRange&#x27;: &#x27;HxPortIntTextN&#x27;</span><br></pre></td></tr></table></figure><h3 id="Image-Curvature-Image-Statistics"><a href="#Image-Curvature-Image-Statistics" class="headerlink" title="Image Curvature + Image Statistics"></a>Image Curvature + Image Statistics</h3><ul><li>接收到处理结果</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result1 = curvature3d.results[<span class="number">0</span>]</span><br><span class="line">result2 = curvature3d.results[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result1.portnames)</span><br><span class="line"><span class="built_in">print</span>(result2.portnames)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;master&#x27;, &#x27;histogramInfo&#x27;, &#x27;sharedColormap&#x27;, &#x27;preview&#x27;]</span><br><span class="line">[&#x27;master&#x27;, &#x27;histogramInfo&#x27;, &#x27;sharedColormap&#x27;, &#x27;preview&#x27;]</span><br></pre></td></tr></table></figure><ul><li>处理结果运行分析</li></ul><p>对两个曲率进行统计</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">imagestatic1 = hx_project.create(<span class="string">&#x27;statistics&#x27;</span>)</span><br><span class="line">imagestatic2 = hx_project.create(<span class="string">&#x27;statistics&#x27;</span>)</span><br><span class="line"></span><br><span class="line">imagestatic1.ports.inputImage.connect(result1)</span><br><span class="line">imagestatic2.ports.inputImage.connect(result2)</span><br><span class="line"></span><br><span class="line">imagestatic1.ports.doIt.was_hit = <span class="literal">True</span></span><br><span class="line">imagestatic2.ports.doIt.was_hit = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">imagestatic1.compute()</span><br><span class="line">imagestatic2.compute()</span><br></pre></td></tr></table></figure><ul><li>接收到统计结果</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">staticresult1 = imagestatic1.results[<span class="number">0</span>]</span><br><span class="line">staticresult12 = imagestatic2.results[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(staticresult1.portnames)</span><br><span class="line"><span class="built_in">print</span>(staticresult12.portnames)</span><br><span class="line"></span><br><span class="line">staticresult1.method</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;master&#x27;, &#x27;table&#x27;, &#x27;DataClass&#x27;]</span><br><span class="line">[&#x27;master&#x27;, &#x27;table&#x27;, &#x27;DataClass&#x27;]</span><br></pre></td></tr></table></figure><h3 id="读取表格"><a href="#读取表格" class="headerlink" title="读取表格"></a>读取表格</h3><p>参看API中<code>hx.core.HxSpreadSheetInterface</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ssi = staticresult1.all_interfaces.HxSpreadSheetInterface</span><br><span class="line"><span class="built_in">print</span>(ssi.tables[<span class="number">0</span>].columns[<span class="number">1</span>].name)<span class="comment"># NbPixels</span></span><br><span class="line"><span class="built_in">print</span>(ssi.tables[<span class="number">0</span>].columns[<span class="number">5</span>].name)<span class="comment"># Mean</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ssi.tables[<span class="number">0</span>].items[<span class="number">0</span>,<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(ssi.tables[<span class="number">0</span>].row[<span class="number">0</span>].items[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h2 id="阈值分割计算曲率案例"><a href="#阈值分割计算曲率案例" class="headerlink" title="阈值分割计算曲率案例"></a>阈值分割计算曲率案例</h2><h3 id="2D"><a href="#2D" class="headerlink" title="2D"></a>2D</h3><p>1、加载图像<br>2、计算曲率<br>3、曲率结果处理</p><ul><li>2D图片加载弹窗问题</li></ul><p>暂时无法解决，获取不到对话框，只可以认为用鼠标连点器点击</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage.io <span class="keyword">import</span> imread_collection</span><br><span class="line"></span><br><span class="line">hx_project.remove_all()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imgfloder_src = [<span class="string">&#x27;E:/digitalrock/eleven_sandstones_dataset/1_Berea/Berea_2d25um_binary_tif_1-0_test/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">img_suffix = <span class="string">&#x27;*.tif&#x27;</span></span><br><span class="line"></span><br><span class="line">pixel_size = <span class="number">2.25e-6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> imgfloder_src:</span><br><span class="line">    imgfloder_seq = imread_collection(i + img_suffix)</span><br><span class="line">    <span class="comment">#print(imgfloder_seq)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> imgfloder_seq.files:</span><br><span class="line">        img_name = j.split(<span class="string">&#x27;\\&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">        filepath = i + img_name</span><br><span class="line">        <span class="comment">#print(filepath)</span></span><br><span class="line">        data = hx_project.load(filepath)</span><br><span class="line">        curvaturemodel = hx_project.create(<span class="string">&#x27;curvature2d&#x27;</span>)</span><br><span class="line">        curvaturemodel.ports.inputImage.connect(data)</span><br><span class="line">        curvaturemodel.ports.doIt.was_hit = <span class="literal">True</span></span><br><span class="line">        curvaturemodel.compute()</span><br><span class="line"></span><br><span class="line">        gasscurvature = curvaturemodel.results[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        gasscurvaturestatistics = hx_project.create(<span class="string">&#x27;statistics&#x27;</span>)</span><br><span class="line">        gasscurvaturestatistics.ports.inputImage.connect(gasscurvature)</span><br><span class="line">        gasscurvaturestatistics.ports.doIt.was_hit = <span class="literal">True</span></span><br><span class="line">        gasscurvaturestatistics.execute()</span><br><span class="line">        gasscurvaturestatisticsresult = gasscurvaturestatistics.results[<span class="number">0</span>]</span><br><span class="line">        ssi1 = gasscurvaturestatisticsresult.all_interfaces.HxSpreadSheetInterface</span><br><span class="line">        <span class="built_in">print</span>(img_name + <span class="string">&quot;  &quot;</span> + <span class="string">&quot;&#123;:.4e&#125;&quot;</span>.<span class="built_in">format</span>((ssi1.tables[<span class="number">0</span>].items[<span class="number">0</span>, <span class="number">5</span>]) / pixel_size))  <span class="comment"># m-1</span></span><br><span class="line">        hx_project.remove_all()</span><br></pre></td></tr></table></figure><h3 id="3D-单个"><a href="#3D-单个" class="headerlink" title="3D-单个"></a>3D-单个</h3><p>1、加载图像<br>2、计算曲率<br>3、曲率结果处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = hx_project.load(<span class="string">&#x27;E:/digitalrock/eleven_sandstones_dataset/1_Berea/Berea_2d25um_binary_1-0_test.tif&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建3维曲率计算模块</span></span><br><span class="line">curvaturemodel = hx_project.create(<span class="string">&#x27;curvature3d&#x27;</span>)</span><br><span class="line"><span class="comment"># 设置曲率计算模块输入</span></span><br><span class="line">curvaturemodel.ports.inputImage.connect(data)</span><br><span class="line"><span class="comment"># 设置曲率计算方法打开doIt</span></span><br><span class="line">curvaturemodel.ports.doIt.was_hit = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 执行计算</span></span><br><span class="line">curvaturemodel.compute()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 高斯曲率计算结果</span></span><br><span class="line">gasscurvature = curvaturemodel.results[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 平均曲率计算结果</span></span><br><span class="line">avgcurvature = curvaturemodel.results[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建高斯曲率统计模块</span></span><br><span class="line">gasscurvaturestatistics = hx_project.create(<span class="string">&#x27;statistics&#x27;</span>)</span><br><span class="line"><span class="comment"># 设置高斯曲率计算模块输入</span></span><br><span class="line">gasscurvaturestatistics.ports.inputImage.connect(gasscurvature)</span><br><span class="line"><span class="comment"># 设置高斯曲率统计模块打开doIt</span></span><br><span class="line">gasscurvaturestatistics.ports.doIt.was_hit = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 执行高斯曲率统计计算</span></span><br><span class="line">gasscurvaturestatistics.execute()</span><br><span class="line"><span class="comment"># 高斯曲率统计结果</span></span><br><span class="line">gasscurvaturestatisticsresult = gasscurvaturestatistics.results[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 查看高斯曲率整体均值</span></span><br><span class="line">ssi1 = gasscurvaturestatisticsresult.all_interfaces.HxSpreadSheetInterface</span><br><span class="line"><span class="built_in">print</span>(ssi1.tables[<span class="number">0</span>].items[<span class="number">0</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建平均曲率统计模块</span></span><br><span class="line">avgcurvaturestatistics = hx_project.create(<span class="string">&#x27;statistics&#x27;</span>)</span><br><span class="line"><span class="comment"># 设置平均曲率计算模块输入</span></span><br><span class="line">avgcurvaturestatistics.ports.inputImage.connect(avgcurvature)</span><br><span class="line"><span class="comment"># 设置平均曲率统计模块打开doIt</span></span><br><span class="line">avgcurvaturestatistics.ports.doIt.was_hit = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 执行平均曲率统计计算</span></span><br><span class="line">avgcurvaturestatistics.execute()</span><br><span class="line"><span class="comment"># 平均曲率统计结果</span></span><br><span class="line">avgcurvaturestatisticsresult = avgcurvaturestatistics.results[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 查看平均曲率整体均值</span></span><br><span class="line">ssi2 = avgcurvaturestatisticsresult.all_interfaces.HxSpreadSheetInterface</span><br><span class="line"><span class="built_in">print</span>(ssi2.tables[<span class="number">0</span>].items[<span class="number">0</span>,<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># exec(open(&#x27;F://PYCharmWorkSpace//DigitalCoreFeatureExtraction//curvature//curvature_avizo_3d_test1.py&#x27;).read())</span></span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exec</span>(<span class="built_in">open</span>(<span class="string">&#x27;F://PYCharmWorkSpace//DigitalCoreFeatureExtraction//curvature//curvature_avizo_3d_test1.py&#x27;</span>).read())</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0.04535578936338425</span><br><span class="line">-0.028369586914777756</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AVIZO自动化</summary>
    
    
    
    <category term="AvizoUsersGuide" scheme="http://hibiscidai.com/categories/AvizoUsersGuide/"/>
    
    
    <category term="Avizo" scheme="http://hibiscidai.com/tags/Avizo/"/>
    
    <category term="石油地质" scheme="http://hibiscidai.com/tags/%E7%9F%B3%E6%B2%B9%E5%9C%B0%E8%B4%A8/"/>
    
    <category term="数字岩心" scheme="http://hibiscidai.com/tags/%E6%95%B0%E5%AD%97%E5%B2%A9%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-26H-7</title>
    <link href="http://hibiscidai.com/2024/08/20/PyTorch-26H-7/"/>
    <id>http://hibiscidai.com/2024/08/20/PyTorch-26H-7/</id>
    <published>2024-08-20T12:00:00.000Z</published>
    <updated>2024-08-22T14:17:05.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/20/PyTorch-26H-7/PyTorch-26H-7.png" class="" title="PyTorch-26H-7"><p>PyTorch-26H-7</p><span id="more"></span><h1 id="PyTorch-26H-7"><a href="#PyTorch-26H-7" class="headerlink" title="PyTorch-26H-7"></a>PyTorch-26H-7</h1><p>主页：<a href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p><p>youtub：<a href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p><p>github：<a href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p><p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p><p>PyTorch documentation：<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>]]></content>
    
    
    <summary type="html">PyTorch-26H-7</summary>
    
    
    
    <category term="PyTorch" scheme="http://hibiscidai.com/categories/PyTorch/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="PyTorch" scheme="http://hibiscidai.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-26H-6</title>
    <link href="http://hibiscidai.com/2024/08/19/PyTorch-26H-6/"/>
    <id>http://hibiscidai.com/2024/08/19/PyTorch-26H-6/</id>
    <published>2024-08-19T12:00:00.000Z</published>
    <updated>2024-08-22T14:16:32.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/19/PyTorch-26H-6/PyTorch-26H-6.png" class="" title="PyTorch-26H-6"><p>PyTorch-26H-6</p><span id="more"></span><h1 id="PyTorch-26H-6"><a href="#PyTorch-26H-6" class="headerlink" title="PyTorch-26H-6"></a>PyTorch-26H-6</h1><p>主页：<a href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p><p>youtub：<a href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p><p>github：<a href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p><p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p><p>PyTorch documentation：<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>]]></content>
    
    
    <summary type="html">PyTorch-26H-6</summary>
    
    
    
    <category term="PyTorch" scheme="http://hibiscidai.com/categories/PyTorch/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="PyTorch" scheme="http://hibiscidai.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-26H-5</title>
    <link href="http://hibiscidai.com/2024/08/18/PyTorch-26H-5/"/>
    <id>http://hibiscidai.com/2024/08/18/PyTorch-26H-5/</id>
    <published>2024-08-18T12:00:00.000Z</published>
    <updated>2024-08-22T14:16:00.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/18/PyTorch-26H-5/PyTorch-26H-5.png" class="" title="PyTorch-26H-5"><p>PyTorch-26H-5</p><span id="more"></span><h1 id="PyTorch-26H-5"><a href="#PyTorch-26H-5" class="headerlink" title="PyTorch-26H-5"></a>PyTorch-26H-5</h1><p>主页：<a href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p><p>youtub：<a href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p><p>github：<a href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p><p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p><p>PyTorch documentation：<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>]]></content>
    
    
    <summary type="html">PyTorch-26H-5</summary>
    
    
    
    <category term="PyTorch" scheme="http://hibiscidai.com/categories/PyTorch/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="PyTorch" scheme="http://hibiscidai.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-26H-4</title>
    <link href="http://hibiscidai.com/2024/08/17/PyTorch-26H-4/"/>
    <id>http://hibiscidai.com/2024/08/17/PyTorch-26H-4/</id>
    <published>2024-08-17T12:00:00.000Z</published>
    <updated>2024-08-22T14:15:33.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/17/PyTorch-26H-4/PyTorch-26H-4.png" class="" title="PyTorch-26H-4"><p>PyTorch-26H-4</p><span id="more"></span><h1 id="PyTorch-26H-4"><a href="#PyTorch-26H-4" class="headerlink" title="PyTorch-26H-4"></a>PyTorch-26H-4</h1><p>主页：<a href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p><p>youtub：<a href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p><p>github：<a href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p><p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p><p>PyTorch documentation：<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>]]></content>
    
    
    <summary type="html">PyTorch-26H-4</summary>
    
    
    
    <category term="PyTorch" scheme="http://hibiscidai.com/categories/PyTorch/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="PyTorch" scheme="http://hibiscidai.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-26H-3</title>
    <link href="http://hibiscidai.com/2024/08/16/PyTorch-26H-3/"/>
    <id>http://hibiscidai.com/2024/08/16/PyTorch-26H-3/</id>
    <published>2024-08-16T12:00:00.000Z</published>
    <updated>2024-10-22T10:21:44.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3.png" class="" title="PyTorch-26H-3"><p>PyTorch-26H-3</p><span id="more"></span><h1 id="PyTorch-26H-3"><a href="#PyTorch-26H-3" class="headerlink" title="PyTorch-26H-3"></a>PyTorch-26H-3</h1><p>主页：<a href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p><p>youtub：<a href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p><p>github：<a href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p><p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p><p>PyTorch documentation：<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><h1 id="What-is-a-classification-problem-什么是分类问题？"><a href="#What-is-a-classification-problem-什么是分类问题？" class="headerlink" title="What is a classification problem? 什么是分类问题？"></a>What is a classification problem? 什么是分类问题？</h1><p><a href="https://en.wikipedia.org/wiki/Statistical_classification">classification problem</a></p><div class="table-container"><table><thead><tr><th style="text-align:center">问题类型</th><th style="text-align:center">解释</th><th style="text-align:center">例子</th></tr></thead><tbody><tr><td style="text-align:center">二元分类(Binary classification)</td><td style="text-align:center">目标可以是两个选项之一，例如是或否</td><td style="text-align:center">根据某人的健康参数预测他是否患有心脏病。</td></tr><tr><td style="text-align:center">多类别分类(Multi-class classification)</td><td style="text-align:center">目标可以是两个以上选项之一</td><td style="text-align:center">确定照片中是食物、人还是狗。</td></tr><tr><td style="text-align:center">多标签分类(Multi-label classification)</td><td style="text-align:center">目标可以分配多个选项</td><td style="text-align:center">预测应为维基百科文章分配哪些类别（例如数学、科学和哲学）。</td></tr></tbody></table></div><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-1.png" class="" title="PyTorch-26H-3-1"><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-2.png" class="" title="PyTorch-26H-3-2"><p>分类和回归是最常见的机器学习问题类型之一。</p><p>换句话说，获取一组输入并预测该组输入属于哪个类别。</p><h1 id="What-we’re-going-to-cover-我们将要讨论的内容"><a href="#What-we’re-going-to-cover-我们将要讨论的内容" class="headerlink" title="What we’re going to cover 我们将要讨论的内容"></a>What we’re going to cover 我们将要讨论的内容</h1><ul><li>Architecture of a neural network classification model</li><li><p>神经网络分类模型的架构</p></li><li><p>Input shapes and output shapes of a classification model (features and labels)</p></li><li><p>分类模型的输入形状和输出形状（特征和标签）</p></li><li><p>Creating custom data to view, fit on and predict on</p></li><li><p>创建自定义数据以查看、拟合和预测</p></li><li><p>Steps in modelling</p></li><li><p>建模步骤</p></li><li><p>Creating a model, setting a loss function and optimiser, creating a training loop, evaluating a<br>model</p></li><li><p>创建模型、设置损失函数和优化器、创建训练循环、评估</p></li><li><p>Saving and loading models</p></li><li><p>保存和加载模型</p></li><li><p>Harnessing the power of non-linearity</p></li><li><p>利用非线性的力量</p></li><li><p>Different classification evaluation methods</p></li><li>不同的分类评估方法</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">话题</th><th style="text-align:center">内容</th></tr></thead><tbody><tr><td style="text-align:center">0. 分类神经网络的架构</td><td style="text-align:center">神经网络几乎可以具有任何形状和大小，但它们通常遵循类似的平面图。</td></tr><tr><td style="text-align:center">1. 准备二元分类数据</td><td style="text-align:center">数据几乎可以是任何东西，但首先我们将创建一个简单的二元分类数据集。</td></tr><tr><td style="text-align:center">2.构建 PyTorch 分类模型</td><td style="text-align:center">在这里我们将创建一个模型来学习数据中的模式，我们还将选择一个损失函数、优化器并构建一个特定于分类的训练循环。</td></tr><tr><td style="text-align:center">3. 将模型拟合到数据（训练）</td><td style="text-align:center">我们有数据和模型，现在让我们让模型（尝试）在（训练）数据中寻找模式。</td></tr><tr><td style="text-align:center">4. 做出预测并评估模型（推理）</td><td style="text-align:center">我们的模型在数据中发现了模式，让我们将它的发现与实际（测试）数据进行比较。</td></tr><tr><td style="text-align:center">5. 改进模型（从模型角度）</td><td style="text-align:center">我们已经训练并评估了一个模型，但它不起作用，让我们尝试一些方法来改进它。</td></tr><tr><td style="text-align:center">6.非线性</td><td style="text-align:center">到目前为止，我们的模型只具有对直线进行建模的能力，那么非线性（非直线）线又如何呢？</td></tr><tr><td style="text-align:center">7. 复制非线性函数</td><td style="text-align:center">我们使用非线性函数来帮助建模非线性数据，但是这些函数是什么样子的？</td></tr><tr><td style="text-align:center">8. 将所有内容与多类别分类结合起来</td><td style="text-align:center">让我们将迄今为止为二元分类所做的一切与多类分类问题放在一起。</td></tr></tbody></table></div><h1 id="0-Architecture-of-a-classification-neural-network-分类神经网络的架构"><a href="#0-Architecture-of-a-classification-neural-network-分类神经网络的架构" class="headerlink" title="0. Architecture of a classification neural network 分类神经网络的架构"></a>0. Architecture of a classification neural network 分类神经网络的架构</h1><p>分类神经网络的一般架构：</p><div class="table-container"><table><thead><tr><th style="text-align:center">超参数</th><th style="text-align:center">二元分类</th><th style="text-align:center">多类分类</th></tr></thead><tbody><tr><td style="text-align:center">输入层形状 Input layer shape (in_features)</td><td style="text-align:center">与特征数量相同（例如，心脏病预测中的年龄、性别、身高、体重、吸烟状况为 5）</td><td style="text-align:center">与二元分类相同</td></tr><tr><td style="text-align:center">隐藏层 Hidden layer(s)</td><td style="text-align:center">针对具体问题，最小值 = 1，最大值 = 无限制</td><td style="text-align:center">与二元分类相同</td></tr><tr><td style="text-align:center">每个隐藏层的神经元 Neurons per hidden layer</td><td style="text-align:center">具体问题具体分析，一般为 10 到 512</td><td style="text-align:center">与二元分类相同</td></tr><tr><td style="text-align:center">输出层形状 Output layer shape (out_features)</td><td style="text-align:center">1（一个类或另一个类）</td><td style="text-align:center">每类 1 张（例如，食物、人物或狗的照片各 3 张）</td></tr><tr><td style="text-align:center">隐藏层激活 Hidden layer activation</td><td style="text-align:center">通常是<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU</a>（整流线性单元），<a href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions">其他激活</a></td><td style="text-align:center">与二元分类相同</td></tr><tr><td style="text-align:center">输出激活 Output activation</td><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a> <a href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html">torch.sigmoid</a></td><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">torch.softmax</a></td></tr><tr><td style="text-align:center">损失函数 Loss function</td><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Cross-entropy#Cross-entropy_loss_function_and_logistic_regression">二元交叉熵Binary crossentropy</a> <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">torch.nn.BCELoss</a></td><td style="text-align:center">交叉熵 <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">torch.nn.CrossEntropyLoss</a></td></tr><tr><td style="text-align:center">优化器 Optimizer</td><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD stochastic gradient descent</a> ，<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam</a>，<a href="https://pytorch.org/docs/stable/optim.html">torch.optim</a></td><td style="text-align:center">与二元分类相同</td></tr></tbody></table></div><p>这个分类神经网络组件的成分列表会根据您正在处理的问题而有所不同。</p><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-2_1.png" class="" title="PyTorch-26H-3-2_1"><h1 id="1-Make-classification-data-and-get-it-ready-分类数据制作及准备"><a href="#1-Make-classification-data-and-get-it-ready-分类数据制作及准备" class="headerlink" title="1. Make classification data and get it ready 分类数据制作及准备"></a>1. Make classification data and get it ready 分类数据制作及准备</h1><p>使用 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html">make_circles()</a> 中的 <code>Scikit-Learn</code> 方法生成两个具有不同颜色的圆圈。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conda install scikit-learn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make 1000 samples </span></span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create circles</span></span><br><span class="line">X, y = make_circles(n_samples,</span><br><span class="line">                    noise = <span class="number">0.03</span>, <span class="comment"># a little bit of noise to the dots</span></span><br><span class="line">                    random_state = <span class="number">42</span>) <span class="comment"># keep random state so we get the same values</span></span><br></pre></td></tr></table></figure><p>查看前5个X值y。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First 5 X features:\n<span class="subst">&#123;X[:<span class="number">5</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nFirst 5 y labels:\n<span class="subst">&#123;y[:<span class="number">5</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">First 5 X features:</span><br><span class="line">[[ 0.75424625  0.23148074]</span><br><span class="line"> [-0.75615888  0.15325888]</span><br><span class="line"> [-0.81539193  0.17328203]</span><br><span class="line"> [-0.39373073  0.69288277]</span><br><span class="line"> [ 0.44220765 -0.89672343]]</span><br><span class="line"></span><br><span class="line">First 5 y labels:</span><br><span class="line">[1 1 1 1 0]</span><br></pre></td></tr></table></figure><p>可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make DataFrame of circle data</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">circles = pd.DataFrame(&#123;<span class="string">&quot;X1&quot;</span>: X[:, <span class="number">0</span>],</span><br><span class="line">       <span class="string">&quot;X2&quot;</span>: X[:, <span class="number">1</span>],</span><br><span class="line">       <span class="string">&quot;label&quot;</span>: y&#125;)</span><br><span class="line">circles.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X1X2label</span><br><span class="line">00.7542460.2314811</span><br><span class="line">1-0.7561590.1532591</span><br><span class="line">2-0.8153920.1732821</span><br><span class="line">3-0.3937310.6928831</span><br><span class="line">40.442208-0.8967230</span><br><span class="line">5-0.4796460.6764351</span><br><span class="line">6-0.0136480.8033491</span><br><span class="line">70.7715130.1477601</span><br><span class="line">8-0.169322-0.7934561</span><br><span class="line">9-0.1214861.0215090</span><br></pre></td></tr></table></figure><p>看起来每对X特征（X1和X2）都有一个标签（y）值，即 0 或 1。</p><p>这告诉我们我们的问题是二元分类，因为只有两个选项（0 或 1）。</p><p>每个类别有多少个值？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check different labels</span></span><br><span class="line">circles.label.value_counts()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1    500</span><br><span class="line">0    500</span><br><span class="line">Name: label, dtype: int64</span><br></pre></td></tr></table></figure><p>0和1各五百个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize with a plot</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(x=X[:, <span class="number">0</span>], </span><br><span class="line">            y=X[:, <span class="number">1</span>], </span><br><span class="line">            c=y, </span><br><span class="line">            cmap=plt.cm.RdYlBu);</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-3.png" class="" title="PyTorch-26H-3-3"><p>如何构建 PyTorch 神经网络来将点分类为红色（0）或蓝色（1）。</p><blockquote><p>在机器学习中，这个数据集通常被视为玩具问题（用于尝试和测试事物的问题）。但它代表了分类的主要关键，您有一些以数值表示的数据，并且您想要构建一个能够对其进行分类的模型，在我们的例子中，将其分成红点或蓝点。</p></blockquote><p><a href="https://scikit-learn.org/1.5/datasets/toy_dataset.html">scikit-learn-toy datasets</a></p><h2 id="1-1-Input-and-output-shapes-输入和输出形状"><a href="#1-1-Input-and-output-shapes-输入和输出形状" class="headerlink" title="1.1 Input and output shapes 输入和输出形状"></a>1.1 Input and output shapes 输入和输出形状</h2><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-3_1.png" class="" title="PyTorch-26H-3-3_1"><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-3_2.png" class="" title="PyTorch-26H-3-3_2"><p>可以设置32的batch size。使用大型 minibatch 进行训练对测试错误不利。</p><p>深度学习中最常见的错误之一是形状错误。</p><p>张量形状和张量运算不匹配将导致模型出现错误。</p><p>我们将会在整个课程中看到很多这样的情况。</p><p>输入和输出形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the shapes of our features and labels</span></span><br><span class="line">X.shape, y.shape</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((1000, 2), (1000,))</span><br></pre></td></tr></table></figure><p>看起来我们在每个维度的第一维度上都找到了匹配项。</p><p>有 1000 个 X 和 1000 个 y。</p><p>但是 X 的第二维度是什么？</p><p>查看单个样本（特征和标签）的值和形状通常很有帮助。</p><p>这样做将帮助您了解您希望从模型中获得什么样的输入和输出形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View the first example of features and labels</span></span><br><span class="line">X_sample = X[<span class="number">0</span>]</span><br><span class="line">y_sample = y[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Values for one sample of X: <span class="subst">&#123;X_sample&#125;</span> and the same for y: <span class="subst">&#123;y_sample&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shapes for one sample of X: <span class="subst">&#123;X_sample.shape&#125;</span> and the same for y: <span class="subst">&#123;y_sample.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1</span><br><span class="line">Shapes for one sample of X: (2,) and the same for y: ()</span><br></pre></td></tr></table></figure><p>这告诉我们 X 的第二个维度意味着它有两个特征（向量vector），而 y 只有一个特征（标量scalar）。</p><p>我们有两个输入和一个输出。</p><h2 id="1-2-Turn-data-into-tensors-and-create-train-and-test-splits-将数据转换为张量并创建训练和测试分割"><a href="#1-2-Turn-data-into-tensors-and-create-train-and-test-splits-将数据转换为张量并创建训练和测试分割" class="headerlink" title="1.2 Turn data into tensors and create train and test splits 将数据转换为张量并创建训练和测试分割"></a>1.2 Turn data into tensors and create train and test splits 将数据转换为张量并创建训练和测试分割</h2><p>1、将我们的数据转换成张量（现在我们的数据在 NumPy 数组中，PyTorch 更喜欢使用 PyTorch 张量）。<br>2、<code>X</code>将我们的数据分成训练集和测试集（我们将在训练集上训练一个模型来学习和之间的模式，<code>y</code>然后在测试数据集上评估这些学习到的模式）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn data into tensors</span></span><br><span class="line"><span class="comment"># Otherwise this causes issues with computations later on</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X = torch.from_numpy(X).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line">y = torch.from_numpy(y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the first five samples</span></span><br><span class="line">X[:<span class="number">5</span>], y[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.7542,  0.2315],</span><br><span class="line">         [-0.7562,  0.1533],</span><br><span class="line">         [-0.8154,  0.1733],</span><br><span class="line">         [-0.3937,  0.6929],</span><br><span class="line">         [ 0.4422, -0.8967]]),</span><br><span class="line"> tensor([1., 1., 1., 1., 0.]))</span><br></pre></td></tr></table></figure><p>现在我们的数据是张量格式，让我们将其分成训练集和测试集。</p><p>使用 Scikit-Learn 中 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split()</a> 函数。</p><p>我们将使用<code>test_size=0.2</code>（80％训练，20％测试），并且由于分割在数据中随机发生，<code>random_state=42</code>因此我们使用可重现的分割。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split data into train and test sets</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, </span><br><span class="line">                                                    y, </span><br><span class="line">                                                    test_size=<span class="number">0.2</span>, <span class="comment"># 20% test, 80% train</span></span><br><span class="line">                                                    random_state=<span class="number">42</span>) <span class="comment"># make the random split reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(800, 200, 800, 200)</span><br></pre></td></tr></table></figure><p>现在有 800 个训练样本和 200 个测试样本。</p><h1 id="2-Building-a-model-建立模型"><a href="#2-Building-a-model-建立模型" class="headerlink" title="2. Building a model 建立模型"></a>2. Building a model 建立模型</h1><p>模型需要分为几个部分。</p><p>1、设置与设备无关的代码（这样我们的模型可以在 CPU 或 GPU 上运行）。<br>2、通过子类化构建模型 <code>nn.Module</code>。<br>3、定义损失函数和优化器。<br>4、创建训练循环。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Standard PyTorch imports</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make device agnostic code</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">device</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;cuda&#x27;</span><br></pre></td></tr></table></figure><p>我们需要一个模型，能够处理我们的<code>X</code>数据作为输入，并生成与我们的数据形状相同<code>y</code>的输出。</p><p>换句话说，给定<code>X</code>（特征feature），我们希望我们的模型预测<code>y</code>（标签label）。</p><p>这种具有特征和标签的设置称为<code>监督学习</code>。因为你的数据会告诉你的模型，给定某个输入，应该得到什么样的输出。</p><p>要创建这样的模型，需要处理<code>X</code>和的输入和输出形状<code>y</code>。</p><p>创建一个模型类：</p><p>1、子类 <code>nn.Module</code>（几乎所有 PyTorch 模型都是 <code>nn.Module</code> 的子类）。<br>2、在构造函数中创建 2 个 <code>nn.Linear</code> 层，能够处理 <code>X</code> 和 <code>y</code> 的输入和输出形状。<br>3、定义一个包含模型前向传递计算的 <code>forward()</code> 方法。<br>4、实例化模型类并将其发送到目标设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Construct a model class that subclasses nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CircleModelV0</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes</span></span><br><span class="line">        self.layer_1 = nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">5</span>) <span class="comment"># takes in 2 features (X), produces 5 features</span></span><br><span class="line">        self.layer_2 = nn.Linear(in_features=<span class="number">5</span>, out_features=<span class="number">1</span>) <span class="comment"># takes in 5 features, produces 1 feature (y)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. Define a forward method containing the forward pass computation</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Return the output of layer_2, a single feature, the same shape as y</span></span><br><span class="line">        <span class="keyword">return</span> self.layer_2(self.layer_1(x)) <span class="comment"># computation goes through layer_1 first then the output of layer_1 goes through layer_2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create an instance of the model and send it to target device</span></span><br><span class="line">model_0 = CircleModelV0().to(device)</span><br><span class="line">model_0</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CircleModelV0(</span><br><span class="line">  (layer_1): Linear(in_features=2, out_features=5, bias=True)</span><br><span class="line">  (layer_2): Linear(in_features=5, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>唯一的重大变化是 <code>self.layer_1</code> 和 <code>self.layer_2</code> 之间发生的事情。</p><p><code>self.layer_1</code> 接受 2 个输入特征 <code>in_features=2</code> 并产生 5 个输出特征 <code>out_features=5</code>。</p><p>这被称为具有 5 个隐藏单元或神经元。该层将输入数据从 2 个特征变为 5 个特征。</p><p>这使得模型可以从 5 个数字而不是仅仅 2 个数字中学习模式，从而可能产生更好的输出。</p><p>在神经网络层中使用的隐藏单元的数量是一个<code>超参数</code>（可以自己设置的值），并且没有必须使用的固定值。<br>通常情况下，数量越多越好，但也可能太多。您选择的数量取决于您的模型类型和您正在使用的数据集。</p><p>由于我们的数据集很小而且简单，因此我们会将其保持较小。</p><p>隐藏单元的唯一规则是下一层（在我们的例子中为 <code>self.layer_2</code>）必须采用与前一层 <code>out_features</code> 相同的 <code>in_features</code>。</p><p>这就是为什么 <code>self.layer_2</code> 有 <code>in_features=5</code>，它从 <code>self.layer_1</code> 中获取 <code>out_features=5</code> 并对它们执行线性计算，将它们转换为 <code>out_features=1</code>（与 y 相同的形状）。</p><p>与我们刚刚构建的分类神经网络类似的视觉示例。尝试在 <a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.57514&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">TensorFlow Playground</a> 网站上创建一个您自己的神经网络。</p><p>您也可以使用 <code>nn.Sequential</code> 执行与上述相同的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replicate CircleModelV0 with nn.Sequential</span></span><br><span class="line">model_0 = nn.Sequential(</span><br><span class="line">    nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">5</span>),</span><br><span class="line">    nn.Linear(in_features=<span class="number">5</span>, out_features=<span class="number">1</span>)</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line">model_0</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Linear(in_features=2, out_features=5, bias=True)</span><br><span class="line">  (1): Linear(in_features=5, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这看起来比子类化简单多了 <code>nn.Module</code>，为什么不总是使用呢<code>nn.Sequential</code>？</p><p><code>nn.Sequential</code>对于直接计算来说非常棒，但是，正如命名空间所说，它总是按顺序运行。</p><p>因此，如果您希望发生其他事情（而不仅仅是直接的顺序计算），您将需要定义自己的自定义<code>nn.Module</code>子类。</p><p>现们有一个模型，让我们看看当我们通过它传递一些数据时会发生什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions with the model</span></span><br><span class="line">untrained_preds = model_0(X_test.to(device))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of predictions: <span class="subst">&#123;<span class="built_in">len</span>(untrained_preds)&#125;</span>, Shape: <span class="subst">&#123;untrained_preds.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of test samples: <span class="subst">&#123;<span class="built_in">len</span>(y_test)&#125;</span>, Shape: <span class="subst">&#123;y_test.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nFirst 10 predictions:\n<span class="subst">&#123;untrained_preds[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nFirst 10 test labels:\n<span class="subst">&#123;y_test[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Length of predictions: 200, Shape: torch.Size([200, 1])</span><br><span class="line">Length of test samples: 200, Shape: torch.Size([200])</span><br><span class="line"></span><br><span class="line">First 10 predictions:</span><br><span class="line">tensor([[-0.1631],</span><br><span class="line">        [-0.4051],</span><br><span class="line">        [ 0.3693],</span><br><span class="line">        [-0.3135],</span><br><span class="line">        [ 0.2072],</span><br><span class="line">        [ 0.0607],</span><br><span class="line">        [-0.4923],</span><br><span class="line">        [-0.3836],</span><br><span class="line">        [ 0.3753],</span><br><span class="line">        [-0.4231]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">First 10 test labels:</span><br><span class="line">tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])</span><br></pre></td></tr></table></figure><p>预测的数量与测试标签的数量相同，但是预测的形式或形状看起来与测试标签不一样。</p><h2 id="2-1-Setup-loss-function-and-optimizer"><a href="#2-1-Setup-loss-function-and-optimizer" class="headerlink" title="2.1 Setup loss function and optimizer"></a>2.1 Setup loss function and optimizer</h2><p>不同类型的问题需要不同的损失函数。</p><p>回归问题（预测数字）：使用平均绝对误差（MAE）损失。<br>二元分类问题（目前的问题），使用<a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">二元交叉熵</a>作为损失函数。</p><p>相同的优化器函数通常可用于不同的问题空间。</p><p>随机梯度下降优化器（<code>SGD，torch.optim.SGD()</code>）可用于解决一系列问题，Adam 优化器（<code>torch.optim.Adam()</code>）同样适用。</p><div class="table-container"><table><thead><tr><th style="text-align:center">损失函数/优化器</th><th style="text-align:center">问题类型</th><th style="text-align:center">PyTorch代码</th></tr></thead><tbody><tr><td style="text-align:center">随机梯度下降（SGD）优化器</td><td style="text-align:center">分类、回归等</td><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">torch.optim.SGD()</a></td></tr><tr><td style="text-align:center">Adam 优化器</td><td style="text-align:center">分类、回归等</td><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">torch.optim.Adam()</a></td></tr><tr><td style="text-align:center">二元交叉熵损失</td><td style="text-align:center">二元分类</td><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">torch.nn.BCELossWithLogits</a> 或者 <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">torch.nn.BCELoss</a></td></tr><tr><td style="text-align:center">交叉熵损失</td><td style="text-align:center">多类别分类</td><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">torch.nn.CrossEntropyLoss</a></td></tr><tr><td style="text-align:center">平均绝对误差 (MAE) 或 L1 损失</td><td style="text-align:center">回归</td><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html">torch.nn.L1Loss</a></td></tr><tr><td style="text-align:center">均方误差 (MSE) 或 L2 损失</td><td style="text-align:center">回归</td><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss">torch.nn.MSELoss</a></td></tr></tbody></table></div><p>由于我们正在处理二元分类问题，因此我们使用二元交叉熵损失函数。</p><p>PyTorch 有两种二元交叉熵实现：</p><p>1、<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">torch.nn.BCELoss()</a>- 创建一个损失函数，测量目标（标签）和输入（特征）之间的二元交叉熵。<br>2、<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">torch.nn.BCEWithLogitsLoss()</a> - 这与上面的相同，只是它有一个内置的 sigmoid 层 (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html">nn.Sigmoid</a>)</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">torch.nn.BCEWithLogitsLoss()</a> 的<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">文档</a>指出，它比在 <code>nn.Sigmoid</code> 层之后使用 <code>torch.nn.BCELoss()</code> 更具数值稳定性。</p><p>通常，实现 2 是更好的选择。但是对于高级用法，可能希望分离 <code>nn.Sigmoid</code> 和 <code>torch.nn.BCELoss()</code> 的组合，但这超出了本笔记本的范围。</p><p>了解了这一点，让我们创建一个损失函数和一个优化器。</p><p>对于优化器，我们将使用 <code>torch.optim.SGD()</code> 以学习率为 0.1 来优化模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a loss function</span></span><br><span class="line"><span class="comment"># loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in</span></span><br><span class="line">loss_fn = nn.BCEWithLogitsLoss() <span class="comment"># BCEWithLogitsLoss = sigmoid built-in</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_0.parameters(), </span><br><span class="line">                            lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>现在让我们创建一个<code>评估指标</code>。<br>评估指标可用于提供模型运行情况的另一个视角。<br>如果损失函数衡量模型的错误程度，我喜欢将评估指标视为衡量模型的正确程度。<br>当然，您可以说这两者都在做同样的事情，但评估指标提供了不同的视角。<br>毕竟，在评估模型时，最好从多个角度看待事物。<br>有几种评估指标可用于分类问题，但让我们从准确度开始。<br>准确度可以通过将正确预测的总数除以预测总数来衡量。<br>例如，如果一个模型在 100 个预测中做出 99 个正确的预测，则准确度为 99%。<br>让我们编写一个函数来实现这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate accuracy (a classification metric)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy_fn</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    correct = torch.eq(y_true, y_pred).<span class="built_in">sum</span>().item() <span class="comment"># torch.eq() calculates where two tensors are equal</span></span><br><span class="line">    acc = (correct / <span class="built_in">len</span>(y_pred)) * <span class="number">100</span> </span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><p>太棒了！我们现在可以在训练模型时使用此功能来测量其性能和损失。</p><h1 id="3-Train-model"><a href="#3-Train-model" class="headerlink" title="3. Train model"></a>3. Train model</h1><p>PyTorch 训练循环步骤：</p><p>1、前向传递 Forward pass - 模型对所有训练数据进行一次遍历，执行其 forward() 函数计算 (<code>model(x_train)</code>)。<br>2、计算损失 Calculate the loss  - 将模型的输出 (预测) 与基本事实进行比较，并进行评估以查看其错误程度 (<code>loss = loss_fn(y_pred, y_train)</code>)。<br>3、零梯度 Zero gradients  - 优化器梯度设置为零 (默认情况下是累积的)，因此可以为特定的训练步骤重新计算 (<code>optimizer.zero_grad()</code>)。<br>4、对损失执行反向传播 Perform backpropagation on the loss - 针对要更新的每个模型参数 (每个参数的 require_grad=True) 计算损失的梯度。这称为反向传播，因此为“向后”(<code>loss.backward()</code>)。<br>5、步进优化器 (梯度下降) Step the optimizer (gradient descent)  - 使用 <code>require_grad=True</code> 更新参数，以根据损失梯度改进它们 (<code>optimizer.step()</code>)。</p><h2 id="3-1-Going-from-raw-model-outputs-to-predicted-labels-logits-gt-prediction-probabilities-gt-prediction-labels-从原始模型输出到预测标签（logits-gt-预测概率-gt-预测标签）"><a href="#3-1-Going-from-raw-model-outputs-to-predicted-labels-logits-gt-prediction-probabilities-gt-prediction-labels-从原始模型输出到预测标签（logits-gt-预测概率-gt-预测标签）" class="headerlink" title="3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels) 从原始模型输出到预测标签（logits -&gt; 预测概率 -&gt; 预测标签）"></a>3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels) 从原始模型输出到预测标签（logits -&gt; 预测概率 -&gt; 预测标签）</h2><p>在训练循环步骤之前，让我们看看在前向传递过程中我们的模型会产生什么结果（前向传递由方法定义<code>forward()</code>）。</p><p>为此，让我们向模型传递一些数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View the frist 5 outputs of the forward pass on the test data</span></span><br><span class="line">y_logits = model_0(X_test.to(device))[:<span class="number">5</span>]</span><br><span class="line">y_logits</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1631],</span><br><span class="line">        [-0.4051],</span><br><span class="line">        [ 0.3693],</span><br><span class="line">        [-0.3135],</span><br><span class="line">        [ 0.2072]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><p>由于我们的模型尚未经过训练，这些输出基本上是随机的。</p><p>但它们是什么？它们是我们的 <code>forward()</code> 方法的输出。</p><p>它实现了两层 <code>nn.Linear()</code>，它在内部调用以下方程：</p><script type="math/tex; mode=display">\mathbf{y} = x \cdot \mathbf{Weights}^T + \mathbf{bias}</script><p>该方程（$\mathbf{y}$）的原始输出（未修改），反过来，我们模型的原始输出通常被称为<code>logits</code>。</p><p>这就是我们的模型在接受输入数据（等式中的 x 或代码中的 <code>X_test</code>）时输出的内容，<code>logits</code>。</p><p>然而，这些数字很难解释。</p><p>我们希望有一些数字可以与我们的真实标签相媲美。</p><p>为了将我们模型的原始输出（logits）变成这种形式，我们可以使用 <a href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html">sigmoid 激活函数</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use sigmoid on model logits</span></span><br><span class="line">y_pred_probs = torch.sigmoid(y_logits)</span><br><span class="line">y_pred_probs</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4593],</span><br><span class="line">        [0.4001],</span><br><span class="line">        [0.5913],</span><br><span class="line">        [0.4223],</span><br><span class="line">        [0.5516]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SigmoidBackward0&gt;)</span><br></pre></td></tr></table></figure><p>看起来输出现在具有某种一致性（即使它们仍然是随机的）。</p><p>它们现在采用<code>预测概率</code>的形式（我通常将其称为 <code>y_pred_probs</code>），换句话说，这些值现在是模型认为数据点属于一个类或另一个类的程度。</p><p>在我们的例子中，由于我们正在处理二元分类，所以我们的理想输出是 0 或 1。</p><p>因此这些值可以被视为决策边界。</p><p>越接近0，模型越认为该样本属于0类，越接近1，模型越认为该样本属于1类。</p><p>更具体地说：</p><p>如果 <code>y_pred_probs&gt;= 0.5</code>，<code>y=1</code>（第 1 类）<br>如果 <code>y_pred_probs&lt; 0.5</code>，<code>y=0</code>（0 类）</p><p>为了将我们的预测概率转化为预测标签，我们可以对 sigmoid  激活函数的输出进行四舍五入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find the predicted labels (round the prediction probabilities)</span></span><br><span class="line">y_preds = torch.<span class="built_in">round</span>(y_pred_probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In full</span></span><br><span class="line">y_pred_labels = torch.<span class="built_in">round</span>(torch.sigmoid(model_0(X_test.to(device))[:<span class="number">5</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check for equality</span></span><br><span class="line"><span class="built_in">print</span>(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get rid of extra dimension</span></span><br><span class="line">y_preds.squeeze()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True, True], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([0., 0., 1., 0., 1.], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SqueezeBackward0&gt;)</span><br></pre></td></tr></table></figure><p>现在看起来我们的模型的预测与我们的真实标签 ( <code>y_test</code>) 的形式相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_test[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 0., 1., 0., 1.])</span><br></pre></td></tr></table></figure><p>这意味着我们将能够将模型的预测与测试标签进行比较，以了解其表现如何。</p><p>回顾一下，我们使用 sigmoid 激活函数将模型的原始输出 (logits) 转换为预测概率。</p><p>然后通过四舍五入将预测概率转换为预测标签。</p><blockquote><p>注意：sigmoid 激活函数通常仅用于二分类 logits。对于多类分类，我们将考虑使用 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">softmax 激活函数</a>。<br>将模型的原始输出传递给 nn.BCEWithLogitsLoss 时不需要使用 sigmoid激活函数（logits loss 中的“logits”是因为它适用于模型的原始 logits 输出），这是因为它内置了 sigmoid函数。</p></blockquote><h2 id="3-2-Building-a-training-and-testing-loop-建立训练和测试循环"><a href="#3-2-Building-a-training-and-testing-loop-建立训练和测试循环" class="headerlink" title="3.2 Building a training and testing loop 建立训练和测试循环"></a>3.2 Building a training and testing loop 建立训练和测试循环</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data to target device</span></span><br><span class="line">X_train, y_train = X_train.to(device), y_train.to(device)</span><br><span class="line">X_test, y_test = X_test.to(device), y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build training and evaluation loop</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_0.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass (model outputs raw logits)</span></span><br><span class="line">    y_logits = model_0(X_train).squeeze() <span class="comment"># squeeze to remove extra `1` dimensions, this won&#x27;t work unless model and data are on same device </span></span><br><span class="line">    y_pred = torch.<span class="built_in">round</span>(torch.sigmoid(y_logits)) <span class="comment"># turn logits -&gt; pred probs -&gt; pred labls</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 2. Calculate loss/accuracy</span></span><br><span class="line">    <span class="comment"># loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()</span></span><br><span class="line">    <span class="comment">#                y_train) </span></span><br><span class="line">    loss = loss_fn(y_logits, <span class="comment"># Using nn.BCEWithLogitsLoss works with raw logits</span></span><br><span class="line">                   y_train) </span><br><span class="line">    acc = accuracy_fn(y_true=y_train, </span><br><span class="line">                      y_pred=y_pred) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_0.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        <span class="comment"># 1. Forward pass</span></span><br><span class="line">        test_logits = model_0(X_test).squeeze() </span><br><span class="line">        test_pred = torch.<span class="built_in">round</span>(torch.sigmoid(test_logits))</span><br><span class="line">        <span class="comment"># 2. Caculate loss/accuracy</span></span><br><span class="line">        test_loss = loss_fn(test_logits,</span><br><span class="line">                            y_test)</span><br><span class="line">        test_acc = accuracy_fn(y_true=y_test,</span><br><span class="line">                               y_pred=test_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening every 10 epochs</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Accuracy: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% | Test loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>, Test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Loss: 0.71041, Accuracy: 49.50% | Test loss: 0.69582, Test acc: 53.00%</span><br><span class="line">Epoch: 10 | Loss: 0.70593, Accuracy: 49.12% | Test loss: 0.69352, Test acc: 54.00%</span><br><span class="line">Epoch: 20 | Loss: 0.70281, Accuracy: 49.25% | Test loss: 0.69213, Test acc: 54.00%</span><br><span class="line">Epoch: 30 | Loss: 0.70055, Accuracy: 49.12% | Test loss: 0.69132, Test acc: 54.50%</span><br><span class="line">Epoch: 40 | Loss: 0.69888, Accuracy: 49.12% | Test loss: 0.69087, Test acc: 54.00%</span><br><span class="line">Epoch: 50 | Loss: 0.69762, Accuracy: 48.75% | Test loss: 0.69066, Test acc: 53.00%</span><br><span class="line">Epoch: 60 | Loss: 0.69666, Accuracy: 48.75% | Test loss: 0.69061, Test acc: 53.50%</span><br><span class="line">Epoch: 70 | Loss: 0.69592, Accuracy: 48.88% | Test loss: 0.69067, Test acc: 54.50%</span><br><span class="line">Epoch: 80 | Loss: 0.69535, Accuracy: 49.00% | Test loss: 0.69079, Test acc: 54.00%</span><br><span class="line">Epoch: 90 | Loss: 0.69489, Accuracy: 49.00% | Test loss: 0.69096, Test acc: 54.00%</span><br></pre></td></tr></table></figure><p>每次数据分割的准确率几乎不超过 50%。</p><p>因为我们正在处理平衡的二元分类问题，所以这意味着我们的模型表现与随机猜测一样好（有 500 个 0 类和 1 类样本，每次预测 1 类的模型准确率都会达到 50%）。</p><h1 id="4-Make-predictions-and-evaluate-the-model-做出预测并评估模型"><a href="#4-Make-predictions-and-evaluate-the-model-做出预测并评估模型" class="headerlink" title="4. Make predictions and evaluate the model 做出预测并评估模型"></a>4. Make predictions and evaluate the model 做出预测并评估模型</h1><p>对于50%准确率的模型，几乎等于瞎猜。</p><p>我们将编写一些代码，从Learn PyTorch for Deep Learning仓库下载并导入helper_functions.py脚本。</p><p>它包含一个名为plot_decision_boundary（）的有用函数，该函数创建了一个NumPy网格，以直观地绘制我们的模型预测某些类的不同点。</p><p>将结果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path </span><br><span class="line"></span><br><span class="line"><span class="comment"># Download helper functions from Learn PyTorch repo (if not already downloaded)</span></span><br><span class="line"><span class="keyword">if</span> Path(<span class="string">&quot;helper_functions.py&quot;</span>).is_file():</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;helper_functions.py already exists, skipping download&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;Downloading helper_functions.py&quot;</span>)</span><br><span class="line">  request = requests.get(<span class="string">&quot;https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py&quot;</span>)</span><br><span class="line">  <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;helper_functions.py&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(request.content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> helper_functions <span class="keyword">import</span> plot_predictions, plot_decision_boundary</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot decision boundaries for training and test sets</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_0, X_train, y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_0, X_test, y_test)</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-4.png" class="" title="PyTorch-26H-3-4"><p>由于数据是圆形的，因此画一条直线最多只能将其从中间切开。</p><p>用机器学习术语来说，模型拟合不足<code>underfitting</code>，意味着它没有从数据中学习预测模式。</p><h1 id="5-Improving-a-model-from-a-model-perspective-改进模型（从模型角度）"><a href="#5-Improving-a-model-from-a-model-perspective-改进模型（从模型角度）" class="headerlink" title="5. Improving a model (from a model perspective) 改进模型（从模型角度）"></a>5. Improving a model (from a model perspective) 改进模型（从模型角度）</h1><p>修复模型的欠拟合问题。</p><p>特别关注模型（而不是数据），我们可以通过几种方式来做到这一点。</p><div class="table-container"><table><thead><tr><th style="text-align:center">模型改进技术</th><th style="text-align:center">作用</th></tr></thead><tbody><tr><td style="text-align:center">添加更多层 Add more layers</td><td style="text-align:center">每一层都可能增加模型的学习能力，因为每一层都能够学习数据中的某种新模式。更多层通常被称为使神经网络更深。</td></tr><tr><td style="text-align:center">添加更多隐藏单元 Add more hidden units</td><td style="text-align:center">与上述类似，每层隐藏单元越多，模型的学习能力就越强。更多隐藏单元通常被称为使神经网络更宽。</td></tr><tr><td style="text-align:center">更长训练时间（更多循环） Fitting for longer (more epochs)</td><td style="text-align:center">如果您的模型有更多机会查看数据，它可能会学到更多东西。</td></tr><tr><td style="text-align:center">改变激活函数 Changing the activation functions</td><td style="text-align:center">有些数据无法仅用直线来拟合（就像我们所看到的），使用非线性激活函数可以帮助解决这个问题（提示，提示）。</td></tr><tr><td style="text-align:center">改变学习率 Change the learning rate</td><td style="text-align:center">虽然与模型不太相关，但仍然相关，优化器的学习率决定了模型每一步应该改变多少参数，太多则模型过度修正，太少则学习不够。</td></tr><tr><td style="text-align:center">改变损失函数 Change the loss function</td><td style="text-align:center">同样，虽然模型特定性不强但仍然很重要，不同的问题需要不同的损失函数。例如，二元交叉熵损失函数不适用于多类分类问题。</td></tr><tr><td style="text-align:center">使用迁移学习 Use transfer learning</td><td style="text-align:center">从与您的问题领域类似的问题中获取预训练模型，并根据您自己的问题进行调整。</td></tr></tbody></table></div><blockquote><p>可以手动调整→超参数<br>机器学习为一半科学一半艺术，需要通过不断实验进行。</p></blockquote><p>让我们看看如果我们在模型中添加一个额外的层，适应更长的时间（<code>epochs=1000</code> 而不是 <code>epochs=100</code>），并将隐藏单元的数量从 <code>5</code> 增加到 <code>10</code>，会发生什么。</p><p>我们将遵循上述相同的步骤，但会更改一些超参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CircleModelV1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer_1 = nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        self.layer_2 = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>) <span class="comment"># extra layer</span></span><br><span class="line">        self.layer_3 = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># note: always make sure forward is spelt correctly!</span></span><br><span class="line">        <span class="comment"># Creating a model like this is the same as below, though below</span></span><br><span class="line">        <span class="comment"># generally benefits from speedups where possible.</span></span><br><span class="line">        <span class="comment"># z = self.layer_1(x)</span></span><br><span class="line">        <span class="comment"># z = self.layer_2(z)</span></span><br><span class="line">        <span class="comment"># z = self.layer_3(z)</span></span><br><span class="line">        <span class="comment"># return z</span></span><br><span class="line">        <span class="keyword">return</span> self.layer_3(self.layer_2(self.layer_1(x)))</span><br><span class="line"></span><br><span class="line">model_1 = CircleModelV1().to(device)</span><br><span class="line">model_1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CircleModelV1(</span><br><span class="line">  (layer_1): Linear(in_features=2, out_features=10, bias=True)</span><br><span class="line">  (layer_2): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">  (layer_3): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>现在我们有了一个模型，我们将使用与之前相同的设置重新创建一个损失函数和优化器实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># loss_fn = nn.BCELoss() # Requires sigmoid on input</span></span><br><span class="line">loss_fn = nn.BCEWithLogitsLoss() <span class="comment"># Does not require sigmoid on input</span></span><br><span class="line">optimizer = torch.optim.SGD(model_1.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>这次我们将进行更长时间的训练（epochs=1000 vs epochs=100），看看它是否能改进我们的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">1000</span> <span class="comment"># Train for longer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data to target device</span></span><br><span class="line">X_train, y_train = X_train.to(device), y_train.to(device)</span><br><span class="line">X_test, y_test = X_test.to(device), y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_logits = model_1(X_train).squeeze()</span><br><span class="line">    y_pred = torch.<span class="built_in">round</span>(torch.sigmoid(y_logits)) <span class="comment"># logits -&gt; prediction probabilities -&gt; prediction labels</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate loss/accuracy</span></span><br><span class="line">    loss = loss_fn(y_logits, y_train)</span><br><span class="line">    acc = accuracy_fn(y_true=y_train, </span><br><span class="line">                      y_pred=y_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_1.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        <span class="comment"># 1. Forward pass</span></span><br><span class="line">        test_logits = model_1(X_test).squeeze() </span><br><span class="line">        test_pred = torch.<span class="built_in">round</span>(torch.sigmoid(test_logits))</span><br><span class="line">        <span class="comment"># 2. Caculate loss/accuracy</span></span><br><span class="line">        test_loss = loss_fn(test_logits,</span><br><span class="line">                            y_test)</span><br><span class="line">        test_acc = accuracy_fn(y_true=y_test,</span><br><span class="line">                               y_pred=test_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening every 10 epochs</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Accuracy: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% | Test loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>, Test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%</span><br><span class="line">Epoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%</span><br><span class="line">Epoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%</span><br><span class="line">Epoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%</span><br><span class="line">Epoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%</span><br><span class="line">Epoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%</span><br><span class="line">Epoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</span><br><span class="line">Epoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</span><br><span class="line">Epoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</span><br><span class="line">Epoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</span><br></pre></td></tr></table></figure><p>我们的模型训练的时间更长，并且增加了一层，但它看起来仍然没有学到比随机猜测更好的模式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot decision boundaries for training and test sets</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_1, X_train, y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_1, X_test, y_test)</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-5.png" class="" title="PyTorch-26H-3-5"><p>我们的模型仍然在红点和蓝点之间画一条直线。</p><p>如果我们的模型画的是直线，那么它能模拟线性数据吗？</p><h2 id="5-1-Preparing-data-to-see-if-our-model-can-model-a-straight-line-准备数据，看看我们的模型是否能建模直线"><a href="#5-1-Preparing-data-to-see-if-our-model-can-model-a-straight-line-准备数据，看看我们的模型是否能建模直线" class="headerlink" title="5.1 Preparing data to see if our model can model a straight line 准备数据，看看我们的模型是否能建模直线"></a>5.1 Preparing data to see if our model can model a straight line 准备数据，看看我们的模型是否能建模直线</h2><p>创建一些线性数据来看看我们的模型是否能够对其进行建模，而不仅仅是使用一个无法学习任何东西的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some data (same as notebook 01)</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data</span></span><br><span class="line">X_regression = torch.arange(start, end, step).unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">y_regression = weight * X_regression + bias <span class="comment"># linear regression formula</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(X_regression))</span><br><span class="line">X_regression[:<span class="number">5</span>], y_regression[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">100</span><br><span class="line">(tensor([[0.0000],</span><br><span class="line">         [0.0100],</span><br><span class="line">         [0.0200],</span><br><span class="line">         [0.0300],</span><br><span class="line">         [0.0400]]),</span><br><span class="line"> tensor([[0.3000],</span><br><span class="line">         [0.3070],</span><br><span class="line">         [0.3140],</span><br><span class="line">         [0.3210],</span><br><span class="line">         [0.3280]]))</span><br></pre></td></tr></table></figure><p>将数据分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create train and test splits</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X_regression)) <span class="comment"># 80% of data used for training set</span></span><br><span class="line">X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]</span><br><span class="line">X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the lengths of each split</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(X_train_regression), </span><br><span class="line">    <span class="built_in">len</span>(y_train_regression), </span><br><span class="line">    <span class="built_in">len</span>(X_test_regression), </span><br><span class="line">    <span class="built_in">len</span>(y_test_regression))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">80 80 20 20</span><br></pre></td></tr></table></figure><p>漂亮，让我们看看数据是什么样子的。</p><p>为此，我们将使用我们在笔记本 01 中创建的 plot_predictions() 函数。</p><p>它包含在我们上面下载的 Learn PyTorch for Deep Learning 存储库中的 helper_functions.py 脚本中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions(train_data=X_train_regression,</span><br><span class="line">    train_labels=y_train_regression,</span><br><span class="line">    test_data=X_test_regression,</span><br><span class="line">    test_labels=y_test_regression</span><br><span class="line">)</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-6.png" class="" title="PyTorch-26H-3-6"><h2 id="5-2-Adjusting-model-1-to-fit-a-straight-line-调整-model-1-以适合直线"><a href="#5-2-Adjusting-model-1-to-fit-a-straight-line-调整-model-1-以适合直线" class="headerlink" title="5.2 Adjusting model_1 to fit a straight line 调整 model_1 以适合直线"></a>5.2 Adjusting <code>model_1</code> to fit a straight line 调整 <code>model_1</code> 以适合直线</h2><p>重新创建<code>model_1</code>，但使用适合我们的回归数据的损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Same architecture as model_1 (but using nn.Sequential)</span></span><br><span class="line">model_2 = nn.Sequential(</span><br><span class="line">    nn.Linear(in_features=<span class="number">1</span>, out_features=<span class="number">10</span>),</span><br><span class="line">    nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>),</span><br><span class="line">    nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">1</span>)</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line">model_2</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Linear(in_features=1, out_features=10, bias=True)</span><br><span class="line">  (1): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">  (2): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>将损失函数设置为<code>nn.L1Loss()</code>（与平均绝对误差相同），并将优化器设置为<code>torch.optim.SGD()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss and optimizer</span></span><br><span class="line">loss_fn = nn.L1Loss()</span><br><span class="line">optimizer = torch.optim.SGD(model_2.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>现在让我们使用常规训练循环步骤来训练模型，<code>epochs=1000</code>（就像<code>model_1</code>一样）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data to target device</span></span><br><span class="line">X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)</span><br><span class="line">X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training </span></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_pred = model_2(X_train_regression)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. Calculate loss (no accuracy since it&#x27;s a regression problem, not classification)</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train_regression)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_2.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass</span></span><br><span class="line">      test_pred = model_2(X_test_regression)</span><br><span class="line">      <span class="comment"># 2. Calculate the loss </span></span><br><span class="line">      test_loss = loss_fn(test_pred, y_test_regression)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>: </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Train loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Test loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Train loss: 0.75986, Test loss: 0.54143</span><br><span class="line">Epoch: 100 | Train loss: 0.09309, Test loss: 0.02901</span><br><span class="line">Epoch: 200 | Train loss: 0.07376, Test loss: 0.02850</span><br><span class="line">Epoch: 300 | Train loss: 0.06745, Test loss: 0.00615</span><br><span class="line">Epoch: 400 | Train loss: 0.06107, Test loss: 0.02004</span><br><span class="line">Epoch: 500 | Train loss: 0.05698, Test loss: 0.01061</span><br><span class="line">Epoch: 600 | Train loss: 0.04857, Test loss: 0.01326</span><br><span class="line">Epoch: 700 | Train loss: 0.06109, Test loss: 0.02127</span><br><span class="line">Epoch: 800 | Train loss: 0.05599, Test loss: 0.01426</span><br><span class="line">Epoch: 900 | Train loss: 0.05571, Test loss: 0.00603</span><br></pre></td></tr></table></figure><p>好的，与分类数据上的 <code>model_1</code> 不同，<code>model_2</code> 的损失似乎实际上在下降。</p><p>让我们绘制它的预测图，看看是否如此。</p><p>请记住，由于我们的模型和数据正在使用目标设备，并且该设备可能是 GPU，因此我们的绘图函数使用 <code>matplotlib</code>，而 <code>matplotlib</code> 无法处理 GPU 上的数据。</p><p>为了处理这个问题，当我们将所有数据传递给 <code>plot_predictions()</code> 时，我们将使用 <code>.cpu()</code> 将所有数据发送到 CPU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn on evaluation mode</span></span><br><span class="line">model_2.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions (inference)</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_preds = model_2(X_test_regression)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot data and predictions with data on the CPU (matplotlib can&#x27;t handle data on the GPU)</span></span><br><span class="line"><span class="comment"># (try removing .cpu() from one of the below and see what happens)</span></span><br><span class="line">plot_predictions(train_data=X_train_regression.cpu(),</span><br><span class="line">                 train_labels=y_train_regression.cpu(),</span><br><span class="line">                 test_data=X_test_regression.cpu(),</span><br><span class="line">                 test_labels=y_test_regression.cpu(),</span><br><span class="line">                 predictions=y_preds.cpu());</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-7.png" class="" title="PyTorch-26H-3-7"><p>模型比在直线上随机猜测要好得多。这意味着我们的模型至少具有一定的学习能力。</p><blockquote><p>构建深度学习模型时，一个有用的故障排除步骤是先从尽可能小的模型开始，看看模型是否有效，然后再将其扩大。<br>这可能意味着从一个简单的神经网络（层数不多，隐藏神经元也不多）和一个小的数据集（就像我们制作的数据集）开始，然后在这个小例子上进行过度拟合overfitting（使模型表现得太好了），然后再增加数据量或模型 大小 / 设计 以减少过度拟合。</p></blockquote><h1 id="6-The-missing-piece-non-linearity-缺失的部分：非线性"><a href="#6-The-missing-piece-non-linearity-缺失的部分：非线性" class="headerlink" title="6. The missing piece: non-linearity 缺失的部分：非线性"></a>6. The missing piece: non-linearity 缺失的部分：非线性</h1><p>由于模型具有线性层，因此它可以绘制直线（线性）。</p><p>但是我们如何赋予它绘制非直线（非线性）线条的能力呢？</p><h2 id="6-1-Recreating-non-linear-data-red-and-blue-circles-重新创建非线性数据（红色和蓝色圆圈）"><a href="#6-1-Recreating-non-linear-data-red-and-blue-circles-重新创建非线性数据（红色和蓝色圆圈）" class="headerlink" title="6.1 Recreating non-linear data (red and blue circles) 重新创建非线性数据（红色和蓝色圆圈）"></a>6.1 Recreating non-linear data (red and blue circles) 重新创建非线性数据（红色和蓝色圆圈）</h2><p>重新创建数据以从头开始。我们将使用与之前相同的设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make and plot data</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line"></span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">X, y = make_circles(n_samples=<span class="number">1000</span>,</span><br><span class="line">    noise=<span class="number">0.03</span>,</span><br><span class="line">    random_state=<span class="number">42</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.RdBu);</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-8.png" class="" title="PyTorch-26H-3-8"><p>太棒了！现在让我们将其分成训练集和测试集，其中 80% 的数据用于训练，20% 的数据用于测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert to tensors and split into train and test sets</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn data into tensors</span></span><br><span class="line">X = torch.from_numpy(X).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line">y = torch.from_numpy(y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split into train and test sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, </span><br><span class="line">                                                    y, </span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">X_train[:<span class="number">5</span>], y_train[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.6579, -0.4651],</span><br><span class="line">         [ 0.6319, -0.7347],</span><br><span class="line">         [-1.0086, -0.1240],</span><br><span class="line">         [-0.9666, -0.2256],</span><br><span class="line">         [-0.1666,  0.7994]]),</span><br><span class="line"> tensor([1., 0., 0., 0., 1.]))</span><br></pre></td></tr></table></figure><h2 id="6-2-Building-a-model-with-non-linearity-建立非线性模型"><a href="#6-2-Building-a-model-with-non-linearity-建立非线性模型" class="headerlink" title="6.2 Building a model with non-linearity 建立非线性模型"></a>6.2 Building a model with non-linearity 建立非线性模型</h2><p>可以用无限的直线（线性）和非直线（非线性）绘制什么样的图案？</p><p>到目前为止，我们的神经网络仅使用线性（直线）函数。</p><p>但我们处理的数据是非线性的（圆圈）。</p><p>当我们为模型引入使用非线性激活函数的能力时</p><p>PyTorch 有一堆<a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">现成的非线性激活函数</a>，它们可以执行类似但不同的事情。</p><p>最常见且性能最好的一种是<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks">ReLU</a>)（整流线性单元，<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html">torch.nn.ReLU()</a>）。</p><p>将它放在神经网络中前向传递的隐藏层之间，看看会发生什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build model with non-linear activation function</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CircleModelV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer_1 = nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        self.layer_2 = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        self.layer_3 = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">1</span>)</span><br><span class="line">        self.relu = nn.ReLU() <span class="comment"># &lt;- add in ReLU activation function</span></span><br><span class="line">        <span class="comment"># Can also put sigmoid in the model </span></span><br><span class="line">        <span class="comment"># This would mean you don&#x27;t need to use it on the predictions</span></span><br><span class="line">        <span class="comment"># self.sigmoid = nn.Sigmoid()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">      <span class="comment"># Intersperse the ReLU activation function between layers</span></span><br><span class="line">       <span class="keyword">return</span> self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))</span><br><span class="line"></span><br><span class="line">model_3 = CircleModelV2().to(device)</span><br><span class="line"><span class="built_in">print</span>(model_3)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CircleModelV2(</span><br><span class="line">  (layer_1): Linear(in_features=2, out_features=10, bias=True)</span><br><span class="line">  (layer_2): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">  (layer_3): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">  (relu): ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>与我们刚刚构建的分类神经网络（使用 ReLU 激活）类似的分类神经网络的视觉示例。尝试在 TensorFlow Playground 网站上创建一个您自己的神经网络。</p><blockquote><p>问题：构建神经网络时，我应该把非线性激活函数放在哪里？<br>经验法则是将它们放在隐藏层之间，紧接着输出层，但是，没有一成不变的选择。随着您对神经网络和深度学习的了解越来越多，您会发现很多不同的组合方法。与此同时，最好不断实验、实验、再实验。</p></blockquote><p>现在我们已经准备好了模型，让我们创建一个二元分类损失函数以及一个优化器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup loss and optimizer </span></span><br><span class="line">loss_fn = nn.BCEWithLogitsLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model_3.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h2 id="6-3-Training-a-model-with-non-linearity-训练非线性模型"><a href="#6-3-Training-a-model-with-non-linearity-训练非线性模型" class="headerlink" title="6.3 Training a model with non-linearity 训练非线性模型"></a>6.3 Training a model with non-linearity 训练非线性模型</h2><p>训练、模型、损失函数、优化器已准备就绪，让我们创建一个训练和测试循环。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put all data on target device</span></span><br><span class="line">X_train, y_train = X_train.to(device), y_train.to(device)</span><br><span class="line">X_test, y_test = X_test.to(device), y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_logits = model_3(X_train).squeeze()</span><br><span class="line">    y_pred = torch.<span class="built_in">round</span>(torch.sigmoid(y_logits)) <span class="comment"># logits -&gt; prediction probabilities -&gt; prediction labels</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. Calculate loss and accuracy</span></span><br><span class="line">    loss = loss_fn(y_logits, y_train) <span class="comment"># BCEWithLogitsLoss calculates loss using logits</span></span><br><span class="line">    acc = accuracy_fn(y_true=y_train, </span><br><span class="line">                      y_pred=y_pred)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_3.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass</span></span><br><span class="line">      test_logits = model_3(X_test).squeeze()</span><br><span class="line">      test_pred = torch.<span class="built_in">round</span>(torch.sigmoid(test_logits)) <span class="comment"># logits -&gt; prediction probabilities -&gt; prediction labels</span></span><br><span class="line">      <span class="comment"># 2. Calculate loss and accuracy</span></span><br><span class="line">      test_loss = loss_fn(test_logits, y_test)</span><br><span class="line">      test_acc = accuracy_fn(y_true=y_test,</span><br><span class="line">                             y_pred=test_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Accuracy: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% | Test Loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>, Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%</span><br><span class="line">Epoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%</span><br><span class="line">Epoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%</span><br><span class="line">Epoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%</span><br><span class="line">Epoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%</span><br><span class="line">Epoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%</span><br><span class="line">Epoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%</span><br><span class="line">Epoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%</span><br><span class="line">Epoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%</span><br><span class="line">Epoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%</span><br></pre></td></tr></table></figure><h2 id="6-4-Evaluating-a-model-trained-with-non-linear-activation-functions-评估用非线性激活函数训练的模型"><a href="#6-4-Evaluating-a-model-trained-with-non-linear-activation-functions-评估用非线性激活函数训练的模型" class="headerlink" title="6.4 Evaluating a model trained with non-linear activation functions 评估用非线性激活函数训练的模型"></a>6.4 Evaluating a model trained with non-linear activation functions 评估用非线性激活函数训练的模型</h2><p>还记得我们的圆形数据是非线性的吗？好吧，让我们看看现在模型的预测结果如何，该模型已经用非线性激活函数进行了训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions</span></span><br><span class="line">model_3.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_preds = torch.<span class="built_in">round</span>(torch.sigmoid(model_3(X_test))).squeeze()</span><br><span class="line">y_preds[:<span class="number">10</span>], y[:<span class="number">10</span>] <span class="comment"># want preds in same format as truth labels</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device=&#x27;cuda:0&#x27;),</span><br><span class="line"> tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot decision boundaries for training and test sets</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_1, X_train, y_train) <span class="comment"># model_1 = no non-linearity</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_3, X_test, y_test) <span class="comment"># model_3 = has non-linearity</span></span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-9.png" class="" title="PyTorch-26H-3-9"><h1 id="7-Replicating-non-linear-activation-functions-复制非线性激活函数"><a href="#7-Replicating-non-linear-activation-functions-复制非线性激活函数" class="headerlink" title="7. Replicating non-linear activation functions 复制非线性激活函数"></a>7. Replicating non-linear activation functions 复制非线性激活函数</h1><blockquote><p>您在自然中遇到的大部分数据都是非线性的（或线性和非线性的组合）。现在我们一直在处理二维图上的点。但想象一下，如果您有想要分类的植物图像，会有很多不同的植物形状。或者您想要总结的维基百科文本，有很多不同的单词组合方式（线性和非线性模式）。</p></blockquote><p>但是非线性激活是什么样的？我们如何复制一些并看看它们的作用如何？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a toy tensor (similar to the data going into our model(s))</span></span><br><span class="line">A = torch.arange(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">A</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,</span><br><span class="line">          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the toy tensor</span></span><br><span class="line">plt.plot(A);</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-10.png" class="" title="PyTorch-26H-3-10"><p>一条直线。</p><p>现在让我们看看 ReLU 激活函数如何影响它。</p><p>我们不会使用 PyTorch 的 ReLU (<code>torch.nn.ReLU</code>)，而是自己重新创建它。</p><p>ReLU 函数将所有负值变为 0，并保持正值不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create ReLU function by hand </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> torch.maximum(torch.tensor(<span class="number">0</span>), x) <span class="comment"># inputs must be tensors</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pass toy tensor through ReLU function</span></span><br><span class="line">relu(A)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,</span><br><span class="line">        8., 9.])</span><br></pre></td></tr></table></figure><p>看起来我们的 ReLU 函数起作用了，所有负值都是零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot ReLU activated toy tensor</span></span><br><span class="line">plt.plot(relu(A));</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-11.png" class="" title="PyTorch-26H-3-11"><p>太棒了！这看起来和 ReLU <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks">维基百科页面上的 ReLU 函数</a>) 形状一模一样。</p><p>我们试试我们一直在使用的 <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid函数</a> 怎么样？</p><p>sigmoid 函数公式如下：</p><script type="math/tex; mode=display">out_i = \frac{1}{1+e^{-input_i}}</script><p>Or using $x$ as input:</p><script type="math/tex; mode=display">S(x) = \frac{1}{1+e^{-x_i}}</script><p>其中 $S$ 代表 sigmoid 函数，$e$ 代表<a href="">指数</a>（<a href="">torch.exp()</a>），$i$ 代表张量中的特定元素。</p><p>让我们用 PyTorch 构建一个函数来复制 sigmoid 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a custom sigmoid function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + torch.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test custom sigmoid on toy tensor</span></span><br><span class="line">sigmoid(A)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,</span><br><span class="line">        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,</span><br><span class="line">        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,</span><br><span class="line">        9.9966e-01, 9.9988e-01])</span><br></pre></td></tr></table></figure><p>这些值看起来很像我们之前看到的预测概率，让我们看看它们的可视化效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot sigmoid activated toy tensor</span></span><br><span class="line">plt.plot(sigmoid(A));</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-12.png" class="" title="PyTorch-26H-3-12"><p>看起来不错！我们已经从直线变成了曲线。</p><p>现在 PyTorch 中存在许多我们尚未尝试过的<a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">非线性激活函数</a>。</p><p>但这两个是最常见的两个。</p><p>问题仍然存在，您可以使用无限数量的线性（直线）和非线性（非直线）线来绘制什么图案？</p><p>几乎任何东西都可以，对吗？</p><p>当我们结合线性和非线性函数时，这正是我们的模型所做的事情。</p><p>我们不是告诉模型要做什么，而是给它工具来找出如何最好地发现数据中的模式。</p><p>这些工具是线性和非线性函数。</p><h1 id="8-Putting-things-together-by-building-a-multi-class-PyTorch-model-通过构建多类-PyTorch-模型将所有内容整合在一起"><a href="#8-Putting-things-together-by-building-a-multi-class-PyTorch-model-通过构建多类-PyTorch-模型将所有内容整合在一起" class="headerlink" title="8. Putting things together by building a multi-class PyTorch model 通过构建多类 PyTorch 模型将所有内容整合在一起"></a>8. Putting things together by building a multi-class PyTorch model 通过构建多类 PyTorch 模型将所有内容整合在一起</h1><p>使用多类分类问题将它们放在一起。</p><p><code>二元分类</code>问题是将某物归类为两个选项之一（例如，将一张照片归类为猫的照片或狗的照片）。而<code>多类分类</code>问题是从两个以上的选项列表中对某物进行分类（例如，将一张照片归类为猫、狗或鸡）。</p><h2 id="8-1-Creating-multi-class-classification-data-创建多类别分类数据"><a href="#8-1-Creating-multi-class-classification-data-创建多类别分类数据" class="headerlink" title="8.1 Creating multi-class classification data 创建多类别分类数据"></a>8.1 Creating multi-class classification data 创建多类别分类数据</h2><p>为了开始多类分类问题，让我们创建一些多类数据。</p><p>为此，我们可以利用 <code>Scikit-Learn</code> 的 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">make_blobs()</a> 方法。</p><p>此方法将创建我们想要的任意数量的类（使用 <code>centers</code> 参数）。</p><p>具体来说，我们可以这样做：</p><p>1、使用 <code>make_blobs()</code> 创建一些多类数据。<br>2、将数据转换为张量（默认 <code>make_blobs()</code> 使用NumPy数组）。<br>3、使用 <code>train_test_split()</code> 将数据分为训练集和测试集 。<br>4、使数据可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import dependencies</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the hyperparameters for data creation</span></span><br><span class="line">NUM_CLASSES = <span class="number">4</span></span><br><span class="line">NUM_FEATURES = <span class="number">2</span></span><br><span class="line">RANDOM_SEED = <span class="number">42</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create multi-class data</span></span><br><span class="line">X_blob, y_blob = make_blobs(n_samples=<span class="number">1000</span>,</span><br><span class="line">    n_features=NUM_FEATURES, <span class="comment"># X features</span></span><br><span class="line">    centers=NUM_CLASSES, <span class="comment"># y labels </span></span><br><span class="line">    cluster_std=<span class="number">1.5</span>, <span class="comment"># give the clusters a little shake up (try changing this to 1.0, the default)</span></span><br><span class="line">    random_state=RANDOM_SEED</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Turn data into tensors</span></span><br><span class="line">X_blob = torch.from_numpy(X_blob).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line">y_blob = torch.from_numpy(y_blob).<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line"><span class="built_in">print</span>(X_blob[:<span class="number">5</span>], y_blob[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Split into train and test sets</span></span><br><span class="line">X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,</span><br><span class="line">    y_blob,</span><br><span class="line">    test_size=<span class="number">0.2</span>,</span><br><span class="line">    random_state=RANDOM_SEED</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Plot data</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plt.scatter(X_blob[:, <span class="number">0</span>], X_blob[:, <span class="number">1</span>], c=y_blob, cmap=plt.cm.RdYlBu);</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-8.4134,  6.9352],</span><br><span class="line">        [-5.7665, -6.4312],</span><br><span class="line">        [-6.0421, -6.7661],</span><br><span class="line">        [ 3.9508,  0.6984],</span><br><span class="line">        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-13.png" class="" title="PyTorch-26H-3-13"><p>准备好了一些多类数据。建立一个模型来分离彩色斑点。</p><p>问题：这个数据集需要非线性吗？或者你可以画出一系列直线来分离它吗？</p><h2 id="8-2-Building-a-multi-class-classification-model-in-PyTorch-在-PyTorch-中构建多类分类模型"><a href="#8-2-Building-a-multi-class-classification-model-in-PyTorch-在-PyTorch-中构建多类分类模型" class="headerlink" title="8.2 Building a multi-class classification model in PyTorch 在 PyTorch 中构建多类分类模型"></a>8.2 Building a multi-class classification model in PyTorch 在 PyTorch 中构建多类分类模型</h2><p>到目前为止，我们已经在 PyTorch 中创建了一些模型。</p><p>您或许还开始了解神经网络的灵活性。</p><p>如何构建一个类似<code>model_3</code>但仍然能够处理多类数据的系统呢？</p><p>创建一个<code>nn.Module</code>包含三个超参数的子类：</p><ul><li><code>input_features</code> X 进入模型的特征数量。</li><li><code>output_features</code> 我们想要的输出特征的理想数量（这将等同于NUM_CLASSES或等于多类分类问题中的类数）。</li><li><code>hidden_units</code>  我们希望每个隐藏层使用的隐藏神经元的数量。</li></ul><p>然后我们将使用上面的超参数创建模型类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create device agnostic code</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">device</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;cuda&#x27;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BlobModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, output_features, hidden_units=<span class="number">8</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Initializes all required hyperparameters for a multi-class classification model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            input_features (int): Number of input features to the model.</span></span><br><span class="line"><span class="string">            out_features (int): Number of output features of the model</span></span><br><span class="line"><span class="string">              (how many classes there are).</span></span><br><span class="line"><span class="string">            hidden_units (int): Number of hidden units between layers, default 8.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear_layer_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_features, out_features=hidden_units),</span><br><span class="line">            <span class="comment"># nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)</span></span><br><span class="line">            nn.Linear(in_features=hidden_units, out_features=hidden_units),</span><br><span class="line">            <span class="comment"># nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)</span></span><br><span class="line">            nn.Linear(in_features=hidden_units, out_features=output_features), <span class="comment"># how many classes are there?</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear_layer_stack(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an instance of BlobModel and send it to the target device</span></span><br><span class="line">model_4 = BlobModel(input_features=NUM_FEATURES, </span><br><span class="line">                    output_features=NUM_CLASSES, </span><br><span class="line">                    hidden_units=<span class="number">8</span>).to(device)</span><br><span class="line">model_4</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BlobModel(</span><br><span class="line">  (linear_layer_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=2, out_features=8, bias=True)</span><br><span class="line">    (1): Linear(in_features=8, out_features=8, bias=True)</span><br><span class="line">    (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="8-3-Creating-a-loss-function-and-optimizer-for-a-multi-class-PyTorch-model-为多类-PyTorch-模型创建损失函数和优化器"><a href="#8-3-Creating-a-loss-function-and-optimizer-for-a-multi-class-PyTorch-model-为多类-PyTorch-模型创建损失函数和优化器" class="headerlink" title="8.3 Creating a loss function and optimizer for a multi-class PyTorch model 为多类 PyTorch 模型创建损失函数和优化器"></a>8.3 Creating a loss function and optimizer for a multi-class PyTorch model 为多类 PyTorch 模型创建损失函数和优化器</h2><p>由于我们正在研究多类分类问题，我们将使用该<code>nn.CrossEntropyLoss()</code>方法作为我们的损失函数。</p><p>我们将坚持使用学习率为 0.1 的 SGD 来优化我们的<code>model_4</code>参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create loss and optimizer</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model_4.parameters(), </span><br><span class="line">                            lr=<span class="number">0.1</span>) <span class="comment"># exercise: try changing the learning rate here and seeing what happens to the model&#x27;s performance</span></span><br></pre></td></tr></table></figure><h2 id="8-4-Getting-prediction-probabilities-for-a-multi-class-PyTorch-model-获取多类-PyTorch-模型的预测概率"><a href="#8-4-Getting-prediction-probabilities-for-a-multi-class-PyTorch-model-获取多类-PyTorch-模型的预测概率" class="headerlink" title="8.4 Getting prediction probabilities for a multi-class PyTorch model 获取多类 PyTorch 模型的预测概率"></a>8.4 Getting prediction probabilities for a multi-class PyTorch model 获取多类 PyTorch 模型的预测概率</h2><p>准备好了损失函数和优化器，并且准备好训练我们的模型，但在此之前，让我们对我们的模型进行一次前向传递，看看它是否有效。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Perform a single forward pass on the data (we&#x27;ll need to put it to the target device for it to work)</span></span><br><span class="line">model_4(X_blob_train.to(device))[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-1.2711, -0.6494, -1.4740, -0.7044],</span><br><span class="line">        [ 0.2210, -1.5439,  0.0420,  1.1531],</span><br><span class="line">        [ 2.8698,  0.9143,  3.3169,  1.4027],</span><br><span class="line">        [ 1.9576,  0.3125,  2.2244,  1.1324],</span><br><span class="line">        [ 0.5458, -1.2381,  0.4441,  1.1804]], device=&#x27;cuda:0&#x27;,</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><p>为每个样本的每个特征都获得了一个值。检查一下形状以确认。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># How many elements in a single prediction sample?</span></span><br><span class="line">model_4(X_blob_train.to(device))[<span class="number">0</span>].shape, NUM_CLASSES </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([4]), 4)</span><br></pre></td></tr></table></figure><p>模型正在为每个类别预测一个值。</p><p>你还记得我们模型的原始输出叫什么吗？</p><p>提示：它与“frog splits”押韵（在制作这些材料时没有伤害任何动物）。</p><p>如果你猜是 logits，那你就猜对了。</p><p>所以现在我们的模型正在输出 logits，但如果我们想弄清楚样本到底是哪个标签，该怎么办？</p><p>如何从 <code>logits</code> -&gt; <code>prediction probabilities</code> -&gt; <code>prediction labels</code>，就像我们处理二元分类问题一样？</p><p>这就是 <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax 激活函数</a> 发挥作用的地方。</p><p>softmax 函数计算每个预测类相对于所有其他可能类成为实际预测类的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make prediction logits with model</span></span><br><span class="line"><span class="comment"># 使用模型进行预测逻辑</span></span><br><span class="line">y_logits = model_4(X_blob_test.to(device))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform softmax calculation on logits across dimension 1 to get prediction probabilities</span></span><br><span class="line"><span class="comment"># 对 1 维上的 logits 执行 softmax 计算，得到预测概率</span></span><br><span class="line">y_pred_probs = torch.softmax(y_logits, dim=<span class="number">1</span>) </span><br><span class="line"><span class="built_in">print</span>(y_logits[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(y_pred_probs[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-1.2549, -0.8112, -1.4795, -0.5696],</span><br><span class="line">        [ 1.7168, -1.2270,  1.7367,  2.1010],</span><br><span class="line">        [ 2.2400,  0.7714,  2.6020,  1.0107],</span><br><span class="line">        [-0.7993, -0.3723, -0.9138, -0.5388],</span><br><span class="line">        [-0.4332, -1.6117, -0.6891,  0.6852]], device=&#x27;cuda:0&#x27;,</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line">tensor([[0.1872, 0.2918, 0.1495, 0.3715],</span><br><span class="line">        [0.2824, 0.0149, 0.2881, 0.4147],</span><br><span class="line">        [0.3380, 0.0778, 0.4854, 0.0989],</span><br><span class="line">        [0.2118, 0.3246, 0.1889, 0.2748],</span><br><span class="line">        [0.1945, 0.0598, 0.1506, 0.5951]], device=&#x27;cuda:0&#x27;,</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><p>softmax 函数的输出可能看起来仍然是乱码（确实如此，因为我们的模型尚未经过训练，并且使用随机模式进行预测），但每个样本都有非常具体的区别。</p><p>将 logits 传递到 softmax 函数后，每个样本现在都加到 1（或非常接近）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sum the first sample output of the softmax activation function</span></span><br><span class="line"><span class="comment"># 对softmax激活函数的第一个样本输出求和</span></span><br><span class="line">torch.<span class="built_in">sum</span>(y_pred_probs[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(1., device=&#x27;cuda:0&#x27;, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure><p>这些预测概率本质上说明了模型认为目标 X 样本（输入）映射到每个类的程度。</p><p>由于 <code>y_pred_probs</code> 中每个类都有一个值，因此最高值的索引就是模型认为特定数据样本最属于的类。</p><p>我们可以使用 <code>torch.argmax()</code> 检查哪个索引具有最高值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Which class does the model think is *most* likely at the index 0 sample?</span></span><br><span class="line"><span class="comment"># 模型认为在索引 0 样本中哪个类最有可能？</span></span><br><span class="line"><span class="built_in">print</span>(y_pred_probs[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.argmax(y_pred_probs[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.1872, 0.2918, 0.1495, 0.3715], device=&#x27;cuda:0&#x27;,</span><br><span class="line">       grad_fn=&lt;SelectBackward0&gt;)</span><br><span class="line">tensor(3, device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><p>您可以看到 <code>torch.argmax()</code> 的输出返回 3，因此对于索引 0 处的样本的特征 (<code>X</code>)，模型预测最可能的类值 (<code>y</code>) 是 3。</p><p>当然，现在这只是随机猜测，所以它有 25% 的正确率（因为有四个类）。但我们可以通过训练模型来提高这些机会。</p><blockquote><p>模型的原始输出称为 logits。<br>对于多类分类问题，要将 logits 转换为预测概率，请使用 softmax 激活函数 (torch.softmax)。<br>具有最高预测概率的值的索引是模型认为在给定该样本的输入特征的情况下最有可能的类号（虽然这是一个预测，但并不意味着它是正确的）。</p></blockquote><h2 id="8-5-Creating-a-training-and-testing-loop-for-a-multi-class-PyTorch-model-为多类-PyTorch-模型创建训练和测试循环"><a href="#8-5-Creating-a-training-and-testing-loop-for-a-multi-class-PyTorch-model-为多类-PyTorch-模型创建训练和测试循环" class="headerlink" title="8.5 Creating a training and testing loop for a multi-class PyTorch model 为多类 PyTorch 模型创建训练和测试循环"></a>8.5 Creating a training and testing loop for a multi-class PyTorch model 为多类 PyTorch 模型创建训练和测试循环</h2><p>好了，现在我们已经完成了所有准备步骤，让我们编写一个训练和测试循环来改进和评估我们的模型。</p><p>我们之前已经完成了很多这些步骤，所以其中很多都是练习。</p><p>唯一的区别是，我们将调整步骤，将模型输出（<code>logits</code>）转换为预测概率（使用softmax激活函数），然后转换为预测标签（通过取softmax激活函数输出的argmax）。</p><p>让我们训练模型<code>epochs=100</code>，并每10个epochs评估一次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set number of epochs</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data to target device</span></span><br><span class="line">X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)</span><br><span class="line">X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_4.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_logits = model_4(X_blob_train) <span class="comment"># model outputs raw logits </span></span><br><span class="line">    y_pred = torch.softmax(y_logits, dim=<span class="number">1</span>).argmax(dim=<span class="number">1</span>) <span class="comment"># go from logits -&gt; prediction probabilities -&gt; prediction labels</span></span><br><span class="line">    <span class="comment"># print(y_logits)</span></span><br><span class="line">    <span class="comment"># 2. Calculate loss and accuracy</span></span><br><span class="line">    loss = loss_fn(y_logits, y_blob_train) </span><br><span class="line">    acc = accuracy_fn(y_true=y_blob_train,</span><br><span class="line">                      y_pred=y_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_4.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass</span></span><br><span class="line">      test_logits = model_4(X_blob_test)</span><br><span class="line">      test_pred = torch.softmax(test_logits, dim=<span class="number">1</span>).argmax(dim=<span class="number">1</span>)</span><br><span class="line">      <span class="comment"># 2. Calculate test loss and accuracy</span></span><br><span class="line">      test_loss = loss_fn(test_logits, y_blob_test)</span><br><span class="line">      test_acc = accuracy_fn(y_true=y_blob_test,</span><br><span class="line">                             y_pred=test_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Acc: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% | Test Loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>, Test Acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>) </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%</span><br><span class="line">Epoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%</span><br><span class="line">Epoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%</span><br><span class="line">Epoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%</span><br><span class="line">Epoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%</span><br><span class="line">Epoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%</span><br><span class="line">Epoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%</span><br><span class="line">Epoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%</span><br><span class="line">Epoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%</span><br><span class="line">Epoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%</span><br><span class="line">Epoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%</span><br><span class="line">Epoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%</span><br><span class="line">Epoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%</span><br><span class="line">Epoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%</span><br><span class="line">Epoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%</span><br><span class="line">Epoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%</span><br><span class="line">Epoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%</span><br></pre></td></tr></table></figure><h2 id="8-6-Making-and-evaluating-predictions-with-a-PyTorch-multi-class-model-使用-PyTorch-多类模型进行预测并评估预测"><a href="#8-6-Making-and-evaluating-predictions-with-a-PyTorch-multi-class-model-使用-PyTorch-多类模型进行预测并评估预测" class="headerlink" title="8.6 Making and evaluating predictions with a PyTorch multi-class model 使用 PyTorch 多类模型进行预测并评估预测"></a>8.6 Making and evaluating predictions with a PyTorch multi-class model 使用 PyTorch 多类模型进行预测并评估预测</h2><p>看起来我们训练过的模型表现得相当不错。</p><p>但为了确保这一点，让我们做一些预测并将它们可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions</span></span><br><span class="line">model_4.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_logits = model_4(X_blob_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the first 10 predictions</span></span><br><span class="line">y_logits[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  4.3377,  10.3539, -14.8948,  -9.7642],</span><br><span class="line">        [  5.0142, -12.0371,   3.3860,  10.6699],</span><br><span class="line">        [ -5.5885, -13.3448,  20.9894,  12.7711],</span><br><span class="line">        [  1.8400,   7.5599,  -8.6016,  -6.9942],</span><br><span class="line">        [  8.0726,   3.2906, -14.5998,  -3.6186],</span><br><span class="line">        [  5.5844, -14.9521,   5.0168,  13.2890],</span><br><span class="line">        [ -5.9739, -10.1913,  18.8655,   9.9179],</span><br><span class="line">        [  7.0755,  -0.7601,  -9.5531,   0.1736],</span><br><span class="line">        [ -5.5918, -18.5990,  25.5309,  17.5799],</span><br><span class="line">        [  7.3142,   0.7197, -11.2017,  -1.2011]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><p>看起来我们模型的预测仍然是 <code>logit</code> 形式。</p><p>但为了评估它们，它们必须与我们的标签 (<code>y_blob_test</code>) 具有相同的形式，后者是整数形式。</p><p>让我们将模型的预测 <code>logit</code> 转换为预测概率（使用 <code>torch.softmax()</code>），然后转换为预测标签（通过获取每个样本的 <code>argmax()</code>）。</p><blockquote><p>可以跳过 <code>torch.softmax()</code> 函数，直接在 <code>logits</code> 上调用 <code>torch.argmax()</code>，从预测 <code>logits</code> -&gt; <code>predicted labels</code> 直接进入。<br>例如，<code>y_preds = torch.argmax(y_logits, dim=1)</code>，这节省了一个计算步骤（没有 <code>torch.softmax()</code>），但导致没有可用的预测概率。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn predicted logits in prediction probabilities</span></span><br><span class="line"><span class="comment"># 将预测的逻辑转换为预测概率</span></span><br><span class="line">y_pred_probs = torch.softmax(y_logits, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn prediction probabilities into prediction labels</span></span><br><span class="line"><span class="comment"># 将预测概率转化为预测标签</span></span><br><span class="line">y_preds = y_pred_probs.argmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare first 10 model preds and test labels</span></span><br><span class="line"><span class="comment"># 比较前 10 个模型预测和测试标签</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predictions: <span class="subst">&#123;y_preds[:<span class="number">10</span>]&#125;</span>\nLabels: <span class="subst">&#123;y_blob_test[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test accuracy: <span class="subst">&#123;accuracy_fn(y_true=y_blob_test, y_pred=y_preds)&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Predictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device=&#x27;cuda:0&#x27;)</span><br><span class="line">Labels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device=&#x27;cuda:0&#x27;)</span><br><span class="line">Test accuracy: 99.5%</span><br></pre></td></tr></table></figure><p>模型预测现在与测试标签的形式相同。</p><p>使用 <code>plot_decision_boundary()</code> 将它们可视化，请记住，因为我们的数据在 GPU 上，所以我们必须将其移动到 CPU 以便与 matplotlib 一起使用（<code>plot_decision_boundary()</code> 会自动为我们执行此操作）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_4, X_blob_train, y_blob_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_4, X_blob_test, y_blob_test)</span><br></pre></td></tr></table></figure><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-14.png" class="" title="PyTorch-26H-3-14"><h1 id="9-More-classification-evaluation-metrics-更多分类评估指标"><a href="#9-More-classification-evaluation-metrics-更多分类评估指标" class="headerlink" title="9. More classification evaluation metrics 更多分类评估指标"></a>9. More classification evaluation metrics 更多分类评估指标</h1><p>到目前为止，我们仅介绍了评估分类模型的几种方法（准确性、损失和可视化预测）。</p><p>这些是您会遇到的一些最常见的方法，并且是一个很好的起点。</p><p>可能希望使用更多指标来评估分类模型，例如：</p><div class="table-container"><table><thead><tr><th style="text-align:center">指标名称/评估方法</th><th style="text-align:center">定义</th><th style="text-align:center">代码</th></tr></thead><tbody><tr><td style="text-align:center">预测精度Accuracy</td><td style="text-align:center">在 100 个预测中，您的模型有多少个预测正确？例如，95% 的准确率意味着 100 个预测中有 95 个正确。</td><td style="text-align:center"><a href="https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#id3">torchmetrics.Accuracy()</a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">sklearn.metrics.accuracy_score()</a></td></tr><tr><td style="text-align:center">准确率Precision</td><td style="text-align:center">真阳性与样本总数的比例。精度越高，假阳性越少（模型预测为 1，但实际应该是 0）。</td><td style="text-align:center"><a href="https://torchmetrics.readthedocs.io/en/stable/classification/precision.html#id4">torchmetrics.Precision()</a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">sklearn.metrics.precision_score()</a></td></tr><tr><td style="text-align:center">召回 Recall</td><td style="text-align:center">真阳性占真阳性和假阴性总数的比例（模型预测为 0，但实际应为 1）。召回率越高，假阴性越少。</td><td style="text-align:center"><a href="https://torchmetrics.readthedocs.io/en/stable/classification/recall.html#id5">torchmetrics.Recall()</a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html">sklearn.metrics.recall_score()</a></td></tr><tr><td style="text-align:center">F1分数 F1-score</td><td style="text-align:center">将精度和召回率结合为一个指标。1 表示最好，0 表示最差。</td><td style="text-align:center"><a href="https://torchmetrics.readthedocs.io/en/stable/classification/f1_score.html#f1score">torchmetrics.F1Score()</a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">sklearn.metrics.f1_score()</a></td></tr><tr><td style="text-align:center">混淆矩阵 <a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">Confusion matrix</a></td><td style="text-align:center">以表格方式将预测值与真实值进行比较，如果 100% 正确，矩阵中的所有值将从左上角到右下角（对角线）。</td><td style="text-align:center"><a href="https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html#confusionmatrix">torchmetrics.ConfusionMatrix</a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions">sklearn.metrics.plot_confusion_matrix()</a></td></tr><tr><td style="text-align:center">分类报告 Classification report</td><td style="text-align:center">收集一些主要的分类指标，例如精确度、召回率和 f1 分数。</td><td style="text-align:center"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html">sklearn.metrics.classification_report()</a></td></tr></tbody></table></div><p>Scikit-Learn（一个流行的、世界一流的机器学习库）对上述指标有许多实现，如果你正在寻找一个类似 PyTorch 的版本，请查看 <a href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics</a>，尤其是 <a href="https://torchmetrics.readthedocs.io/en/stable/pages/classification.html">TorchMetrics 分类部分</a>。</p><p>尝试一下 <code>torchmetrics.Accuracy</code> 指标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> torchmetrics <span class="keyword">import</span> Accuracy</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    !pip install torchmetrics==<span class="number">0.9</span><span class="number">.3</span> <span class="comment"># this is the version we&#x27;re using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)</span></span><br><span class="line">    <span class="keyword">from</span> torchmetrics <span class="keyword">import</span> Accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup metric and make sure it&#x27;s on the target device</span></span><br><span class="line">torchmetrics_accuracy = Accuracy(task=<span class="string">&#x27;multiclass&#x27;</span>, num_classes=<span class="number">4</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate accuracy</span></span><br><span class="line">torchmetrics_accuracy(y_preds, y_blob_test)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.9950, device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><p>所有练习都集中于练习以上部分中的代码。</p><p>您应该能够通过参考每个部分或按照链接的资源来完成它们。</p><p>所有练习都应使用<a href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">设备激动代码</a>来完成。</p><p>资源：</p><ul><li><a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/02_pytorch_classification_exercises.ipynb">练习模板笔记本02</a></li><li><a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/02_pytorch_classification_exercise_solutions.ipynb">02 的示例解决方案笔记本</a>（在查看<em>之前先</em>尝试练习）</li></ul><ol><li><p>使用 Scikit-Learn 的函数创建二元分类数据集  <code>make_moons()</code>。</p><ul><li>为了一致性，数据集应该有 1000 个样本和一个<code>random_state=42</code>。</li><li>将数据转换为 PyTorch 张量。将数据分为训练集和测试集，<code>train_test_split</code>其中 80% 用于训练，20% 用于测试。</li></ul></li><li><p>通过子类化构建一个模型<code>nn.Module</code>，该模型包含非线性激活函数，并且能够拟合您在 1 中创建的数据。</p><ul><li>请随意使用您想要的 PyTorch 层（线性和非线性）的任意组合。</li></ul></li><li><p>设置二元分类兼容的损失函数和优化器，以便在训练模型时使用。</p></li><li><p>创建一个训练和测试循环，以使您在 2 中创建的模型适合您在 1 中创建的数据。</p><ul><li><a href="https://torchmetrics.readthedocs.io/en/latest/">为了测量模型准确性，您可以创建自己的准确性函数或使用TorchMetrics</a>中的准确性函数。</li><li>对模型进行足够长时间的训练，以达到 96% 以上的准确率。</li><li>训练循环应该每 10 个时期输出一次模型训练和测试集损失和准确率的进度。</li></ul></li><li><p>使用训练好的模型进行预测，并使用<code>plot_decision_boundary()</code>此笔记本中创建的函数绘制它们。</p></li><li><p>在纯 PyTorch 中复制 Tanh（双曲正切）激活函数。</p><ul><li>请随意参考<a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh">ML 备忘单网站</a>来获取该公式。</li></ul></li><li><p>使用<a href="https://cs231n.github.io/neural-networks-case-study/">CS231n 中的螺旋数据创建功能</a> 创建多类数据集（代码见下文）。</p><ul><li>构建一个能够拟合数据的模型（您可能需要线性和非线性层的组合）。</li><li>构建一个能够处理多类数据的损失函数和优化器（可选扩展：使用 Adam 优化器而不是 SGD，您可能必须尝试不同的学习率值才能使其发挥作用）。</li><li>对多类数据进行训练和测试循环，并在其上训练模型以达到 95% 以上的测试准确率（您可以在此处使用任何您喜欢的准确率测量函数）。</li><li>根据模型预测在螺旋数据集上绘制决策边界，该<code>plot_decision_boundary()</code>函数也适用于该数据集。</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Code for creating a spiral dataset from CS231n</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">N = <span class="number">100</span> <span class="comment"># number of points per class</span></span><br><span class="line">D = <span class="number">2</span> <span class="comment"># dimensionality</span></span><br><span class="line">K = <span class="number">3</span> <span class="comment"># number of classes</span></span><br><span class="line">X = np.zeros((N*K,D)) <span class="comment"># data matrix (each row = single example)</span></span><br><span class="line">y = np.zeros(N*K, dtype=<span class="string">&#x27;uint8&#x27;</span>) <span class="comment"># class labels</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">  ix = <span class="built_in">range</span>(N*j,N*(j+<span class="number">1</span>))</span><br><span class="line">  r = np.linspace(<span class="number">0.0</span>,<span class="number">1</span>,N) <span class="comment"># radius</span></span><br><span class="line">  t = np.linspace(j*<span class="number">4</span>,(j+<span class="number">1</span>)*<span class="number">4</span>,N) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># theta</span></span><br><span class="line">  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]</span><br><span class="line">  y[ix] = j</span><br><span class="line"><span class="comment"># lets visualize the data</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="Extra-curriculum-课外活动"><a href="#Extra-curriculum-课外活动" class="headerlink" title="Extra-curriculum 课外活动"></a>Extra-curriculum 课外活动</h1><ul><li>写下 3 个您认为机器分类可能有用的问题（可以是任何问题，您可以发挥创造力，例如，根据购买金额和购买地点特征将信用卡交易分类为欺诈或非欺诈）。</li><li>研究基于梯度的优化器（如 SGD 或 Adam）中的“动量”概念，它是什么意思？</li><li>花 10 分钟阅读<a href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions">Wikipedia 上关于不同激活函数的页面</a>，其中有多少个你能与<a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">PyTorch 的激活函数</a>相媲美？</li><li>研究何时准确度可能不是一个好的衡量标准（提示：阅读<a href="https://willkoehrsen.github.io/statistics/learning/beyond-accuracy-precision-and-recall/">Will Koehrsen 的《超越准确度》</a>来获取想法）。</li><li><strong>观看：</strong>要了解我们的神经网络内部发生的情况以及它们如何学习，请观看<a href="https://youtu.be/7sB052Pz0sQ">麻省理工学院的深度学习简介视频</a>。</li></ul>]]></content>
    
    
    <summary type="html">PyTorch-26H-3</summary>
    
    
    
    <category term="PyTorch" scheme="http://hibiscidai.com/categories/PyTorch/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="PyTorch" scheme="http://hibiscidai.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-26H-2</title>
    <link href="http://hibiscidai.com/2024/08/15/PyTorch-26H-2/"/>
    <id>http://hibiscidai.com/2024/08/15/PyTorch-26H-2/</id>
    <published>2024-08-15T12:00:00.000Z</published>
    <updated>2024-10-17T13:14:12.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2.png" class="" title="PyTorch-26H-2"><p>PyTorch-26H-2</p><span id="more"></span><h1 id="PyTorch-26H-2"><a href="#PyTorch-26H-2" class="headerlink" title="PyTorch-26H-2"></a>PyTorch-26H-2</h1><p>主页：<a href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p><p>youtub：<a href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p><p>github：<a href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p><p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p><p>PyTorch documentation：<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><h1 id="Chapter-1-–-PyTorch-Workflow-Fundamentals"><a href="#Chapter-1-–-PyTorch-Workflow-Fundamentals" class="headerlink" title="Chapter 1 – PyTorch Workflow Fundamentals"></a>Chapter 1 – PyTorch Workflow Fundamentals</h1><p>机器学习和深度学习的本质是从过去获取一些数据，建立一种算法（如神经网络）来发现其中的模式，并利用发现的模式来预测未来。</p><p>从一条直线开始，构建一个 PyTorch 模型来学习直线的模式并进行匹配。</p><h2 id="What-we’re-going-to-cover"><a href="#What-we’re-going-to-cover" class="headerlink" title="What we’re going to cover"></a>What we’re going to cover</h2><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-1.png" class="" title="PyTorch-26H-2-1"><div class="table-container"><table><thead><tr><th style="text-align:center">话题</th><th style="text-align:center">内容</th></tr></thead><tbody><tr><td style="text-align:center">1. 准备数据</td><td style="text-align:center">数据可以是任何东西，但首先我们要创建一条简单的直线</td></tr><tr><td style="text-align:center">2. 建立模型</td><td style="text-align:center">在这里我们将创建一个模型来学习数据中的模式，我们还将选择一个损失函数、优化器并建立一个训练循环。</td></tr><tr><td style="text-align:center">3. 将模型拟合到数据（训练）</td><td style="text-align:center">我们有数据和模型，现在让我们让模型（尝试）在（训练）数据中寻找模式。</td></tr><tr><td style="text-align:center">4. 做出预测并评估模型（推理）</td><td style="text-align:center">我们的模型在数据中发现了模式，让我们将它的发现与实际（测试）数据进行比较。</td></tr><tr><td style="text-align:center">5. 保存和加载模型</td><td style="text-align:center">在其他地方使用模型，或者稍后再回来。</td></tr><tr><td style="text-align:center">6. 综合起来</td><td style="text-align:center">让我们把以上所有内容结合起来。</td></tr></tbody></table></div><h2 id="Where-can-you-get-help"><a href="#Where-can-you-get-help" class="headerlink" title="Where can you get help?"></a>Where can you get help?</h2><p>本课程的所有材料均可在 <a href="https://github.com/mrdbourke/pytorch-deep-learning">GitHub</a> 上找到。</p><p>如果您遇到麻烦，您也可以在<a href="https://github.com/mrdbourke/pytorch-deep-learning/discussions">讨论页面</a>上提问。</p><p>还有<a href="https://discuss.pytorch.org/">PyTorch 开发者论坛</a>，这是一个对所有 PyTorch 相关事宜非常有用的地方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">what_were_covering = &#123;<span class="number">1</span>: <span class="string">&quot;data (prepare and load)&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;build model&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;fitting the model to data (training)&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;making predictions and evaluating a model (inference)&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;saving and loading a model&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;putting it all together&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">what_were_covering = &#123;<span class="number">1</span>: <span class="string">&quot;数据（准备和加载）&quot;</span>,</span><br><span class="line"><span class="number">2</span>: <span class="string">&quot;构建模型&quot;</span>,</span><br><span class="line"><span class="number">3</span>: <span class="string">&quot;将模型与数据拟合（训练）&quot;</span>,</span><br><span class="line"><span class="number">4</span>: <span class="string">&quot;进行预测和评估模型（推理）&quot;</span>,</span><br><span class="line"><span class="number">5</span>: <span class="string">&quot;保存和加载模型&quot;</span>,</span><br><span class="line"><span class="number">6</span>: <span class="string">&quot;将所有内容整合在一起&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>模块导入</p><p>我们将获得<code>torch</code>，<code>torch.nn</code>（nn代表神经网络，这个包包含在 PyTorch 中创建神经网络的构建块）和<code>matplotlib</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn <span class="comment"># nn contains all of PyTorch&#x27;s building blocks for neural networks</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check PyTorch version</span></span><br><span class="line">torch.__version__</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;2.4.1&#x27;</span><br></pre></td></tr></table></figure><h2 id="1-Data-preparing-and-loading"><a href="#1-Data-preparing-and-loading" class="headerlink" title="1. Data (preparing and loading)"></a>1. Data (preparing and loading)</h2><p>机器学习中的“数据”几乎可以是任何你能想象到的东西。数字表（比如一个大的 Excel 电子表格）、任何类型的图像、视频（YouTube 上有大量数据！）、歌曲或播客等音频文件、蛋白质结构、文本等等。</p><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-2.png" class="" title="PyTorch-26H-2-2"><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-3.png" class="" title="PyTorch-26H-2-3"><p>机器学习是一个由两部分组成的游戏：</p><p>1、<strong>数据（无论它是什么）转换为数字（一种表示）。</strong><br>2、<strong>选择或建立一个模型来尽可能好地学习表示。</strong></p><p>有时一项和两项可以同时进行。但是如果没有数据怎么办？嗯，这就是我们现在的情况。没有数据。但我们可以创造一些。我们将数据创建为一条直线。</p><p>我们将使用 <a href="https://en.wikipedia.org/wiki/Linear_regression">线性回归</a> 来创建具有已知参数（模型可以学习的东西）的数据，然后我们将使用 PyTorch 来查看是否可以构建模型来使用 <a href="https://en.wikipedia.org/wiki/Gradient_descent">梯度下降</a> 来估计这些参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create *known* parameters</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.02</span></span><br><span class="line">X = torch.arange(start, end, step).unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">y = weight * X + bias</span><br><span class="line"></span><br><span class="line">X[:<span class="number">10</span>], y[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0.0000],</span><br><span class="line">         [0.0200],</span><br><span class="line">         [0.0400],</span><br><span class="line">         [0.0600],</span><br><span class="line">         [0.0800],</span><br><span class="line">         [0.1000],</span><br><span class="line">         [0.1200],</span><br><span class="line">         [0.1400],</span><br><span class="line">         [0.1600],</span><br><span class="line">         [0.1800]]),</span><br><span class="line"> tensor([[0.3000],</span><br><span class="line">         [0.3140],</span><br><span class="line">         [0.3280],</span><br><span class="line">         [0.3420],</span><br><span class="line">         [0.3560],</span><br><span class="line">         [0.3700],</span><br><span class="line">         [0.3840],</span><br><span class="line">         [0.3980],</span><br><span class="line">         [0.4120],</span><br><span class="line">         [0.4260]]))</span><br></pre></td></tr></table></figure><p>开始构建一个可以学习X（特征）和y（标签）之间关系的模型。</p><h3 id="Split-data-into-training-and-test-sets"><a href="#Split-data-into-training-and-test-sets" class="headerlink" title="Split data into training and test sets"></a>Split data into training and test sets</h3><p>在建立模型之前，我们需要将其拆分。</p><p>机器学习项目中最重要的步骤之一是创建训练和测试集（必要时还要创建验证集）。</p><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-3_2.png" class="" title="PyTorch-26H-2-3_2"><div class="table-container"><table><thead><tr><th style="text-align:center">分类</th><th style="text-align:center">目的</th><th style="text-align:center">总数据量</th><th style="text-align:center">使用频率</th></tr></thead><tbody><tr><td style="text-align:center">训练集</td><td style="text-align:center">模型从这些数据中学习（例如您在学期期间学习的课程材料）。</td><td style="text-align:center">~60-80％</td><td style="text-align:center">Always</td></tr><tr><td style="text-align:center">验证集</td><td style="text-align:center">模型会根据这些数据进行调整（就像期末考试之前进行的模拟考试一样）。</td><td style="text-align:center">~10-20%</td><td style="text-align:center">Often but not always</td></tr><tr><td style="text-align:center">测试集</td><td style="text-align:center">模型会根据这些数据进行评估，以测试其所学到的知识（就像学期末参加的期末考试一样）。</td><td style="text-align:center">~10-20%</td><td style="text-align:center">Always</td></tr></tbody></table></div><p>只使用训练和测试集，这意味着我们将拥有一个数据集供我们的模型学习和评估。</p><p>通过分割X和Y张量来创建它们。</p><blockquote><p>处理真实数据时，此步骤通常在项目开始时完成（测试集应始终与所有其他数据分开）。我们希望我们的模型从训练数据中学习，然后在测试数据上对其进行评估，以了解它对未见过的示例的推广效果如何。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create train/test split</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X)) <span class="comment"># 80% of data used for training set, 20% for testing </span></span><br><span class="line">X_train, y_train = X[:train_split], y[:train_split]</span><br><span class="line">X_test, y_test = X[train_split:], y[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(X_train), len(y_train), len(X_test), len(y_test)</span><br></pre></td></tr></table></figure><p>40 个样本用于训练（X_train &amp; y_train）和 10 个样本用于测试（X_test &amp; y_test）。</p><p>创建的模型将尝试学习X_train &amp; y_train之间的关系，然后我们将评估它在 X_test 和 y_test 上的学习内容。</p><p>创建函数可视化数字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_predictions</span>(<span class="params">train_data=X_train, </span></span><br><span class="line"><span class="params">                     train_labels=y_train, </span></span><br><span class="line"><span class="params">                     test_data=X_test, </span></span><br><span class="line"><span class="params">                     test_labels=y_test, </span></span><br><span class="line"><span class="params">                     predictions=<span class="literal">None</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Plots training data, test data and compares predictions.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Plot training data in blue</span></span><br><span class="line">  plt.scatter(train_data, train_labels, c=<span class="string">&quot;b&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Training data&quot;</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Plot test data in green</span></span><br><span class="line">  plt.scatter(test_data, test_labels, c=<span class="string">&quot;g&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Testing data&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> predictions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># Plot the predictions in red (predictions were made on the test data)</span></span><br><span class="line">    plt.scatter(test_data, predictions, c=<span class="string">&quot;r&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Predictions&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Show the legend</span></span><br><span class="line">  plt.legend(prop=&#123;<span class="string">&quot;size&quot;</span>: <span class="number">14</span>&#125;)</span><br><span class="line"></span><br><span class="line">plot_predictions()</span><br></pre></td></tr></table></figure><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-4.png" class="" title="PyTorch-26H-2-4"><h2 id="2-Build-model"><a href="#2-Build-model" class="headerlink" title="2. Build model"></a>2. Build model</h2><p>建立一个模型，使用蓝点来预测绿点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a Linear Regression model class</span></span><br><span class="line"><span class="comment"># 创建线性回归模型类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module): <span class="comment"># &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)PyTorch 中的几乎所有东西都是 nn.Module（可以将其视为神经网络乐高积木）</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="comment"># &lt;- start with random weights (this will get adjusted as the model learns)从随机权重开始（这将随着模型的学习而进行调整）</span></span><br><span class="line">                                                dtype=torch.<span class="built_in">float</span>), <span class="comment"># &lt;- PyTorch loves float32 by defaultPyTorch 默认喜欢 float32</span></span><br><span class="line">                                   requires_grad=<span class="literal">True</span>) <span class="comment"># &lt;- can we update this value with gradient descent?)我们可以用梯度下降来更新这个值吗？）</span></span><br><span class="line"></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="comment"># &lt;- start with random bias (this will get adjusted as the model learns)从随机偏差开始（这将随着模型的学习而进行调整）</span></span><br><span class="line">                                            dtype=torch.<span class="built_in">float</span>), <span class="comment"># &lt;- PyTorch loves float32 by default</span></span><br><span class="line">                                requires_grad=<span class="literal">True</span>) <span class="comment"># &lt;- can we update this value with gradient descent?))我们可以用梯度下降来更新这个值吗？）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward defines the computation in the model Forward 定义模型中的计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor: <span class="comment"># &lt;- &quot;x&quot; is the input data (e.g. training/testing features) “x”是输入数据（例如训练/测试特征）</span></span><br><span class="line">        <span class="keyword">return</span> self.weights * x + self.bias <span class="comment"># &lt;- this is the linear regression formula (y = m*x + b) 这是线性回归公式 (y = m*x + b)</span></span><br></pre></td></tr></table></figure><p>Start with random values (weight &amp; bias)<br>从随机值开始（权重和偏差）</p><p>Look at training data and adjust the random values to better represent (or get closer to) the ideal values (the weight &amp; bias values we used to create the data)<br>查看训练数据并调整随机值以更好地表示（或更接近）理想值（我们用于创建数据的权重和偏差值）</p><p>Through two main algorithms:<br>通过两种主要算法：</p><ol><li>Gradient descent：<a href="https://youtu.be/IHZwWFHWa-w">https://youtu.be/IHZwWFHWa-w</a></li><li>梯度下降：<a href="https://youtu.be/IHZwWFHWa-w">https://youtu.be/IHZwWFHWa-w</a></li><li>Backpropagation：<a href="https://youtu.be/llg3gGewQ5U">https://youtu.be/llg3gGewQ5U</a></li><li>反向传播：<a href="https://youtu.be/llg3gGewQ5U">https://youtu.be/llg3gGewQ5U</a></li></ol><p><a href="https://realpython.com/python3-object-oriented-programming/">python3面向对象编程指南</a></p><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-4_2.png" class="" title="PyTorch-26H-2-4_2"><p>Subclass nn.Module(this contains all the building blocks for neural networks)子类 nn.Module（包含神经网络的所有构建块）</p><p>Initialise model parameters to be used in various computations (these could be diMerent layers from torch.nn, single parameters, hard-coded values or functions)初始化用于各种计算的模型参数（这些参数可能是来自 torch.nn 的不同层、单个参数、硬编码值或函数）</p><p><code>requires_grad =True</code> means PyTorch will track the gradients of this speciLc parameter for use with torch.autograd and gradient descent (for many torch.nn modules, <code>requires_grad =True</code> is set by default)<code>require_grad =True</code> 表示 PyTorch 将跟踪此特定参数的梯度，以便与 torch.autograd 和梯度下降一起使用（对于许多 torch.nn 模块，<code>requires_grad =True</code> 是默认设置的）</p><p>Any subclass of nn.Module needs to override <code>forward()</code> (this deLnes the forward computation of the model)nn.Module 的任何子类都需要重写 <code>forward()</code>（这定义了模型的前向计算）</p><h3 id="PyTorch-model-building-essentials-PyTorch-模型构建要点"><a href="#PyTorch-model-building-essentials-PyTorch-模型构建要点" class="headerlink" title="PyTorch model building essentials PyTorch 模型构建要点"></a>PyTorch model building essentials PyTorch 模型构建要点</h3><p>PyTorch 有四个（大约）基本模块，<a href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a>、<a href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a>、<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code>torch.utils.data.Dataset</code></a>、<a href="https://pytorch.org/docs/stable/data.html"><code>torch.utils.data.DataLoader</code></a>你可以用它们来创建几乎任何你能想到的神经网络。</p><div class="table-container"><table><thead><tr><th style="text-align:center">PyTorch模块</th><th style="text-align:center">作用</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://pytorch.org/docs/stable/nn.html">torch.nn</a></td><td style="text-align:center">包含计算图的所有构建块（本质上是以特定方式执行的一系列计算）。</td></tr><tr><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter">torch.nn.Parameter</a></td><td style="text-align:center">存储可以与 一起使用的张量nn.Module。如果requires_grad=True梯度（用于通过<a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html">梯度下降</a>更新模型参数）是自动计算的，这通常被称为“autograd”。</td></tr><tr><td style="text-align:center"><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">torch.nn.Module</a></td><td style="text-align:center">所有神经网络模块的基类，神经网络的所有构建块都是子类。如果你在 PyTorch 中构建神经网络，你的模型应该是子类nn.Module。需要forward()实现一个方法。</td></tr><tr><td style="text-align:center"><a href="https://pytorch.org/docs/stable/optim.html">torch.optim</a></td><td style="text-align:center">包含各种优化算法（这些算法告诉存储的模型参数nn.Parameter如何最好地改变以改善梯度下降并进而减少损失）。</td></tr><tr><td style="text-align:center">def forward()</td><td style="text-align:center">所有nn.Module子类都需要一种方法，它定义了传递给特定的数据（例如上面的线性回归公式）forward()将进行的计算。nn.Module</td></tr></tbody></table></div><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-4_3.png" class="" title="PyTorch-26H-2-4_3"><p>PyTorch 神经网络中的几乎所有内容都来自<code>torch.nn</code>。</p><p><code>nn.Module</code>包含较大的构建块（层）<br><code>nn.Parameter</code>包含较小的参数，如权重和偏差（将它们放在一起形成<code>nn.Module(s)</code>）<br><code>forward()</code>告诉较大的块如何在 <code>nn.Module(s)</code> 内对输入（充满数据的张量）进行计算<br><code>torch.optim</code>包含关于如何改进参数<code>nn.Parameter</code>以更好地表示输入数据的优化方法</p><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-5.png" class="" title="PyTorch-26H-2-5"><p>子类 <code>nn.Module</code>（包含神经网络的所有构建块）<br>初始化用于各种计算的<code>模型参数</code>（这些参数可能是来自torch.nn 的不同层、单个参数、硬编码值或函数）<br><code>require_grad=True</code> 表示 PyTorch 将跟踪此特定参数的梯度，以便与 <code>torch.autograd</code> 和梯度下降一起使用（对于许多 <code>torch.nn</code> 模块，<code>requires_grad=True</code> 是默认设置）<br><code>nn.Module</code> 的任何子类都需要重写 <code>forward()</code>（这定义了模型的前向计算）</p><p>通过子类化创建 PyTorch 模型的基本构建块<code>nn.Module</code>。对于子类化的对象<code>nn.Module</code>，<code>forward()</code>必须定义方法。</p><p>在 <a href="https://pytorch.org/tutorials/beginner/ptcheat.html">PyTorch Cheat Sheet</a> 中查看更多这些基本模块及其用例。</p><h3 id="Checking-the-contents-of-a-PyTorch-model-检查-PyTorch-模型的内容"><a href="#Checking-the-contents-of-a-PyTorch-model-检查-PyTorch-模型的内容" class="headerlink" title="Checking the contents of a PyTorch model 检查 PyTorch 模型的内容"></a>Checking the contents of a PyTorch model 检查 PyTorch 模型的内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set manual seed since nn.Parameter are randomly initialized由于 nn.Parameter 是随机初始化的，因此请设置手动种子</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))创建模型的实例（这是包含 nn.Parameter(s) 的 nn.Module 的子类）</span></span><br><span class="line">model_0 = LinearRegressionModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the nn.Parameter(s) within the nn.Module subclass we created检查我们创建的 nn.Module 子类中的 nn.Parameter(s)</span></span><br><span class="line"><span class="built_in">list</span>(model_0.parameters())</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[Parameter containing:</span><br><span class="line"> tensor([0.3367], requires_grad=True),</span><br><span class="line"> Parameter containing:</span><br><span class="line"> tensor([0.1288], requires_grad=True)]</span><br></pre></td></tr></table></figure><p>我们还可以使用 获取模型的状态（模型包含的内容）<code>.state_dict()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List named parameters </span></span><br><span class="line">model_0.state_dict()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([(&#x27;weights&#x27;, tensor([0.3367])), (&#x27;bias&#x27;, tensor([0.1288]))])</span><br></pre></td></tr></table></figure><p>注意 <code>model_0.state_dict()</code> 中的权重和偏差的值是如何作为随机浮点张量出现的吗？<br>这是因为我们上面使用 <code>torch.randn()</code> 初始化了它们。<br>本质上，我们希望从随机参数开始，并让模型将它们更新为最适合我们数据的参数（我们在创建直线数据时设置的硬编码 <code>weight</code> 和 <code>bias</code>）。</p><p>尝试改变上面两个单元格的 <code>torch.manual_seed()</code> 值，看看权重和偏差值会发生什么变化。<br>因为我们的模型从随机值开始，所以现在它的预测能力较差。</p><h3 id="Making-predictions-using-torch-inference-mode"><a href="#Making-predictions-using-torch-inference-mode" class="headerlink" title="Making predictions using torch.inference_mode()"></a>Making predictions using <code>torch.inference_mode()</code></h3><p>将测试数据传递给它，<code>X_test</code> 看看它的预测有多接近 <code>y_test</code>。</p><p>将数据传递给模型时，它将通过模型的 <code>forward()</code> 方法并使用我们定义的计算产生结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions with model</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode(): </span><br><span class="line">    y_preds = model_0(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: in older PyTorch code you might also see torch.no_grad()</span></span><br><span class="line"><span class="comment"># with torch.no_grad():</span></span><br><span class="line"><span class="comment">#   y_preds = model_0(X_test)</span></span><br></pre></td></tr></table></figure><p>使用 <code>torch.inference_mode()</code> 作为<a href="https://realpython.com/python-with-statement/">上下文管理器</a>（这就是 <code>torch.inference_mode()</code>: 的作用）来进行预测。</p><p>顾名思义，<code>torch.inference_mode()</code> 用于使用模型进行推理（做出预测）。</p><p><code>torch.inference_mode()</code> 关闭了许多功能（例如梯度跟踪，这对于训练是必需的，但对于推理不是必需的），以使前向传递（数据通过 forward() 方法）更快。</p><p>在较旧的 PyTorch 代码中，您可能还会看到 <code>torch.no_grad()</code> 用于推理。虽然 <code>torch.inference_mode()</code> 和 <code>torch.no_grad()</code> 的作用类似，但 <code>torch.inference_mode()</code> 较新，可能更快且更受欢迎。有关更多信息，请参阅 <a href="https://twitter.com/PyTorch/status/1437838231505096708?s=20">Tweet from PyTorch</a> 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the predictions</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of testing samples: <span class="subst">&#123;<span class="built_in">len</span>(X_test)&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of predictions made: <span class="subst">&#123;<span class="built_in">len</span>(y_preds)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted values:\n<span class="subst">&#123;y_preds&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Number of testing samples: 10</span><br><span class="line">Number of predictions made: 10</span><br><span class="line">Predicted values:</span><br><span class="line">tensor([[0.3982],</span><br><span class="line">        [0.4049],</span><br><span class="line">        [0.4116],</span><br><span class="line">        [0.4184],</span><br><span class="line">        [0.4251],</span><br><span class="line">        [0.4318],</span><br><span class="line">        [0.4386],</span><br><span class="line">        [0.4453],</span><br><span class="line">        [0.4520],</span><br><span class="line">        [0.4588]])</span><br></pre></td></tr></table></figure><p>请注意每个测试样本有一个预测值。</p><p>这是因为我们使用的数据类型。对于我们的直线，一个X值对应一个y值。</p><p>然而，机器学习模型非常灵活。可以将 100 个X值映射到一个、两个、三个或 10 个y值。这完全取决于正在处理的内容。</p><p>预测仍然是页面上的数字，使用<code>plot_predictions()</code>上面创建的函数将它们可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions(predictions=y_preds)</span><br></pre></td></tr></table></figure><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-6.png" class="" title="PyTorch-26H-2-6"><p>对比预测结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_test - y_preds</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4618],</span><br><span class="line">        [0.4691],</span><br><span class="line">        [0.4764],</span><br><span class="line">        [0.4836],</span><br><span class="line">        [0.4909],</span><br><span class="line">        [0.4982],</span><br><span class="line">        [0.5054],</span><br><span class="line">        [0.5127],</span><br><span class="line">        [0.5200],</span><br><span class="line">        [0.5272]])</span><br></pre></td></tr></table></figure><p>使用随机参数做出预测，没有进行观察的结果，差距很大。</p><h2 id="3-Train-model-训练模型"><a href="#3-Train-model-训练模型" class="headerlink" title="3. Train model 训练模型"></a>3. Train model 训练模型</h2><p>模型正在使用随机参数进行计算进行预测，这基本上是猜测（随机）。</p><p>更新其内部参数（将参数称为模式），使用<code>weights</code>和<code>bias</code>随机设置的值<code>nn.Parameter()</code>，<code>torch.randn()</code>以便更好地表示数据。</p><p>可以对此进行硬编码（默认值weight=0.7和bias=0.3）</p><h3 id="Creating-a-loss-function-and-optimizer-in-PyTorch-在-PyTorch-中创建损失函数和优化器"><a href="#Creating-a-loss-function-and-optimizer-in-PyTorch-在-PyTorch-中创建损失函数和优化器" class="headerlink" title="Creating a loss function and optimizer in PyTorch 在 PyTorch 中创建损失函数和优化器"></a>Creating a loss function and optimizer in PyTorch 在 PyTorch 中创建损失函数和优化器</h3><div class="table-container"><table><thead><tr><th style="text-align:center">功能</th><th style="text-align:center">作用</th><th style="text-align:center">位置</th><th style="text-align:center">价值</th></tr></thead><tbody><tr><td style="text-align:center">损失函数</td><td style="text-align:center">衡量模型预测与真实标签相比的误差程度。误差越低越好。</td><td style="text-align:center">内置损失函数<code>torch.nn</code></td><td style="text-align:center">回归问题平均绝对误差(MAE)<code>torch.nn.L1Loss()</code>,二元分类问题的二元交叉熵<code>torch.nn.BCELoss()</code></td></tr><tr><td style="text-align:center">优化器</td><td style="text-align:center">告诉模型如何更新其内部参数以最好的降低损失。</td><td style="text-align:center">优化函数实现 <code>torch.optim</code></td><td style="text-align:center">随机梯度下降 <code>torch.optim.SGD()</code> , Adam优化器<code>torch.optim.Adam()</code></td></tr></tbody></table></div><p>据处理的问题类型，将决定使用的损失函数和优化器。</p><p>经验：SGD（随机梯度下降）或 Adam 优化器，效果很好。用于回归问题（预测数字）的 MAE（平均绝对误差）损失函数或用于分类问题（预测一件事或另一件事）的二元交叉熵损失函数。</p><p>对于我们的问题，因为我们正在预测一个数字，所以我们使用 PyTorch 中的 MAE（位于 <code>torch.nn.L1Loss()</code> 下）作为我们的损失函数。</p><p>平均绝对误差 (MAE，在 PyTorch 中为：<code>torch.nn.L1Loss</code>) 测量两点（预测和标签）之间的绝对差异，然后对所有示例取平均值。</p><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-7.png" class="" title="PyTorch-26H-2-7"><p>我们将使用 SGD，<code>torch.optim.SGD(params, lr)</code>，其中：</p><p><code>params</code> 是您想要优化的目标模型参数（例如我们之前随机设置的<code>weights</code>和<code>bias</code>）。<br><code>lr</code> 是您希望优化器更新参数的学习率，越高意味着优化器将尝试更大的更新（这些更新有时可能太大，优化器将无法工作），越低意味着优化器将尝试较小的更新（这些更新有时可能太小，优化器将花费太长时间才能找到理想值）。学习率被视为超参数（因为它是由机器学习工程师设置的）。学习率的常见起始值为 0.01、0.001、0.0001，但是，这些值也可以随着时间的推移进行调整（这称为<a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">学习率调度</a>）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the loss function 创建损失函数</span></span><br><span class="line">loss_fn = nn.L1Loss() <span class="comment"># MAE loss is same as L1Loss MAE 损失与 L1Loss 相同</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the optimizer 创建优化器</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_0.parameters(), <span class="comment"># parameters of target model to optimize 待优化目标模型参数</span></span><br><span class="line">                            lr=<span class="number">0.01</span>) <span class="comment"># learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))学习率（优化器在每一步应该改变多少参数，越高=越多（越不稳定），越低=越少（可能需要很长时间））</span></span><br></pre></td></tr></table></figure><h3 id="Creating-an-optimization-loop-in-PyTorch-在-PyTorch-中创建优化循环"><a href="#Creating-an-optimization-loop-in-PyTorch-在-PyTorch-中创建优化循环" class="headerlink" title="Creating an optimization loop in PyTorch 在 PyTorch 中创建优化循环"></a>Creating an optimization loop in PyTorch 在 PyTorch 中创建优化循环</h3><p>训练循环涉及模型 遍历训练数据并学习<code>features</code>和<code>labels</code>之间的关系。</p><p>测试循环涉及检查测试数据并评估模型在训练数据上学习到的模式的优劣（模型在训练期间永远不会看到测试数据）。</p><p>每个都称为一个“循环”，因为我们希望我们的模型查看（循环）每个数据集中的每个样本。</p><h3 id="PyTorch-training-loop-PyTorch-训练循环"><a href="#PyTorch-training-loop-PyTorch-训练循环" class="headerlink" title="PyTorch training loop PyTorch 训练循环"></a>PyTorch training loop PyTorch 训练循环</h3><p>训练步骤：</p><div class="table-container"><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">步骤</th><th style="text-align:center">作用</th><th style="text-align:center">示例</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">Forward pass</td><td style="text-align:center">该模型会遍历所有训练数据一次，并执行其 <code>forward()</code> 函数计算。</td><td style="text-align:center"><code>model(x_train)</code></td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">Calculate the loss</td><td style="text-align:center">将模型的输出（预测）与基本事实进行比较，并进行评估以查看其错误程度。</td><td style="text-align:center"><code>loss = loss_fn(y_pred, y_train)</code></td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">Zero gradients</td><td style="text-align:center">优化器的梯度设置为零（默认情况下是累积的），因此可以针对特定的训练步骤重新计算它们。</td><td style="text-align:center"><code>optimizer.zero_grad()</code></td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">Perform backpropagation on the loss（Loss backward）</td><td style="text-align:center">计算每个要更新的模型参数的损失梯度（每个参数的 <code>require_grad=True</code>）。这称为反向传播，因此是“向后”的。</td><td style="text-align:center"><code>loss.backward()</code></td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">Update the optimizer (gradient descent)</td><td style="text-align:center">使用 <code>require_grad=True</code> 来根据损失梯度更新参数，以改进它们。</td><td style="text-align:center"><code>optimizer.step()</code></td></tr></tbody></table></div><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-8.png" class="" title="PyTorch-26H-2-8"><p>Pass the data through the model for a number of epochs (e.g. 100 for 100 passes of the data)<br>将数据通过模型传递若干个时期（例如，100 次数据传递为 100 个时期）</p><p>Pass the data through the model, this will perform the <code>forward()</code> method located within the model object<br>通过模型传递数据，这将执行位于模型对象内的 <code>forward()</code> 方法</p><p>Calculate the loss value (how wrong the model’s predictions are)<br>计算损失值（模型预测的错误程度）</p><p>Zero the optimizer gradients (they accumulate every epoch, zero them to start fresh each forward pass)<br>将优化器梯度归零（它们在每个时期都会累积，在每次前向传递时将它们归零以重新开始）</p><p>Perform backpropagation on the loss function (compute the gradient of every parameter with <code>requires_grad=True</code>)<br>对损失函数进行反向传播（使用 <code>require_grad=True</code> 计算每个参数的梯度）</p><p>Step the optimizer to update the model’s parameters with respect to the gradients calculated by <code>loss.backward()</code><br>让优化器根据 <code>loss.backward()</code> 计算出的梯度来更新模型的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">1</span></span><br><span class="line"><span class="comment"># Pass the data through the model for a number of epochs (e.g. 100)</span></span><br><span class="line"><span class="comment"># 将数据通过模型传递若干个时期（例如 100）</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs) :</span><br><span class="line">    <span class="comment"># Put model in training mode (this is the default state of a model)</span></span><br><span class="line">    <span class="comment"># 将模型置于训练模式（这是模型的默认状态）</span></span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. Forward pass on train data using the forward() method inside</span></span><br><span class="line">    <span class="comment"># 1. 使用内部的 forward() 方法向前传递训练数据</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate the Loss (how different are the model&#x27;s predictions to the true values</span></span><br><span class="line">    <span class="comment"># 2. 计算损失（模型的预测与真实值有多大差异</span></span><br><span class="line">    Loss = Loss_fn(y_pred, y_true)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. Zero the gradients of the optimizer (they accumulate by default)</span></span><br><span class="line">    <span class="comment"># 3. 将优化器的梯度归零（默认情况下它们会累积）</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. Perform backpropagation on the loss</span></span><br><span class="line">    <span class="comment"># 4. 对损失进行反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. Progress/step the optimizer ( gradient descent)</span></span><br><span class="line">    <span class="comment"># 5. 推进/步进优化器（梯度下降）</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><blockquote><p>以上只是步骤排序或描述的一个例子。随着经验的积累，你会发现制作 PyTorch 训练循环可以非常灵活。</p></blockquote><p>训练循环歌曲:<br><a href="https://www.youtube.com/watch?v=Nutpusq_AFw">The Unofficial PyTorch Optimization Loop Song</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">It&#x27;s train time!</span><br><span class="line">do the forward pass,</span><br><span class="line">calculate the loss,</span><br><span class="line">optimizer zero grad,</span><br><span class="line">losssss backwards!</span><br><span class="line"></span><br><span class="line">Optimizer step step step</span><br><span class="line"></span><br><span class="line">Let&#x27;s test now!</span><br><span class="line">with torch no grad:</span><br><span class="line">do the forward pass,</span><br><span class="line">calculate the loss,</span><br><span class="line">watch it go down down down!</span><br></pre></td></tr></table></figure><p>至于事物的顺序，以上是一个很好的默认顺序，但你可能会看到略有不同的顺序。一些经验法则：</p><ul><li>在对损失执行反向传播 <code>(loss.backward())</code> 之前，先计算损失 <code>(loss = ...)</code>。</li><li>在针对每个模型参数 <code>(loss.backward())</code> 计算损失的梯度之前，先将梯度归零 <code>(optimizer.zero_grad())</code>。</li><li>在对损失执行反向传播 <code>(loss.backward())</code> 之后，逐步执行优化器 <code>(optimizer.step())</code>。</li></ul><p>有关帮助理解反向传播和梯度下降幕后情况的资源，请参阅课外部分。</p><h3 id="PyTorch-testing-loop-PyTorch-测试循环"><a href="#PyTorch-testing-loop-PyTorch-测试循环" class="headerlink" title="PyTorch testing loop PyTorch 测试循环"></a>PyTorch testing loop PyTorch 测试循环</h3><div class="table-container"><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">步骤</th><th style="text-align:center">作用</th><th style="text-align:center">例子</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">Forward pass</td><td style="text-align:center">该模型会遍历所有测试数据一次，并执行其 <code>forward()</code> 函数计算。</td><td style="text-align:center"><code>model(x_test)</code></td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">Calculate the loss</td><td style="text-align:center">将模型的输出（预测）与基本事实进行比较，并进行评估以查看其错误程度。</td><td style="text-align:center"><code>loss = loss_fn(y_pred, y_test)</code></td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">Calulate evaluation metrics (optional)</td><td style="text-align:center">除了损失值之外，您可能还想计算其他评估指标，例如测试集的准确性。</td><td style="text-align:center">Custom functions</td></tr></tbody></table></div><p>请注意，测试循环不包含执行反向传播（<code>loss.backward()</code>）或步进优化器（<code>optimizer.step()</code>），这是因为在测试期间模型中的任何参数都不会改变，它们已经被计算出来了。对于测试，我们只对通过模型的前向传递的输出感兴趣。</p><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-9.png" class="" title="PyTorch-26H-2-9"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup empty lists to keep track of model progress</span></span><br><span class="line"><span class="comment"># 设置空列表来跟踪模型进度</span></span><br><span class="line">epoch_count = []</span><br><span class="line">train_loss_values = []</span><br><span class="line">test_loss_values = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pass the data through the model for a number of epochs (e.g. 100) pochs):</span></span><br><span class="line"><span class="comment"># 将数据通过模型传递若干个时期（例如 100 个时期）：</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span> (epochs):</span><br><span class="line">    <span class="comment">### Training Loop code here ###</span></span><br><span class="line">    <span class="comment">### Testing starts ###</span></span><br><span class="line">    <span class="comment"># Put the model in evaluation mode </span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># Turn on inference mode context manager :</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        <span class="comment"># 1. Forward pass on test data</span></span><br><span class="line">        test_pred = model(X_test)</span><br><span class="line">        <span class="comment"># 2. Caculate loss on test data</span></span><br><span class="line">        test_loss = Loss_fn(test_pred, y_test) </span><br><span class="line"><span class="comment"># Print out what&#x27;s happening every 10 epochs</span></span><br><span class="line"><span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">    epoch_count.append(epoch)</span><br><span class="line">    train_Loss_values.append(loss)</span><br><span class="line">    test_loss_values.append(test_loss)</span><br><span class="line">    <span class="built_in">print</span>( <span class="string">f&quot; Epoch: <span class="subst">&#123;epoch&#125;</span>| MAE Train Loss: <span class="subst">&#123;loss&#125;</span>I MAE Test Loss: <span class="subst">&#123;test_loss&#125;</span></span></span><br></pre></td></tr></table></figure><p>Create empty lists for storing useful values (helpful for tracking model progress)<br>创建空列表来存储有用的值（有助于跟踪模型进度）</p><p>Tell the model we want to evaluate rather than train (this turns off functionality used for training but not evaluation)<br>告诉模型我们想要评估而不是训练（这会关闭用于训练但不用于评估的功能）</p><p>Turn on <code>torch.inference_mode()</code> context manager to disable functionality such as gradient tracking for inference (gradient tracking not needed for inference)<br>打开 <code>torch.inference_mode()</code> 上下文管理器以禁用推理的梯度跟踪等功能（推理不需要梯度跟踪）</p><p>Pass the test data through the model (this will call the model’s implemented <code>forward()</code> method)<br>通过模型传递测试数据（这将调用模型实现的 <code>forward()</code> 方法）</p><p>Calculate the test loss value (how wrong the model’s predictions are on the test dataset, lower is better)<br>计算测试损失值（模型对测试数据集的预测错误程度，越低越好）</p><p>Display information outputs for how the model is doing during training/testing every ~10 epochs (note: what gets printed out here can be adjusted for speciLc problems)<br>每~10 个时期显示模型在训练/测试过程中的运行情况的信息输出（注意：此处打印的内容可针对具体问题进行调整）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs (how many times the model will pass over the training data)</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty loss lists to track values</span></span><br><span class="line">train_loss_values = []</span><br><span class="line">test_loss_values = []</span><br><span class="line">epoch_count = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put model in training mode (this is the default state of a model)</span></span><br><span class="line">    model_0.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass on train data using the forward() method inside </span></span><br><span class="line">    y_pred = model_0(X_train)</span><br><span class="line">    <span class="comment"># print(y_pred)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate the loss (how different are our models predictions to the ground truth)</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Zero grad of the optimizer</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Progress the optimizer</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the model in evaluation mode</span></span><br><span class="line">    model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass on test data</span></span><br><span class="line">      test_pred = model_0(X_test)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 2. Caculate loss on test data</span></span><br><span class="line">      test_loss = loss_fn(test_pred, y_test.<span class="built_in">type</span>(torch.<span class="built_in">float</span>)) <span class="comment"># predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">      <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            epoch_count.append(epoch)</span><br><span class="line">            train_loss_values.append(loss.detach().numpy())</span><br><span class="line">            test_loss_values.append(test_loss.detach().numpy())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | MAE Train Loss: <span class="subst">&#123;loss&#125;</span> | MAE Test Loss: <span class="subst">&#123;test_loss&#125;</span> &quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 </span><br><span class="line">Epoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 </span><br><span class="line">Epoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 </span><br><span class="line">Epoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 </span><br><span class="line">Epoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 </span><br><span class="line">Epoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 </span><br><span class="line">Epoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 </span><br><span class="line">Epoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 </span><br><span class="line">Epoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 </span><br><span class="line">Epoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819</span><br></pre></td></tr></table></figure><p>查看损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the loss curves</span></span><br><span class="line">plt.plot(epoch_count, train_loss_values, label = <span class="string">&quot;Train loss&quot;</span>)</span><br><span class="line">plt.plot(epoch_count, test_loss_values, label = <span class="string">&quot;Test loss&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training and test loss curves&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epochs&quot;</span>)</span><br><span class="line">plt.legend();</span><br></pre></td></tr></table></figure><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-10.png" class="" title="PyTorch-26H-2-10"><p>损失曲线显示损失随时间下降。请记住，损失是衡量模型错误程度的指标，因此损失越低越好。</p><p>由于损失函数和优化器，模型的内部参数（<code>weights</code> 和 <code>bias</code>）得到了更新，以更好地反映数据中的底层模式。</p><p>让我们检查模型的 <code>.state_dict()</code> 来查看模型与我们为权重和偏差设置的原始值有多接近。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find our model&#x27;s learned parameters</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The model learned the following values for weights and bias:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_0.state_dict())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nAnd the original values for weights and bias are:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;weights: <span class="subst">&#123;weight&#125;</span>, bias: <span class="subst">&#123;bias&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The model learned the following values for weights and bias:</span><br><span class="line">OrderedDict([(&#x27;weights&#x27;, tensor([0.5784])), (&#x27;bias&#x27;, tensor([0.3513]))])</span><br><span class="line"></span><br><span class="line">And the original values for weights and bias are:</span><br><span class="line">weights: 0.7, bias: 0.3</span><br></pre></td></tr></table></figure><p>我们的模型非常接近计算<code>weight</code>和的精确原始值<code>bias</code>（如果我们训练更长时间，它可能会更加接近）。</p><p>当epochs=200：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | MAE Train Loss: 0.024458957836031914 | MAE Test Loss: 0.05646304413676262 </span><br><span class="line">Epoch: 10 | MAE Train Loss: 0.021020207554101944 | MAE Test Loss: 0.04819049686193466 </span><br><span class="line">Epoch: 20 | MAE Train Loss: 0.01758546568453312 | MAE Test Loss: 0.04060482233762741 </span><br><span class="line">Epoch: 30 | MAE Train Loss: 0.014155393466353416 | MAE Test Loss: 0.03233227878808975 </span><br><span class="line">Epoch: 40 | MAE Train Loss: 0.010716589167714119 | MAE Test Loss: 0.024059748277068138 </span><br><span class="line">Epoch: 50 | MAE Train Loss: 0.0072835334576666355 | MAE Test Loss: 0.016474086791276932 </span><br><span class="line">Epoch: 60 | MAE Train Loss: 0.0038517764769494534 | MAE Test Loss: 0.008201557211577892 </span><br><span class="line">Epoch: 70 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 80 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 90 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 100 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 110 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 120 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 130 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 140 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 150 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 160 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 170 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 180 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 190 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882</span><br></pre></td></tr></table></figure><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-11.png" class="" title="PyTorch-26H-2-11"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The model learned the following values for weights and bias:</span><br><span class="line">OrderedDict([(&#x27;weights&#x27;, tensor([0.6990])), (&#x27;bias&#x27;, tensor([0.3093]))])</span><br><span class="line"></span><br><span class="line">And the original values for weights and bias are:</span><br><span class="line">weights: 0.7, bias: 0.3</span><br></pre></td></tr></table></figure><h2 id="4-Making-predictions-with-a-trained-PyTorch-model-inference-使用训练好的-PyTorch-模型进行预测（推理）"><a href="#4-Making-predictions-with-a-trained-PyTorch-model-inference-使用训练好的-PyTorch-模型进行预测（推理）" class="headerlink" title="4. Making predictions with a trained PyTorch model (inference) 使用训练好的 PyTorch 模型进行预测（推理）"></a>4. Making predictions with a trained PyTorch model (inference) 使用训练好的 PyTorch 模型进行预测（推理）</h2><p>使用 <code>PyTorch</code> 模型进行预测（也称为执行推理）时，需要记住三件事：</p><ul><li>将模型设置为评估模式 (<code>model.eval()</code>)。</li><li>使用推理模式上下文管理器进行预测（使用 <code>torch.inference_mode(): ...</code>）。</li><li>所有预测都应使用同一设备上的对象进行（例如，仅在 GPU 上的数据和模型或仅在 CPU 上的数据和模型）。</li></ul><p>前两项确保 PyTorch 在训练期间在后台使用但对推理不必要的所有有用计算和设置均已关闭（这可加快计算速度）。第三项确保您不会遇到跨设备错误。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Set the model in evaluation mode</span></span><br><span class="line">model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Setup the inference mode context manager</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">  <span class="comment"># 3. Make sure the calculations are done with the model and data on the same device</span></span><br><span class="line">  <span class="comment"># in our case, we haven&#x27;t setup device-agnostic code yet so our data and model are</span></span><br><span class="line">  <span class="comment"># on the CPU by default.</span></span><br><span class="line">  <span class="comment"># model_0.to(device)</span></span><br><span class="line">  <span class="comment"># X_test = X_test.to(device)</span></span><br><span class="line">  y_preds = model_0(X_test)</span><br><span class="line">y_preds</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.8141],</span><br><span class="line">        [0.8256],</span><br><span class="line">        [0.8372],</span><br><span class="line">        [0.8488],</span><br><span class="line">        [0.8603],</span><br><span class="line">        [0.8719],</span><br><span class="line">        [0.8835],</span><br><span class="line">        [0.8950],</span><br><span class="line">        [0.9066],</span><br><span class="line">        [0.9182]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions(predictions=y_preds)</span><br></pre></td></tr></table></figure><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-12.png" class="" title="PyTorch-26H-2-12"><h2 id="5-Saving-and-loading-a-PyTorch-model-保存和加载-PyTorch-模型"><a href="#5-Saving-and-loading-a-PyTorch-model-保存和加载-PyTorch-模型" class="headerlink" title="5. Saving and loading a PyTorch model 保存和加载 PyTorch 模型"></a>5. Saving and loading a PyTorch model 保存和加载 PyTorch 模型</h2><p>如果您已经训练了 PyTorch 模型，那么您可能会想要保存它并将其导出到某个地方。</p><p>例如，您可能在 Google Colab 或使用 GPU 的本地机器上训练它，但现在您想将其导出到其他人可以使用的某种应用程序中。</p><p>或者您可能想保存模型的进度，稍后再回来加载它。</p><p>对于在 PyTorch 中保存和加载模型，您应该了解三种主要方法（以下所有内容均取自 <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference">PyTorch 保存和加载模型指南</a>）：</p><div class="table-container"><table><thead><tr><th style="text-align:center">方法</th><th style="text-align:center">作用</th></tr></thead><tbody><tr><td style="text-align:center"><code>torch.save</code></td><td style="text-align:center">使用 Python 的 <code>pickle</code> 实用程序将序列化对象保存到磁盘。可以使用 <code>torch.save</code> 保存模型、张量和各种其他 <code>Python</code> 对象（如字典）。</td></tr><tr><td style="text-align:center"><code>torch.load</code></td><td style="text-align:center">使用 pickle 的 <code>unpickling</code> 功能对 <code>pickle</code> 的 Python 对象文件（如模型、张量或字典）进行反序列化并将其加载到内存中。您还可以设置将对象加载到哪个设备（CPU、GPU 等）。</td></tr><tr><td style="text-align:center"><code>torch.nn.Module.load_state_dict</code></td><td style="text-align:center">使用已保存的 <code>state_dict()</code> 对象加载模型的参数字典 (<code>model.state_dict()</code>)。</td></tr></tbody></table></div><p>正如 <a href="https://docs.python.org/3/library/pickle.html">Python 的 pickle 文档</a>所述，pickle模块实现了用于序列化和反序列化 Python 对象结构的二进制协议，pickle 模块并不安全。这意味着您只应解开（加载）您信任的数据。这也适用于加载 PyTorch 模型。只使用您信任的来源保存的 PyTorch 模型。</p><h3 id="Saving-a-PyTorch-model’s-state-dict-保存-PyTorch-模型的state-dict"><a href="#Saving-a-PyTorch-model’s-state-dict-保存-PyTorch-模型的state-dict" class="headerlink" title="Saving a PyTorch model’s state_dict() 保存 PyTorch 模型的state_dict()"></a>Saving a PyTorch model’s <code>state_dict()</code> 保存 PyTorch 模型的<code>state_dict()</code></h3><p>什么是<code>state_dict()</code>？</p><p>在 PyTorch 中，模型的可学习参数（即权重和偏差） <code>torch.nn.Module</code>包含在模型的参数中（通过 访问<code>model.parameters()</code>）。 <code>Astate_dict</code>只是一个 Python 字典对象，它将每个层映射到其参数张量。<br><code>state_dict</code> 对象是 Python 字典，因此可以轻松保存、更新、更改和恢复它们，从而为 PyTorch 模型和优化器增加大量模块化。请注意，只有具有可学习参数（卷积层、线性层等）和已注册缓冲区（<code>batchnorm</code> 的 <code>running_mean</code>）的层才会在模型的 <code>state_dict</code> 中拥有条目。优化器对象 (<code>torch.optim</code>) 也有一个 <code>state_dict</code>，其中包含有关优化器状态以及所用超参数的信息。</p><p>保存和加载模型进行推理（进行预测）的<a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference">推荐方法</a>是保存和加载模型的 <code>state_dict()</code>。</p><p>让我们看看如何通过几个步骤做到这一点：</p><ul><li>我们将使用 Python 的 <code>pathlib</code> 模块创建一个目录，用于将模型保存到调用的模型中。</li><li>我们将创建一个文件路径来保存模型。</li><li>我们将调用 <code>torch.save(obj, f)</code>，其中 <code>obj</code> 是目标模型的 <code>state_dict()</code>，<code>f</code> 是保存模型的文件名。</li></ul><p>注意：PyTorch 保存的模型或对象通常以 <code>.pt</code> 或 <code>.pth</code> 结尾，例如 <code>saved_model_01.pth</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create models directory </span></span><br><span class="line">MODEL_PATH = Path(<span class="string">&quot;models&quot;</span>)</span><br><span class="line">MODEL_PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create model save path </span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;01_pytorch_workflow_model_0.pth&quot;</span></span><br><span class="line">MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Save the model state dict </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Saving model to: <span class="subst">&#123;MODEL_SAVE_PATH&#125;</span>&quot;</span>)</span><br><span class="line">torch.save(obj=model_0.state_dict(), <span class="comment"># only saving the state_dict() only saves the models learned parameters</span></span><br><span class="line">           f=MODEL_SAVE_PATH) </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Saving model to: models\01_pytorch_workflow_model_0.pth</span><br></pre></td></tr></table></figure><h3 id="Loading-a-saved-PyTorch-model’s-state-dict-加载已保存的-PyTorch-模型的-state-dict"><a href="#Loading-a-saved-PyTorch-model’s-state-dict-加载已保存的-PyTorch-模型的-state-dict" class="headerlink" title="Loading a saved PyTorch model’s state_dict() 加载已保存的 PyTorch 模型的 state_dict()"></a>Loading a saved PyTorch model’s <code>state_dict()</code> 加载已保存的 PyTorch 模型的 <code>state_dict()</code></h3><p>由于我们现在在 <code>models/01_pytorch_workflow_model_0.pth</code> 处有一个保存的模型 <code>state_dict()</code>，我们现在可以使用 <code>torch.nn.Module.load_state_dict(torch.load(f))</code> 加载它，其中 <code>f</code> 是我们保存的模型 <code>state_dict()</code> 的文件路径。</p><p>为什么在 <code>torch.nn.Module.load_state_dict()</code> 里面调用 <code>torch.load()</code>？</p><p>因为我们只保存了模型的 <code>state_dict()</code>（它是学习参数的字典）而不是整个模型，所以我们首先必须使用 <code>torch.load()</code> 加载 <code>state_dict()</code>，然后将该 <code>state_dict()</code> 传递给我们模型的新实例（它是 <code>nn.Module</code> 的一个子类）。</p><p>为什么不保存整个模型？</p><p><a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model">保存整个模型</a>而不是仅仅保存 <code>state_dict()</code> 更为直观，但是，引用 PyTorch 文档：</p><p>保存整个模型的缺点是序列化数据与保存模型时使用的特定类和确切的目录结构绑定在一起……<br>因此，在其他项目中使用或重构后，您的代码可能会以各种方式中断。</p><p>因此，我们使用灵活的方法来保存和加载 <code>state_dict()</code>，它基本上是一个模型参数的字典。</p><p>让我们通过创建另一个 <code>LinearRegressionModel()</code> 实例来测试它，它是 <code>torch.nn.Module</code> 的一个子类，因此具有内置方法 <code>load_state_dict()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate a new instance of our model (this will be instantiated with random weights)</span></span><br><span class="line"><span class="comment"># 实例化我们模型的新实例（这将使用随机权重实例化）</span></span><br><span class="line">loaded_model_0 = LinearRegressionModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the state_dict of our saved model (this will update the new instance of our model with trained weights)</span></span><br><span class="line"><span class="comment"># 加载我们保存的模型的 state_dict （这将使用训练后的权重更新我们模型的新实例）</span></span><br><span class="line">loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure><p>PyTorch 推理规则：</p><ul><li>将模型设置为评估模式 (<code>model.eval()</code>)。</li><li>使用推理模式上下文管理器进行预测（使用 <code>torch.inference_mode(): ...</code>）。</li><li>所有预测都应使用同一设备上的对象进行（例如，仅在 GPU 上的数据和模型或仅在 CPU 上的数据和模型）。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Put the loaded model into evaluation mode</span></span><br><span class="line"><span class="comment"># 1. 将加载的模型置于评估模式</span></span><br><span class="line">loaded_model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Use the inference mode context manager to make predictions</span></span><br><span class="line"><span class="comment"># 2. 使用推理模式上下文管理器进行预测</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    loaded_model_preds = loaded_model_0(X_test) <span class="comment"># perform a forward pass on the test data with the loaded model# 使用加载的模型对测试数据执行前向传递</span></span><br></pre></td></tr></table></figure><p>现在我们已经使用加载的模型做出了一些预测，让我们看看它们是否与之前的预测相同。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Compare previous model predictions with loaded model predictions (these should be the same)</span><br><span class="line">y_preds == loaded_model_preds</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True]])</span><br></pre></td></tr></table></figure><p>看起来加载的模型预测与之前的模型预测（保存前所做的预测）相同。这表明我们的模型正在按预期保存和加载。</p><p>还有更多方法可以保存和加载 PyTorch 模型，但我将把这些方法留到课外和进一步阅读中。有关更多信息，请参阅 <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-and-loading-models">PyTorch 保存和加载模型指南</a>。</p><h2 id="6-Putting-it-all-together"><a href="#6-Putting-it-all-together" class="headerlink" title="6. Putting it all together"></a>6. Putting it all together</h2><p>首先导入所需的标准库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import PyTorch and matplotlib</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn <span class="comment"># nn contains all of PyTorch&#x27;s building blocks for neural networks</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check PyTorch version</span></span><br><span class="line">torch.__version__</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;2.4.1&#x27;</span><br></pre></td></tr></table></figure><p>通过设置来使我们的代码与设备无关，device=”cuda”如果它可用，否则它将默认为device=”cpu”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup device agnostic code</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using device: <span class="subst">&#123;device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using device: cuda</span><br></pre></td></tr></table></figure><h3 id="6-1-Data"><a href="#6-1-Data" class="headerlink" title="6.1 Data"></a>6.1 Data</h3><p>首先，我们将对一些权重和偏差值进行硬编码。</p><p>然后，我们将在 0 到 1 之间设置一个数字范围，这些数字将是我们的 X 值。</p><p>最后，我们将使用 X 值以及权重和偏差值，通过线性回归公式 (y = 权重 * X + 偏差) 创建 y。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create weight and bias</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create range values</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.02</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create X and y (features and labels)</span></span><br><span class="line">X = torch.arange(start, end, step).unsqueeze(dim=<span class="number">1</span>) <span class="comment"># without unsqueeze, errors will happen later on (shapes within linear layers)</span></span><br><span class="line">y = weight * X + bias</span><br><span class="line">X[:<span class="number">10</span>], y[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0.0000],</span><br><span class="line">         [0.0200],</span><br><span class="line">         [0.0400],</span><br><span class="line">         [0.0600],</span><br><span class="line">         [0.0800],</span><br><span class="line">         [0.1000],</span><br><span class="line">         [0.1200],</span><br><span class="line">         [0.1400],</span><br><span class="line">         [0.1600],</span><br><span class="line">         [0.1800]]),</span><br><span class="line"> tensor([[0.3000],</span><br><span class="line">         [0.3140],</span><br><span class="line">         [0.3280],</span><br><span class="line">         [0.3420],</span><br><span class="line">         [0.3560],</span><br><span class="line">         [0.3700],</span><br><span class="line">         [0.3840],</span><br><span class="line">         [0.3980],</span><br><span class="line">         [0.4120],</span><br><span class="line">         [0.4260]]))</span><br></pre></td></tr></table></figure><p>现在我们有了一些数据，让我们将其分成训练集和测试集。</p><p>我们将使用 80/20 的分割方式，即 80% 的训练数据和 20% 的测试数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split data</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X))</span><br><span class="line">X_train, y_train = X[:train_split], y[:train_split]</span><br><span class="line">X_test, y_test = X[train_split:], y[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(40, 40, 10, 10)</span><br></pre></td></tr></table></figure><p>太好了，让我们将它们可视化以确保它们看起来不错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: If you&#x27;ve reset your runtime, this function won&#x27;t work, </span></span><br><span class="line"><span class="comment"># you&#x27;ll have to rerun the cell above where it&#x27;s instantiated.</span></span><br><span class="line">plot_predictions(X_train, y_train, X_test, y_test)</span><br></pre></td></tr></table></figure><h3 id="6-2-Building-a-PyTorch-linear-model"><a href="#6-2-Building-a-PyTorch-linear-model" class="headerlink" title="6.2 Building a PyTorch linear model"></a>6.2 Building a PyTorch linear model</h3><p>太棒了，让我们来看一下。我们已经有了一些数据，现在是时候创建一个模型了。</p><p>我们将创建与以前相同风格的模型，只是这次，我们不再使用 <code>nn.Parameter()</code> 手动定义模型的权重和偏差参数，而是使用 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">nn.Linear(in_features, out_features)</a> 来为我们完成这项工作。</p><p>其中 <code>in_features</code> 是输入数据的维度数，<code>out_features</code> 是您希望输出到的维度数。</p><p>在我们的例子中，这两个都是 <code>1</code>，因为我们的数据每个标签 (<code>y</code>) 有 1 个输入特征 (<code>X</code>)。对它们进行大小调整以确保它们看起来不错。</p><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-13.png" class="" title="PyTorch-26H-2-13"><p>使用 <code>nn.Parameter</code> 创建线性回归模型，而不是使用 <code>nn.Linear</code>。<code>torch.nn</code> 模块具有预构建计算的示例还有很多，包括许多流行且有用的神经网络层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Subclass nn.Module to make our model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModelV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># Use nn.Linear() for creating the model parameters</span></span><br><span class="line">        self.linear_layer = nn.Linear(in_features=<span class="number">1</span>, </span><br><span class="line">                                      out_features=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the forward computation (input data x flows through nn.Linear())</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">return</span> self.linear_layer(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the manual seed when creating the model (this isn&#x27;t always needed but is used for demonstrative purposes, try commenting it out and seeing what happens)</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">model_1 = LinearRegressionModelV2()</span><br><span class="line">model_1, model_1.state_dict()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(LinearRegressionModelV2(</span><br><span class="line">   (linear_layer): Linear(in_features=1, out_features=1, bias=True)</span><br><span class="line"> ),</span><br><span class="line"> OrderedDict([(&#x27;linear_layer.weight&#x27;, tensor([[0.7645]])),</span><br><span class="line">              (&#x27;linear_layer.bias&#x27;, tensor([0.8300]))]))</span><br></pre></td></tr></table></figure><p>注意model_1.state_dict()的输出，nn.Linear()层为我们创建了一个随机权重和偏差参数。</p><p>现在让我们将模型放到 GPU 上（如果可用）。</p><p>我们可以使用 .to(device) 更改 PyTorch 对象所在的设备。</p><p>首先让我们检查模型的当前设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check model device</span></span><br><span class="line"><span class="built_in">next</span>(model_1.parameters()).device</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(type=&#x27;cpu&#x27;)</span><br></pre></td></tr></table></figure><p>太棒了，看起来模型默认在 CPU 上运行。</p><p>让我们将其改为在 GPU 上运行（如果可用的话）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set model to GPU if it&#x27;s available, otherwise it&#x27;ll default to CPU</span></span><br><span class="line">model_1.to(device) <span class="comment"># the device variable was set above to be &quot;cuda&quot; if available or &quot;cpu&quot; if not</span></span><br><span class="line"><span class="built_in">next</span>(model_1.parameters()).device</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(type=&#x27;cuda&#x27;, index=0)</span><br></pre></td></tr></table></figure><p>太棒了！由于我们的代码与设备无关，因此无论 GPU 是否可用，上述单元都可以工作。</p><h3 id="6-3-Training"><a href="#6-3-Training" class="headerlink" title="6.3 Training"></a>6.3 Training</h3><p>是时候构建训练和测试循环了。</p><p>首先，我们需要一个损失函数loss function和一个优化器optimizer。</p><p>让我们使用之前使用的相同函数，<code>nn.L1Loss()</code> 和 <code>torch.optim.SGD()</code>。</p><p>我们必须将新模型的参数 (<code>model.parameters()</code>) 传递给优化器，以便它在训练期间进行调整。</p><p><code>0.01</code> 的学习率之前也很好用，所以让我们再次使用它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create loss function</span></span><br><span class="line">loss_fn = nn.L1Loss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_1.parameters(), <span class="comment"># optimize newly created model&#x27;s parameters</span></span><br><span class="line">                            lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>损失函数和优化器已准备就绪，现在让我们使用训练和测试循环来训练和评估我们的模型。</p><p>与之前的训练循环相比，我们在此步骤中要做的唯一不同的事情是将数据放在目标设备上。</p><p>我们已经使用 <code>model_1.to(device)</code> 将我们的模型放在目标设备上。</p><p>我们可以对数据执行相同的操作。</p><p>这样，如果模型在 GPU 上，数据就在 GPU 上（反之亦然）。</p><p>这次让我们更进一步，设置 <code>epochs=1000</code>。</p><p>如果您需要 PyTorch 训练循环步骤的提醒，请参见下文。</p><p>PyTorch 训练循环步骤</p><ul><li>前向传递 - 模型对所有训练数据进行一次遍历，执行其 forward() 函数计算 (model(x_train))。</li><li>计算损失 - 将模型的输出 (预测) 与基本事实进行比较，并进行评估以查看它们的错误程度 (loss = loss_fn(y_pred, y_train)。</li><li>零梯度 - 优化器梯度设置为零 (默认情况下是累积的)，因此可以为特定的训练步骤重新计算它们 (optimizer.zero_grad())。</li><li>对损失执行反向传播 - 针对要更新的每个模型参数 (每个参数的 require_grad=True) 计算损失的梯度。这称为反向传播，因此是“向后”(loss.backward())。</li><li>步进优化器 (梯度下降) - 使用 require_grad=True 更新参数，以根据损失梯度来改进它们 (optimizer.step())。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs </span></span><br><span class="line">epochs = <span class="number">1000</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data on the available device</span></span><br><span class="line"><span class="comment"># Without this, error will happen (not all model/data on device)</span></span><br><span class="line">X_train = X_train.to(device)</span><br><span class="line">X_test = X_test.to(device)</span><br><span class="line">y_train = y_train.to(device)</span><br><span class="line">y_test = y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_1.train() <span class="comment"># train mode is on by default after construction</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_pred = model_1(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Zero grad optimizer</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Step the optimizer</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_1.<span class="built_in">eval</span>() <span class="comment"># put the model in evaluation mode for testing (inference)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        test_pred = model_1(X_test)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 2. Calculate the loss</span></span><br><span class="line">        test_loss = loss_fn(test_pred, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Train loss: <span class="subst">&#123;loss&#125;</span> | Test loss: <span class="subst">&#123;test_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Train loss: 0.5551779866218567 | Test loss: 0.5739762187004089</span><br><span class="line">Epoch: 100 | Train loss: 0.006215683650225401 | Test loss: 0.014086711220443249</span><br><span class="line">Epoch: 200 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 300 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 400 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 500 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 600 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 700 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 800 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 900 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br></pre></td></tr></table></figure><blockquote><p>注意：由于机器学习的随机性，您可能会得到略有不同的结果（不同的损失和预测值），具体取决于您的模型是在 CPU 还是 GPU 上训练的。即使您在任一设备上使用相同的随机种子，情况也是如此。如果差异很大，您可能需要查找错误，但是，如果差异很小（理想情况下很小），您可以忽略它。</p></blockquote><p>检查一下模型学习到的参数，并将它们与我们硬编码的原始参数进行比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find our model&#x27;s learned parameters</span></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint <span class="comment"># pprint = pretty print, see: https://docs.python.org/3/library/pprint.html </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The model learned the following values for weights and bias:&quot;</span>)</span><br><span class="line">pprint(model_1.state_dict())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nAnd the original values for weights and bias are:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;weights: <span class="subst">&#123;weight&#125;</span>, bias: <span class="subst">&#123;bias&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The model learned the following values for weights and bias:</span><br><span class="line">OrderedDict([(&#x27;linear_layer.weight&#x27;, tensor([[0.6968]], device=&#x27;cuda:0&#x27;)),</span><br><span class="line">             (&#x27;linear_layer.bias&#x27;, tensor([0.3025], device=&#x27;cuda:0&#x27;))])</span><br><span class="line"></span><br><span class="line">And the original values for weights and bias are:</span><br><span class="line">weights: 0.7, bias: 0.3</span><br></pre></td></tr></table></figure><p>请记住，在实践中，你很少会提前知道完美的参数。</p><p>如果你提前知道了模型必须学习的参数，机器学习还有什么乐趣呢？</p><p>此外，在许多现实世界的机器学习问题中，参数的数量可能超过数千万。</p><h3 id="6-4-Making-predictions"><a href="#6-4-Making-predictions" class="headerlink" title="6.4 Making predictions"></a>6.4 Making predictions</h3><p>现在我们已经有一个训练好的模型，让我们打开它的评估模式并做出一些预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn model into evaluation mode</span></span><br><span class="line">model_1.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test data</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_preds = model_1(X_test)</span><br><span class="line">y_preds</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.8600],</span><br><span class="line">        [0.8739],</span><br><span class="line">        [0.8878],</span><br><span class="line">        [0.9018],</span><br><span class="line">        [0.9157],</span><br><span class="line">        [0.9296],</span><br><span class="line">        [0.9436],</span><br><span class="line">        [0.9575],</span><br><span class="line">        [0.9714],</span><br><span class="line">        [0.9854]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><p>如果您使用 GPU 上的数据进行预测，您可能会注意到上面的输出在末尾有 device=’cuda:0’。这意味着数据位于 CUDA 设备 0 上（由于零索引，您的系统可以访问的第一个 GPU），如果您将来最终使用多个 GPU，这个数字可能会更高。</p><p>现在让我们绘制模型的预测。</p><blockquote><p>注意：许多数据科学库（例如 pandas、matplotlib 和 NumPy）无法使用存储在 GPU 上的数据。因此，当您尝试使用其中一个库中的函数处理未存储在 CPU 上的张量数据时，可能会遇到一些问题。要解决此问题，您可以在目标张量上调用 .cpu() 以返回 CPU 上的目标张量的副本。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot_predictions(predictions=y_preds) # -&gt; won&#x27;t work... data not on CPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data on the CPU and plot it</span></span><br><span class="line">plot_predictions(predictions=y_preds.cpu())</span><br></pre></td></tr></table></figure><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-14.png" class="" title="PyTorch-26H-2-14"><h3 id="6-5-Saving-and-loading-a-model-保存和加载模型"><a href="#6-5-Saving-and-loading-a-model-保存和加载模型" class="headerlink" title="6.5 Saving and loading a model 保存和加载模型"></a>6.5 Saving and loading a model 保存和加载模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create models directory </span></span><br><span class="line">MODEL_PATH = Path(<span class="string">&quot;models&quot;</span>)</span><br><span class="line">MODEL_PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create model save path </span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;01_pytorch_workflow_model_1.pth&quot;</span></span><br><span class="line">MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Save the model state dict </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Saving model to: <span class="subst">&#123;MODEL_SAVE_PATH&#125;</span>&quot;</span>)</span><br><span class="line">torch.save(obj=model_1.state_dict(), <span class="comment"># only saving the state_dict() only saves the models learned parameters</span></span><br><span class="line">           f=MODEL_SAVE_PATH) </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Saving model to: models\01_pytorch_workflow_model_1.pth</span><br></pre></td></tr></table></figure><p>为了确保一切正常，我们将其重新加载。</p><ul><li>创建 <code>LinearRegressionModelV2()</code> 类的新实例</li><li>使用 <code>torch.nn.Module.load_state_dict()</code> 加载模型状态字典</li><li>将模型的新实例发送到目标设备（以确保我们的代码与设备无关）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate a fresh instance of LinearRegressionModelV2</span></span><br><span class="line">loaded_model_1 = LinearRegressionModelV2()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load model state dict </span></span><br><span class="line">loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)</span></span><br><span class="line">loaded_model_1.to(device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loaded model:\n<span class="subst">&#123;loaded_model_1&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model on device:\n<span class="subst">&#123;<span class="built_in">next</span>(loaded_model_1.parameters()).device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Loaded model:</span><br><span class="line">LinearRegressionModelV2(</span><br><span class="line">  (linear_layer): Linear(in_features=1, out_features=1, bias=True)</span><br><span class="line">)</span><br><span class="line">Model on device:</span><br><span class="line">cuda:0</span><br></pre></td></tr></table></figure><p>现在我们可以评估加载的模型，看看它的预测是否与保存之前的预测一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate loaded model</span></span><br><span class="line">loaded_model_1.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    loaded_model_1_preds = loaded_model_1(X_test)</span><br><span class="line">y_preds == loaded_model_1_preds</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2><p>1、使用线性回归公式 () 创建直线数据集<code>weight * X + bias</code>。</p><ul><li>设置<code>weight=0.3</code>并且<code>bias=0.9</code>总共应该至少有 100 个数据点。</li><li>将数据分成 80％ 用于训练，20％ 用于测试。</li><li>绘制训练和测试数据，使其变得可视化。</li></ul><p>2、通过子类化构建 PyTorch 模型<code>nn.Module</code>。</p><ul><li>里面应该有一个随机初始化<code>nn.Parameter()</code>的<code>requires_grad=True</code>，一个为<code>weights</code>，一个为<code>bias</code>。</li><li>实现<code>forward()</code>在1中创建数据集时使用的计算线性回归函数的方法。</li><li>一旦构建了模型，就创建它的一个实例并检查它的<code>state_dict()</code>。</li><li>注意：如果您愿意，<code>nn.Linear()</code>也<code>nn.Parameter()</code>可以使用。</li></ul><p>3、<code>nn.L1Loss()</code>分别使用和创建损失函数和优化器<code>torch.optim.SGD(params, lr)</code>。</p><ul><li>将优化器的学习率设置为 0.01，要优化的参数应该是您在 2 中创建的模型的模型参数。</li><li>编写一个训练循环来执行 300 个时期的适当训练步骤。</li><li>训练循环应该每 20 个时期在测试数据集上测试模型。</li></ul><p>4、使用训练好的模型对测试数据进行预测。</p><ul><li>根据原始训练和测试数据对这些预测进行可视化（注意：如果您想使用不支持 CUDA 的库（例如 matplotlib 来绘图，则可能需要确保预测不在GPU 上）。</li></ul><p>5、将您训练的模型保存<code>state_dict()</code>到文件中。</p><ul><li>创建您在 2 中创建的模型类的新实例，并加载<code>state_dict()</code>您刚刚保存的内容。</li><li>使用加载的模型对测试数据执行预测，并确认它们与 4 中的原始模型预测相匹配。</li></ul><h2 id="Extra-curriculum"><a href="#Extra-curriculum" class="headerlink" title="Extra-curriculum"></a>Extra-curriculum</h2><ul><li>阅读Jeremy Howard 的《到底<a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">是什么<code>torch.nn</code>？》，</a>以更深入地了解 PyTorch 中最重要的模块之一的工作原理。</li><li>花 10 分钟浏览并查看<a href="https://pytorch.org/tutorials/beginner/ptcheat.html">PyTorch 文档备忘单，</a>了解您可能会遇到的所有不同 PyTorch 模块。</li><li>花 10 分钟阅读<a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch 网站上的加载和保存文档，</a>以熟悉 PyTorch 中的不同保存和加载选项。</li><li>花费 1-2 小时阅读/观看以下内容，了解梯度下降和反向传播的内部原理，这两种主要算法一直在后台运行，帮助我们的模型学习。</li><li><a href="https://en.wikipedia.org/wiki/Gradient_descent">梯度下降的维基百科页面</a></li><li>梯度下降算法——Robert Kwiatkowski 的<a href="https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21">深入探讨</a></li><li><a href="https://youtu.be/IHZwWFHWa-w">梯度下降，神经网络如何学习视频</a>（3Blue1Brown 拍摄）</li><li><a href="https://youtu.be/Ilg3gGewQ5U">反向传播到底在做什么？</a>视频由 3Blue1Brown 提供</li><li><a href="https://en.wikipedia.org/wiki/Backpropagation">反向传播维基百科页面</a></li></ul>]]></content>
    
    
    <summary type="html">PyTorch-26H-2</summary>
    
    
    
    <category term="PyTorch" scheme="http://hibiscidai.com/categories/PyTorch/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="PyTorch" scheme="http://hibiscidai.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-26H-1</title>
    <link href="http://hibiscidai.com/2024/08/14/PyTorch-26H-1/"/>
    <id>http://hibiscidai.com/2024/08/14/PyTorch-26H-1/</id>
    <published>2024-08-14T12:00:00.000Z</published>
    <updated>2024-10-11T13:34:53.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1.png" class="" title="PyTorch-26H-1"><p>PyTorch-26H-1</p><span id="more"></span><h1 id="PyTorch-26H-1"><a href="#PyTorch-26H-1" class="headerlink" title="PyTorch-26H-1"></a>PyTorch-26H-1</h1><p>主页：<a href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p><p>youtub：<a href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p><p>github：<a href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p><p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p><p>PyTorch documentation：<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><h1 id="Chapter-0-–-PyTorch-Fundamentals"><a href="#Chapter-0-–-PyTorch-Fundamentals" class="headerlink" title="Chapter 0 – PyTorch Fundamentals"></a>Chapter 0 – PyTorch Fundamentals</h1><h2 id="what-is-deep-learning"><a href="#what-is-deep-learning" class="headerlink" title="what is deep learning?"></a>what is deep learning?</h2><p>Machine learning is turning things (data) into numbers and finding patterns in those numbers.</p><p>Deep Learning ∈ Machine Learning ∈ Aritfical Intelligence</p><p>传统程序：输入+规则→输出<br>机器学些：输入+输出→规则</p><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-1.png" class="" title="PyTorch-26H-1-1"><h2 id="Why-use-machine-deep-learning"><a href="#Why-use-machine-deep-learning" class="headerlink" title="Why use machine/deep learning?"></a>Why use machine/deep learning?</h2><p>对于复杂的问题，无法找到所有的规则。</p><h2 id="The-number-one-rule-of-ML"><a href="#The-number-one-rule-of-ML" class="headerlink" title="The number one rule of ML"></a>The number one rule of ML</h2><p>“If you can build a simple rule-based system that doesn’t require machine learning, do that.”</p><ul><li><p>A wise software engineer… (actually rule 1 of Google’s Machine Learning Handbook)</p></li><li><p>What deep learning is good for</p></li></ul><p>Problems with long lists of rules- when the traditional approach fails, machine learning/deep learning may help.<br>规则列表过长的问题——当传统方法失败时，机器学习/深度学习可能会有所帮助。</p><p>Continually changing environments- deep learning can adapt (learn’) to new scenarios.<br>不断变化的环境——深度学习可以适应（学习）新场景。</p><p>Discovering insights within large collections of data- can you imagine trying to hand-craft rules for what 101 different kinds of food look like?<br>在大量数据中发现见解——你能想象尝试手工制定 101 种不同食物的规则吗？</p><ul><li>What deep learning is not good for</li></ul><p>When you need explainability- -the patterns learned by a deep learning model are typically uninterpretable by a human.<br>当你需要可解释性时——深度学习模型学习到的模式通常无法被人类解释。</p><p>When the traditional approach is a better option一if you can accomplish what you need with a simple rule-based system.<br>当传统方法是更好的选择时——如果你可以使用简单的基于规则的系统完成所需的工作。</p><p>When errors are unacceptable一since the outputs of deep learning model aren’t always predictable.<br>当错误不可接受时——因为深度学习模型的输出并不总是可预测的。</p><p>When you don’t have much data一deep learning models usually require a fairly large amount of data to produce great results.<br>当你没有太多数据时——深度学习模型通常需要相当大量的数据才能产生很好的结果。</p><h2 id="Machine-learning-vs-deep-learning"><a href="#Machine-learning-vs-deep-learning" class="headerlink" title="Machine learning vs deep learning"></a>Machine learning vs deep learning</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-2.png" class="" title="PyTorch-26H-1-2"><ul><li>Machine learning</li></ul><p>适合处理结构化数据</p><p>常见算法：<br>Random forest 随机森林<br>Gradient boosted models 梯度提升模型<br>Naive Bayes 朴素贝叶斯<br>Nearest neighbour 最近邻<br>Support vector machine 支持向量机</p><ul><li>Deep learning</li></ul><p>适合处理非结构化数据</p><p>常见算法：<br>Neural networks 神经网络<br>Fully connected neural network 全连接神经网络<br>Convolutional neural network 卷积神经网络<br>Recurrent neuralnetwork 循环神经网络<br>Transformer</p><h2 id="Anatomy-of-neural-networks"><a href="#Anatomy-of-neural-networks" class="headerlink" title="Anatomy of neural networks"></a>Anatomy of neural networks</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-3.png" class="" title="PyTorch-26H-1-3"><p>数据→数字→神经网络→权重→输出</p><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-4.png" class="" title="PyTorch-26H-1-4"><h2 id="Different-learning-paradigms"><a href="#Different-learning-paradigms" class="headerlink" title="Different learning paradigms"></a>Different learning paradigms</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-5.png" class="" title="PyTorch-26H-1-5"><p>监督学习：大量已知数据标注。</p><p>无监督学习：自动分析数据。</p><p>迁移学习：将学习到的模式嵌入到新的模型中。</p><p>强化学习reinforcement learning：奖励想要的结果。</p><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-6.png" class="" title="PyTorch-26H-1-6"><h2 id="What-can-deep-learning-be-used-for"><a href="#What-can-deep-learning-be-used-for" class="headerlink" title="What can deep learning be used for?"></a>What can deep learning be used for?</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-7.png" class="" title="PyTorch-26H-1-7"><p>CV</p><p>NATURAL LANGUAGE PROGRESS</p><p>SEQUENCE IN AND SEQUENCE OUT</p><h2 id="What-is-why-PyTorch"><a href="#What-is-why-PyTorch" class="headerlink" title="What is/why PyTorch?"></a>What is/why PyTorch?</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-8.png" class="" title="PyTorch-26H-1-8"><p>Most popular research deep learning framework<br>最流行的深度学习研究框架</p><p>Write fast deep learning code in Python (able to run on a GPU/many GPUs)<br>用 Python 编写快速的深度学习代码（可在 GPU/多个 GPU 上运行）</p><p>Able to access many pre-built deep learning models (Torch Hub/torchvision.models)<br>能够访问许多预构建的深度学习模型（Torch Hub/torchvision.models）</p><p>Whole stack: preprocess data, model data, deploy model in your application/cloud<br>整个堆栈：预处理数据、建立数据模型、在应用程序/云中部署模型</p><p>Originally designed and used in-house by Facebook/Meta (now opensource and used by companies such as Tesla, Microsoft, OpenAI)<br>最初由 Facebook/Meta 内部设计和使用（现已开源，并被特斯拉、微软、OpenAI 等公司使用）</p><p><a href="www.paperswithcode.com/trends">paperswithcode</a></p><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-9.png" class="" title="PyTorch-26H-1-9"><p>GPU(Graphics Processing Unit)</p><p>TPU(Tensor Processing Unit)</p><h2 id="What-are-tensors"><a href="#What-are-tensors" class="headerlink" title="What are tensors?"></a>What are tensors?</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-10.png" class="" title="PyTorch-26H-1-10"><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-11.png" class="" title="PyTorch-26H-1-11"><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>Now:</p><ul><li>PyTorch basics &amp; fundamentals (dealing with tensors and tensor operations)</li><li>PyTorch 基础和基本原理（处理张量和张量运算）</li></ul><p>Later:</p><ul><li>Preprocessing data (getting it into tensors)</li><li>预处理数据（将数据转化为张量）</li><li>Building and using pretrained deep learning models</li><li>构建和使用预训练的深度学习模型</li><li>Fitting a model to the data (learning patterns)</li><li>根据数据拟合模型（学习模式）</li><li>Making predictions with a model (using patterns)</li><li>使用模型进行预测（使用模式）</li><li>Evaluating model predictions</li><li>评估模型预测</li><li>Saving and loading models</li><li>保存和加载模型</li><li>Using a trained model to make predictions on custom data</li><li>使用训练好的模型对自定义数据进行预测</li></ul><p>一点科学，一点艺术。</p><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-12.png" class="" title="PyTorch-26H-1-12"><h2 id="How-to-and-how-not-to-approach-this-course"><a href="#How-to-and-how-not-to-approach-this-course" class="headerlink" title="How to (and how not to) approach this course"></a>How to (and how not to) approach this course</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-13.png" class="" title="PyTorch-26H-1-13"><p>1、code alone</p><p>2、explore and experiment</p><p>3、visuallize what you don’t understand</p><p>4、ask questions</p><p>5、do the exercises</p><p>6、share your work</p><h2 id="Important-resources"><a href="#Important-resources" class="headerlink" title="Important resources"></a>Important resources</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-14.png" class="" title="PyTorch-26H-1-14"><p>课程资料：<br><a href="https://www.github.com/mrdbourke/pytorch-deep-learning">Course materials</a></p><p>问答：<br><a href="https://www.github.com/mrdbourke/pytorch-deep-learning/discussions">Course Q&amp;A</a></p><p>书籍：<br><a href="https://learnpytorch.io">Course online book</a></p><p><a href="https://pytorch.org/">pytorch_org</a></p><p><a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a></p><p>出问题去讨论区</p><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-15.png" class="" title="PyTorch-26H-1-15"><h2 id="Getting-setup"><a href="#Getting-setup" class="headerlink" title="Getting setup"></a>Getting setup</h2><p><a href="https://colab.research.google.com/">Google colab</a></p><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-16.png" class="" title="PyTorch-26H-1-16"><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-17.png" class="" title="PyTorch-26H-1-17"><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-18.png" class="" title="PyTorch-26H-1-18"><p><code>00_pytorch_fundamentals_vedio.ipynb</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello I&#x27;m excited to learn PyTorch!&quot;</span>)</span><br><span class="line"></span><br><span class="line">&gt;Hello I<span class="string">&#x27;m excited to learn PyTorch!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">!nvidia-smi</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Mon Aug 26 11:28:50 2024       </span></span><br><span class="line"><span class="string">+---------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="string">| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |</span></span><br><span class="line"><span class="string">|-----------------------------------------+----------------------+----------------------+</span></span><br><span class="line"><span class="string">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span></span><br><span class="line"><span class="string">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span></span><br><span class="line"><span class="string">|                                         |                      |               MIG M. |</span></span><br><span class="line"><span class="string">|=========================================+======================+======================|</span></span><br><span class="line"><span class="string">|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |</span></span><br><span class="line"><span class="string">| N/A   49C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |</span></span><br><span class="line"><span class="string">|                                         |                      |                  N/A |</span></span><br><span class="line"><span class="string">+-----------------------------------------+----------------------+----------------------+</span></span><br><span class="line"><span class="string">                                                                                         </span></span><br><span class="line"><span class="string">+---------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="string">| Processes:                                                                            |</span></span><br><span class="line"><span class="string">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span></span><br><span class="line"><span class="string">|        ID   ID                                                             Usage      |</span></span><br><span class="line"><span class="string">|=======================================================================================|</span></span><br><span class="line"><span class="string">|  No running processes found                                                           |</span></span><br><span class="line"><span class="string">+---------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">import torch</span></span><br><span class="line"><span class="string">import pandas as pd</span></span><br><span class="line"><span class="string">import numpy as np</span></span><br><span class="line"><span class="string">import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="string">print(torch.__version__)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&gt;2.3.1+cu121</span></span><br></pre></td></tr></table></figure><h2 id="Introduction-to-tensors"><a href="#Introduction-to-tensors" class="headerlink" title="Introduction to tensors"></a>Introduction to tensors</h2><p><a href="https://pytorch.org/docs/stable/tensors.html">torch.Tensor</a></p><p>A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.<br>张量是包含单一数据类型元素的多维矩阵。</p><h3 id="scalar→标量"><a href="#scalar→标量" class="headerlink" title="scalar→标量"></a><strong>scalar→标量</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scalar</span></span><br><span class="line">scalar = torch.tensor(<span class="number">7</span>)</span><br><span class="line">scalar</span><br><span class="line"><span class="comment">#&gt;tensor(7)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看张量的维度</span></span><br><span class="line">scalar.ndim</span><br><span class="line"><span class="comment">#&gt;0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#取回张量→int</span></span><br><span class="line">scalar.item()</span><br><span class="line"><span class="comment">#&gt;7</span></span><br></pre></td></tr></table></figure><h3 id="vecotr→向量"><a href="#vecotr→向量" class="headerlink" title="vecotr→向量"></a><strong>vecotr→向量</strong></h3><p>拥有大小和方向</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Vector</span></span><br><span class="line">vector = torch.tensor([<span class="number">7</span>,<span class="number">7</span>])</span><br><span class="line">vector</span><br><span class="line"><span class="comment">#&gt;tensor([7, 7])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看向量的维度</span></span><br><span class="line">vector.ndim</span><br><span class="line"><span class="comment">#&gt;1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看向量的形状</span></span><br><span class="line">vector.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([2])</span></span><br></pre></td></tr></table></figure><h3 id="MATRIX→矩阵"><a href="#MATRIX→矩阵" class="headerlink" title="MATRIX→矩阵"></a><strong>MATRIX→矩阵</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">MATRIX = torch.tensor([[<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">[<span class="number">9</span>,<span class="number">10</span>]])</span><br><span class="line">MATRIX</span><br><span class="line"></span><br><span class="line"><span class="comment">#&gt;tensor([[ 7,  8],</span></span><br><span class="line"><span class="comment">#&gt;        [ 9, 10]])</span></span><br><span class="line"></span><br><span class="line">MATRIX.ndim</span><br><span class="line"><span class="comment">#&gt;2</span></span><br><span class="line"></span><br><span class="line">MATRIX[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([7, 8])</span></span><br><span class="line"></span><br><span class="line">MATRIX[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([ 9, 10])</span></span><br><span class="line"></span><br><span class="line">MATRIX.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([2, 2])</span></span><br></pre></td></tr></table></figure><h3 id="TENSOR→张量"><a href="#TENSOR→张量" class="headerlink" title="TENSOR→张量"></a><strong>TENSOR→张量</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">TENSOR = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]]])</span><br><span class="line">TENSOR</span><br><span class="line"><span class="comment">#&gt;tensor([[[1, 2, 3],</span></span><br><span class="line"><span class="comment">#&gt;         [3, 6, 9],</span></span><br><span class="line"><span class="comment">#&gt;         [2, 4, 6]]])</span></span><br><span class="line"></span><br><span class="line">TENSOR.ndim</span><br><span class="line"><span class="comment">#&gt;3</span></span><br><span class="line"></span><br><span class="line">TENSOR.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([1, 3, 3])</span></span><br><span class="line"><span class="comment">## 一维 3x3形状的张量</span></span><br><span class="line"></span><br><span class="line">TENSOR[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#&gt;        [3, 6, 9],</span></span><br><span class="line"><span class="comment">#&gt;        [2, 4, 6]])</span></span><br></pre></td></tr></table></figure><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-19.png" class="" title="PyTorch-26H-1-19"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TENSOR = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>]]])</span><br><span class="line">TENSOR.ndim</span><br><span class="line"><span class="comment">#&gt;3</span></span><br><span class="line"></span><br><span class="line">TENSOR.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([1, 3, 2])</span></span><br><span class="line"></span><br><span class="line">TENSOR[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([[1, 2],</span></span><br><span class="line"><span class="comment">#&gt;        [3, 6],</span></span><br><span class="line"><span class="comment">#&gt;        [2, 4]])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">TENSOR = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>]],</span><br><span class="line">   [[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">4</span>]]])</span><br><span class="line">TENSOR.ndim</span><br><span class="line"><span class="comment">#&gt;3</span></span><br><span class="line"></span><br><span class="line">TENSOR.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([2, 3, 2])</span></span><br><span class="line"></span><br><span class="line">TENSOR[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([[1, 2],</span></span><br><span class="line"><span class="comment">#&gt;        [3, 6],</span></span><br><span class="line"><span class="comment">#&gt;        [2, 4]])</span></span><br><span class="line"></span><br><span class="line">TENSOR[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([[1, 2],</span></span><br><span class="line"><span class="comment">#&gt;        [3, 6],</span></span><br><span class="line"><span class="comment">#&gt;        [2, 4]])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">TENSOR = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>]],</span><br><span class="line">   [[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">4</span>]]])</span><br><span class="line"><span class="comment"># ValueError: expected sequence of length 3 at dim 1 (got 4)</span></span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:center">Name</th><th style="text-align:center">解释</th><th style="text-align:center">维度</th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">scalar标量</td><td style="text-align:center">一个数字</td><td style="text-align:center">0</td><td style="text-align:center">Lower(a)</td></tr><tr><td style="text-align:center">vector向量</td><td style="text-align:center">带有方向的数字（例如带有方向的风速），但也可以有许多其他数字</td><td style="text-align:center">1</td><td style="text-align:center">Lower(y)</td></tr><tr><td style="text-align:center">matrix矩阵</td><td style="text-align:center">二维数字数组</td><td style="text-align:center">2</td><td style="text-align:center">Upper(Q)</td></tr><tr><td style="text-align:center">tensor张量</td><td style="text-align:center">n 维数字数组</td><td style="text-align:center">n,0 维张量是标量，1 维张量是矢量</td><td style="text-align:center">Upper(X)</td></tr></tbody></table></div><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-20.png" class="" title="PyTorch-26H-1-20"><h2 id="Creating-tensors"><a href="#Creating-tensors" class="headerlink" title="Creating tensors"></a>Creating tensors</h2><p>Why random tensors?<br>Random tensors are important because the way many neural networks learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data .<br>随机张量很重要，因为许多神经网络的学习方式是从充满随机数的张量开始，然后调整这些随机数以更好地表示数据。</p><p>start with random numbersy → look at data → update random numbers → look at data → update random numbers</p><ul><li>创建随机张量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a random tensor of size (3, 4)</span></span><br><span class="line">random_tensor = torch.rand(size=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">random_tensor, random_tensor.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0.6541, 0.4807, 0.2162, 0.6168],</span><br><span class="line">         [0.4428, 0.6608, 0.6194, 0.8620],</span><br><span class="line">         [0.2795, 0.6055, 0.4958, 0.5483]]),</span><br><span class="line"> torch.float32)</span><br></pre></td></tr></table></figure><ul><li>创建随机图像张量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a random tensor with similar shape to an image tensor</span></span><br><span class="line">random_image_size_tensor = torch.rand(size=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)) <span class="comment"># height, width, colour channels (R, G, B)</span></span><br><span class="line">random_image_size_tensor.shape, random_image_size_tensor.ndim</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([224, 224, 3]), 3)</span><br></pre></td></tr></table></figure><p>图像表示为具有形状的张量，[3, 224, 224]这意味着[colour_channels, height, width]，图像具有3颜色通道（红色、绿色、蓝色）、像素高度224和像素宽度224。</p><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-21.png" class="" title="PyTorch-26H-1-21"><h3 id="Zeros-and-ones"><a href="#Zeros-and-ones" class="headerlink" title="Zeros and ones"></a>Zeros and ones</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor of all zeros</span></span><br><span class="line">zeros = torch.zeros(size=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">zeros</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0.]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor of all ones</span></span><br><span class="line">ones = torch.ones(size=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">ones</span><br><span class="line">ones.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.]])</span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure><h3 id="Creating-a-range-of-tensors-and-tensors-like"><a href="#Creating-a-range-of-tensors-and-tensors-like" class="headerlink" title="Creating a range of tensors and tensors-like"></a>Creating a range of tensors and tensors-like</h3><ul><li>数字范围</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use torch.range() and get deprecated message, use torch.arange()</span></span><br><span class="line">one_to_ten = torch.arange(start=<span class="number">1</span>, end=<span class="number">11</span>, step=<span class="number">1</span>)</span><br><span class="line">one_to_ten</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])</span><br></pre></td></tr></table></figure><ul><li>相同形状填充零或一的张量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating tensors like</span></span><br><span class="line">ten_zeros = torch.zeros_like(<span class="built_in">input</span>=one_to_ten)</span><br><span class="line">ten_zeros</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating tensors like</span></span><br><span class="line">ten_zeros = torch.oness_like(<span class="built_in">input</span>=one_to_ten)</span><br><span class="line">ten_zeros</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</span><br></pre></td></tr></table></figure><h2 id="Tensor-datatypes"><a href="#Tensor-datatypes" class="headerlink" title="Tensor datatypes"></a>Tensor datatypes</h2><p><a href="https://pytorch.org/docs/stable/tensors.html#data-types">torch.Tensor-Data tpyes</a></p><p><a href="https://en.wikipedia.org/wiki/Precision_(computer_science">Precision in computing</a>#:~:text=In%20computer%20science%2C%20the%20precision,used%20to%20express%20a%20value)</p><p>张量数据类型是使用 PyTorch 和深度学习时会遇到的 3 个大错误之一：</p><ol><li>张量数据类型不正确datatype</li><li>张量形状不正确shape</li><li>张量不在正确的设备上device</li></ol><ul><li>默认为float32</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Float 32 tensor</span></span><br><span class="line">float_32_tensor = torch.tensor([<span class="number">3.0</span>, <span class="number">6.0</span>, <span class="number">9.0</span>],</span><br><span class="line">                               dtype=<span class="literal">None</span>)</span><br><span class="line">float_32_tensor</span><br><span class="line">float_32_tensor.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([3., 6., 9.])</span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure><ul><li>修改默认数据类型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Float 32 tensor</span></span><br><span class="line">float_32_tensor = torch.tensor([<span class="number">3.0</span>, <span class="number">6.0</span>, <span class="number">9.0</span>],</span><br><span class="line">                               dtype=torch.float16)</span><br><span class="line">float_32_tensor</span><br><span class="line">float_32_tensor.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([3., 6., 9.]), dtype=torch.float16</span><br><span class="line">torch.float16</span><br></pre></td></tr></table></figure><ul><li>设备和记录操作</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Float 32 tensor</span></span><br><span class="line">float_32_tensor = torch.tensor([<span class="number">3.0</span>, <span class="number">6.0</span>, <span class="number">9.0</span>],</span><br><span class="line">                               dtype=<span class="literal">None</span>,<span class="comment">#张量数据类型</span></span><br><span class="line">                               device=<span class="literal">None</span>,<span class="comment">#cpu or cuda</span></span><br><span class="line">                               requires_grad=<span class="literal">False</span>)<span class="comment">#是否使用此张量操作跟踪</span></span><br><span class="line">float_32_tensor</span><br><span class="line">float_32_tensor.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([3., 6., 9.])</span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure><ul><li>张量转换32 → 16</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">float_16_tensor = float_32_tensor.<span class="built_in">type</span>(torch.float16)</span><br><span class="line">float_16_tensor</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([3., 6., 9.], dtype=torch.float16)</span><br></pre></td></tr></table></figure><ul><li>不同数据类型张量相乘</li></ul><p>进行了自动类型转换，向上转换</p><p>float16 × float32 → float32</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test = float_16_tensor * float_32_tensor</span><br><span class="line">test, test.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 9., 36., 81.]), torch.float32)</span><br></pre></td></tr></table></figure><p>int64 × float32 → float32</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int_32_tensor = torch.tensor([<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>], dtype=torch.int64)</span><br><span class="line">int_32_tensor, int_32_tensor.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([3, 6, 9]), torch.int64)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test = float_32_tensor * int_32_tensor</span><br><span class="line">test, test.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 9., 36., 81.]), torch.float32)</span><br></pre></td></tr></table></figure><p>long × float32 → float32</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int_32_tensor = torch.tensor([<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>], dtype=torch.long)</span><br><span class="line">int_32_tensor, int_32_tensor.dtpye</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([3, 6, 9]), torch.int64)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test = float_32_tensor * int_32_tensor</span><br><span class="line">test, test.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 9., 36., 81.]), torch.float32)</span><br></pre></td></tr></table></figure><h2 id="Tensor-attributes-information-about-tensors"><a href="#Tensor-attributes-information-about-tensors" class="headerlink" title="Tensor attributes (information about tensors)"></a>Tensor attributes (information about tensors)</h2><ul><li>shape/size()：形状</li><li>dtpye：数据类型</li><li>device：设备</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor</span></span><br><span class="line">some_tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find out details about it</span></span><br><span class="line"><span class="built_in">print</span>(some_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;some_tensor.shape&#125;</span>&quot;</span>)<span class="comment">#some_tensor.shape=some_tensor.size()</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;some_tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;some_tensor.device&#125;</span>&quot;</span>) <span class="comment"># default to CPU</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4688, 0.0055, 0.8551, 0.0646],</span><br><span class="line">        [0.6538, 0.5157, 0.4071, 0.2109],</span><br><span class="line">        [0.9960, 0.3061, 0.9369, 0.7008]])</span><br><span class="line">Shape of tensor: torch.Size([3, 4])</span><br><span class="line">Datatype of tensor: torch.float32</span><br><span class="line">Device tensor is stored on: cpu</span><br></pre></td></tr></table></figure><h2 id="Manipulating-tensors"><a href="#Manipulating-tensors" class="headerlink" title="Manipulating tensors"></a>Manipulating tensors</h2><ul><li>Addition：加法</li><li>Substraction：减法</li><li>Multiplication (element-wise)：乘法</li><li>Division：除法</li><li>Matrix multiplication：矩阵乘法</li></ul><h3 id="basic-operations"><a href="#basic-operations" class="headerlink" title="basic operations"></a>basic operations</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor of values and add a number to it</span></span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor + <span class="number">10</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([11, 12, 13])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Multiply it by 10</span></span><br><span class="line">tensor * <span class="number">10</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([10, 20, 30])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Tensors don&#x27;t change unless reassigned</span><br><span class="line">tensor</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Subtract and reassign</span></span><br><span class="line">tensor = tensor - <span class="number">10</span></span><br><span class="line">tensor</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-9, -8, -7])</span><br></pre></td></tr></table></figure><p>PyTorch也有一些内置函数，如<code>torch.mul()</code>(乘法的缩写)和<code>torch.add()</code>来执行基本操作。</p><h3 id="Matrix-multiplication"><a href="#Matrix-multiplication" class="headerlink" title="Matrix multiplication"></a>Matrix multiplication</h3><p><a href="https://www.mathsisfun.com/algebra/matrix-multiplying.html">How to Multiply Matrices</a></p><p>矩阵乘法需要记住的两个主要规则是：</p><ol><li><p><strong>内部尺寸必须匹配</strong>：<br>(3, 2) @ (3, 2)不起作用<br>(2, 3) @ (3, 2)会起作用<br>(3, 2) @ (2, 3)会起作用</p></li><li><p><strong>得到的矩阵具有外部尺寸的形状</strong>：<br>(2, 3) @ (3, 2)-&gt;(2, 2)<br>(3, 2) @ (2, 3)-&gt;(3, 3)</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor.shape</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3])</span><br></pre></td></tr></table></figure><ul><li>元素乘法：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor * tensor</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 4, 9])</span><br></pre></td></tr></table></figure><ul><li>矩阵乘法：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.matmul(tensor, tensor)</span><br><span class="line"><span class="comment"># 1 * 1 + 2 * 2 + 3 * 3</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(14)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(tensor, tensor)</span><br><span class="line"><span class="comment"># 1 * 1 + 2 * 2 + 3 * 3</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(14)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor @ tensor</span><br><span class="line"><span class="comment"># 1 * 1 + 2 * 2 + 3 * 3</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(14)</span><br></pre></td></tr></table></figure><ul><li>内置<code>torch.matmul()</code>方法更快。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"><span class="comment"># Matrix multiplication by hand </span></span><br><span class="line"><span class="comment"># (avoid doing operations with for loops at all cost, they are computationally expensive)</span></span><br><span class="line">value = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tensor)):</span><br><span class="line">  value += tensor[i] * tensor[i]</span><br><span class="line">value</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CPU times: user 773 µs, sys: 0 ns, total: 773 µs</span><br><span class="line">Wall time: 499 µs</span><br><span class="line">tensor(14)</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">%time</span></span><br><span class="line">torch.matmul(tensor, tensor)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CPU times: user 146 µs, sys: 83 µs, total: 229 µs</span><br><span class="line">Wall time: 171 µs</span><br><span class="line">tensor(14)</span><br></pre></td></tr></table></figure><h3 id="深度学习中最常见的错误之一（形状错误）"><a href="#深度学习中最常见的错误之一（形状错误）" class="headerlink" title="深度学习中最常见的错误之一（形状错误）"></a>深度学习中最常见的错误之一（形状错误）</h3><p><a href="http://matrixmultiplication.xyz/">matrixmultiplication矩阵可视化</a></p><p>由于深度学习的大部分内容是对矩阵进行乘法和执行运算，并且矩阵对于可以组合的形状和大小有严格的规则，因此在深度学习中遇到的最常见错误之一就是形状不匹配。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shapes need to be in the right way  </span></span><br><span class="line">tensor_A = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                         [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">                         [<span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">tensor_B = torch.tensor([[<span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">                         [<span class="number">8</span>, <span class="number">11</span>], </span><br><span class="line">                         [<span class="number">9</span>, <span class="number">12</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">torch.matmul(tensor_A, tensor_B) <span class="comment"># (this will error)</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">RuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)</span><br></pre></td></tr></table></figure><p>通过矩阵的内部维度匹配来使矩阵乘法在tensor_A和之间进行tensor_B。</p><p><strong>使用转置（切换给定张量的维度）。</strong></p><p>您可以使用以下任一方式在 PyTorch 中执行转置：</p><ul><li><code>torch.transpose(input, dim0, dim1)</code>， 其中 <code>input</code> 是需要转置的张量， 和 <code>dim0</code> 是 <code>dim1</code> 需要交换的维度。</li><li><code>tensor.T</code>，<code>tensor</code> 是需要转置的张量。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tensor([[1., 2.],</span></span><br><span class="line"><span class="comment">#        [3., 4.],</span></span><br><span class="line"><span class="comment">#        [5., 6.]])</span></span><br><span class="line"><span class="comment">#tensor([[ 7., 10.],</span></span><br><span class="line"><span class="comment">#        [ 8., 11.],</span></span><br><span class="line"><span class="comment">#        [ 9., 12.]])</span></span><br><span class="line"><span class="built_in">print</span>(tensor_A)</span><br><span class="line"><span class="built_in">print</span>(tensor_B.T)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.],</span><br><span class="line">        [5., 6.]])</span><br><span class="line">tensor([[ 7.,  8.,  9.],</span><br><span class="line">        [10., 11., 12.]])</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">The operation works when tensor_B is transposed</span></span><br><span class="line">print(f&quot;Original shapes: tensor_A = &#123;tensor_A.shape&#125;, tensor_B = &#123;tensor_B.shape&#125;\n&quot;)</span><br><span class="line">print(f&quot;New shapes: tensor_A = &#123;tensor_A.shape&#125; (same as above), tensor_B.T = &#123;tensor_B.T.shape&#125;\n&quot;)</span><br><span class="line">print(f&quot;Multiplying: &#123;tensor_A.shape&#125; * &#123;tensor_B.T.shape&#125; &lt;- inner dimensions match\n&quot;)</span><br><span class="line">print(&quot;Output:\n&quot;)</span><br><span class="line">output = torch.matmul(tensor_A, tensor_B.T)</span><br><span class="line">print(output) </span><br><span class="line">print(f&quot;\nOutput shape: &#123;output.shape&#125;&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])</span><br><span class="line"></span><br><span class="line">New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])</span><br><span class="line"></span><br><span class="line">Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"></span><br><span class="line">tensor([[ 27.,  30.,  33.],</span><br><span class="line">        [ 61.,  68.,  75.],</span><br><span class="line">        [ 95., 106., 117.]])</span><br><span class="line"></span><br><span class="line">Output shape: torch.Size([3, 3])</span><br></pre></td></tr></table></figure><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-22.gif" class="" title="PyTorch-26H-1-22"><h3 id="像这样的矩阵乘法也称为两个矩阵的点积。"><a href="#像这样的矩阵乘法也称为两个矩阵的点积。" class="headerlink" title="像这样的矩阵乘法也称为两个矩阵的点积。"></a>像这样的矩阵乘法也称为两个矩阵的点积。</h3><p>神经网络充满了矩阵乘法和点积。</p><p>该<code>torch.nn.Linear()</code>模块（我们稍后会看到它的实际作用），也称为前馈层或全连接层，实现输入<code>x</code>和权重矩阵之间的矩阵乘法<code>A</code>。</p><script type="math/tex; mode=display">y = x\cdot{A^T} + b</script><ul><li>x是该层的输入（深度学习是将多个层<code>torch.nn.Linear()</code>和其他层堆叠在一起）。</li><li>A是由该层创建的权重矩阵，它开始是随机数，随着神经网络学习更好地表示数据中的模式，这些随机数会进行调整（请注意“ T”，这是因为权重矩阵被转置了）。</li><li>注意：您可能还经常看到<code>W</code>或另一个字母，<code>X</code>用于展示权重矩阵。</li><li><code>b</code>是用于稍微偏移权重和输入的偏差项。</li><li><code>y</code>是输出（对输入进行操作以期发现其中的模式）。</li></ul><p>这是一个线性函数（你可能在高中或其他地方见过类似 $y = mx+b$ 的函数），可以用来画一条直线！</p><p>让我们尝试一下线性层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Since the linear layer starts with a random weights matrix, let&#x27;s make it reproducible (more on this later)由于线性层以随机权重矩阵开始，因此让我们使其可重现</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># This uses matrix multiplication使用矩阵乘法</span></span><br><span class="line">linear = torch.nn.Linear(in_features=<span class="number">2</span>, <span class="comment"># in_features = matches inner dimension of input 匹配输入的内维度</span></span><br><span class="line">                         out_features=<span class="number">6</span>) <span class="comment"># out_features = describes outer value 描述外部值</span></span><br><span class="line">x = tensor_A</span><br><span class="line">output = linear(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Input shape: <span class="subst">&#123;x.shape&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output:\n<span class="subst">&#123;output&#125;</span>\n\nOutput shape: <span class="subst">&#123;output.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Input shape: torch.Size([3, 2])</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],</span><br><span class="line">        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],</span><br><span class="line">        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],</span><br><span class="line">       grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Output shape: torch.Size([3, 6])</span><br></pre></td></tr></table></figure><h2 id="Finding-the-min-max-mean-amp-sum"><a href="#Finding-the-min-max-mean-amp-sum" class="headerlink" title="Finding the min, max, mean &amp; sum"></a>Finding the min, max, mean &amp; sum</h2><h3 id="最小最大平均求和"><a href="#最小最大平均求和" class="headerlink" title="最小最大平均求和"></a>最小最大平均求和</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor</span></span><br><span class="line">x = torch.arange(<span class="number">0</span>, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Minimum: <span class="subst">&#123;x.<span class="built_in">min</span>()&#125;</span>&quot;</span>)<span class="comment">#torch.min(x)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Maximum: <span class="subst">&#123;x.<span class="built_in">max</span>()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(f&quot;Mean: &#123;x.mean()&#125;&quot;) # this will error</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Mean: <span class="subst">&#123;x.<span class="built_in">type</span>(torch.float32).mean()&#125;</span>&quot;</span>) <span class="comment"># won&#x27;t work without float datatype</span></span><br><span class="line"><span class="comment"># torch.mean(x.type(torch.float32))</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Sum: <span class="subst">&#123;x.<span class="built_in">sum</span>()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Minimum: 0</span><br><span class="line">Maximum: 90</span><br><span class="line">Mean: 45.0</span><br><span class="line">Sum: 450</span><br></pre></td></tr></table></figure><h3 id="查找最大最小索引位置"><a href="#查找最大最小索引位置" class="headerlink" title="查找最大最小索引位置"></a>查找最大最小索引位置</h3><p><code>torch.argmax()</code>，<code>torch.argmin()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor</span></span><br><span class="line">tensor = torch.arange(<span class="number">10</span>, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor: <span class="subst">&#123;tensor&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Returns index of max and min values</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Index where max value occurs: <span class="subst">&#123;tensor.argmax()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Index where min value occurs: <span class="subst">&#123;tensor.argmin()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])</span><br><span class="line">Index where max value occurs: 8</span><br><span class="line">Index where min value occurs: 0</span><br></pre></td></tr></table></figure><h3 id="更改数据类型"><a href="#更改数据类型" class="headerlink" title="更改数据类型"></a>更改数据类型</h3><p><code>torch.Tensor.type(dtype=None)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor and check its datatype</span></span><br><span class="line">tensor = torch.arange(<span class="number">10.</span>, <span class="number">100.</span>, <span class="number">10.</span>)</span><br><span class="line">tensor.dtype</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.float32</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a float16 tensor</span></span><br><span class="line">tensor_float16 = tensor.<span class="built_in">type</span>(torch.float16)</span><br><span class="line">tensor_float16</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create an int8 tensor</span></span><br><span class="line">tensor_int8 = tensor.<span class="built_in">type</span>(torch.int8)</span><br><span class="line">tensor_int8</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)</span><br></pre></td></tr></table></figure><blockquote><p>数字越小（例如 32、16、8），计算机存储的值就越不精确。存储量越少，通常计算速度越快，整体模型就越小。基于移动端的神经网络通常使用 8 位整数，与 float32 相比，它们更小、运行速度更快，但准确度较低。</p></blockquote><p><a href="https://pytorch.org/docs/stable/tensors.html">torch.Tensor-doc</a></p><h2 id="重塑、视图、堆叠、压缩、解压、置换"><a href="#重塑、视图、堆叠、压缩、解压、置换" class="headerlink" title="重塑、视图、堆叠、压缩、解压、置换"></a>重塑、视图、堆叠、压缩、解压、置换</h2><ul><li>Reshaping - reshapes an input tensor to a defined shape</li><li>View - return a view of a input tensor of certain shape but keep the same memory as the original tensor</li><li>Stacking - combine multiple tnesors on top of each other (vstack) or side by side(hstack)</li><li>Squeeze - removes all 1 dimensions from a tensor</li><li>Unsqueeze - add a 1 dimensions to a target tensor</li><li><p>Permute - Return a view of the input with dimensions permuted(swapped) in a certain way</p></li><li><p>重塑 - 将输入张量重塑为定义的形状</p></li><li>视图 - 返回特定形状的输入张量的视图，但保留与原始张量相同的内存</li><li>堆叠 - 将多个张量组合在一起（vstack）或并排组合（hstack）</li><li>压缩 - 从张量中删除所有 1 维</li><li>解压 - 向目标张量添加 1 维</li><li>置换 - 返回以特定方式置换（交换）维度的输入视图</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">方法</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">torch.reshape(input, shape)</td><td style="text-align:center">重塑input为shape（如果兼容），也可以使用torch.Tensor.reshape()。</td></tr><tr><td style="text-align:center">Tensor.view(shape)</td><td style="text-align:center">shape返回与原始张量不同的但共享相同数据的原始张量的视图。</td></tr><tr><td style="text-align:center">torch.stack(tensors, dim=0)</td><td style="text-align:center">沿新维度（dim）连接一系列张量（tensor），所有tensors必须大小相同。</td></tr><tr><td style="text-align:center">torch.squeeze(input)</td><td style="text-align:center">挤压input以删除所有具有值的维度1。</td></tr><tr><td style="text-align:center">torch.unsqueeze(input, dim)</td><td style="text-align:center">返回在 处添加input的维度值。1dim</td></tr><tr><td style="text-align:center">torch.permute(input, dims)</td><td style="text-align:center">返回原始输入的视图，其尺寸已置换（重新排列）为 dims。</td></tr></tbody></table></div><h3 id="Reshaping-重塑"><a href="#Reshaping-重塑" class="headerlink" title="Reshaping-重塑"></a>Reshaping-重塑</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">1.</span>, <span class="number">8.</span>)</span><br><span class="line">x, x.shape</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add an extra dimension</span></span><br><span class="line">x_reshaped = x.reshape(<span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">x_reshaped, x_reshaped.shape</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</span><br></pre></td></tr></table></figure><p><code>x.reshape(1, 8)</code> → error，因为元素数和原来数组不相同。</p><p><code>x.reshape(2, 7)</code> → error，因为在不增加元素数量的情况情况下将元素数量增加一倍。</p><p><code>x.reshape(7, 1)</code> → 可以运行，行列转换。</p><h3 id="View-视图"><a href="#View-视图" class="headerlink" title="View-视图"></a>View-视图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Change view (keeps same data as original but changes view)</span></span><br><span class="line"><span class="comment"># See more: https://stackoverflow.com/a/54507446/7900723</span></span><br><span class="line">z = x.view(<span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">z, z.shape</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</span><br></pre></td></tr></table></figure><p>改变张量的视图实际上只会创建同<code>torch.view()</code>一张量的新视图。</p><p>因此<strong>改变视图也会改变原始张量。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Changing z changes x</span></span><br><span class="line">z[:, <span class="number">0</span>] = <span class="number">5</span></span><br><span class="line">z, x</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))</span><br></pre></td></tr></table></figure><h3 id="Stacking-堆叠"><a href="#Stacking-堆叠" class="headerlink" title="Stacking-堆叠"></a>Stacking-堆叠</h3><p>新的张量堆叠在其自身之上五次，<code>torch.stack()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stack tensors on top of each other</span></span><br><span class="line">x_stacked = torch.stack([x, x, x, x], dim=<span class="number">0</span>)</span><br><span class="line">x_stacked</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5., 2., 3., 4., 5., 6., 7.],</span><br><span class="line">        [5., 2., 3., 4., 5., 6., 7.],</span><br><span class="line">        [5., 2., 3., 4., 5., 6., 7.],</span><br><span class="line">        [5., 2., 3., 4., 5., 6., 7.]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stack tensors on top of each other</span></span><br><span class="line">x_stacked = torch.stack([x, x, x, x], dim=<span class="number">1</span>)</span><br><span class="line">x_stacked</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5., 5., 5., 5.],</span><br><span class="line">        [2., 2., 2., 2.],</span><br><span class="line">        [3., 3., 3., 3.],</span><br><span class="line">        [4., 4., 4., 4.],</span><br><span class="line">        [5., 5., 5., 5.],</span><br><span class="line">        [6., 6., 6., 6.],</span><br><span class="line">        [7., 7., 7., 7.]])</span><br></pre></td></tr></table></figure><p><code>x_stacked = torch.stack([x, x, x, x], dim=2)</code> 维度为2不行，因为原来的形状与二维不兼容。</p><h4 id="torch-stack-和-torch-vstack-和-torch-hstack"><a href="#torch-stack-和-torch-vstack-和-torch-hstack" class="headerlink" title="torch.stack 和 torch.vstack 和 torch.hstack"></a>torch.stack 和 torch.vstack 和 torch.hstack</h4><ul><li>stack</li></ul><p><a href="https://pytorch.org/docs/main/generated/torch.stack.html">torch.stack</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack(tensors, dim=0, *, out=None) → Tensor</span><br></pre></td></tr></table></figure><p>沿新维度连接一系列张量。</p><p>所有张量都需要具有相同的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">x, x.size()</span><br><span class="line"></span><br><span class="line">x1 = torch.stack((x, x)) <span class="comment"># same as torch.stack((x, x), dim=0)</span></span><br><span class="line">x1, x1.size()</span><br><span class="line"></span><br><span class="line">x2 = torch.stack((x, x), dim=<span class="number">1</span>)</span><br><span class="line">x2, x2.size()</span><br><span class="line"></span><br><span class="line">x3 = torch.stack((x, x), dim=<span class="number">2</span>)</span><br><span class="line">x3, x3.size()</span><br><span class="line"></span><br><span class="line">x4 = torch.stack((x, x), dim=-<span class="number">1</span>)</span><br><span class="line">x4, x4.size()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.1801,  0.1566, -2.0349],</span><br><span class="line">         [ 0.0183, -0.0088, -0.6409]]),</span><br><span class="line"> torch.Size([2, 3]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(tensor([[[ 1.3646, -0.4994, -0.6540],</span><br><span class="line">          [-0.3804, -0.2373,  1.8952]],</span><br><span class="line"> </span><br><span class="line">         [[ 1.3646, -0.4994, -0.6540],</span><br><span class="line">          [-0.3804, -0.2373,  1.8952]]]),</span><br><span class="line"> torch.Size([2, 2, 3]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(tensor([[[ 1.3646, -0.4994, -0.6540],</span><br><span class="line">          [ 1.3646, -0.4994, -0.6540]],</span><br><span class="line"> </span><br><span class="line">         [[-0.3804, -0.2373,  1.8952],</span><br><span class="line">          [-0.3804, -0.2373,  1.8952]]]),</span><br><span class="line"> torch.Size([2, 2, 3]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(tensor([[[ 1.3646,  1.3646],</span><br><span class="line">          [-0.4994, -0.4994],</span><br><span class="line">          [-0.6540, -0.6540]],</span><br><span class="line"> </span><br><span class="line">         [[-0.3804, -0.3804],</span><br><span class="line">          [-0.2373, -0.2373],</span><br><span class="line">          [ 1.8952,  1.8952]]]),</span><br><span class="line"> torch.Size([2, 3, 2]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(tensor([[[ 1.3646,  1.3646],</span><br><span class="line">          [-0.4994, -0.4994],</span><br><span class="line">          [-0.6540, -0.6540]],</span><br><span class="line"> </span><br><span class="line">         [[-0.3804, -0.3804],</span><br><span class="line">          [-0.2373, -0.2373],</span><br><span class="line">          [ 1.8952,  1.8952]]]),</span><br><span class="line"> torch.Size([2, 3, 2]))</span><br></pre></td></tr></table></figure><ul><li>vstack</li></ul><p><a href="https://pytorch.org/docs/stable/generated/torch.vstack.html">torch.vstack</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.vstack(tensors, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>按垂直顺序（按行）堆叠张量。</p><p>这相当于在所有一维张量被重塑后沿第一个轴进行连接<code>torch.atleast_2d()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">torch.vstack((a,b))</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])</span><br><span class="line">torch.vstack((a,b))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6]])</span><br><span class="line"></span><br><span class="line">tensor([[1],</span><br><span class="line">        [2],</span><br><span class="line">        [3],</span><br><span class="line">        [4],</span><br><span class="line">        [5],</span><br><span class="line">        [6]])</span><br></pre></td></tr></table></figure><ul><li>hstack</li></ul><p><a href="https://pytorch.org/docs/stable/generated/torch.hstack.html">torch.hstack</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.hstack(tensors, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>按水平顺序（按列）堆叠张量。</p><p>这相当于沿第一个轴对一维张量进行连接，并沿第二个轴对所有其他张量进行连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">torch.hstack((a,b))</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])</span><br><span class="line">torch.hstack((a,b))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3, 4, 5, 6])</span><br><span class="line"></span><br><span class="line">tensor([[1, 4],</span><br><span class="line">        [2, 5],</span><br><span class="line">        [3, 6]])</span><br></pre></td></tr></table></figure><h3 id="Squeeze-压缩"><a href="#Squeeze-压缩" class="headerlink" title="Squeeze-压缩"></a>Squeeze-压缩</h3><p>从张量中删除所有单一维度（将张量压缩为仅具有超过 1 的维度）</p><p><a href="https://pytorch.org/docs/main/generated/torch.squeeze.html">torch.squeeze</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.squeeze(<span class="built_in">input</span>: Tensor, dim: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">int</span>, <span class="type">List</span>[<span class="built_in">int</span>]]]) → Tensor</span><br></pre></td></tr></table></figure><p>返回已删除所有指定尺寸为input1的张量。</p><p>input：(A × 1 × B × C × 1 × D)<br>method：input.squeeze()<br>output：(A × B × C × D)</p><p>input：(A × 1 × B)<br>method：squeeze(input, 0)<br>output：(A × 1 × B)</p><p>input：(A × 1 × B)<br>method：squeeze(input, 1)<br>output：(A × B)</p><blockquote><p>返回的张量与输入张量共享存储，因此改变一个张量的内容也会改变另一个张量的内容。<br>果张量的批处理维度为 1，则squeeze(input) 也会删除批处理维度，这可能会导致意外错误。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous tensor: <span class="subst">&#123;x_reshaped&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous shape: <span class="subst">&#123;x_reshaped.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove extra dimension from x_reshaped</span></span><br><span class="line">x_squeezed = x_reshaped.squeeze()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nNew tensor: <span class="subst">&#123;x_squeezed&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New shape: <span class="subst">&#123;x_squeezed.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])</span><br><span class="line">Previous shape: torch.Size([1, 7])</span><br><span class="line"></span><br><span class="line">New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])</span><br><span class="line">New shape: torch.Size([7])</span><br></pre></td></tr></table></figure><h3 id="Unsqueeze-解压"><a href="#Unsqueeze-解压" class="headerlink" title="Unsqueeze-解压"></a>Unsqueeze-解压</h3><p>在特定索引处添加维度值 1</p><p><a href="https://pytorch.org/docs/main/generated/torch.unsqueeze.html">torch.unsqueeze</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim) → Tensor</span><br></pre></td></tr></table></figure><p>返回在指定位置插入一个维度为一的新张量。</p><p>返回的张量与该张量共享相同的底层数据。</p><p>可以使用dim范围内的值。负数将对应于= 处的应用。[-input.dim() - 1, input.dim() + 1)dimunsqueeze()dimdim + input.dim() + 1</p><p>可以使用 <code>[-input.dim() - 1, input.dim() + 1)</code> 范围内的 dim 值。负 dim 将对应于在 <code>dim = dim + input.dim() + 1</code> 处应用的 unsqueeze()。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous tensor: <span class="subst">&#123;x_squeezed&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous shape: <span class="subst">&#123;x_squeezed.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Add an extra dimension with unsqueeze</span></span><br><span class="line">x_unsqueezed = x_squeezed.unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nNew tensor: <span class="subst">&#123;x_unsqueezed&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New shape: <span class="subst">&#123;x_unsqueezed.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_unsqueezed = x_squeezed.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nNew tensor: <span class="subst">&#123;x_unsqueezed&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New shape: <span class="subst">&#123;x_unsqueezed.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])</span><br><span class="line">Previous shape: torch.Size([7])</span><br><span class="line"></span><br><span class="line">New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])</span><br><span class="line">New shape: torch.Size([1, 7])</span><br><span class="line"></span><br><span class="line">New tensor: tensor([[5.],</span><br><span class="line">[2.],</span><br><span class="line">                    [3.],</span><br><span class="line">                    [4.],</span><br><span class="line">                    [5.],</span><br><span class="line">                    [6.],</span><br><span class="line">                    [7.]])</span><br><span class="line">New shape: torch.Size([7, 1])</span><br></pre></td></tr></table></figure><h3 id="Permute-置换"><a href="#Permute-置换" class="headerlink" title="Permute-置换"></a>Permute-置换</h3><p>重新排列轴值的顺序</p><p><a href="https://pytorch.org/docs/stable/generated/torch.permute.html">torch.permute</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.permute(<span class="built_in">input</span>, dims) → Tensor</span><br></pre></td></tr></table></figure><p>返回维度已排列的原始张量输入的视图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.permute(x, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).size()</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create tensor with specific shape</span></span><br><span class="line">x_original = torch.rand(size=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Permute the original tensor to rearrange the axis order</span></span><br><span class="line">x_permuted = x_original.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment"># shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous shape: <span class="subst">&#123;x_original.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New shape: <span class="subst">&#123;x_permuted.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Previous shape: torch.Size([224, 224, 3])</span><br><span class="line">New shape: torch.Size([3, 224, 224])</span><br></pre></td></tr></table></figure><blockquote><p>因为排列返回一个视图（与原始共享相同的数据），所以排列张量中的值将与原始张量相同，如果更改视图中的值，它将更改原始的值。</p></blockquote><h2 id="Selecting-data-indexing"><a href="#Selecting-data-indexing" class="headerlink" title="Selecting data (indexing)"></a>Selecting data (indexing)</h2><p>从张量中选择特定数据（例如，仅第一列或第二行）。可以使用索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor </span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">10</span>).reshape(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x, x.shape</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[[1, 2, 3],</span><br><span class="line">          [4, 5, 6],</span><br><span class="line">          [7, 8, 9]]]),</span><br><span class="line"> torch.Size([1, 3, 3]))</span><br></pre></td></tr></table></figure><p>索引值从外部维度 -&gt; 内部维度（检查方括号）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let&#x27;s index bracket by bracket</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First square bracket:\n<span class="subst">&#123;x[<span class="number">0</span>]&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Second square bracket: <span class="subst">&#123;x[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Third square bracket: <span class="subst">&#123;x[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">First square bracket:</span><br><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6],</span><br><span class="line">        [7, 8, 9]])</span><br><span class="line">Second square bracket: tensor([1, 2, 3])</span><br><span class="line">Third square bracket: 1</span><br></pre></td></tr></table></figure><p><code>:</code>指定“此维度中的所有值”，然后使用逗号 ( <code>,</code>) 添加另一个维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get all values of 0th dimension and the 0 index of 1st dimension, 获取第 0 维的所有值以及第 1 维的 0 索引</span></span><br><span class="line">x[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 2, 3]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get all values of 0th &amp; 1st dimensions but only index 1 of 2nd dimension, 获取第 0 维和第 1 维的所有值，但仅获取第 2 维的索引 1</span></span><br><span class="line">x[:, :, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[2, 5, 8]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension,获取 0 维的所有值，但仅获取第 1 维和第 2 维的 1 索引值</span></span><br><span class="line">x[:, <span class="number">1</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([5])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get index 0 of 0th and 1st dimension and all values of 2nd dimension,获取第 0 维和第 1 维的索引 0 以及第 2 维的所有值</span></span><br><span class="line">x[<span class="number">0</span>, <span class="number">0</span>, :] <span class="comment"># same as x[0][0]</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Index on x to return 9</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>][<span class="number">2</span>][<span class="number">2</span>])</span><br><span class="line"><span class="comment">#x[0, 2, 2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Index on x to return 3, 6, 9</span></span><br><span class="line"><span class="built_in">print</span>(x[:, :, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># x[0, :, 2]</span></span><br></pre></td></tr></table></figure><h2 id="PyTorch-tensors-and-NumPy"><a href="#PyTorch-tensors-and-NumPy" class="headerlink" title="PyTorch tensors and NumPy"></a>PyTorch tensors and NumPy</h2><p>NumPy 是一个流行的 Python 数值计算库，因此 PyTorch 具有与其良好交互的功能。</p><p>从 NumPy 到 PyTorch（以及返回）需要使用的两种主要方法是：</p><ul><li><code>torch.from_numpy(ndarray)</code> ： NumPy 数组 -&gt; PyTorch 张量。</li><li><code>torch.Tensor.numpy()</code> ： PyTorch 张量 -&gt; NumPy 数组。</li></ul><h3 id="NumPy-array-to-tensor"><a href="#NumPy-array-to-tensor" class="headerlink" title="NumPy array to tensor"></a>NumPy array to tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NumPy array to tensor</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">array = np.arange(<span class="number">1.0</span>, <span class="number">8.0</span>)</span><br><span class="line">tensor = torch.from_numpy(array)</span><br><span class="line"></span><br><span class="line">array, tensor</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([1., 2., 3., 4., 5., 6., 7.]),</span><br><span class="line"> tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</span><br></pre></td></tr></table></figure><p>默认情况下，NumPy 数组是使用数据类型创建的float64，如果将其转换为 PyTorch 张量，将保留相同的数据类型（如上）。</p><p>许多 PyTorch 计算默认使用float32。</p><p>转换 NumPy 数组 (float64) -&gt; PyTorch 张量 (float64) -&gt; PyTorch 张量 (float32)，使用<code>tensor = torch.from_numpy(array).type(torch.float32)</code>。</p><h3 id="改变数组，张量不变"><a href="#改变数组，张量不变" class="headerlink" title="改变数组，张量不变"></a>改变数组，张量不变</h3><p>tensor上面重新分配了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改变数组，保留张量数组</span></span><br><span class="line"><span class="comment"># Change the array, keep the tensor</span></span><br><span class="line">array = array + <span class="number">1</span></span><br><span class="line">array, tensor</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([2., 3., 4., 5., 6., 7., 8.]),</span><br><span class="line"> tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</span><br></pre></td></tr></table></figure><h3 id="Tensor-to-NumPy-array"><a href="#Tensor-to-NumPy-array" class="headerlink" title="Tensor to NumPy array"></a>Tensor to NumPy array</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tensor to NumPy array</span></span><br><span class="line">tensor = torch.ones(<span class="number">7</span>) <span class="comment"># create a tensor of ones with dtype=float32</span></span><br><span class="line">numpy_tensor = tensor.numpy() <span class="comment"># will be dtype=float32 unless changed</span></span><br><span class="line">tensor, numpy_tensor</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1., 1., 1., 1., 1., 1., 1.]),</span><br><span class="line"> array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</span><br></pre></td></tr></table></figure><h3 id="改变张量，数组不变"><a href="#改变张量，数组不变" class="headerlink" title="改变张量，数组不变"></a>改变张量，数组不变</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Change the tensor, keep the array the same</span></span><br><span class="line">tensor = tensor + <span class="number">1</span></span><br><span class="line">tensor, numpy_tensor</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([2., 2., 2., 2., 2., 2., 2.]),</span><br><span class="line"> array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</span><br></pre></td></tr></table></figure><h2 id="Reproducibility"><a href="#Reproducibility" class="headerlink" title="Reproducibility"></a>Reproducibility</h2><p><a href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch-Reproducibility</a></p><p><a href="https://en.wikipedia.org/wiki/Random_seed">Random seed-WiKi</a></p><p>伪随机性。</p><p>计算机的设计从根本上来说就是确定性的（每个步骤都是可预测的），所以它们产生的随机性是模拟随机性。</p><p>神经网络从随机数开始描述数据中的模式（这些数字是糟糕的描述），并尝试使用张量运算（以及一些我们尚未讨论的其他内容）来改进这些随机数，以更好地描述数据中的模式。</p><p>流程：start with random numbers -&gt; tensor operations -&gt; try to make better (again and again and again)</p><p>尽管随机性很好而且很强大，但有时还是希望随机性少一点，因此可以进行可重复的实验。</p><p>A计算机上运行与B计算机上运行相同的代码是否可以获得相同（或非常相似）的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create two random tensors</span></span><br><span class="line">random_tensor_A = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">random_tensor_B = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor A:\n<span class="subst">&#123;random_tensor_A&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor B:\n<span class="subst">&#123;random_tensor_B&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does Tensor A equal Tensor B? (anywhere)&quot;</span>)</span><br><span class="line">random_tensor_A == random_tensor_B</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Tensor A:</span><br><span class="line">tensor([[0.8016, 0.3649, 0.6286, 0.9663],</span><br><span class="line">        [0.7687, 0.4566, 0.5745, 0.9200],</span><br><span class="line">        [0.3230, 0.8613, 0.0919, 0.3102]])</span><br><span class="line"></span><br><span class="line">Tensor B:</span><br><span class="line">tensor([[0.9536, 0.6002, 0.0351, 0.6826],</span><br><span class="line">        [0.3743, 0.5220, 0.1336, 0.9666],</span><br><span class="line">        [0.9754, 0.8474, 0.8988, 0.1105]])</span><br><span class="line"></span><br><span class="line">Does Tensor A equal Tensor B? (anywhere)</span><br><span class="line">tensor([[False, False, False, False],</span><br><span class="line">        [False, False, False, False],</span><br><span class="line">        [False, False, False, False]])</span><br></pre></td></tr></table></figure><p>创建两个具有相同值的随机张量。</p><p>张量仍然包含随机值，但它们具有相同的特性。</p><p>这就是<code>torch.manual_seed(seed)</code>出现的位置，其中seed是一个整数（类似于42但可以是任何东西），它决定了随机性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Set the random seed</span></span><br><span class="line">RANDOM_SEED=<span class="number">42</span> <span class="comment"># try changing this to different values and see what happens to the numbers below</span></span><br><span class="line">torch.manual_seed(seed=RANDOM_SEED) </span><br><span class="line">random_tensor_C = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Have to reset the seed every time a new rand() is called </span></span><br><span class="line"><span class="comment"># Without this, tensor_D would be different to tensor_C </span></span><br><span class="line">torch.random.manual_seed(seed=RANDOM_SEED) <span class="comment"># try commenting this line out and seeing what happens</span></span><br><span class="line">random_tensor_D = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor C:\n<span class="subst">&#123;random_tensor_C&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor D:\n<span class="subst">&#123;random_tensor_D&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does Tensor C equal Tensor D? (anywhere)&quot;</span>)</span><br><span class="line">random_tensor_C == random_tensor_D</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Tensor C:</span><br><span class="line">tensor([[0.8823, 0.9150, 0.3829, 0.9593],</span><br><span class="line">        [0.3904, 0.6009, 0.2566, 0.7936],</span><br><span class="line">        [0.9408, 0.1332, 0.9346, 0.5936]])</span><br><span class="line"></span><br><span class="line">Tensor D:</span><br><span class="line">tensor([[0.8823, 0.9150, 0.3829, 0.9593],</span><br><span class="line">        [0.3904, 0.6009, 0.2566, 0.7936],</span><br><span class="line">        [0.9408, 0.1332, 0.9346, 0.5936]])</span><br><span class="line"></span><br><span class="line">Does Tensor C equal Tensor D? (anywhere)</span><br><span class="line">tensor([[True, True, True, True],</span><br><span class="line">        [True, True, True, True],</span><br><span class="line">        [True, True, True, True]])</span><br></pre></td></tr></table></figure><h2 id="在-GPU-上运行张量（并进行更快的计算）"><a href="#在-GPU-上运行张量（并进行更快的计算）" class="headerlink" title="在 GPU 上运行张量（并进行更快的计算）"></a>在 GPU 上运行张量（并进行更快的计算）</h2><p>深度学习算法需要大量的数值运算。</p><p>默认情况下这些操作通常在 CPU（计算机处理单元）上完成。</p><p>然而，还有另一种常见的硬件，称为 GPU（图形处理单元），它在执行神经网络所需的特定类型的操作（矩阵乘法）时通常比 CPU 快得多。</p><h3 id="获取GPU"><a href="#获取GPU" class="headerlink" title="获取GPU"></a>获取GPU</h3><p><a href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/">Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning</a></p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Method</strong></th><th style="text-align:center"><strong>Difficulty to setup</strong></th><th style="text-align:center"><strong>Pros</strong></th><th style="text-align:center"><strong>Cons</strong></th><th style="text-align:center"><strong>How to setup</strong></th></tr></thead><tbody><tr><td style="text-align:center">Google Colab</td><td style="text-align:center">Easy</td><td style="text-align:center">Free to use, almost zero setup required, can share work with others as easy as a link</td><td style="text-align:center">Doesn’t save your data outputs, limited compute, subject to timeouts</td><td style="text-align:center"><a href="https://colab.research.google.com/notebooks/gpu.ipynb">Follow the Google Colab Guide</a></td></tr><tr><td style="text-align:center">Use your own</td><td style="text-align:center">Medium</td><td style="text-align:center">Run everything locally on your own machine</td><td style="text-align:center">GPUs aren’t free, require upfront cost</td><td style="text-align:center">Follow the <a href="https://pytorch.org/get-started/locally/">PyTorch   installation guidelines</a></td></tr><tr><td style="text-align:center">Cloud computing (AWS, GCP, Azure)</td><td style="text-align:center">Medium-Hard</td><td style="text-align:center">Small upfront cost, access to almost infinite compute</td><td style="text-align:center">Can get expensive if running continually, takes some time to setup right</td><td style="text-align:center">Follow the <a href="https://pytorch.org/get-started/cloud-partners/">PyTorch installation guidelines</a></td></tr></tbody></table></div><p>检查本机GPU</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\windows11&gt;nvidia-smi</span><br><span class="line">Fri Oct 11 20:12:11 2024</span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 551.68                 Driver Version: 551.68         CUDA Version: 12.4     |</span><br><span class="line">|-----------------------------------------+------------------------+----------------------+</span><br><span class="line">| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                        |               MIG M. |</span><br><span class="line">|=========================================+========================+======================|</span><br><span class="line">|   0  NVIDIA GeForce RTX 3090      WDDM  |   00000000:73:00.0  On |                  N/A |</span><br><span class="line">| 30%   44C    P8             30W /  350W |    2290MiB /  24576MiB |      3%      Default |</span><br><span class="line">|                                         |                        |                  N/A |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br></pre></td></tr></table></figure><h3 id="让-PyTorch-在-GPU-上运行"><a href="#让-PyTorch-在-GPU-上运行" class="headerlink" title="让 PyTorch 在 GPU 上运行"></a>让 PyTorch 在 GPU 上运行</h3><h4 id="检查cuda包"><a href="#检查cuda包" class="headerlink" title="检查cuda包"></a>检查cuda包</h4><p>一旦您准备好访问 GPU，下一步就是使用 PyTorch 来存储数据（张量）和计算数据（对张量执行操作）。</p><p>为此，您可以使用该<code>torch.cuda</code>包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check for GPU</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Fri Oct 11 12:55:41 2024       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |</span><br><span class="line">| N/A   37C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>如果上述输出为True，则 PyTorch 可以看到并使用 GPU；如果输出False，则它看不到 GPU，在这种情况下，您必须返回安装步骤。</p><h4 id="设置device"><a href="#设置device" class="headerlink" title="设置device"></a>设置device</h4><p>让我们创建一个device变量来存储可用的设备类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set device type</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">device</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;cuda&#x27;</span><br></pre></td></tr></table></figure><p>如果输出上述内容，”cuda”则意味着我们可以将所有 PyTorch 代码设置为使用可用的 CUDA 设备（GPU），如果输出”cpu”，则我们的 PyTorch 代码将坚持使用 CPU。</p><blockquote><p>在 PyTorch 中，最佳做法是编写与设备无关的代码。这意味着代码将在 CPU（始终可用）或 GPU（如果可用）上运行。</p></blockquote><p>best-practices：<a href="https://pytorch.org/docs/main/notes/cuda.html">PyTroch CUDA semantics</a></p><h4 id="多个GPU计算"><a href="#多个GPU计算" class="headerlink" title="多个GPU计算"></a>多个GPU计算</h4><p>如果您想要进行更快的计算，则可以使用 GPU，但如果您想进行更快的计算，则可以使用多个 GPU。</p><p>您可以计算 PyTorch 可以使用的 GPU 数量<code>torch.cuda.device_count()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Count number of devices</span></span><br><span class="line">torch.cuda.device_count()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><p>了解 PyTorch 可以访问的 GPU 数量很有帮助，以防您想在一个 GPU 上运行特定进程，而在另一个 GPU 上运行另一个进程（PyTorch 还具有允许您在所有GPU 上运行进程的功能）。</p><h3 id="将张量（和模型）放在-GPU-上"><a href="#将张量（和模型）放在-GPU-上" class="headerlink" title="将张量（和模型）放在 GPU 上"></a>将张量（和模型）放在 GPU 上</h3><p>您可以通过对张量（和模型，我们稍后会看到）调用 <code>to(device)</code> 来将其放置在特定设备上。其中 <code>device</code> 是您希望张量（或模型）转到的目标设备。</p><p>为什么要这么做？</p><p>GPU 提供的数值计算速度比 CPU 快得多，并且如果 GPU 不可用，由于我们的设备无关代码（参见上文），它将在 CPU 上运行。</p><p>使用 <code>to(device)</code> 将张量放在 GPU 上（例如 <code>some_tensor.to(device)</code>）将返回该张量的副本，例如，相同的张量将存在于 CPU 和 GPU 上。要覆盖张量，请重新分配它们：<code>some_tensor = some_tensor.to(device)</code></p><h4 id="创建一个张量并将其放在-GPU-上"><a href="#创建一个张量并将其放在-GPU-上" class="headerlink" title="创建一个张量并将其放在 GPU 上"></a>创建一个张量并将其放在 GPU 上</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create tensor (default on CPU)</span></span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensor not on GPU</span></span><br><span class="line"><span class="built_in">print</span>(tensor, tensor.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move tensor to GPU (if available)</span></span><br><span class="line">tensor_on_gpu = tensor.to(device)</span><br><span class="line">tensor_on_gpu</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3]) cpu</span><br><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><p>二个张量有device=’cuda:0’，这意味着它存储在第 0 个可用的 GPU 上（GPU 的索引为 0，如果有两个可用的 GPU，则它们分别为’cuda:0’和’cuda:1’，最多为’cuda:n’）。</p><h3 id="将张量移回-CPU"><a href="#将张量移回-CPU" class="headerlink" title="将张量移回 CPU"></a>将张量移回 CPU</h3><p>使用 NumPy 与张量进行交互（NumPy 不利用 GPU），您需要执行此操作。</p><p><code>torch.Tensor.numpy()</code>使用方法<code>tensor_on_gpu</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If tensor is on GPU, can&#x27;t transform it to NumPy (this will error)</span></span><br><span class="line">tensor_on_gpu.numpy()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: can&#x27;t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.</span><br></pre></td></tr></table></figure><p>相反，为了将张量返回到 CPU 并可供 NumPy 使用，使用<code>Tensor.cpu()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instead, copy the tensor back to cpu</span></span><br><span class="line">tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()</span><br><span class="line">tensor_back_on_cpu</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 2, 3])</span><br></pre></td></tr></table></figure><p>上面返回了 CPU 内存中 GPU 张量的副本，原始张量仍然在 GPU 上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor_on_gpu</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><h3 id="GPU上的随机种子"><a href="#GPU上的随机种子" class="headerlink" title="GPU上的随机种子"></a>GPU上的随机种子</h3><p>在cpu上的随机种子，使用<code>torch.manual_seed()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)  <span class="comment"># 设置CPU种子</span></span><br><span class="line">torch.cuda.manual_seed(<span class="number">42</span>)  <span class="comment"># 设置当前GPU的种子</span></span><br><span class="line">torch.cuda.manual_seed_all(<span class="number">42</span>)  <span class="comment"># 如果使用多个GPU，则为所有GPU设置种子</span></span><br><span class="line"></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span><span class="comment">#确保卷积操作的确定性，强制使用确定性的算法。</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span><span class="comment">#禁用动态算法选择，这样算法的选择不会因为输入数据的变化而改变。</span></span><br></pre></td></tr></table></figure><p>通过上述设置，可以确保在 CPU 和 GPU 上运行的 PyTorch 代码具有可重复性。</p><h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><p><code>pytorch-deep-learning/extras/exercises</code></p><p>…\pytorch-deep-learning\extras\exercises\00_pytorch_fundamentals_exercises.ipynb<br>…\pytorch-deep-learning\extras\solutions\00_pytorch_fundamentals_exercise_solutions.ipynb</p><h2 id="配置pytroch环境"><a href="#配置pytroch环境" class="headerlink" title="配置pytroch环境"></a>配置pytroch环境</h2><p><a href="https://pytorch.org/get-started/locally/#windows-anaconda">PyTorch Get Started</a></p><p>pytorch 2.4.1<br>windows<br>conda<br>python<br>CUDA12.4</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pytorch241 python=3.8</span><br><span class="line"></span><br><span class="line">conda activate pytorch241</span><br><span class="line"></span><br><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia</span><br><span class="line"></span><br><span class="line">conda env remove -n pytorch241</span><br><span class="line">conda clean -i</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">PyTorch-26H-1</summary>
    
    
    
    <category term="PyTorch" scheme="http://hibiscidai.com/categories/PyTorch/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="PyTorch" scheme="http://hibiscidai.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Linux登录提示语</title>
    <link href="http://hibiscidai.com/2024/08/13/Linux%E7%99%BB%E5%BD%95%E6%8F%90%E7%A4%BA%E8%AF%AD/"/>
    <id>http://hibiscidai.com/2024/08/13/Linux%E7%99%BB%E5%BD%95%E6%8F%90%E7%A4%BA%E8%AF%AD/</id>
    <published>2024-08-13T02:00:00.000Z</published>
    <updated>2024-08-13T06:52:11.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/13/Linux%E7%99%BB%E5%BD%95%E6%8F%90%E7%A4%BA%E8%AF%AD/Linux%E7%99%BB%E5%BD%95%E6%8F%90%E7%A4%BA%E8%AF%AD.png" class="" title="Linux登录提示语"><p>Linux登录提示语</p><span id="more"></span><h1 id="Linux登录提示语"><a href="#Linux登录提示语" class="headerlink" title="Linux登录提示语"></a>Linux登录提示语</h1><p><code>/etc/issue</code> 本地（虚拟控制台KVM等）登录前提示语，支持转义字符</p><p><code>/etc/issue.net</code> 远程（telnet，ssh）登录前提示语，不支持转义字符</p><p><code>/etc/motd</code> 登录后提示语</p><p><code>/etc/issue</code> 和 <code>/etc/issue.net</code> ：这2个文件是你在登录之前显示的，区别一个负责本地登录前显示，一个负责网络登录前显示。也即 <code>/etc/issue</code> 是显示在控制台登录前（非图形界面），而 <code>/etc/issue.net</code> 是显示在 Telnet (SSH默认不开启）远程登录前，另外 <code>/etc/issue.net</code> 不支持转义字符。</p><p><code>/etc/motd</code> ：这个文件是在你登录之后显示的，不管你是 TTY 还是 PTS 登录，也不管是  Telnet 或 SSH 都显示这个文件里面的信息。</p><p>配置更改后，需要重启SSH服务。</p><ul><li><code>\d</code> : 插入目前日期。</li><li><code>\t</code> : 插入当前时间</li><li><code>\s</code> : 插入系统名称，操作系统名称</li><li><code>\r</code> : 插入操作系统版本号，例如1.1.9.</li><li><code>\v</code> : 插入操作系统的版本</li><li><code>\m</code> : 展示设备的架构标记符，例如i486</li><li><code>\n</code> : 插入设备主机名</li><li><code>\o</code> : 插入设备域名</li><li><code>\l</code> : 插入当前tty终端名称</li><li><code>\u</code> : 插入当前登录用户数</li><li><code>\U</code> : 插入当前登录用户数，以 “1 user” or “ users” 形式</li></ul><h1 id="Ubuntu-系统登录提示"><a href="#Ubuntu-系统登录提示" class="headerlink" title="Ubuntu 系统登录提示"></a>Ubuntu 系统登录提示</h1><p>一般的静态MOTD在<code>/etc/motd</code>中存放，而动态的MOTD在<code>/run/motd.dynamic</code>中存放。</p><p><code>update-motd.d</code>中的东西会存放在<code>motd.dynamic</code>中，然后<code>motd.dynamic</code>再通过<code>pam_motd</code>执行。</p><p>这里的<code>pam_motd</code>其实就是<code>pam_motd.so</code>用于执行<code>update-motd.d</code>文件夹中可执行文件的。</p><p>文件夹：<code>/etc/update-motd.d</code></p><p><code>00-header</code><br><code>10-help-text</code><br><code>50-landscape-sysinfo</code><br><code>50-motd-news</code><br><code>85-fvupd</code><br><code>90-updates-available</code><br><code>91-contract-ua-esm-status</code><br><code>91-release-upgrade</code><br><code>92-nattended-upgrades</code><br><code>95-hwe-eol</code><br><code>97-overlayroot</code><br><code>98-fsck-at-reboot</code><br><code>98-reboot-required</code></p><p>这几个脚本文件，前面的 数字决定了执行顺序，数字越小执行顺序越靠前。</p><p>修改文件内容即可</p><h1 id="命令行美化"><a href="#命令行美化" class="headerlink" title="命令行美化"></a>命令行美化</h1><ul><li><p>figlet: 将英文字母转换为ASCII字符画</p></li><li><p>jp2a: 将图片转换为ASCII字符画</p></li><li><p>asciitable 输出好看的ASCII表格</p></li><li><p><a href="https://motd.bakaya.ro/?ref=xavier.wang">图片转Linux Shell彩色文本</a></p></li><li><p><a href="https://rivers.chaitin.cn/tools/figlet">在线figlet</a></p></li></ul><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><p>文件位置：<code>/etc/update-motd.d/xx.txt</code></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  _  _  _        ____   _____     _     ____       _____  _   _  ___  ____         _  _  _ </span><br><span class="line"> | || || |      |  _ \ | ____|   / \   |  _ \     |_   _|| | | ||_ _|/ ___|       | || || |</span><br><span class="line"> | || || |      | |_) ||  _|    / _ \  | | | |      | |  | |_| | | | \___ \       | || || |</span><br><span class="line"> |_||_||_|      |  _ &lt; | |___  / ___ \ | |_| |      | |  |  _  | | |  ___) |      |_||_||_|</span><br><span class="line"> (_)(_)(_)      |_| \_\|_____|/_/   \_\|____/       |_|  |_| |_||___||____/       (_)(_)(_)</span><br><span class="line">                                                                                                                                                                 </span><br><span class="line">                                                                      </span><br><span class="line">！！！    不按要求操作，删除账号    ！！！</span><br><span class="line">！！！    If you do not follow the instructions, your account will be deleted    ！！！</span><br><span class="line"></span><br><span class="line">https://docs.qq.com/doc/xxxx</span><br><span class="line"></span><br><span class="line">  _  _  _        ____   _____     _     ____       _____  _   _  ___  ____         _  _  _ </span><br><span class="line"> | || || |      |  _ \ | ____|   / \   |  _ \     |_   _|| | | ||_ _|/ ___|       | || || |</span><br><span class="line"> | || || |      | |_) ||  _|    / _ \  | | | |      | |  | |_| | | | \___ \       | || || |</span><br><span class="line"> |_||_||_|      |  _ &lt; | |___  / ___ \ | |_| |      | |  |  _  | | |  ___) |      |_||_||_|</span><br><span class="line"> (_)(_)(_)      |_| \_\|_____|/_/   \_\|____/       |_|  |_| |_||___||____/       (_)(_)(_)</span><br><span class="line">                                                                                          </span><br><span class="line">···</span><br><span class="line"></span><br><span class="line">printf &quot;******************************************************************\n&quot;</span><br><span class="line">printf &quot;******************************************************************\n&quot;</span><br><span class="line">printf &quot; * 使用前必读-YOU MUST READ IT: https://docs.qq.com/doc/DRWNzTWNsSlRtY0lV\n&quot;</span><br><span class="line">printf &quot;******************************************************************\n&quot;</span><br><span class="line">printf &quot;******************************************************************\n&quot;</span><br></pre></td></tr></table></figure><p>修改<code>/etc/update-motd.d/92-unattended-upgrades</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/update-motd.d/xxx.txt</span><br></pre></td></tr></table></figure><p>查询效果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run-parts /etc/update-motd.d</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Linux登录提示语</summary>
    
    
    
    <category term="Linux" scheme="http://hibiscidai.com/categories/Linux/"/>
    
    
    <category term="Linux" scheme="http://hibiscidai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>drd-Digital_Rocks_Data</title>
    <link href="http://hibiscidai.com/2024/08/12/drd-Digital_Rocks_Data/"/>
    <id>http://hibiscidai.com/2024/08/12/drd-Digital_Rocks_Data/</id>
    <published>2024-08-12T07:00:00.000Z</published>
    <updated>2024-08-12T07:46:12.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/12/drd-Digital_Rocks_Data/drd-Digital_Rocks_Data.png" class="" title="drd-Digital_Rocks_Data"><p>drd-Digital_Rocks_Data</p><span id="more"></span><h1 id="drd-Digital-Rocks-Data"><a href="#drd-Digital-Rocks-Data" class="headerlink" title="drd-Digital_Rocks_Data"></a>drd-Digital_Rocks_Data</h1><p><a href="https://pypi.org/project/drd/">drd-pypi</a></p><p><a href="https://github.com/lukasmosser/digital_rocks_data">digital_rocks_data_github</a></p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Digital Rock Images are three-dimensional datasets of rocks and other porous media.<br>数字岩石图像是岩石和其他多孔介质的三维数据集。</p><p>These are typically acquired using three-dimensional imaging techniques such as Micro-Computer Tomography (MicroCT).<br>这些图像通常使用三维成像技术（例如微型计算机断层扫描 (MicroCT)）获取。</p><p>They represent a rich dataset that form a basis for characterization of physical processes involving porous media.<br>它们代表了丰富的数据集，为表征涉及多孔介质的物理过程奠定了基础。</p><p>Digital Rock Images are scattered throughout the web on various hosting sites such as the Digital Rocks Portal, Zenodo, or university specific sites. This library aims to make downloading these datasets easy through a python interface so they can be used in automated image processing workflows, reproducible research, or data science and machine learning worfklows.<br>数字岩石图像分散在网络上的各种托管网站上，例如 Digital Rocks Portal、Zenodo 或大学专用网站。该库旨在通过 Python 界面轻松下载这些数据集，以便它们可用于自动图像处理工作流程、可重复研究或数据科学和机器学习工作流程。</p><p>Furthermore, these images are associated with metadata about their spatial dimensions which should be considered when loading these image datasets.<br>The library therefore requires these metadata to be available and creates an xarray DataArray which can keep spatial scale information when loading an image dataset.<br>此外，这些图像与有关其空间维度的元数据相关联，在加载这些图像数据集时应考虑这些元数据。<br>因此，该库要求这些元数据可用，并创建一个 xarray DataArray，它可以在加载图像数据集时保留空间尺度信息。</p><p>Each dataset is linked in this library i.e. no hosting is done by the library itself.<br>每个数据集都链接到此库中，即库本身不进行任何托管。</p><p><a href="https://www.digitalrocksportal.org/">Digital Rocks Portal</a>：<br><a href="https://www.digitalrocksportal.org/projects/317">Eleven Sandstones Dataset</a></p><ul><li>Berea</li><li>Bandera Brown</li><li>Bandera Gray</li><li>Bentheimer</li><li>Berea Sister Gray</li><li>Berea Upper Gray</li><li>Buff Berea</li><li>Castle Gate</li><li>Kirby</li><li>Leopard</li><li>Parker</li></ul><p><a href="https://www.imperial.ac.uk/earth-science/research/research-groups/pore-scale-modelling/micro-ct-images-and-networks/">Imperial College London</a>：</p><ul><li>MicroCT Images of Sandstones and Carbonates 2015</li><li><a href="https://figshare.com/projects/micro-CT_Images_-_2009/2275">icroCT Images of Sandstones and Carbonates 2009</a></li></ul><h1 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h1><ul><li>安装使用</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install drd</span><br></pre></td></tr></table></figure><ul><li><code>plot_eleven_sandstones_dataset_example.py</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Eleven Sandstones Plotting Example</span></span><br><span class="line"><span class="string">=========================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This example shows how we can load an image from the eleven sandstones dataset.</span></span><br><span class="line"><span class="string">这个例子展示了如何从十一块砂岩数据集中加载图像。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> drd.datasets.eleven_sandstones <span class="keyword">import</span> load_eleven_sandstones</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="comment"># Loading the Image</span></span><br><span class="line"><span class="comment"># ------------------------</span></span><br><span class="line"><span class="comment"># 我们将使用名为“load_eleven_sandstones”的实用函数来生成 xarray DataArray</span></span><br><span class="line"><span class="comment"># We will use one of the utility functions called `load_eleven_sandstones` to generate an xarray DataArray </span></span><br><span class="line"><span class="comment"># 它已经包含所有预先配置的空间轴信息和缩放比例。</span></span><br><span class="line"><span class="comment"># which already contains all the spatial axis information and scaling preconfigured.</span></span><br><span class="line"><span class="comment"># 这样，我们将根据空间坐标系对图像数据有一个适当的定义。</span></span><br><span class="line"><span class="comment"># This way we will have a proper definition of the image data in terms of a spatial coordinate system.</span></span><br><span class="line"></span><br><span class="line">img = load_eleven_sandstones(<span class="string">&quot;Berea&quot;</span>, <span class="string">&quot;Berea_2d25um_grayscale.raw&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是“代码块”的结尾（如果使用上述 IDE）。</span></span><br><span class="line"><span class="comment"># This is the end of the &#x27;code block&#x27; (if using an above IDE). All code within</span></span><br><span class="line"><span class="comment"># 此块可轻松一次性执行。</span></span><br><span class="line"><span class="comment"># this block can be easily executed all at once.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="comment"># 使用 xarray 绘制图像数据</span></span><br><span class="line"><span class="comment"># Plotting the Image Data using xarray</span></span><br><span class="line"><span class="comment"># ------------------------</span></span><br><span class="line"><span class="comment"># 我们将使用 xarray 的功能来汇总或选择我们的数据以绘制 z 维度的平均值。</span></span><br><span class="line"><span class="comment"># We will use xarray&#x27;s ability to summarize or select our data to plot an average over the z dimension.</span></span><br><span class="line"></span><br><span class="line">img.mean(dim=<span class="string">&#x27;z&#x27;</span>).plot()</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="comment"># 我们可以清楚地看到我们如何首先从网络下载图像数据，然后计算微型 CT 图像数据集的平均值。</span></span><br><span class="line"><span class="comment"># We can clearly see how we first downloaded the image data from the web and subsequently compute an average over the micro-ct image dataset.</span></span><br></pre></td></tr></table></figure><ul><li><code>plot_icl_sandstones_carbonates_2009_example.py</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Imperial College London Sandstones &amp; Carbonates 2009 Dataset</span></span><br><span class="line"><span class="string">=========================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This example shows how we can load an image from the Imperial College London Sandstones &amp; Carbonates 2009 dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> drd.datasets.icl_sandstones_carbonates_2009 <span class="keyword">import</span> load_icl_sandstones_carbonates_2009</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="comment"># Loading the Image</span></span><br><span class="line"><span class="comment"># ------------------------</span></span><br><span class="line"><span class="comment"># 我们将使用名为“load_icl_sandstones_carbonates_2009”的实用函数来生成 xarray DataArray</span></span><br><span class="line"><span class="comment"># We will use one of the utility functions called `load_icl_sandstones_carbonates_2009` to generate an xarray DataArray </span></span><br><span class="line"><span class="comment"># 它已经包含所有预先配置的空间轴信息和缩放比例。</span></span><br><span class="line"><span class="comment"># which already contains all the spatial axis information and scaling preconfigured.</span></span><br><span class="line"><span class="comment"># 这样，我们将根据空间坐标系对图像数据有一个适当的定义。</span></span><br><span class="line"><span class="comment"># This way we will have a proper definition of the image data in terms of a spatial coordinate system.</span></span><br><span class="line"></span><br><span class="line">img = load_icl_sandstones_carbonates_2009(<span class="string">&quot;Berea&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是“代码块”的结尾（如果使用上述 IDE）。</span></span><br><span class="line"><span class="comment"># This is the end of the &#x27;code block&#x27; (if using an above IDE). All code within</span></span><br><span class="line"><span class="comment"># 此块可轻松一次性执行。</span></span><br><span class="line"><span class="comment"># this block can be easily executed all at once.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="comment"># Plotting the Image Data using xarray</span></span><br><span class="line"><span class="comment"># ------------------------</span></span><br><span class="line"><span class="comment"># We will use xarray&#x27;s ability to summarize or select our data to plot an average over the z dimension.</span></span><br><span class="line"></span><br><span class="line">img.mean(dim=<span class="string">&#x27;z&#x27;</span>).plot()</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="comment"># We can clearly see how we first downloaded the image data from the web and subsequently compute an average over the micro-ct image dataset.</span></span><br></pre></td></tr></table></figure><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br></pre></td><td class="code"><pre><span class="line">DATASET_METADATA = &#123;</span><br><span class="line">    <span class="string">&quot;Berea&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;Berea_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223451/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Berea_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223452/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Berea_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223453/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;BanderaBrown&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;BanderaBrown_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223448/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BanderaBrown_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223454/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BanderaBrown_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223455/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;BanderaGray&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;BanderaGray_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223459/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BanderaGray_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223457/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BanderaGray_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223458/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;Bentheimer&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;Bentheimer_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223461/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Bentheimer_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223462/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Bentheimer_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223463/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;BSG&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;BSG_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223464/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BSG_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223465/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BSG_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223466/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;BUG&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;BUG_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223467/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BUG_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223468/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BUG_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223469/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;BB&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;BB_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223470/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BB_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223471/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;BB_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223472/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;CastleGate&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;CastleGate_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223473/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;CastleGate_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223474/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;CastleGate_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223475/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;Kirby&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;Kirby_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223476/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Kirby_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223477/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Kirby_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223478/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;Leopard&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;Leopard_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223479/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Leopard_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223480/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Leopard_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223481/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;Parker&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;Parker_2d25um_grayscale.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223482/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Parker_2d25um_grayscale_filtered.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223483/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>:  <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Parker_2d25um_binary.raw&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.digitalrocksportal.org/projects/317/images/223484/download/&quot;</span>,</span><br><span class="line">            <span class="string">&quot;voxel_length&quot;</span>: [<span class="number">2.25</span>, <span class="number">2.25</span>, <span class="number">2.25</span>],</span><br><span class="line">            <span class="string">&quot;metric_voxel_length_unit&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="string">&quot;width&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;height&quot;</span>: <span class="number">1000</span>, </span><br><span class="line">            <span class="string">&quot;number_of_slices&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">            <span class="string">&quot;byte_order&quot;</span>: <span class="string">&quot;little-endian&quot;</span>,</span><br><span class="line">            <span class="string">&quot;image_type&quot;</span>: np.uint8</span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><p>创建conda环境，通过pip安装drd</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> drd.datasets.eleven_sandstones <span class="keyword">import</span> load_eleven_sandstones</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># xarray DataArray with image data</span></span><br><span class="line">img = load_eleven_sandstones(<span class="string">&quot;Berea&quot;</span>, <span class="string">&quot;Berea_2d25um_grayscale.raw&quot;</span>, <span class="string">&quot;/home/daijin/data/drd_data/&quot;</span>)</span><br><span class="line"><span class="comment">#img = load_eleven_sandstones(&quot;Berea&quot;, &quot;Berea_2d25um_grayscale_filtered.raw&quot;, &quot;/home/daijin/data/drd_data/&quot;)</span></span><br><span class="line"><span class="comment">#img = load_eleven_sandstones(&quot;Berea&quot;, &quot;Berea_2d25um_binary.raw&quot;, &quot;/home/daijin/data/drd_data/&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot average over z dimension</span></span><br><span class="line">img.mean(dim=<span class="string">&#x27;z&#x27;</span>).plot()</span><br><span class="line"><span class="comment">#img.mean(dim=&#x27;x&#x27;).plot()</span></span><br><span class="line"><span class="comment">#img.mean(dim=&#x27;y&#x27;).plot()</span></span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>程序运行会检查文件是否下载，下载完成后进行文件校验，校验完成后进行数据读取。</p><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_grayscale_z.png" class="" title="Berea_2d25um_grayscale_z"><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_grayscale_filtered_z.png" class="" title="Berea_2d25um_grayscale_filtered_z"><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_binary_z.png" class="" title="Berea_2d25um_binary_z"><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_grayscale_x.png" class="" title="Berea_2d25um_grayscale_x"><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_grayscale_filtered_x.png" class="" title="Berea_2d25um_grayscale_filtered_x"><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_binary_x.png" class="" title="Berea_2d25um_binary_x"><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_grayscale_y.png" class="" title="Berea_2d25um_grayscale_y"><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_grayscale_filtered_y.png" class="" title="Berea_2d25um_grayscale_filtered_y"><img src="/2024/08/12/drd-Digital_Rocks_Data/Berea_2d25um_binary_y.png" class="" title="Berea_2d25um_binary_y">]]></content>
    
    
    <summary type="html">drd-Digital_Rocks_Data</summary>
    
    
    
    <category term="岩石物理" scheme="http://hibiscidai.com/categories/%E5%B2%A9%E7%9F%B3%E7%89%A9%E7%90%86/"/>
    
    
    <category term="数字岩心" scheme="http://hibiscidai.com/tags/%E6%95%B0%E5%AD%97%E5%B2%A9%E5%BF%83/"/>
    
    <category term="岩石物理" scheme="http://hibiscidai.com/tags/%E5%B2%A9%E7%9F%B3%E7%89%A9%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>WebDAV-使用指南</title>
    <link href="http://hibiscidai.com/2024/08/12/WebDAV-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <id>http://hibiscidai.com/2024/08/12/WebDAV-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</id>
    <published>2024-08-12T06:00:00.000Z</published>
    <updated>2024-08-12T06:10:29.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/12/WebDAV-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/WebDAV-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97.png" class="" title="WebDAV-使用指南"><p>WebDAV-使用指南</p><span id="more"></span><h1 id="WebDAV-使用指南"><a href="#WebDAV-使用指南" class="headerlink" title="WebDAV-使用指南"></a>WebDAV-使用指南</h1><p><a href="https://kb.synology.cn/zh-cn/DSM/tutorial/How_to_access_files_on_Synology_NAS_with_WebDAV">如何使用 WebDAV 访问 Synology NAS 上的文件？</a></p><p><a href="https://kb.synology.cn/zh-cn/DSM/help/WebDAVServer/webdav_server?version=7">WebDAV Server</a></p><p><a href="https://www.raidrive.com/">RaiDriver</a></p><h1 id="WebDAV是啥"><a href="#WebDAV是啥" class="headerlink" title="WebDAV是啥"></a>WebDAV是啥</h1><p>基于Web的分布式编写和版本控制（英语：Web-based Distributed Authoring and Versioning，缩写：WebDAV）是超文本传输协议（HTTP）的扩展，有利于用户间协同编辑和管理存储在万维网服务器文档。</p><p>WebDAV协议为用户在服务器上创建、更改和移动文档提供了一个框架。WebDAV协议最重要的功能包括作者或修改日期等属性的维护、命名空间管理、集合和覆盖保护。为属性维护所提供的功能包括创建、删除和查询文件信息等；命名空间管理处理在服务器名称空间内复制和移动网页的能力；集合（Collections）处理各种资源的创建、删除和列举；覆盖保护处理与锁定文件相关的问题。WebDAV协议利用TLS、HTTP摘要认证、XML等技术来满足这些需求。</p><h1 id="Synology设置要求"><a href="#Synology设置要求" class="headerlink" title="Synology设置要求"></a>Synology设置要求</h1><p>参看前文连接</p><h1 id="RaiDriver配置"><a href="#RaiDriver配置" class="headerlink" title="RaiDriver配置"></a>RaiDriver配置</h1><p>下载RaiDriver<br>安装RaiDriver</p><img src="/2024/08/12/WebDAV-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/RaiDriver%E9%85%8D%E7%BD%AE-1.png" class="" title="RaiDriver配置-1"><p>此软件只保证在win上通过WebDAV协议进行访问，可以测试连通性。<br>测试成功后可以在本地电脑磁盘上看到映射的文件</p><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>群辉自带内网穿透，http写：<br><a href="https://{内网ip}.{quickconnect}.quickconnect.cn:{端口}/{映射文件夹}">https://{内网ip}.{quickconnect}.quickconnect.cn:{端口}/{映射文件夹}</a></p><p>账户密码可以用一个，但建议创建新用户，防止密码泄密后访问所有文件。</p>]]></content>
    
    
    <summary type="html">WebDAV-使用指南</summary>
    
    
    
    <category term="软件" scheme="http://hibiscidai.com/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    
    <category term="软件" scheme="http://hibiscidai.com/tags/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="WebDAV" scheme="http://hibiscidai.com/tags/WebDAV/"/>
    
  </entry>
  
  <entry>
    <title>Cesium-使用指南</title>
    <link href="http://hibiscidai.com/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <id>http://hibiscidai.com/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</id>
    <published>2024-08-06T08:00:00.000Z</published>
    <updated>2024-08-08T09:22:57.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97.png" class="" title="Cesium-使用指南"><p>Cesium-使用指南</p><span id="more"></span><h1 id="Cesium-使用指南"><a href="#Cesium-使用指南" class="headerlink" title="Cesium-使用指南"></a>Cesium-使用指南</h1><p><a href="https://cesium.com/">Cesium: The Platform for 3D Geospatial</a></p><p><a href="https://github.com/CesiumGS/cesium">Cesium-github</a></p><p><a href="https://cesium.com/learn/cesiumjs-sandcastle/">Cesium-doc</a></p><p><a href="https://sandcastle.cesium.com/">Cesium-沙盒</a></p><p><a href="https://ion.cesium.com/">Cesium-用户控制面板</a></p><p><a href="http://lbs.tianditu.gov.cn/">国家地理信息公共服务平台天地图</a></p><p><a href="http://lbs.tianditu.gov.cn/server/MapService.html">天地图-地图API</a></p><p><a href=""></a></p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>CesiumJS is a JavaScript library for creating 3D globes and 2D maps in a web browser without a plugin. It uses WebGL for hardware-accelerated graphics, and is cross-platform, cross-browser, and tuned for dynamic-data visualization.<br>CesiumJS 是一个 JavaScript 库，用于在 Web 浏览器中创建 3D 地球仪和 2D 地图，无需插件。它使用 WebGL 进行硬件加速图形处理，并且跨平台、跨浏览器，并针对动态数据可视化进行了调整。</p><p>Built on open formats, CesiumJS is designed for robust interoperability and scaling for massive datasets.<br>CesiumJS 基于开放格式构建，旨在实现强大的互操作性和对海量数据集的扩展。</p><h1 id="注册"><a href="#注册" class="headerlink" title="注册"></a>注册</h1><p>登陆cesium官网注册账号，然后前往用户控制面板查看自己的<code>Access Tokens</code>，最好新建一个token，打开全部权限。</p><p>登陆天地图官网进行注册，进入用户平台。创建新的应用，得到应用的key。</p><h1 id="安装CesiumJS"><a href="#安装CesiumJS" class="headerlink" title="安装CesiumJS"></a>安装CesiumJS</h1><h2 id="CDN安装-主要安装方式"><a href="#CDN安装-主要安装方式" class="headerlink" title="CDN安装-主要安装方式"></a>CDN安装-主要安装方式</h2><p>Below is a complete HTML page that will load the required JavaScript and CSS files and initialize the scene at San Francisco. If you don’t have a development environment, you can create a file containing this HTML and view it in a browser.<br>下面是一个完整的 HTML 页面，它将加载所需的 JavaScript 和 CSS 文件并初始化旧金山的场景。如果您没有开发环境，您可以创建一个包含此 HTML 的文件并在浏览器中查看它。</p><p>Just replace your_access_token with your Cesium ion access token.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;utf-8&quot;</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- Include the CesiumJS JavaScript and CSS files --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cesium.com/downloads/cesiumjs/releases/1.120/Build/Cesium/Cesium.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">&quot;https://cesium.com/downloads/cesiumjs/releases/1.120/Build/Cesium/Widgets/widgets.css&quot;</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;cesiumContainer&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;module&quot;</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Your access token can be found at: https://ion.cesium.com/tokens.</span></span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Replace `your_access_token` with your Cesium ion access token.</span></span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">    <span class="title class_">Cesium</span>.<span class="property">Ion</span>.<span class="property">defaultAccessToken</span> = <span class="string">&#x27;your_access_token&#x27;</span>;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Initialize the Cesium Viewer in the HTML element with the `cesiumContainer` ID.</span></span></span><br><span class="line"><span class="language-javascript">    <span class="keyword">const</span> viewer = <span class="keyword">new</span> <span class="title class_">Cesium</span>.<span class="title class_">Viewer</span>(<span class="string">&#x27;cesiumContainer&#x27;</span>, &#123;</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">terrain</span>: <span class="title class_">Cesium</span>.<span class="property">Terrain</span>.<span class="title function_">fromWorldTerrain</span>(),</span></span><br><span class="line"><span class="language-javascript">    &#125;);    </span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Fly the camera to San Francisco at the given longitude, latitude, and height.</span></span></span><br><span class="line"><span class="language-javascript">    viewer.<span class="property">camera</span>.<span class="title function_">flyTo</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">destination</span>: <span class="title class_">Cesium</span>.<span class="property">Cartesian3</span>.<span class="title function_">fromDegrees</span>(-<span class="number">122.4175</span>, <span class="number">37.655</span>, <span class="number">400</span>),</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">orientation</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">        <span class="attr">heading</span>: <span class="title class_">Cesium</span>.<span class="property">Math</span>.<span class="title function_">toRadians</span>(<span class="number">0.0</span>),</span></span><br><span class="line"><span class="language-javascript">        <span class="attr">pitch</span>: <span class="title class_">Cesium</span>.<span class="property">Math</span>.<span class="title function_">toRadians</span>(-<span class="number">15.0</span>),</span></span><br><span class="line"><span class="language-javascript">      &#125;</span></span><br><span class="line"><span class="language-javascript">    &#125;);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Add Cesium OSM Buildings, a global 3D buildings layer.</span></span></span><br><span class="line"><span class="language-javascript">    <span class="keyword">const</span> buildingTileset = <span class="keyword">await</span> <span class="title class_">Cesium</span>.<span class="title function_">createOsmBuildingsAsync</span>();</span></span><br><span class="line"><span class="language-javascript">    viewer.<span class="property">scene</span>.<span class="property">primitives</span>.<span class="title function_">add</span>(buildingTileset);   </span></span><br><span class="line"><span class="language-javascript">  </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/CesiumJSHelloWorld.png" class="" title="CesiumJSHelloWorld"><h2 id="挂载服务器"><a href="#挂载服务器" class="headerlink" title="挂载服务器"></a>挂载服务器</h2><p>在本地打开的html可以预览格式和js，但是调用cesium的地图服务需要采用http传输协议，所以在本地容易出现卡地图加载不出来的情况。</p><p>需要将静态页面部署到有公网ip的服务器上，进行静态页面访问即可。</p><h3 id="宝塔新建静态网页"><a href="#宝塔新建静态网页" class="headerlink" title="宝塔新建静态网页"></a>宝塔新建静态网页</h3><p>默认网页端口是80，如果想创建多个网站，可以使用多端口映射。</p><p>先去ecs控制台打开81（也可以自己定义）端口，然后宝塔界面安全组也打开。</p><p>网站→添加站点<br>域名：xxx.xxx.xxx.xxx:81<br>根目录：www/wwwroot/新建网站文件夹<br>FTP：无<br>数据库：无<br>PHP版本：纯静态<br>网站分类：默认分类</p><p>创建完成之后根目录中index.html即可<br>可以通过<code>xxx.xxx.xxxx.xxx:81</code>访问index<br>根目录中其他网页可以输入<code>xxx.xxx.xxxx.xxx:81/xxx.html</code>访问</p><h2 id="NPM安装-参考安装，本文不采用"><a href="#NPM安装-参考安装，本文不采用" class="headerlink" title="NPM安装-参考安装，本文不采用"></a>NPM安装-参考安装，本文不采用</h2><p>If you’re building your application using a module bundler such as Webpack, Parcel, or Rollup, you can install CesiumJS by running:<br>如果您使用模块捆绑器（例如 Webpack、Parcel 或 Rollup）构建应用程序，则可以通过运行以下命令安装 CesiumJS：</p><p>npm install cesium</p><p>The code below loads the required JavaScript and CSS files.<br>以下代码加载所需的 JavaScript 和 CSS 文件。</p><p>Just replace your_access_token with your Cesium ion access token.<br>只需将 your_access_token 替换为您的 Cesium ion 访问令牌即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">// The URL on your server where CesiumJS<span class="string">&#x27;s static files are hosted.</span></span><br><span class="line"><span class="string">window.CESIUM_BASE_URL = &#x27;</span>/<span class="string">&#x27;;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">import &#123; Cartesian3, createOsmBuildingsAsync, Ion, Math as CesiumMath, Terrain, Viewer &#125; from &#x27;</span>cesium<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">import &quot;cesium/Build/Cesium/Widgets/widgets.css&quot;;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// Your access token can be found at: https://ion.cesium.com/tokens.</span></span><br><span class="line"><span class="string">// Replace `your_access_token` with your Cesium ion access token.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ion.defaultAccessToken = &#x27;</span>your_access_token<span class="string">&#x27;;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// Initialize the Cesium Viewer in the HTML element with the `cesiumContainer` ID.</span></span><br><span class="line"><span class="string">const viewer = new Viewer(&#x27;</span>cesiumContaine<span class="string">r&#x27;, &#123;</span></span><br><span class="line"><span class="string">  terrain: Terrain.fromWorldTerrain(),</span></span><br><span class="line"><span class="string">&#125;);    </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// Fly the camera to San Francisco at the given longitude, latitude, and height.</span></span><br><span class="line"><span class="string">viewer.camera.flyTo(&#123;</span></span><br><span class="line"><span class="string">  destination: Cartesian3.fromDegrees(-122.4175, 37.655, 400),</span></span><br><span class="line"><span class="string">  orientation: &#123;</span></span><br><span class="line"><span class="string">    heading: CesiumMath.toRadians(0.0),</span></span><br><span class="line"><span class="string">    pitch: CesiumMath.toRadians(-15.0),</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">&#125;);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// Add Cesium OSM Buildings, a global 3D buildings layer.</span></span><br><span class="line"><span class="string">const buildingTileset = await createOsmBuildingsAsync();</span></span><br><span class="line"><span class="string">viewer.scene.primitives.add(buildingTileset);   </span></span><br></pre></td></tr></table></figure><h3 id="Configuring-CESIUM-BASE-URL"><a href="#Configuring-CESIUM-BASE-URL" class="headerlink" title="Configuring CESIUM_BASE_URL"></a>Configuring CESIUM_BASE_URL</h3><p>CesiumJS requires a few static files to be hosted on your server, like web workers and SVG icons. Configure your module bundler to copy the following four directories and serve them as static files:<br>CesiumJS 需要在您的服务器上托管一些静态文件，例如 Web Worker 和 SVG 图标。配置您的模块捆绑器以复制以下四个目录并将它们作为静态文件提供：</p><ul><li><code>node_modules/cesium/Build/Cesium/Workers</code></li><li><code>node_modules/cesium/Build/Cesium/ThirdParty</code></li><li><code>node_modules/cesium/Build/Cesium/Assets</code></li><li><code>node_modules/cesium/Build/Cesium/Widgets</code></li></ul><p>The <code>window.CESIUM_BASE_URL</code> global variable must be set before CesiumJS is imported. It must point to the URL where those four directories are served.</p><p>在导入 CesiumJS 之前必须设置 <code>window.CESIUM_BASE_URL</code> 全局变量。它必须指向提供这四个目录的 URL。</p><p>For example, if the image at <code>Assets/Images/cesium_credit.png</code> is served with a <code>static/Cesium/</code> prefix under <a href="http://localhost:8080/static/Cesium/Assets/Images/cesium_credit.png">http://localhost:8080/static/Cesium/Assets/Images/cesium_credit.png</a>, then you would set the base URL as follows:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">window.CESIUM_BASE_URL = &#x27;/static/Cesium/&#x27;;</span><br></pre></td></tr></table></figure><p>See the Cesium Webpack Example for a complete Webpack config.<br>请参阅 Cesium Webpack 示例以了解完整的 Webpack 配置。</p><h1 id="地图"><a href="#地图" class="headerlink" title="地图"></a>地图</h1><h2 id="Cesium平台提供的地图"><a href="#Cesium平台提供的地图" class="headerlink" title="Cesium平台提供的地图"></a>Cesium平台提供的地图</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;utf-8&quot;</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Include the CesiumJS JavaScript and CSS files --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cesium.com/downloads/cesiumjs/releases/1.120/Build/Cesium/Cesium.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">&quot;https://cesium.com/downloads/cesiumjs/releases/1.120/Build/Cesium/Widgets/widgets.css&quot;</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;cesiumContainer&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">            <span class="comment">// Natural Earth II with Shaded Relief, Water, and Drainages from http://www.naturalearthdata.com</span></span></span><br><span class="line"><span class="language-javascript">            <span class="keyword">const</span> viewer = <span class="keyword">new</span> <span class="title class_">Cesium</span>.<span class="title class_">Viewer</span>(<span class="string">&quot;cesiumContainer&quot;</span>, &#123;</span></span><br><span class="line"><span class="language-javascript"><span class="attr">baseLayer</span>: <span class="title class_">Cesium</span>.<span class="property">ImageryLayer</span>.<span class="title function_">fromProviderAsync</span>(</span></span><br><span class="line"><span class="language-javascript"><span class="title class_">Cesium</span>.<span class="property">IonImageryProvider</span>.<span class="title function_">fromAssetId</span>(<span class="number">3813</span>)</span></span><br><span class="line"><span class="language-javascript">),</span></span><br><span class="line"><span class="language-javascript">            &#125;);</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;csiumContain&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>添加个人token会导致无法调用地图。</p></blockquote><p>其中<code>AssetId</code>为官方提供的id地址</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="keyword">const</span> viewer = <span class="keyword">new</span> <span class="title class_">Cesium</span>.<span class="title class_">Viewer</span>(<span class="string">&#x27;cesiumContainer&#x27;</span>, </span></span><br><span class="line"><span class="language-javascript">    &#123;</span></span><br><span class="line"><span class="language-javascript"><span class="attr">baseLayer</span>: <span class="title class_">Cesium</span>.<span class="property">ImageryLayer</span>.<span class="title function_">fromProviderAsync</span>(</span></span><br><span class="line"><span class="language-javascript"><span class="title class_">Cesium</span>.<span class="property">IonImageryProvider</span>.<span class="title function_">fromAssetId</span>(<span class="number">3</span>)</span></span><br><span class="line"><span class="language-javascript">),</span></span><br><span class="line"><span class="language-javascript">    &#125;</span></span><br><span class="line"><span class="language-javascript">&#125;);</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在cesium-用户控制台中<code>My Assets</code>可以找到对应的id</p><div class="table-container"><table><thead><tr><th style="text-align:center">ID</th><th style="text-align:center">Name</th><th style="text-align:center">Type</th><th style="text-align:center">Data added</th><th style="text-align:center">Size</th></tr></thead><tbody><tr><td style="text-align:center">2275207</td><td style="text-align:center">Google Photorealistic 3D Tiles</td><td style="text-align:center">3D Tiles</td><td style="text-align:center">2023-9-13</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">96188</td><td style="text-align:center">Cesium OSM Buildings</td><td style="text-align:center">3D Tiles</td><td style="text-align:center">2020-5-1</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">Bing Maps Road</td><td style="text-align:center">Imagery</td><td style="text-align:center">2016-10-27</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">Bing Maps Aerial with Labels</td><td style="text-align:center">Imagery</td><td style="text-align:center">2016-10-27</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">Bing Maps Aerial</td><td style="text-align:center">Imagery</td><td style="text-align:center">2016-10-27</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">Cesium World Terrain</td><td style="text-align:center">Terrain</td><td style="text-align:center">2016-10-18</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">3831</td><td style="text-align:center">Natural Earth II</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr></tbody></table></div><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/Google%20Photorealistic%203D%20Tiles.png" class="" alt="Google Photorealistic 3D Tiles"><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/Cesium%20OSM%20Buildings.png" class="" alt="Cesium OSM Buildings"><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/Bing%20Maps%20Road.png" class="" alt="Bing Maps Road"><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/Bing%20Maps%20Aerial%20with%20Labels.png" class="" alt="Bing Maps Aerial with Labels"><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/Bing%20Maps%20Aerial.png" class="" alt="Bing Maps Aerial"><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/Cesium%20World%20Terrain.png" class="" alt="Cesium World Terrain"><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/Natural%20Earth%20II.png" class="" alt="Natural Earth II"><h2 id="天地图提供的地图"><a href="#天地图提供的地图" class="headerlink" title="天地图提供的地图"></a>天地图提供的地图</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;utf-8&quot;</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Include the CesiumJS JavaScript and CSS files --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cesium.com/downloads/cesiumjs/releases/1.120/Build/Cesium/Cesium.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">&quot;https://cesium.com/downloads/cesiumjs/releases/1.120/Build/Cesium/Widgets/widgets.css&quot;</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;cesiumContainer&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="comment">// 这个 tk 只能在本域名下使用</span></span></span><br><span class="line"><span class="language-javascript"><span class="keyword">var</span> token = <span class="string">&#x27;xxx&#x27;</span>;</span></span><br><span class="line"><span class="language-javascript"><span class="comment">// 服务域名</span></span></span><br><span class="line"><span class="language-javascript"><span class="comment">//var tdtUrl = &#x27;https://t&#123;s&#125;.tianditu.gov.cn/&#x27;;</span></span></span><br><span class="line"><span class="language-javascript"><span class="comment">// 服务负载子域</span></span></span><br><span class="line"><span class="language-javascript"><span class="keyword">var</span> subdomains=[<span class="string">&#x27;0&#x27;</span>,<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;2&#x27;</span>,<span class="string">&#x27;3&#x27;</span>,<span class="string">&#x27;4&#x27;</span>,<span class="string">&#x27;5&#x27;</span>,<span class="string">&#x27;6&#x27;</span>,<span class="string">&#x27;7&#x27;</span>];</span></span><br><span class="line"><span class="language-javascript"><span class="keyword">var</span> viewer = <span class="keyword">new</span> <span class="title class_">Cesium</span>.<span class="title class_">Viewer</span>(<span class="string">&#x27;cesiumContainer&#x27;</span>,&#123;</span></span><br><span class="line"><span class="language-javascript"><span class="attr">shouldAnimate</span>: <span class="literal">true</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">selectionIndicator</span>:<span class="literal">true</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">animation</span>:<span class="literal">true</span>,       <span class="comment">//动画</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">homeButton</span>:<span class="literal">true</span>,       <span class="comment">//home键</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">geocoder</span>:<span class="literal">true</span>,         <span class="comment">//地址编码</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">baseLayerPicker</span>:<span class="literal">true</span>, <span class="comment">//图层选择控件</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">timeline</span>:<span class="literal">true</span>,        <span class="comment">//时间轴</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">fullscreenButton</span>:<span class="literal">true</span>, <span class="comment">//全屏显示</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">infoBox</span>:<span class="literal">true</span>,         <span class="comment">//点击要素之后浮窗</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">sceneModePicker</span>:<span class="literal">true</span>,  <span class="comment">//投影方式  三维/二维</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">navigationInstructionsInitiallyVisible</span>:<span class="literal">true</span>, <span class="comment">//导航指令</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">navigationHelpButton</span>:<span class="literal">true</span>,     <span class="comment">//帮助信息</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">selectionIndicator</span>:<span class="literal">true</span>, <span class="comment">// 选择</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">imageryProvider</span>: <span class="keyword">new</span> <span class="variable language_">window</span>.<span class="property">Cesium</span>.<span class="title class_">WebMapTileServiceImageryProvider</span>(&#123;</span></span><br><span class="line"><span class="language-javascript"><span class="comment">//影像底图,</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">url</span>: <span class="string">&quot;http://t0.tianditu.gov.cn/ter_c/wmts?tk=&quot;</span>+token,<span class="comment">//天地图的地图晕眩-经纬度</span></span></span><br><span class="line"><span class="language-javascript"><span class="comment">//url: &quot;http://t0.tianditu.gov.cn/ter_w/wmts?tk=&quot;+token,//天地图的地图晕眩-球面墨卡托</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">subdomains</span>: subdomains,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">layer</span>: <span class="string">&quot;tdtImgLayer&quot;</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">style</span>: <span class="string">&quot;default&quot;</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">format</span>: <span class="string">&quot;image/jpeg&quot;</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">tileMatrixSetID</span>: <span class="string">&quot;GoogleMapsCompatible&quot;</span>,<span class="comment">//使用谷歌的瓦片切片方式</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">show</span>: <span class="literal">true</span></span></span><br><span class="line"><span class="language-javascript">&#125;)</span></span><br><span class="line"><span class="language-javascript">&#125;);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">viewer.<span class="property">imageryLayers</span>.<span class="title function_">addImageryProvider</span>(<span class="keyword">new</span> <span class="variable language_">window</span>.<span class="property">Cesium</span>.<span class="title class_">WebMapTileServiceImageryProvider</span>(&#123;</span></span><br><span class="line"><span class="language-javascript"><span class="comment">//影像注记</span></span></span><br><span class="line"><span class="language-javascript"><span class="comment">//url: &quot;http://t0.tianditu.gov.cn/cta_c/wmts?tk=&quot;+token,//天地图的地形标注-经纬度</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">url</span>: <span class="string">&quot;http://t0.tianditu.gov.cn/cta_w/wmts?tk=&quot;</span>+token,<span class="comment">//天地图的地形标注-球面墨卡托</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">subdomains</span>: subdomains,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">layer</span>: <span class="string">&quot;tdtCiaLayer&quot;</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">style</span>: <span class="string">&quot;default&quot;</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">format</span>: <span class="string">&quot;image/jpeg&quot;</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">tileMatrixSetID</span>: <span class="string">&quot;GoogleMapsCompatible&quot;</span>,</span></span><br><span class="line"><span class="language-javascript"><span class="attr">show</span>: <span class="literal">true</span></span></span><br><span class="line"><span class="language-javascript">&#125;));</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">viewer.<span class="property">camera</span>.<span class="title function_">setView</span>(&#123;</span></span><br><span class="line"><span class="language-javascript"><span class="comment">// fromDegrees()方法，将经纬度和高程转换为世界坐标</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">destination</span>:<span class="title class_">Cesium</span>.<span class="property">Cartesian3</span>.<span class="title function_">fromDegrees</span>(-<span class="number">121.9068641</span>,<span class="number">56.20149076</span>,<span class="number">20000000</span>),</span></span><br><span class="line"><span class="language-javascript"><span class="attr">orientation</span>:&#123;</span></span><br><span class="line"><span class="language-javascript"><span class="comment">// 指向</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">heading</span>:<span class="title class_">Cesium</span>.<span class="property">Math</span>.<span class="title function_">toRadians</span>(<span class="number">270</span>,<span class="number">0</span>),</span></span><br><span class="line"><span class="language-javascript"><span class="comment">// 视角</span></span></span><br><span class="line"><span class="language-javascript"><span class="attr">pitch</span>:<span class="title class_">Cesium</span>.<span class="property">Math</span>.<span class="title function_">toRadians</span>(-<span class="number">90</span>),</span></span><br><span class="line"><span class="language-javascript"><span class="attr">roll</span>:<span class="number">0.0</span></span></span><br><span class="line"><span class="language-javascript">&#125;</span></span><br><span class="line"><span class="language-javascript">&#125;);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">viewer.<span class="property">scene</span>.<span class="property">mode</span> = <span class="title class_">Cesium</span>.<span class="property">SceneMode</span>.<span class="property">SCENE2D</span>;<span class="comment">// 2D 模式</span></span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>矢量底图</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E7%9F%A2%E9%87%8F%E5%BA%95%E5%9B%BE.png" class="" title="天地图_矢量底图"><p>经纬度投影：<a href="http://t0.tianditu.gov.cn/vec_c/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/vec_c/wmts?tk=您的密钥</a></p><p>球面墨卡托投影：<a href="http://t0.tianditu.gov.cn/vec_w/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/vec_w/wmts?tk=您的密钥</a></p><ul><li>矢量注记</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E7%9F%A2%E9%87%8F%E6%B3%A8%E8%AE%B0.png" class="" title="天地图_矢量注记"><p>经纬度投影：<a href="http://t0.tianditu.gov.cn/cva_c/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/cva_c/wmts?tk=您的密钥</a></p><p>球面墨卡托投影：<a href="http://t0.tianditu.gov.cn/cva_w/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/cva_w/wmts?tk=您的密钥</a></p><ul><li>影像底图</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E5%BD%B1%E5%83%8F%E5%BA%95%E5%9B%BE.png" class="" title="天地图_影像底图"><p>经纬度投影：<a href="http://t0.tianditu.gov.cn/img_c/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/img_c/wmts?tk=您的密钥</a></p><p>球面墨卡托投影：<a href="http://t0.tianditu.gov.cn/img_w/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/img_w/wmts?tk=您的密钥</a></p><ul><li>影像注记</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E5%BD%B1%E5%83%8F%E6%B3%A8%E8%AE%B0.png" class="" title="天地图_影像注记"><p>经纬度投影：<a href="http://t0.tianditu.gov.cn/cia_c/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/cia_c/wmts?tk=您的密钥</a></p><p>球面墨卡托投影：<a href="http://t0.tianditu.gov.cn/cia_w/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/cia_w/wmts?tk=您的密钥</a></p><ul><li>地形晕眩</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E5%9C%B0%E5%BD%A2%E6%99%95%E7%9C%A9.png" class="" title="天地图_地形晕眩"><p>经纬度投影：<a href="http://t0.tianditu.gov.cn/ter_c/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/ter_c/wmts?tk=您的密钥</a></p><p>球面墨卡托投影：<a href="http://t0.tianditu.gov.cn/ter_w/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/ter_w/wmts?tk=您的密钥</a></p><ul><li>地形注记</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E5%9C%B0%E5%BD%A2%E6%B3%A8%E8%AE%B0.png" class="" title="天地图_地形注记"><p>经纬度投影：<a href="http://t0.tianditu.gov.cn/cta_c/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/cta_c/wmts?tk=您的密钥</a></p><p>球面墨卡托投影：<a href="http://t0.tianditu.gov.cn/cta_w/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/cta_w/wmts?tk=您的密钥</a></p><ul><li>全球境界</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E5%85%A8%E7%90%83%E5%A2%83%E7%95%8C.png" class="" title="天地图_全球境界"><p>经纬度投影：<a href="http://t0.tianditu.gov.cn/ibo_c/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/ibo_c/wmts?tk=您的密钥</a></p><p>球面墨卡托投影：<a href="http://t0.tianditu.gov.cn/ibo_w/wmts?tk=您的密钥">http://t0.tianditu.gov.cn/ibo_w/wmts?tk=您的密钥</a></p><ul><li>三维地名</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E4%B8%89%E7%BB%B4%E5%9C%B0%E5%90%8D.png" class="" title="天地图_三维地名"><p>调用说明：<a href="http://lbs.tianditu.gov.cn/docs/#/sanwei/">http://lbs.tianditu.gov.cn/docs/#/sanwei/</a></p><p>cesuim扩展包：<a href="http://lbs.tianditu.gov.cn/docs/#/sanwei/">http://lbs.tianditu.gov.cn/docs/#/sanwei/</a></p><p>服务地址：https://[ t0-t7 ].tianditu.gov.cn/mapservice/GetTiles?tk=您的密钥</p><ul><li>三维地形</li></ul><img src="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E5%A4%A9%E5%9C%B0%E5%9B%BE_%E4%B8%89%E7%BB%B4%E5%9C%B0%E5%BD%A2.jpg" class="" title="天地图_三维地形"><p>调用说明：<a href="http://lbs.tianditu.gov.cn/docs/#/sanwei/">http://lbs.tianditu.gov.cn/docs/#/sanwei/</a></p><p>cesuim扩展包：<a href="http://lbs.tianditu.gov.cn/docs/#/sanwei/">http://lbs.tianditu.gov.cn/docs/#/sanwei/</a></p><p>服务地址：https://[ t0-t7 ].tianditu.gov.cn/mapservice/swdx?tk=您的密钥</p><ul><li>备注</li></ul><p>天地图地图服务二级域名包括t0-t7，您可以随机选择使用，如<a href="http://t2.tianditu.gov.cn/vec_c/wmts?tk=您的密钥">http://t2.tianditu.gov.cn/vec_c/wmts?tk=您的密钥</a></p><p>元数据查询：<br><a href="http://t0.tianditu.gov.cn/img_w/wmts?request=GetCapabilities&amp;service=wmts">http://t0.tianditu.gov.cn/img_w/wmts?request=GetCapabilities&amp;service=wmts</a></p><p>地图瓦片获取：<br><a href="http://t0.tianditu.gov.cn/img_w/wmts?SERVICE=WMTS&amp;REQUEST=GetTile&amp;VERSION=1.0.0&amp;LAYER=img&amp;STYLE=default&amp;TILEMATRIXSET=w&amp;FORMAT=tiles&amp;TILEMATRIX={z}&amp;TILEROW={y}&amp;TILECOL={x}&amp;tk=您的密钥">http://t0.tianditu.gov.cn/img_w/wmts?SERVICE=WMTS&amp;REQUEST=GetTile&amp;VERSION=1.0.0&amp;LAYER=img&amp;STYLE=default&amp;TILEMATRIXSET=w&amp;FORMAT=tiles&amp;TILEMATRIX={z}&amp;TILEROW={y}&amp;TILECOL={x}&amp;tk=您的密钥</a></p><h1 id="画点"><a href="#画点" class="headerlink" title="画点"></a>画点</h1><h2 id="月球点案例"><a href="#月球点案例" class="headerlink" title="月球点案例"></a>月球点案例</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="keyword">const</span> pointsOfInterest = [</span></span><br><span class="line"><span class="language-javascript">  &#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">text</span>: <span class="string">&quot;Apollo 11&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">latitude</span>: <span class="number">0.67416</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">longitude</span>: <span class="number">23.47315</span>,</span></span><br><span class="line"><span class="language-javascript">  &#125;,</span></span><br><span class="line"><span class="language-javascript">  &#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">text</span>: <span class="string">&quot;Apollo 14&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">latitude</span>: -<span class="number">3.64417</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">longitude</span>: <span class="number">342.52135</span>,</span></span><br><span class="line"><span class="language-javascript">  &#125;,</span></span><br><span class="line"><span class="language-javascript">  &#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">text</span>: <span class="string">&quot;Apollo 15&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">latitude</span>: <span class="number">26.13341</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">longitude</span>: <span class="number">3.6285</span>,</span></span><br><span class="line"><span class="language-javascript">  &#125;,</span></span><br><span class="line"><span class="language-javascript">  &#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">text</span>: <span class="string">&quot;Lunokhod 1&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">latitude</span>: <span class="number">38.2378</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">longitude</span>: -<span class="number">35.0017</span>,</span></span><br><span class="line"><span class="language-javascript">  &#125;,</span></span><br><span class="line"><span class="language-javascript">  &#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">text</span>: <span class="string">&quot;Lunokhod 2&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">latitude</span>: <span class="number">25.83232</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">longitude</span>: <span class="number">30.92215</span>,</span></span><br><span class="line"><span class="language-javascript">  &#125;,</span></span><br><span class="line"><span class="language-javascript">];</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="comment">// Natural Earth II with Shaded Relief, Water, and Drainages from http://www.naturalearthdata.com</span></span></span><br><span class="line"><span class="language-javascript"><span class="keyword">const</span> viewer = <span class="keyword">new</span> <span class="title class_">Cesium</span>.<span class="title class_">Viewer</span>(<span class="string">&quot;cesiumContainer&quot;</span>, &#123;</span></span><br><span class="line"><span class="language-javascript">  <span class="attr">baseLayer</span>: <span class="title class_">Cesium</span>.<span class="property">ImageryLayer</span>.<span class="title function_">fromProviderAsync</span>(</span></span><br><span class="line"><span class="language-javascript">    <span class="title class_">Cesium</span>.<span class="property">IonImageryProvider</span>.<span class="title function_">fromAssetId</span>(<span class="number">3813</span>)</span></span><br><span class="line"><span class="language-javascript">  ),</span></span><br><span class="line"><span class="language-javascript">&#125;);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">viewer.<span class="property">entities</span>.<span class="title function_">add</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">  <span class="attr">position</span>: <span class="title class_">Cesium</span>.<span class="property">Cartesian3</span>.<span class="title function_">fromDegrees</span>(-<span class="number">121.9068641</span>,<span class="number">56.20149076</span>),</span></span><br><span class="line"><span class="language-javascript">  <span class="attr">point</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">color</span>: <span class="title class_">Cesium</span>.<span class="property">Color</span>.<span class="property">YELLOW</span>,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">pixelSize</span>: <span class="number">10</span></span></span><br><span class="line"><span class="language-javascript">  &#125;</span></span><br><span class="line"><span class="language-javascript">&#125;);</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="添加多个点"><a href="#添加多个点" class="headerlink" title="添加多个点"></a>添加多个点</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="comment">// 使用提供的坐标创建点位置数组</span></span></span><br><span class="line"><span class="language-javascript"><span class="keyword">const</span> positions = [</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">56.20149</span>, -<span class="number">121.906864</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">57.00581</span>, -<span class="number">122.605421</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">57.00405</span>, -<span class="number">122.682536</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">57.06126</span>, -<span class="number">122.344514</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">57.54797</span>, -<span class="number">122.754017</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">57.33774</span>, -<span class="number">122.300309</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">57.58799</span>, -<span class="number">122.798906</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">56.23782</span>, -<span class="number">122.059896</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">56.66702</span>, -<span class="number">122.241126</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">56.96679</span>, -<span class="number">122.183874</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">56.99772</span>, -<span class="number">122.251137</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">52.42144</span>, -<span class="number">122.44831</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">56.38861</span>, -<span class="number">122.31356</span>],</span></span><br><span class="line"><span class="language-javascript">  [<span class="number">56.87023</span>, -<span class="number">122.12061</span>]</span></span><br><span class="line"><span class="language-javascript">];</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="comment">// 为每个位置添加一个点实体</span></span></span><br><span class="line"><span class="language-javascript">positions.<span class="title function_">forEach</span>(<span class="function">(<span class="params">pos, index</span>) =&gt;</span> &#123;</span></span><br><span class="line"><span class="language-javascript">  viewer.<span class="property">entities</span>.<span class="title function_">add</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">position</span>: <span class="title class_">Cesium</span>.<span class="property">Cartesian3</span>.<span class="title function_">fromDegrees</span>(pos[<span class="number">1</span>], pos[<span class="number">0</span>]),  <span class="comment">// 注意：经度在前，纬度在后</span></span></span><br><span class="line"><span class="language-javascript">    <span class="attr">point</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">color</span>: <span class="title class_">Cesium</span>.<span class="property">Color</span>.<span class="title function_">fromRandom</span>(&#123;<span class="attr">alpha</span>: <span class="number">1.0</span>&#125;), <span class="comment">// 随机颜色</span></span></span><br><span class="line"><span class="language-javascript">      <span class="attr">pixelSize</span>: <span class="number">10</span>,</span></span><br><span class="line"><span class="language-javascript">    &#125;,</span></span><br><span class="line"><span class="language-javascript">    <span class="attr">label</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">text</span>: <span class="string">`Point <span class="subst">$&#123;index + <span class="number">1</span>&#125;</span>`</span>,</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">font</span>: <span class="string">&#x27;14pt sans-serif&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">horizontalOrigin</span>: <span class="title class_">Cesium</span>.<span class="property">HorizontalOrigin</span>.<span class="property">LEFT</span>,</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">verticalOrigin</span>: <span class="title class_">Cesium</span>.<span class="property">VerticalOrigin</span>.<span class="property">BOTTOM</span>,</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">pixelOffset</span>: <span class="keyword">new</span> <span class="title class_">Cesium</span>.<span class="title class_">Cartesian2</span>(<span class="number">5</span>, <span class="number">5</span>)</span></span><br><span class="line"><span class="language-javascript">    &#125;</span></span><br><span class="line"><span class="language-javascript">  &#125;);</span></span><br><span class="line"><span class="language-javascript">&#125;);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="comment">// 设置相机视角以适应所有点</span></span></span><br><span class="line"><span class="language-javascript">viewer.<span class="title function_">zoomTo</span>(viewer.<span class="property">entities</span>);</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="设置中心点"><a href="#设置中心点" class="headerlink" title="设置中心点"></a>设置中心点</h1><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">viewer.camera.setView(&#123;</span><br><span class="line">    // fromDegrees()方法，将经纬度和高程转换为世界坐标</span><br><span class="line">    destination:Cesium.Cartesian3.fromDegrees(117.48,30.67,15000.0),</span><br><span class="line">    orientation:&#123;</span><br><span class="line">    // 指向</span><br><span class="line">    heading:Cesium.Math.toRadians(90,0),</span><br><span class="line">    // 视角</span><br><span class="line">    pitch:Cesium.Math.toRadians(-90),</span><br><span class="line">    roll:0.0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">viewer.camera.flyTo(&#123;</span><br><span class="line">    // fromDegrees()方法，将经纬度和高程转换为世界坐标</span><br><span class="line">    destination:Cesium.Cartesian3.fromDegrees(117.48,30.67,15000.0),</span><br><span class="line">    orientation:&#123;</span><br><span class="line">    // 指向</span><br><span class="line">    heading:Cesium.Math.toRadians(90,0),</span><br><span class="line">    // 视角</span><br><span class="line">    pitch:Cesium.Math.toRadians(-90),</span><br><span class="line">    roll:0.0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="设置默认显示模式"><a href="#设置默认显示模式" class="headerlink" title="设置默认显示模式"></a>设置默认显示模式</h1><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 2.5D 哥伦布模式</span><br><span class="line">viewer.scene.mode = Cesium.SceneMode.COLUMBUS_VIEW;</span><br><span class="line">// 2D 模式</span><br><span class="line">viewer.scene.mode = Cesium.SceneMode.SCENE2D;</span><br><span class="line">// 3D 模式</span><br><span class="line">viewer.scene.mode = Cesium.SceneMode.SCENE3D;</span><br><span class="line">// 变形模式</span><br><span class="line">viewer.scene.mode = Cesium.SceneMode.MORPHING;</span><br></pre></td></tr></table></figure><h1 id="截图"><a href="#截图" class="headerlink" title="截图"></a>截图</h1><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">    <span class="keyword">function</span> <span class="title function_">saveToFile</span>(<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">let</span> canvas = viewer.<span class="property">scene</span>.<span class="property">canvas</span>;</span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">let</span> image = canvas.<span class="title function_">toDataURL</span>(<span class="string">&quot;image/png&quot;</span>).<span class="title function_">replace</span>(<span class="string">&quot;image/png&quot;</span>, <span class="string">&quot;image/octet-stream&quot;</span>);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">let</span> link = <span class="variable language_">document</span>.<span class="title function_">createElement</span>(<span class="string">&quot;a&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">let</span> blob = <span class="title function_">dataURLtoBlob</span>(image);</span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">let</span> objurl = <span class="variable constant_">URL</span>.<span class="title function_">createObjectURL</span>(blob);</span></span><br><span class="line"><span class="language-javascript">        link.<span class="property">download</span> = <span class="string">&quot;scene.png&quot;</span>;</span></span><br><span class="line"><span class="language-javascript">        link.<span class="property">href</span> = objurl;</span></span><br><span class="line"><span class="language-javascript">        link.<span class="title function_">click</span>();</span></span><br><span class="line"><span class="language-javascript">    &#125;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">    <span class="keyword">function</span> <span class="title function_">dataURLtoBlob</span>(<span class="params">dataurl</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">let</span> arr = dataurl.<span class="title function_">split</span>(<span class="string">&#x27;,&#x27;</span>),</span></span><br><span class="line"><span class="language-javascript">        mime = arr[<span class="number">0</span>].<span class="title function_">match</span>(<span class="regexp">/:(.*?);/</span>)[<span class="number">1</span>],</span></span><br><span class="line"><span class="language-javascript">        bstr = <span class="title function_">atob</span>(arr[<span class="number">1</span>]),</span></span><br><span class="line"><span class="language-javascript">        n = bstr.<span class="property">length</span>,</span></span><br><span class="line"><span class="language-javascript">        u8arr = <span class="keyword">new</span> <span class="title class_">Uint8Array</span>(n);</span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">while</span> (n--) &#123;</span></span><br><span class="line"><span class="language-javascript">        u8arr[n] = bstr.<span class="title function_">charCodeAt</span>(n);</span></span><br><span class="line"><span class="language-javascript">        &#125;</span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Blob</span>([u8arr], &#123; <span class="attr">type</span>: mime &#125;);</span></span><br><span class="line"><span class="language-javascript">    &#125;</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;toolbar&quot;</span> <span class="attr">class</span>=<span class="string">&quot;param-container tool-bar layui-form-item&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">button</span>  <span class="attr">onclick</span>=<span class="string">&quot;saveToFile()&quot;</span>&gt;</span>场景出图<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="沙盒代码"><a href="#沙盒代码" class="headerlink" title="沙盒代码"></a>沙盒代码</h1><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// Natural Earth II 3831</span><br><span class="line">const viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;, &#123;</span><br><span class="line">  baseLayer: Cesium.ImageryLayer.fromProviderAsync(</span><br><span class="line">    Cesium.IonImageryProvider.fromAssetId(3813)</span><br><span class="line">  ),</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">viewer.camera.setView(&#123;</span><br><span class="line">    // fromDegrees()方法，将经纬度和高程转换为世界坐标</span><br><span class="line">    destination:Cesium.Cartesian3.fromDegrees(-121.9068641,56.20149076,20000000),</span><br><span class="line">    orientation:&#123;</span><br><span class="line">    // 指向</span><br><span class="line">    heading:Cesium.Math.toRadians(270,0),</span><br><span class="line">    // 视角</span><br><span class="line">    pitch:Cesium.Math.toRadians(-90),</span><br><span class="line">    roll:0.0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">viewer.entities.add(&#123;</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-121.9068641,56.20149076),</span><br><span class="line">  point: &#123;</span><br><span class="line">    color: Cesium.Color.YELLOW,</span><br><span class="line">    pixelSize: 10</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">// Natural Earth II 3831</span><br><span class="line">const viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;, &#123;</span><br><span class="line">  baseLayer: Cesium.ImageryLayer.fromProviderAsync(</span><br><span class="line">    Cesium.IonImageryProvider.fromAssetId(3813)</span><br><span class="line">  ),</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 设置相机视角以覆盖所有点</span><br><span class="line">viewer.camera.setView(&#123;</span><br><span class="line">  destination: Cesium.Cartesian3.fromDegrees(-122.5, 55, 1000000),</span><br><span class="line">  orientation: &#123;</span><br><span class="line">    heading: Cesium.Math.toRadians(0),</span><br><span class="line">    pitch: Cesium.Math.toRadians(-45),</span><br><span class="line">    roll: 0.0</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 使用提供的坐标创建点位置数组</span><br><span class="line">const positions = [</span><br><span class="line">  [56.20149, -121.906864],</span><br><span class="line">  [57.00581, -122.605421],</span><br><span class="line">  [57.00405, -122.682536],</span><br><span class="line">  [57.06126, -122.344514],</span><br><span class="line">  [57.54797, -122.754017],</span><br><span class="line">  [57.33774, -122.300309],</span><br><span class="line">  [57.58799, -122.798906],</span><br><span class="line">  [56.23782, -122.059896],</span><br><span class="line">  [56.66702, -122.241126],</span><br><span class="line">  [56.96679, -122.183874],</span><br><span class="line">  [56.99772, -122.251137],</span><br><span class="line">  [52.42144, -122.44831],</span><br><span class="line">  [56.38861, -122.31356],</span><br><span class="line">  [56.87023, -122.12061]</span><br><span class="line">];</span><br><span class="line"></span><br><span class="line">// 为每个位置添加一个点实体</span><br><span class="line">positions.forEach((pos, index) =&gt; &#123;</span><br><span class="line">  viewer.entities.add(&#123;</span><br><span class="line">    position: Cesium.Cartesian3.fromDegrees(pos[1], pos[0]),  // 注意：经度在前，纬度在后</span><br><span class="line">    point: &#123;</span><br><span class="line">      color: Cesium.Color.fromRandom(&#123;alpha: 1.0&#125;), // 随机颜色</span><br><span class="line">      pixelSize: 10,</span><br><span class="line">    &#125;,</span><br><span class="line">    label: &#123;</span><br><span class="line">      text: `Point $&#123;index + 1&#125;`,</span><br><span class="line">      font: &#x27;14pt sans-serif&#x27;,</span><br><span class="line">      horizontalOrigin: Cesium.HorizontalOrigin.LEFT,</span><br><span class="line">      verticalOrigin: Cesium.VerticalOrigin.BOTTOM,</span><br><span class="line">      pixelOffset: new Cesium.Cartesian2(5, 5)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 设置相机视角以适应所有点</span><br><span class="line">viewer.zoomTo(viewer.entities);</span><br></pre></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">// Natural Earth II 3831</span><br><span class="line">const viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;, &#123;</span><br><span class="line">  baseLayer: Cesium.ImageryLayer.fromProviderAsync(</span><br><span class="line">    Cesium.IonImageryProvider.fromAssetId(3831)</span><br><span class="line">  ),</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">viewer.camera.setView(&#123;</span><br><span class="line">    // fromDegrees()方法，将经纬度和高程转换为世界坐标</span><br><span class="line">    destination:Cesium.Cartesian3.fromDegrees(-121.9068641,56.20149076,20000000),</span><br><span class="line">    orientation:&#123;</span><br><span class="line">    // 指向</span><br><span class="line">    heading:Cesium.Math.toRadians(0,0),</span><br><span class="line">    // 视角</span><br><span class="line">    pitch:Cesium.Math.toRadians(-90),</span><br><span class="line">    roll:0.0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">viewer.entities.add(&#123;</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-121.9068641,56.20149076),</span><br><span class="line">  point: &#123;</span><br><span class="line">    color: Cesium.Color.YELLOW,</span><br><span class="line">    pixelSize: 10</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 定义多个点的数据</span><br><span class="line">const points = [</span><br><span class="line">    &#123; lon: -121.9068641, lat: 56.20149076, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.6054214, lat: 57.00580928, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.6825364, lat: 57.004054, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.3445136, lat: 57.06126005, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.7540172, lat: 57.54796791, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.3003092, lat: 57.33774351, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.7989062, lat: 57.58799072, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.0598957, lat: 56.23781614, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.2411258, lat: 56.66701811, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.1838739, lat: 56.96678936, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.251137, lat: 56.99771651, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.44831, lat: 52.42144, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.31356, lat: 56.38861, color: Cesium.Color.YELLOW, pixes: 10 &#125;,</span><br><span class="line">    &#123; lon: -122.12061, lat: 56.87023, color: Cesium.Color.YELLOW, pixes: 10 &#125;</span><br><span class="line">];</span><br><span class="line"></span><br><span class="line">// 遍历点数据并添加到地图上</span><br><span class="line">points.forEach(point =&gt; &#123;</span><br><span class="line">    viewer.entities.add(&#123;</span><br><span class="line">        position: Cesium.Cartesian3.fromDegrees(point.lon, point.lat),</span><br><span class="line">        point: &#123;</span><br><span class="line">            color: point.color,</span><br><span class="line">            pixelSize: point.pixes</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Cesium-使用指南</summary>
    
    
    
    <category term="科研利器" scheme="http://hibiscidai.com/categories/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="软件" scheme="http://hibiscidai.com/tags/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="石油地质" scheme="http://hibiscidai.com/tags/%E7%9F%B3%E6%B2%B9%E5%9C%B0%E8%B4%A8/"/>
    
  </entry>
  
  <entry>
    <title>CGAN-TensorFlow</title>
    <link href="http://hibiscidai.com/2024/07/10/CGAN-TensorFlow/"/>
    <id>http://hibiscidai.com/2024/07/10/CGAN-TensorFlow/</id>
    <published>2024-07-10T09:00:00.000Z</published>
    <updated>2024-07-11T09:24:39.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/07/10/CGAN-TensorFlow/CGAN-TensorFlow.png" class="" title="CGAN-TensorFlow"><p>CGAN-TensorFlow</p><span id="more"></span><p>[TOC]</p><h1 id="CGAN-TensorFlow"><a href="#CGAN-TensorFlow" class="headerlink" title="CGAN-TensorFlow"></a>CGAN-TensorFlow</h1><p>原文链接：<a href="https://agustinus.kristia.de/techblog/2016/12/24/conditional-gan-tensorflow/">Conditional Generative Adversarial Nets in TensorFlow</a></p><p>We have seen the Generative Adversarial Nets (GAN) model in the previous post. We have also seen the arch nemesis of GAN, the VAE and its conditional variation: Conditional VAE (CVAE). Hence, it is only proper for us to study conditional variation of GAN, called Conditional GAN or CGAN for short.<br>我们在上一篇文章中看到了生成对抗网络（GAN）模型。我们还看到了 GAN 的死对头，VAE 及其条件变体：条件 VAE（CVAE）。因此，我们研究 GAN 的条件变体（称为条件 GAN 或简称 CGAN）是再合适不过的了。</p><h1 id="CGAN-Formulation-and-Architecture"><a href="#CGAN-Formulation-and-Architecture" class="headerlink" title="CGAN: Formulation and Architecture"></a>CGAN: Formulation and Architecture</h1><p>Recall, in GAN, we have two neural nets: the generator <script type="math/tex">G(z)</script>  and the discriminator <script type="math/tex">D(X)</script>.<br>Now, as we want to condition those networks with some vector <script type="math/tex">y</script>, the easiest way to do it is to feed <script type="math/tex">y</script> into both networks. Hence, our generator and discriminator are now <script type="math/tex">G(z,y)</script>  and  <script type="math/tex">D(X,y)</script> respectively.<br>回想一下，在 GAN 中，我们有两个神经网络：生成器 <script type="math/tex">G(z)</script> 和鉴别器 <script type="math/tex">D(X)</script>。<br>现在，由于我们想用某个向量 <script type="math/tex">y</script> 来调节这些网络，最简单的方法是将 <script type="math/tex">y</script> 输入两个网络。因此，我们的生成器和鉴别器现在分别是 <script type="math/tex">G(z,y)</script> 和 <script type="math/tex">D(X,y)</script>。</p><p>We can see it with a probabilistic point of view. <script type="math/tex">G(z,y)</script> is modeling the distribution of our data, given  <script type="math/tex">z</script> and  <script type="math/tex">y</script>, that is, our data is generated with this scheme <script type="math/tex">X~G(X|z，y)</script>.<br>我们可以用概率的角度来看待它。给定<script type="math/tex">z</script>和<script type="math/tex">y</script>，<script type="math/tex">G(z,y)</script>正在对我们的数据分布进行建模，也就是说，我们的数据是用这个方案<script type="math/tex">X \sim G(X|z，y)</script>生成的。</p><p>Likewise for the discriminator, now it tries to find discriminating label for <script type="math/tex">X</script> and <script type="math/tex">X_G</script>, that are modeled with <script type="math/tex">d \sim D(d|X,y)</script>.<br>同样对于鉴别器来说，现在它试图为 <script type="math/tex">X</script> 和 <script type="math/tex">X_G</script> 找到鉴别标签，用 <script type="math/tex">d \sim D(d|X,y)</script> 建模。</p><p>Hence, we could see that both <script type="math/tex">D</script> and <script type="math/tex">G</script> is jointly conditioned to two variables <script type="math/tex">z</script> or <script type="math/tex">X</script> and <script type="math/tex">y</script>.<br>因此，我们可以看到 <script type="math/tex">D</script> 和 <script type="math/tex">G</script> 都与两个变量 <script type="math/tex">z</script> 或 <script type="math/tex">X</script> 和 <script type="math/tex">y</script> 联合相关。</p><p>Now, the objective function is given by:<br>现在，目标函数如下：</p><script type="math/tex; mode=display">\min_G\max_DV(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x,y)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z,y),y))]</script><p>If we compare the above loss to GAN loss, the difference only lies in the additional parameter <script type="math/tex">y</script> in both <script type="math/tex">D</script> and <script type="math/tex">G</script>.<br>如果我们将上述损失与 GAN 损失进行比较，差异仅在于 <script type="math/tex">D</script> 和 <script type="math/tex">G</script> 中的附加参数 <script type="math/tex">y</script>。</p><p>The architecture of CGAN is now as follows (taken from [1]):<br>CGAN 的架构如下（取自[1]）：</p><img src="/2024/07/10/CGAN-TensorFlow/CGAN%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" class="" title="CGAN网络结构"><p>In contrast with the architecture of GAN, we now has an additional input layer in both discriminator net and generator net.<br>与 GAN 的架构相比，我们现在在鉴别器网络和生成器网络中都增加了一个输入层。</p><h1 id="CGAN-Implementation-in-TensorFlow"><a href="#CGAN-Implementation-in-TensorFlow" class="headerlink" title="CGAN: Implementation in TensorFlow"></a>CGAN: Implementation in TensorFlow</h1><p>I’d like to direct the reader to the previous post about GAN, particularly for the implementation in TensorFlow. Implementing CGAN is so simple that we just need to add a handful of lines to the original GAN implementation. So, here we will only look at those modifications.<br>我想引导读者阅读上一篇关于 GAN 的文章，特别是关于在 TensorFlow 中的实现。实现 CGAN 非常简单，我们只需要在原始 GAN 实现中添加几行代码即可。因此，我们在这里只讨论这些修改。</p><p><a href="https://agustinus.kristia.de/techblog/2016/09/17/gan-tensorflow/">Generative Adversarial Nets in TensorFlow</a></p><p>The first additional code for CGAN is here:<br>CGAN 的第一个附加代码在这里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, y_dim])</span><br></pre></td></tr></table></figure><p>We are adding new input to hold our variable we are conditioning our CGAN to.<br>我们正在添加新输入来保存我们用来调节 CGAN 的变量。</p><p>Next, we add it to both our generator net and discriminator net:<br>接下来，我们将其添加到我们的生成器网络和鉴别器网络中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generator</span>(<span class="params">z, y</span>):</span><br><span class="line">    <span class="comment"># Concatenate z and y</span></span><br><span class="line">    inputs = tf.concat(concat_dim=<span class="number">1</span>, values=[z, y])</span><br><span class="line"></span><br><span class="line">    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)</span><br><span class="line">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class="line">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="comment"># Concatenate x and y</span></span><br><span class="line">    inputs = tf.concat(concat_dim=<span class="number">1</span>, values=[x, y])</span><br><span class="line"></span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)</span><br><span class="line">    D_logit = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    D_prob = tf.nn.sigmoid(D_logit)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D_prob, D_logit</span><br></pre></td></tr></table></figure><p>The problem we have here is how to incorporate the new variable <script type="math/tex">y</script> into <script type="math/tex">D(x)</script> and <script type="math/tex">G(z)</script>. As we are trying to model the joint conditional, the simplest way to do it is to just concatenate both variables. Hence, in <script type="math/tex">G(z,y)</script>, we are concatenating <script type="math/tex">z</script> and <script type="math/tex">y</script> before we feed it into the networks. The same procedure is applied to <script type="math/tex">D(X,y)</script>.<br>我们这里的问题是如何将新变量 <script type="math/tex">y</script> 合并到 <script type="math/tex">D(x)</script> 和 <script type="math/tex">G(z)</script> 中。由于我们试图对联合条件进行建模，最简单的方法就是将两个变量连接起来。因此，在 <script type="math/tex">G(z,y)</script> 中，我们在将其输入网络之前将 <script type="math/tex">z</script> 和 <script type="math/tex">y</script> 连接起来。同样的程序也适用于 <script type="math/tex">D(X,y)</script>。</p><p>Of course, as our inputs for <script type="math/tex">D(X,y)</script> and <script type="math/tex">G(z,y)</script> is now different than the original GAN, we need to modify our weights:<br>当然，由于我们对 <script type="math/tex">D(X,y)</script> 和 <script type="math/tex">G(z,y)</script> 的输入现在与原始 GAN 不同，我们需要修改我们的权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Modify input to hidden weights for discriminator</span></span><br><span class="line"><span class="comment"># 修改鉴别器隐藏权重的输入</span></span><br><span class="line">D_W1 = tf.Variable(shape=[X_dim + y_dim, h_dim]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Modify input to hidden weights for generator</span></span><br><span class="line"><span class="comment"># 修改生成器隐藏权重的输入</span></span><br><span class="line">G_W1 = tf.Variable(shape=[Z_dim + y_dim, h_dim]))</span><br></pre></td></tr></table></figure><p>That is, we just adjust the dimensionality of our weights.<br>也就是说，我们只是调整权重的维数。</p><p>Next, we just use our new networks:<br>接下来，我们只需使用我们的新网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add additional parameter y into all networks</span></span><br><span class="line"><span class="comment"># 在所有网络中添加附加参数 y</span></span><br><span class="line">G_sample = generator(Z, y)</span><br><span class="line">D_real, D_logit_real = discriminator(X, y)</span><br><span class="line">D_fake, D_logit_fake = discriminator(G_sample, y)</span><br></pre></td></tr></table></figure><p>And finally, when training, we also feed the value of <script type="math/tex">y</script> into the networks:<br>最后，在训练时，我们还将 <script type="math/tex">y</script> 的值输入网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_mb, y_mb = mnist.train.next_batch(mb_size)</span><br><span class="line"></span><br><span class="line">Z_sample = sample_Z(mb_size, Z_dim)</span><br><span class="line">_, D_loss_curr = sess.run([D_solver, D_loss], feed_dict=&#123;X: X_mb, Z: Z_sample, y:y_mb&#125;)</span><br><span class="line">_, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=&#123;Z: Z_sample, y:y_mb&#125;)</span><br></pre></td></tr></table></figure><p>As an example above, we are training our GAN with MNIST data, and the conditional variable <script type="math/tex">y</script> is the labels.<br>如上例，我们使用 MNIST 数据训练我们的 GAN，条件变量 <script type="math/tex">y</script> 是标签。</p><h1 id="CGAN-Results"><a href="#CGAN-Results" class="headerlink" title="CGAN: Results"></a>CGAN: Results</h1><p>At test time, we want to generate new data samples with certain label. For example, we set the label to be 5, i.e. we want to generate digit “5”:<br>在测试时，我们希望生成具有特定标签的新数据样本。例如，我们将标签设置为 5，即我们希望生成数字“5”：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_sample = <span class="number">16</span></span><br><span class="line">Z_sample = sample_Z(n_sample, Z_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create conditional one-hot vector, with index 5 = 1</span></span><br><span class="line"><span class="comment"># 创建条件独热向量，索引 5 = 1</span></span><br><span class="line">y_sample = np.zeros(shape=[n_sample, y_dim])</span><br><span class="line">y_sample[:, <span class="number">5</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">samples = sess.run(G_sample, feed_dict=&#123;Z: Z_sample, y:y_sample&#125;)</span><br></pre></td></tr></table></figure><p>Above, we just sample <script type="math/tex">z</script>, and then construct the conditional variables. In our example case, the conditional variables is a collection of one-hot vectors with value 1 in the 5th index. The last thing we need to is to run the network with those variables as inputs.<br>上面，我们只是对 <script type="math/tex">z</script> 进行采样，然后构造条件变量。在我们的示例中，条件变量是第 5 个索引中值为 1 的独热向量集合。我们需要做的最后一件事是使用这些变量作为输入来运行网络。</p><p>Here is the results:<br>结果如下：</p><img src="/2024/07/10/CGAN-TensorFlow/CGAN%E7%BB%93%E6%9E%9C1.png" class="" title="CGAN结果1"><p>Looks pretty much like digit 5, right?<br>看起来很像数字 5，对吧？</p><p>If we set our one-hot vectors to have value of 1 in the 7th index:<br>如果我们将独热向量设置为第 7 个索引中的值为 1：</p><img src="/2024/07/10/CGAN-TensorFlow/CGAN%E7%BB%93%E6%9E%9C2.png" class="" title="CGAN结果2"><p>Those results confirmed that have successfully trained our CGAN.<br>这些结果证实了我们的 CGAN 训练成功。</p><p><a href="https://github.com/wiseodd/generative-models">完整代码</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cgan_pytorch.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;../../MNIST_data&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">mb_size = <span class="number">64</span></span><br><span class="line">Z_dim = <span class="number">100</span></span><br><span class="line">X_dim = mnist.train.images.shape[<span class="number">1</span>]</span><br><span class="line">y_dim = mnist.train.labels.shape[<span class="number">1</span>]</span><br><span class="line">h_dim = <span class="number">128</span></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">lr = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_init</span>(<span class="params">size</span>):</span><br><span class="line">    in_dim = size[<span class="number">0</span>]</span><br><span class="line">    xavier_stddev = <span class="number">1.</span> / np.sqrt(in_dim / <span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> Variable(torch.randn(*size) * xavier_stddev, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ==================== GENERATOR ======================== &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])</span><br><span class="line">bzh = Variable(torch.zeros(h_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Whx = xavier_init(size=[h_dim, X_dim])</span><br><span class="line">bhx = Variable(torch.zeros(X_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">G</span>(<span class="params">z, c</span>):</span><br><span class="line">    inputs = torch.cat([z, c], <span class="number">1</span>)</span><br><span class="line">    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ==================== DISCRIMINATOR ======================== &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">Wxh = xavier_init(size=[X_dim + y_dim, h_dim])</span><br><span class="line">bxh = Variable(torch.zeros(h_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Why = xavier_init(size=[h_dim, <span class="number">1</span>])</span><br><span class="line">bhy = Variable(torch.zeros(<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">D</span>(<span class="params">X, c</span>):</span><br><span class="line">    inputs = torch.cat([X, c], <span class="number">1</span>)</span><br><span class="line">    h = nn.relu(inputs @ Wxh + bxh.repeat(inputs.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G_params = [Wzh, bzh, Whx, bhx]</span><br><span class="line">D_params = [Wxh, bxh, Why, bhy]</span><br><span class="line">params = G_params + D_params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ===================== TRAINING ======================== &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reset_grad</span>():</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data = p.grad.data</span><br><span class="line">            p.grad = Variable(data.new().resize_as_(data).zero_())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G_solver = optim.Adam(G_params, lr=<span class="number">1e-3</span>)</span><br><span class="line">D_solver = optim.Adam(D_params, lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line">ones_label = Variable(torch.ones(mb_size, <span class="number">1</span>))</span><br><span class="line">zeros_label = Variable(torch.zeros(mb_size, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100000</span>):</span><br><span class="line">    <span class="comment"># Sample data</span></span><br><span class="line">    z = Variable(torch.randn(mb_size, Z_dim))</span><br><span class="line">    X, c = mnist.train.next_batch(mb_size)</span><br><span class="line">    X = Variable(torch.from_numpy(X))</span><br><span class="line">    c = Variable(torch.from_numpy(c.astype(<span class="string">&#x27;float32&#x27;</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Dicriminator forward-loss-backward-update</span></span><br><span class="line">    G_sample = G(z, c)</span><br><span class="line">    D_real = D(X, c)</span><br><span class="line">    D_fake = D(G_sample, c)</span><br><span class="line"></span><br><span class="line">    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)</span><br><span class="line">    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)</span><br><span class="line">    D_loss = D_loss_real + D_loss_fake</span><br><span class="line"></span><br><span class="line">    D_loss.backward()</span><br><span class="line">    D_solver.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Housekeeping - reset gradient</span></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generator forward-loss-backward-update</span></span><br><span class="line">    z = Variable(torch.randn(mb_size, Z_dim))</span><br><span class="line">    G_sample = G(z, c)</span><br><span class="line">    D_fake = D(G_sample, c)</span><br><span class="line"></span><br><span class="line">    G_loss = nn.binary_cross_entropy(D_fake, ones_label)</span><br><span class="line"></span><br><span class="line">    G_loss.backward()</span><br><span class="line">    G_solver.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Housekeeping - reset gradient</span></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print and plot every now and then</span></span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Iter-&#123;&#125;; D_loss: &#123;&#125;; G_loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(it, D_loss.data.numpy(), G_loss.data.numpy()))</span><br><span class="line"></span><br><span class="line">        c = np.zeros(shape=[mb_size, y_dim], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        c[:, np.random.randint(<span class="number">0</span>, <span class="number">10</span>)] = <span class="number">1.</span></span><br><span class="line">        c = Variable(torch.from_numpy(c))</span><br><span class="line">        samples = G(z, c).data.numpy()[:<span class="number">16</span>]</span><br><span class="line"></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">        gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">        gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">            ax = plt.subplot(gs[i])</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">            ax.set_xticklabels([])</span><br><span class="line">            ax.set_yticklabels([])</span><br><span class="line">            ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">            plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&#x27;Greys_r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;out/&#x27;</span>):</span><br><span class="line">            os.makedirs(<span class="string">&#x27;out/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        plt.savefig(<span class="string">&#x27;out/&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(cnt).zfill(<span class="number">3</span>)), bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        plt.close(fig)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;../../MNIST_data&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">mb_size = <span class="number">64</span></span><br><span class="line">Z_dim = <span class="number">100</span></span><br><span class="line">X_dim = mnist.train.images.shape[<span class="number">1</span>]</span><br><span class="line">y_dim = mnist.train.labels.shape[<span class="number">1</span>]</span><br><span class="line">h_dim = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_init</span>(<span class="params">size</span>):</span><br><span class="line">    in_dim = size[<span class="number">0</span>]</span><br><span class="line">    xavier_stddev = <span class="number">1.</span> / tf.sqrt(in_dim / <span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; Discriminator Net model &quot;&quot;&quot;</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, y_dim])</span><br><span class="line"></span><br><span class="line">D_W1 = tf.Variable(xavier_init([X_dim + y_dim, h_dim]))</span><br><span class="line">D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class="line"></span><br><span class="line">D_W2 = tf.Variable(xavier_init([h_dim, <span class="number">1</span>]))</span><br><span class="line">D_b2 = tf.Variable(tf.zeros(shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator</span>(<span class="params">x, y</span>):</span><br><span class="line">    inputs = tf.concat(axis=<span class="number">1</span>, values=[x, y])</span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)</span><br><span class="line">    D_logit = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    D_prob = tf.nn.sigmoid(D_logit)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D_prob, D_logit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; Generator Net model &quot;&quot;&quot;</span></span><br><span class="line">Z = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, Z_dim])</span><br><span class="line"></span><br><span class="line">G_W1 = tf.Variable(xavier_init([Z_dim + y_dim, h_dim]))</span><br><span class="line">G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class="line"></span><br><span class="line">G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))</span><br><span class="line">G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))</span><br><span class="line"></span><br><span class="line">theta_G = [G_W1, G_W2, G_b1, G_b2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator</span>(<span class="params">z, y</span>):</span><br><span class="line">    inputs = tf.concat(axis=<span class="number">1</span>, values=[z, y])</span><br><span class="line">    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)</span><br><span class="line">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class="line">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_Z</span>(<span class="params">m, n</span>):</span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(-<span class="number">1.</span>, <span class="number">1.</span>, size=[m, n])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">samples</span>):</span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">    gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">        ax = plt.subplot(gs[i])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        ax.set_xticklabels([])</span><br><span class="line">        ax.set_yticklabels([])</span><br><span class="line">        ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">        plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&#x27;Greys_r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G_sample = generator(Z, y)</span><br><span class="line">D_real, D_logit_real = discriminator(X, y)</span><br><span class="line">D_fake, D_logit_fake = discriminator(G_sample, y)</span><br><span class="line"></span><br><span class="line">D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))</span><br><span class="line">D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))</span><br><span class="line">D_loss = D_loss_real + D_loss_fake</span><br><span class="line">G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))</span><br><span class="line"></span><br><span class="line">D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)</span><br><span class="line">G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;out/&#x27;</span>):</span><br><span class="line">    os.makedirs(<span class="string">&#x27;out/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        n_sample = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">        Z_sample = sample_Z(n_sample, Z_dim)</span><br><span class="line">        y_sample = np.zeros(shape=[n_sample, y_dim])</span><br><span class="line">        y_sample[:, <span class="number">7</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        samples = sess.run(G_sample, feed_dict=&#123;Z: Z_sample, y:y_sample&#125;)</span><br><span class="line"></span><br><span class="line">        fig = plot(samples)</span><br><span class="line">        plt.savefig(<span class="string">&#x27;out/&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(i).zfill(<span class="number">3</span>)), bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        plt.close(fig)</span><br><span class="line"></span><br><span class="line">    X_mb, y_mb = mnist.train.next_batch(mb_size)</span><br><span class="line"></span><br><span class="line">    Z_sample = sample_Z(mb_size, Z_dim)</span><br><span class="line">    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict=&#123;X: X_mb, Z: Z_sample, y:y_mb&#125;)</span><br><span class="line">    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=&#123;Z: Z_sample, y:y_mb&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Iter: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(it))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;D loss: &#123;:.4&#125;&#x27;</span>. <span class="built_in">format</span>(D_loss_curr))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;G_loss: &#123;:.4&#125;&#x27;</span>.<span class="built_in">format</span>(G_loss_curr))</span><br><span class="line">        <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><p>修改后代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要库</span></span><br><span class="line"><span class="comment"># PyTorch用于深度学习</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># NumPy用于数值计算</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Matplotlib用于绘图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="comment"># os用于文件操作</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># TensorFlow用于加载MNIST数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#from tensorflow.examples.tutorials.mnist import input_data</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图像数据标准化到 [0, 1] 范围</span></span><br><span class="line">x_train = x_train.reshape(x_train.shape[<span class="number">0</span>], <span class="number">784</span>).astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255.</span></span><br><span class="line">x_test = x_test.reshape(x_test.shape[<span class="number">0</span>], <span class="number">784</span>).astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转换为 one-hot 编码</span></span><br><span class="line">y_train = tf.keras.utils.to_categorical(y_train, <span class="number">10</span>)</span><br><span class="line">y_test = tf.keras.utils.to_categorical(y_test, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不再使用原来的mnist.train.next_batch()方法，我们需要创建一个函数来模拟这个功能：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">next_batch</span>(<span class="params">num, data, labels</span>):</span><br><span class="line">    idx = np.arange(<span class="number">0</span> , <span class="built_in">len</span>(data))</span><br><span class="line">    np.random.shuffle(idx)</span><br><span class="line">    idx = idx[:num]</span><br><span class="line">    data_shuffle = [data[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    labels_shuffle = [labels[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    <span class="keyword">return</span> np.asarray(data_shuffle), np.asarray(labels_shuffle)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># mnist = input_data.read_data_sets(&#x27;../../MNIST_data&#x27;, one_hot=True) # 加载MNIST数据集，旧的TensorFlow版本</span></span><br><span class="line">mb_size = <span class="number">64</span>    <span class="comment">#设置mini-batch大小为64</span></span><br><span class="line">Z_dim = <span class="number">100</span> <span class="comment">#设置随机向量维度为100</span></span><br><span class="line">X_dim = x_train.shape[<span class="number">1</span>]</span><br><span class="line">y_dim = y_train.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#X_dim = mnist.train.images.shape[1] # 设置输入图像维度(784,因为MNIST图像是28x28=784像素)</span></span><br><span class="line"><span class="comment">#y_dim = mnist.train.labels.shape[1] # 设置标签维度(10,因为MNIST有10个类别)</span></span><br><span class="line">h_dim = <span class="number">128</span> <span class="comment">#设置隐藏层维度为128</span></span><br><span class="line">c = <span class="number">0</span>   <span class="comment">#初始化计数器c</span></span><br><span class="line">lr = <span class="number">1e-3</span>   <span class="comment">#学习率lr</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Xavier初始化方法,用于初始化网络权重，它有助于解决深度网络中的梯度消失问题</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_init</span>(<span class="params">size</span>):</span><br><span class="line">    in_dim = size[<span class="number">0</span>]</span><br><span class="line">    xavier_stddev = <span class="number">1.</span> / np.sqrt(in_dim / <span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> Variable(torch.randn(*size) * xavier_stddev, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ==================== GENERATOR ======================== &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 初始化了生成器的权重和偏置。Wzh和bzh是第一层的权重和偏置,Whx和bhx是第二层的。</span></span><br><span class="line">Wzh = xavier_init(size=[Z_dim, h_dim])</span><br><span class="line">bzh = Variable(torch.zeros(h_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Whx = xavier_init(size=[h_dim, X_dim])</span><br><span class="line">bhx = Variable(torch.zeros(X_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成器函数。它接受噪声z作为输入,通过两层网络生成假图像。第一层使用ReLU激活函数,第二层使用Sigmoid函数。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">G</span>(<span class="params">z</span>):</span><br><span class="line">    h = nn.relu(z @ Wzh + bzh.repeat(z.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ==================== DISCRIMINATOR ======================== &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 初始化判别器的权重和偏置。Wzh和bzh是第一层的权重和偏置,Whx和bhx是第二层的。</span></span><br><span class="line">Wxh = xavier_init(size=[X_dim, h_dim])</span><br><span class="line">bxh = Variable(torch.zeros(h_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Why = xavier_init(size=[h_dim, <span class="number">1</span>])</span><br><span class="line">bhy = Variable(torch.zeros(<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判别器函数。它接受图像X作为输入,输出一个0到1之间的数,表示图像是真实的概率。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">D</span>(<span class="params">X</span>):</span><br><span class="line">    h = nn.relu(X @ Wxh + bxh.repeat(X.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将生成器和判别器的参数分别组织起来,方便后续优化。</span></span><br><span class="line">G_params = [Wzh, bzh, Whx, bhx]</span><br><span class="line">D_params = [Wxh, bxh, Why, bhy]</span><br><span class="line">params = G_params + D_params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ===================== TRAINING ======================== &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重置所有参数的梯度。在每次更新参数之后调用,以准备下一次反向传播。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reset_grad</span>():</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data = p.grad.data</span><br><span class="line">            p.grad = Variable(data.new().resize_as_(data).zero_())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为生成器和判别器分别创建了Adam优化器。</span></span><br><span class="line">G_solver = optim.Adam(G_params, lr=<span class="number">1e-3</span>)</span><br><span class="line">D_solver = optim.Adam(D_params, lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表示&quot;真&quot;和&quot;假&quot;的标签,用于计算损失。</span></span><br><span class="line">ones_label = Variable(torch.ones(mb_size, <span class="number">1</span>))</span><br><span class="line">zeros_label = Variable(torch.zeros(mb_size, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练循环外创建两个列表来存储loss值</span></span><br><span class="line">D_losses = []</span><br><span class="line">G_losses = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 曲线平滑方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">moving_average</span>(<span class="params">data, window_size</span>):</span><br><span class="line">    cumsum = np.cumsum(np.insert(data, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> (cumsum[window_size:] - cumsum[:-window_size]) / window_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存loss曲线</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;out_gan_pytorch_1/&#x27;</span>):</span><br><span class="line">    os.makedirs(<span class="string">&#x27;out_gan_pytorch_1/&#x27;</span>)</span><br><span class="line">csv_file = <span class="built_in">open</span>(<span class="string">&#x27;out_gan_pytorch_1/loss_data.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">csv_writer = csv.writer(csv_file)</span><br><span class="line">csv_writer.writerow([<span class="string">&#x27;Iteration&#x27;</span>, <span class="string">&#x27;D_loss&#x27;</span>, <span class="string">&#x27;G_loss&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主训练循环,总共进行100000次迭代。</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100000</span>):</span><br><span class="line">    <span class="comment"># Sample data</span></span><br><span class="line">    z = Variable(torch.randn(mb_size, Z_dim))   <span class="comment">#生成一个随机噪声批次z作为生成器的输入。</span></span><br><span class="line">    <span class="comment"># X, _ = mnist.train.next_batch(mb_size)  #从MNIST数据集中获取一批真实图像X。</span></span><br><span class="line">    X, _ = next_batch(mb_size, x_train, y_train)</span><br><span class="line">    X = Variable(torch.from_numpy(X))   <span class="comment">#将X转换为PyTorch的Variable。</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; ===================== 判别器训练 ======================== &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Dicriminator forward-loss-backward-update</span></span><br><span class="line">    G_sample = G(z) <span class="comment">#使用生成器G生成一批假图像G_sample</span></span><br><span class="line">    D_real = D(X)   <span class="comment">#判别器D对真实图像X进行评估,得到D_real</span></span><br><span class="line">    D_fake = D(G_sample)    <span class="comment">#判别器D对生成的假图像G_sample进行评估,得到D_fake</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算判别器的损失</span></span><br><span class="line">    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)   <span class="comment">#D_loss_real是真实图像的损失, 目标是使D_real接近1。</span></span><br><span class="line">    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)  <span class="comment">#D_loss_fake是假图像的损失, 目标是使D_fake接近0。</span></span><br><span class="line">    D_loss = D_loss_real + D_loss_fake  <span class="comment">#总损失D_loss是这两部分之和。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对判别器进行反向传播和参数更新。</span></span><br><span class="line">    D_loss.backward()</span><br><span class="line">    D_solver.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Housekeeping - reset gradient</span></span><br><span class="line">    <span class="comment"># 重置所有参数的梯度,为下一步做准备。</span></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; ===================== 生成器训练 ======================== &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Generator forward-loss-backward-update</span></span><br><span class="line">    <span class="comment"># 为训练生成器,我们再次生成假图像并用判别器评估</span></span><br><span class="line">    z = Variable(torch.randn(mb_size, Z_dim))</span><br><span class="line">    G_sample = G(z)</span><br><span class="line">    D_fake = D(G_sample)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算生成器的损失。注意这里的目标是让D_fake接近1, 即欺骗判别器</span></span><br><span class="line">    G_loss = nn.binary_cross_entropy(D_fake, ones_label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对生成器进行反向传播和参数更新。</span></span><br><span class="line">    G_loss.backward()</span><br><span class="line">    G_solver.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Housekeeping - reset gradient</span></span><br><span class="line">    <span class="comment"># 再次重置梯度。</span></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每次迭代都记录loss值</span></span><br><span class="line">    D_losses.append(D_loss.item())</span><br><span class="line">    G_losses.append(G_loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print and plot every now and then</span></span><br><span class="line">    <span class="comment"># 每1000次迭代打印一次当前的损失值。</span></span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Iter-&#123;&#125;; D_loss: &#123;&#125;; G_loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(it, D_loss.data.numpy(), G_loss.data.numpy()))</span><br><span class="line"></span><br><span class="line">        samples = G(z).data.numpy()[:<span class="number">16</span>]    <span class="comment"># 生成16个样本。</span></span><br><span class="line"></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))    <span class="comment"># 创建一个4x4的图像网格。</span></span><br><span class="line">        gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)    <span class="comment"># 将每个生成的样本绘制到网格中。</span></span><br><span class="line">        gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">            ax = plt.subplot(gs[i])</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">            ax.set_xticklabels([])</span><br><span class="line">            ax.set_yticklabels([])</span><br><span class="line">            ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">            plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&#x27;Greys_r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;out_gan_pytorch_1/&#x27;</span>):</span><br><span class="line">            os.makedirs(<span class="string">&#x27;out_gan_pytorch_1/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        plt.savefig(<span class="string">&#x27;out_gan_pytorch_1/&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(c).zfill(<span class="number">3</span>)), bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">        c += <span class="number">1</span></span><br><span class="line">        plt.close(fig)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存当前的loss数据</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;out_gan_pytorch_1/loss_data_ongoing.csv&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">            writer = csv.writer(file)</span><br><span class="line">            <span class="keyword">if</span> it == <span class="number">0</span>:  <span class="comment"># 如果是第一次写入，添加表头</span></span><br><span class="line">                writer.writerow([<span class="string">&quot;Iteration&quot;</span>, <span class="string">&quot;G_loss&quot;</span>, <span class="string">&quot;D_loss&quot;</span>])</span><br><span class="line">            writer.writerow([it, G_loss.item(), D_loss.item()])</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Loss data at iteration <span class="subst">&#123;it&#125;</span> has been appended to &#x27;out_gan_pytorch_1/loss_data_ongoing.csv&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算移动平均-曲线平滑</span></span><br><span class="line">window_size = <span class="number">1000</span></span><br><span class="line">G_losses_avg = moving_average(G_losses, window_size)</span><br><span class="line">D_losses_avg = moving_average(D_losses, window_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制loss曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">plt.title(<span class="string">&quot;Generator and Discriminator Loss During Training&quot;</span>)</span><br><span class="line">plt.plot(G_losses, label=<span class="string">&quot;G&quot;</span>)</span><br><span class="line">plt.plot(D_losses, label=<span class="string">&quot;D&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iterations&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&#x27;out_gan_pytorch_1/loss_curve.png&#x27;</span>)</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存loss数据到CSV文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;out_gan_pytorch_1/loss_data.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    writer.writerow([<span class="string">&quot;Iteration&quot;</span>, <span class="string">&quot;G_loss&quot;</span>, <span class="string">&quot;D_loss&quot;</span>])  <span class="comment"># 写入表头</span></span><br><span class="line">    <span class="keyword">for</span> i, (g_loss, d_loss) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(G_losses, D_losses)):</span><br><span class="line">        writer.writerow([i, g_loss, d_loss])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loss data has been saved to &#x27;out_gan_pytorch_1/loss_data.csv&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算移动平均</span></span><br><span class="line">window_size = <span class="number">1000</span></span><br><span class="line">G_losses_avg = moving_average(G_losses, window_size)</span><br><span class="line">D_losses_avg = moving_average(D_losses, window_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存移动平均后的loss数据到CSV文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;out_gan_pytorch_1/loss_data_avg.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    writer.writerow([<span class="string">&quot;Iteration&quot;</span>, <span class="string">&quot;G_loss_avg&quot;</span>, <span class="string">&quot;D_loss_avg&quot;</span>])  <span class="comment"># 写入表头</span></span><br><span class="line">    <span class="keyword">for</span> i, (g_loss, d_loss) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(G_losses_avg, D_losses_avg)):</span><br><span class="line">        writer.writerow([i+window_size, g_loss, d_loss])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Averaged loss data has been saved to &#x27;out_gan_pytorch_1/loss_data_avg.csv&#x27;&quot;</span>)</span><br><span class="line"><span class="comment"># loss_data.csv：包含所有迭代的原始loss数据</span></span><br><span class="line"><span class="comment"># loss_data_avg.csv：包含移动平均后的loss数据</span></span><br><span class="line"><span class="comment"># loss_data_ongoing.csv：在训练过程中定期保存的loss数据</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">CGAN-TensorFlow</summary>
    
    
    
    <category term="人工智能" scheme="http://hibiscidai.com/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="GAN" scheme="http://hibiscidai.com/tags/GAN/"/>
    
    <category term="人工智能" scheme="http://hibiscidai.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="深度学习" scheme="http://hibiscidai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>小丑在殿堂</title>
    <link href="http://hibiscidai.com/2024/07/03/%E5%B0%8F%E4%B8%91%E5%9C%A8%E6%AE%BF%E5%A0%82/"/>
    <id>http://hibiscidai.com/2024/07/03/%E5%B0%8F%E4%B8%91%E5%9C%A8%E6%AE%BF%E5%A0%82/</id>
    <published>2024-07-03T06:00:00.000Z</published>
    <updated>2024-07-22T13:41:06.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/07/03/%E5%B0%8F%E4%B8%91%E5%9C%A8%E6%AE%BF%E5%A0%82/%E5%B0%8F%E4%B8%91%E5%9C%A8%E6%AE%BF%E5%A0%82.png" class="" title="小丑在殿堂"><p>小丑在殿堂</p><span id="more"></span><p>[TOC]</p><h1 id="小丑在殿堂"><a href="#小丑在殿堂" class="headerlink" title="小丑在殿堂"></a>小丑在殿堂</h1><p>也许你愿意相信的不是真理，而是权威。</p><p>尊长尊老的核心内涵在于对生命的敬畏、对更有深度的生命赞同。</p><ul><li>小人君子论</li></ul><p>定义：<br>对弱者包容，对强者尊重。→君子<br>对弱者欺压，对强者谄媚。→小人</p><p>情况分析：<br>小人见君子 → 小人最优<br>小人见小人 → ？<br>君子见君子 → 君子最优<br>君子见小人 → 君子最差</p><p>由此可得：<br>小人的最优收益望 = 君子的最优收益期望<br>小人的最差收益望 &gt; 君子的最差收益期望</p><p>结论：<br>君子难得！！！</p><p>言语所能传达的，大部分是情绪语调，半小时的交谈，不如一纸文书。</p><p>沟通的前提是平等尊重，带有强烈主观情绪色彩的交谈是单方面强权的霸凌。</p><p>谈事和谈态度傻傻分不清，事情没谈明白开始扯态度，态度可以了又说事不行。</p><p>我想听的不是你要怎么样，我只想听我想听到的。</p><p>在强权背景下，催生了畸形的运转模式，权利和能力被画上了等号，组织中每个人都视而不见，久而久之，权利逐渐盖过了能力，对能力的质疑转变了对权力的质疑，同样的，会受到强权的打压。</p><ul><li>忽有清风化剑气，直斩二十少年意。</li></ul><p>在象牙塔里的时光，容易让一个人意淫，对社会意淫，对未来意淫。当剥去幻想的衣裳，看到的才是赤裸的羞耻。</p><ul><li>小丑在殿堂，大师在流浪。</li></ul><p>开始记录与2024年07月04日，纪念逐渐远离梦想的少年。</p><p>上位者心中只有任务，在上位者的严重，处于权利弱势地位的人不是人，而是工具，为达自己的利益，合理的分配工具资源，才是上位者考虑的事情。</p>]]></content>
    
    
    <summary type="html">小丑在殿堂</summary>
    
    
    
    <category term="日志" scheme="http://hibiscidai.com/categories/%E6%97%A5%E5%BF%97/"/>
    
    
    <category term="日志" scheme="http://hibiscidai.com/tags/%E6%97%A5%E5%BF%97/"/>
    
    <category term="小丑在殿堂" scheme="http://hibiscidai.com/tags/%E5%B0%8F%E4%B8%91%E5%9C%A8%E6%AE%BF%E5%A0%82/"/>
    
  </entry>
  
  <entry>
    <title>PyCharm挂载Linux服务器</title>
    <link href="http://hibiscidai.com/2024/06/25/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://hibiscidai.com/2024/06/25/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8/</id>
    <published>2024-06-25T00:00:00.000Z</published>
    <updated>2024-06-26T06:04:24.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/06/25/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8.png" class="" title="PyCharm挂载Linux服务器"><p>PyCharm挂载Linux服务器</p><span id="more"></span><p>[TOC]</p><h1 id="PyCharm挂载Linux服务器"><a href="#PyCharm挂载Linux服务器" class="headerlink" title="PyCharm挂载Linux服务器"></a>PyCharm挂载Linux服务器</h1><blockquote><p>PyCharm社区版本无法实现ssh连接服务器。</p></blockquote><p><a href="https://www.jetbrains.com/help/pycharm/2024.1/remote-development-starting-page.html">PyCharm官网教程：从 PyCharm 连接到远程服务器</a></p><h2 id="服务器安装Anaconda"><a href="#服务器安装Anaconda" class="headerlink" title="服务器安装Anaconda"></a>服务器安装Anaconda</h2><h3 id="查看PC的Anaconda版本"><a href="#查看PC的Anaconda版本" class="headerlink" title="查看PC的Anaconda版本"></a>查看PC的Anaconda版本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Windows PowerShell</span><br><span class="line">版权所有（C） Microsoft Corporation。保留所有权利。</span><br><span class="line">PS C:\Users\windows11&gt; conda --version</span><br><span class="line">conda 23.1.0</span><br></pre></td></tr></table></figure><p>Conda has been updated to v23.1.0. This installer uses python-3.10.9.</p><h3 id="去官网下载Anaconda"><a href="#去官网下载Anaconda" class="headerlink" title="去官网下载Anaconda"></a>去官网下载Anaconda</h3><p>选择 Anaconda Distribution 安装程序。</p><p>下载<code>Anaconda3-2024.02-1-Linux-x86_64.sh</code>，对应python版本3.11。</p><h3 id="上传安装文件"><a href="#上传安装文件" class="headerlink" title="上传安装文件"></a>上传安装文件</h3><p><strong>用自己账号登录服务器</strong>，上传至服务器，自己用户的根目录中。</p><h3 id="执行安装文件"><a href="#执行安装文件" class="headerlink" title="执行安装文件"></a>执行安装文件</h3><p>在根目录中执行，运行安装：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh Anaconda3-2024.02-1-Linux-x86_64.sh</span><br></pre></td></tr></table></figure></p><h3 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><p>在空白处添加以下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在本用户权限下运行，不会影响其他用户的环境</span></span><br><span class="line">alias Python=&#x27;/home/USERNAME/anaconda3/bin&#x27;   </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这里写anaconda的安装路径</span></span><br><span class="line">export PATH=&quot;/home/USERNAME/anaconda3/bin:$PATH&quot;</span><br></pre></td></tr></table></figure></p><p>加载配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="添加国内conda源"><a href="#添加国内conda源" class="headerlink" title="添加国内conda源"></a>添加国内conda源</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br></pre></td></tr></table></figure><h3 id="conda版本版本更换"><a href="#conda版本版本更换" class="headerlink" title="conda版本版本更换"></a>conda版本版本更换</h3><p>保证和本地版本相同，进行conda版本切换。<br>注意查看对应的python版本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install -n base conda=23.1.0</span><br><span class="line">conda install pyhton=3.7.4</span><br></pre></td></tr></table></figure><p>当然也可以保持现状，在创建conda环境时再进行python版本设置。</p><h2 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h2><h3 id="conda环境迁移"><a href="#conda环境迁移" class="headerlink" title="conda环境迁移"></a>conda环境迁移</h3><p>相同操作系统 的计算机之间复制环境，则可以生成 spec list。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list --explicit &gt; spec-list.txt</span><br></pre></td></tr></table></figure><p>在新机子重现环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name YOUR_ENV_NAME --file spec-list.txt</span><br></pre></td></tr></table></figure><h3 id="创建虚拟环境-1"><a href="#创建虚拟环境-1" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n YOUR_ENV_NAME python=3.7</span><br></pre></td></tr></table></figure><p>查看当前存在哪些虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></table></figure><p>激活虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate YOUR_ENV_NAME</span><br></pre></td></tr></table></figure><p>查看安装了哪些包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><blockquote><p>conda:matplotlib、pandas、numpy<br>pip:networkx、python_igraph、numba、scipy</p></blockquote><p>其他命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda update conda：               检查更新当前conda</span><br><span class="line">conda search XXX                   搜索包，查看可安装版本</span><br><span class="line">conda remove -n py36 --all         删除环境</span><br><span class="line">conda deactivate                   退出虚拟环境，conda4之前版本：source deactivate</span><br><span class="line"> </span><br><span class="line">pip install --upgrade &lt;包的名字&gt;     更新包</span><br><span class="line">pip install python_igraph          （import igraph包）</span><br></pre></td></tr></table></figure><h2 id="配置PyCharm"><a href="#配置PyCharm" class="headerlink" title="配置PyCharm"></a>配置PyCharm</h2><p>打开python项目→文件→设置→项目：xxx→python解释器→添加解释器→SSH</p><p>输入主机ip、端口、用户名</p><p>选择<code>Virtualenv环境</code>，如果识别conda环境可以直接选择conda。<br><code>系统解释器</code>面向root用户，属于全局，不方便使用。</p><p>解释器路径：<code>/home/USERNAME/anaconda3/envs/YOUR_ENV_NAME/bin/python</code><br>同步文件夹：<code>home/USERNAME/data/USERNAME/PROJECTNAME</code></p><img src="/2024/06/25/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8/PyCharm%E9%85%8D%E7%BD%AESSH%E7%8E%AF%E5%A2%83.png" class="" title="PyCharm配置SSH环境"><p>工具→部署，可以调整文件夹同步设置</p><h2 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h2><p>运行代码测试</p><h2 id="直接在服务器运行代码"><a href="#直接在服务器运行代码" class="headerlink" title="直接在服务器运行代码"></a>直接在服务器运行代码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source activate YOUR_ENV_NAME</span><br><span class="line"></span><br><span class="line">nohup python3 -u my.py &gt;&gt; my.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">jobs -l      （当前终端查看全部进程）</span><br><span class="line">ps -aux|grep 进程号  （全局各个新终端查看指定进程）</span><br><span class="line">ps -ef         （全局全部进程）</span><br><span class="line">kill -STOP 进程号（命令可以直接暂停一个后台任务）</span><br><span class="line">kill -CONT 进程号（命令可以直接恢复一个后台任务）</span><br><span class="line">kill -9 进程号     （杀死进程）</span><br></pre></td></tr></table></figure><h2 id="JetBrains-Gateway"><a href="#JetBrains-Gateway" class="headerlink" title="JetBrains Gateway"></a>JetBrains Gateway</h2><p>JetBrains Gateway 是一个轻量级启动器，它将远程服务器与您的本地机器连接起来，在后端下载必要的组件，并在JetBrains Client中打开您的项目。</p><p>您可以将 JetBrains Gateway 用作独立启动器或作为 IDE 的入口点来连接到远程服务器。</p><p>默认情况下，下载的 PyCharm 位于远程服务器上的以下文件夹中：<code>~ /.cache /JetBrains /RemoteDev /dist</code>。</p><p>文件→远程开发→SSH</p><img src="/2024/06/25/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8/JetBrainsGateway_1.png" class="" title="JetBrainsGateway_1"><p>选择对应的pycharm版本<br>会自动列出目前和本机相同的版本和最新版本</p><img src="/2024/06/25/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8/JetBrainsGateway_2.png" class="" title="JetBrainsGateway_2"><p>选择项目目录</p><img src="/2024/06/25/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8/JetBrainsGateway_3.png" class="" title="JetBrainsGateway_3"><p>使用gateway开发</p><img src="/2024/06/25/PyCharm%E6%8C%82%E8%BD%BDLinux%E6%9C%8D%E5%8A%A1%E5%99%A8/JetBrainsGateway_4.png" class="" title="JetBrainsGateway_4"><p>方法A：<br>代码在本地，运行环境在服务器。<br>本地安装pycharm，服务器不安装pyachrm。<br>代码运行通过本地上传至服务器，然后再服务器运行环境下返回运行结果。<br>用户实际操作的是本地pycharm。<br>关闭pycharm后代码中断。</p><p>方法B：<br>代码在服务器，运行环境在服务器。<br>本地安装pycharm，服务器不安装pyachrm。<br>代码直接运行在服务器上。用户可以用本地pycharm操作。<br>用户实际操作的是服务器pycharm。<br>关闭pycharm后代码可以选择不中断，可以选择在服务器后台跑。</p><p>可以先用方法A测试运行，上传部署代码后，使用方法B，可以直接在远程上打开IDE运行测试代码，适合有长时间跑数据的需求。</p><p>方法B中开发环境有些功能会有一些bug，尽量选择全新的IDE版本。</p>]]></content>
    
    
    <summary type="html">PyCharm挂载Linux服务器</summary>
    
    
    
    <category term="Linux" scheme="http://hibiscidai.com/categories/Linux/"/>
    
    
    <category term="Python" scheme="http://hibiscidai.com/tags/Python/"/>
    
    <category term="IDE" scheme="http://hibiscidai.com/tags/IDE/"/>
    
    <category term="Linux" scheme="http://hibiscidai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>核磁共振测井</title>
    <link href="http://hibiscidai.com/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/"/>
    <id>http://hibiscidai.com/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/</id>
    <published>2024-05-14T03:00:00.000Z</published>
    <updated>2024-07-17T12:24:27.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95.png" class="" title="核磁共振测井"><p>核磁共振测井</p><span id="more"></span><p>[TOC]</p><h1 id="核磁共振测井"><a href="#核磁共振测井" class="headerlink" title="核磁共振测井"></a>核磁共振测井</h1><p>用作本人速查</p><h1 id="核磁共振测井物理基础"><a href="#核磁共振测井物理基础" class="headerlink" title="核磁共振测井物理基础"></a>核磁共振测井物理基础</h1><p>核磁共振是量子力学最经典的一个实例。</p><p>核磁共振原理经典六张图。</p><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E5%8E%9F%E7%90%86%E7%BB%8F%E5%85%B8%E5%85%AD%E5%BC%A0%E5%9B%BE-1.png" class="" title="核磁共振测井"><p>很多原子核都可以做核磁共振，但是测井的重点在于氢核，氢核本身在高速运动。<br>氢核本身在做高速运动的时候像一个石头一样有南北极。<br>对于一个原子核来说是有磁性的，但是当很多氢原子核在一起的时候，如果没有外加磁场，每一个氢核都是随机取向的，所以没有什么磁性。</p><p>单独的氢核，快速运动，具有一定的磁性，产生一个N极和S极。<br>当氢核放在外加磁场中，外加磁场强度是B0，氢核会围绕外加磁场方向轴转动，称为进动。进动频率为拉莫尔频率。</p><script type="math/tex; mode=display">f = \frac{\gamma}{2 \pi} B_0</script><script type="math/tex; mode=display">\gamma $$ 叫旋磁比，是原子核的一个性质，核素定了，数值就定了。$$ B_0 $$ 是外加磁场强度。当磁场强度确定的时候，进动频率确定。在同一个磁场强度里面，不同的原子核因为它有不同的旋磁比，所以他们也会有不同的进动频率。在不同的磁场强度里面，那么对同一个原子核，也会不同的进动频率。单个自旋是理想状态，通常任何时候都是多个自旋。在B0磁场内，进动是量子化的，不是一个有很多种连续这个进度频率状态存在的，而是对于氢原子核来说，它有一个进动是沿着B0方向向，还有一种进动是逆着B0方向。那么对应的就是两个态，一个是高能态，一个是低能态，高能态的是逆着B0方向的，低能态顺着B0方向。在磁场里面当多个自旋的时候，有一个能级的分裂。没有外加磁场只有一个能级，加了外加磁场能级就分开了。氢原子核有两个能级。低能态的原子核数量会稍多于高能态的原子核数量。恰恰是因为这两个人群之间有原子核的数量有稍微的差异，所以就能够去做核磁共振。最终原子核数量的差异会产生一个宏观磁化量M0，可测量。不仅是可以测量它，而且可以测量它的变化过程。M0跟这个体系里面的氢原子核的数量成正比，如果能够把M0测出来，那么实际上通过标定，就可以把这个样品里面的原子核的数量测量出来。通过标定以后，实际上可以把孔隙度测量出来。M0它不是一下就形成的，它会有个过程，一个磁化的过程里面，它服从这样一个指数的这样一个规律，就这个原子核这么多原子核放在B0磁场里面的时候，M慢慢长，长的过程叫做纵向弛豫的过程，或者说实际上它是一个磁化的过程，极化的过程。Mt慢慢随着时间的增加是这样的长涨着涨到了一个平衡态，这个平衡态就是最终M0，完全极化。$$ M_t = M_0 * (1 - e^{\frac{-t}{T_1}})</script><p>增长的速率就是用这个时间常数，T1表示，纵向弛豫时间。</p><p>单纯从磁化量变化或者磁化的过程来看，来定义它，但实际上其一它还有更多的物理含义。<br>物理上定义为自旋-晶格弛豫时间。</p><p>我怎么样才能够磁化，让它充分磁化？充分磁化，因为它是一个无限接近的过程。<br>怎么样在比较少的时间里面，就让它代表了充分磁化？<br>T1，后面要讲到孔隙介质T1的时候，它还对应了我们样品的很多的属性，很多的特性，可以做很多的应用。</p><p>在跟B0磁场垂直的方向，加一个B1磁场。<br>B1磁场的频率刚好等于拉摩尔进动频率，此时会发生共振。核加了这样一个磁，核磁，然后再加一个磁场，共振。</p><p>共振：共振的情况就是低能态的原子核会从电磁场里面吸收能量，当拉莫尔频率进动的频率和外加磁场的频率相等的时候，低磁低能级的低能态的这个自旋会从这个磁场里面吸收能量，跃迁到高能态。</p><p>从量子力学的角度来说，它量子跃迁了第一轮胎的这个自旋发生了，量子跃迁到高能上去了。<br>从经典力学的角度来描述，需要一个很重要的概念叫坐标变换，没有这个坐标变换，我们很难来理解很多事儿。<br>目前我们讨论那么进度评论这些都是在叫实验室坐标系里面，实验室坐标系，就是我们现在坐在这里，所以我们能够看到它高速的进洞，看到它自己去在自选看这两个现象。<br>但是磁场加了以后，你要理解后续发生的行为，要做一个坐标系的变换，要在旋转坐标系里面来看待问题。<br>我们自己要跟着进动的自旋跑步跑，跟着进动自旋跑，就是在自旋，围绕着 B0磁场，我们现在要跟着它一起跑，那就叫旋转坐标系，我们是在旋转的，跟着进动频率在旋转。<br>我们加了电磁波加的 b1磁场，它是个脉冲磁场。早期工作的时候是叫连续波，连续波的时候它不是一个脉冲的，但是现在所有的核磁共振技术是脉冲核磁共振技术。<br>脉冲里面有一个宽度脉冲的宽度，然后有一个幅度，然后里面有一个调制频率，调制频率就是拉莫尔的进动频率。<br>所以脉冲电磁波它有三个要素，调制频率，幅度，长度。<br>你加上脉冲以后，如果我们在旋转坐标系里面来看，就有一个很重要的现象。<br>你就发现原来在旋转坐标系里面，我们随着这个来旋转的话，那么原来宏观磁化量是m，然后在旋转坐标系下，加上脉冲后，实际上等效于把m要搬转翻转。翻转到一个角度：<script type="math/tex">\Theta</script></p><script type="math/tex; mode=display">\Theta = \gamma B_1 \tau</script><p>前提条件是脉冲的调制频率等于进动频率，或者等于旋转坐标系的频率，否则它不会共振，否则也看不到板转的过程。</p><p>假设这个条件满足的情况下，那么 M就被扳转一个<script type="math/tex">\Theta</script>角，这个<script type="math/tex">\Theta</script>角完全由脉冲它的高度和它的长度来定义。如果希望把 M翻转到XY平面上来，实际上就是翻转90度，我们把脉冲叫90度脉冲。如果要搬转到180度，也就是从这个方向搬到另外一个方向，我们就把板转角叫做180度脉冲。</p><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E5%8E%9F%E7%90%86%E7%BB%8F%E5%85%B8%E5%85%AD%E5%BC%A0%E5%9B%BE-2.png" class="" title="核磁共振测井"><p>本来是 M0是这个方向的，然后我们加一个90度脉冲意味着什么呢？这个方向的磁化量，我们把它搬走了，XY平面来，90的脉冲激发完了以后，如果我们用线圈去接收的话，你可以接收到一个信号，这个信号就是我们把这个FID叫自由感应衰减。</p><p>这个FID信号是因为我们把它搬走了，XY平面以后，它会散向。在旋转这个系里面，我们把它叫做相位散掉了。相位的分散，实际上对应的是实验室坐标系里面，它的频率不一样。<br>它的频率为什么不一样？因为它的磁场不是完全均匀的，磁场不是完全均匀，所以拉莫尔频率不是完全一样，它就会分散。在我们的旋转坐标系里面，就是它们的相位不一样，那么相位不一样。<br>所以慢慢的随着时间推移，它慢慢散相散相，散相了以后就变成了各个方向都有了，所以在XY平面上，平均值慢慢就归零了，这个过程我们也把它叫做横向弛豫过程。</p><p>FID在理想条件下，我们也是可以把它用一个方程来表达出来的。</p><script type="math/tex; mode=display">M_t = M_0 * e^{\frac {-t} {T_2 ^ * } }</script><p>T2称为横向弛豫时间，<script type="math/tex">T_ 2 ^ *</script> 是理想状态的，<script type="math/tex">T_ 2</script> 是实际测量到的。</p><p>因为这里面可能有磁场的不均匀性等等问题，所以 <script type="math/tex">T_ 2 ^ *</script> 跟我们样品本身的T2可能会有区别，甚至会区别比较大。但是从它的弛豫机制来说，我们把这个过程叫横向弛豫的过程。有一个横向弛豫的速率，叫T2，横向弛豫时间。</p><p>但是由于磁场本身的不均匀性，使得拉莫尔频率不一样，使它的相位在实验室在旋转坐标系里面，它不容易重聚，所以这样得到的T2不是我们本真的T2，而是T2和磁场的非均匀性的一个共同作用的结果。</p><p>用FID是测不到真实的T2的，如何测量到真实的T2？这个时候就提出了一个叫CPMG，叫自旋回波的方法，是测量横向弛豫时间的方法。</p><p>最简单的一个自旋回波的脉冲序列，它包括什么呢？包括一个90度脉冲，把它磁化量翻转90度到xy平面，到xy平面以后，由于磁场的非均匀性，它会散相。这个时候我们再加一个所谓的180的脉冲。<br>一个90的脉冲，然后我们再过一段时间加一个180的脉冲。加一个180的脉冲，然后再等待相同的时间，会产生一个所谓的回波，叫自旋回波。自旋回波就成为我们真实的实验的一个方法，包括我们岩心分析和下面的测井，都来用自旋回波。</p><p>而且自旋回波不光是可以测一个，它还可以测很多一个序列，一个系列，这个系列我们把它叫自旋回波串。<br>就是你在90度脉冲，一个FID，然后我们加一个180的脉冲，形成一个回波，再加一个180的脉冲，再形成回波，再加再形成。你不断的加180的脉冲，它会有一串回波，而这一串回波它是按照我们的横向弛豫时间为时间常数来衰减的，所以我们通过这个方法就得到了所谓的横向弛豫时间的测量值。</p><p>在旋转坐标系里面去理解90脉冲，180脉冲，所谓的脉冲扳转角。<br>我们过去的很多的测量是一个fid信号，甚至我们在很多的化学谱里面，他就测fid信号。<br>磁场非常的均匀条件下，就用这个FID来解析它的谱学的信息。<br>通谱学的信息来获得分子的结构，分子的动力学等等很多的应用。</p><p>很多脉冲序列来做所谓的量子调控，做很多所谓的极化转移。就是它可以通过一大堆的这些脉冲序列对量子调控，然后让分子发生变化，然后来获取分子里面的信息，来实现对分子的表征，对分子结构的解析。高场方向发展。</p><p>我们这里考虑了磁场的非均匀性，因为高场的时候希望磁场越均匀越好，但是我们面对低场，面对我们的孔隙介质，你是不可能做到均匀的。我们也不去用这个FID了，我们来做弛豫的测量。在非均匀磁场的条件下，怎么样把它的持续时间测准。</p><p>怎么产生回波？</p><p>90度脉冲以后，我们仍然在旋转坐标系里面来看的话，那就是相位的问题。因为我们知道这一个磁化量，它实际上是很多单个的自旋磁化量的一个叠加。那么单个的自旋它可能有我们考虑到磁场的非均匀性，它可能是由于我们外加的B0磁场不均匀，也可能是由于我们的样品本身里面产生了一个背景梯度磁场，不管什么原因，就导致了每一个自选所经历的磁场强度可能是不一样的。</p><p>磁场强度不一样，拉莫尔频率不一样，旋转坐标系里面的旋转频率不一样，旋转坐标系里面的相位不一样。</p><p>所以在这个里面我们就看到一部分磁场，一部分自旋，它会往这个方向跑，跑的比这个B0磁场的拉莫尔频率还快，还有一部分可能跟不上，所以你在前面旋转坐标系里面来看，你就比方说我们有三个人的跑步，如果我的速度刚好是正中间的话，那么有一个人跑得比较快的，有一个人跑得比较慢的，我在我看来那个人跑得快的就往那边去跑的，慢的就往后再往回再往那边跑，跑得快的再往前面跑，跑的慢的在后面跑，如果以我作为参照系的话。那么这里也就是一样，如果我们以B0磁场，这个H核和在B0磁场对应的拉莫尔频率作为参照，作为参照系的话，那么一部分磁场不均匀的比较大的一个信号，它就相当于在这个里面往前跑，而一部分磁场强度比较小的相当于往后跑，慢慢就分散了。</p><p>如果我们只考虑两个的话，实际上很多的那么这个过程我们把它叫做散相。那么这个项目的分散了以后，当然慢慢算就没有了，所以成了这个FID。<br>但是当我们再加一个180的脉冲，前面我们讲加一个180的脉冲，就意味着它做180度的翻转。</p><p>180板转有很多可能性，一种就是说我做180度的翻转，实际上把跑得快的翻转的跑的慢的地方去了，把跑得比较慢的搬走的跑得快的地方去，也就是说一般是相当于我们跑步，我们三个人在跑步，然后一个指令下来，全部向后跑，全部向后跑的结果什么我就往那边跑了是吧？原来跑得慢的那个人，他反而变得朝着我的方向跑，跑得比较快的人反而落后于我了，但他也要向着我来跑，原来跑的快的，慢慢的，因为他经历的路径多，他就往那边跑了，然后跑得慢的他就跑得快的往这边。最后在同样的时间里面，他们跑到这个地方来就会重聚，在重聚在一块的结果，我们就会通过我们的线圈就观测了一个很大的信号叫回波信号，回波就是这样来形成的。</p><p>所以这里就有一个旋转坐标系里面来分析脉冲的作用和回波的形成。那么然后180度脉冲以后，到这里来重聚，然后重聚了以后你把回波采集到了，过一段采集完了以后，然后他又要分散了，又要反散相，散相了以后，那么你再来一个180度，又测量了一下，不断地散相、重聚、散相、重聚、散相、重聚。</p><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E5%8E%9F%E7%90%86%E7%BB%8F%E5%85%B8%E5%85%AD%E5%BC%A0%E5%9B%BE-3.png" class="" title="核磁共振测井"><p>我们统一来看：首先我们把样品放到外加磁场里面，放到外加磁场里面，那么B0所以它慢慢的就遵守纵向弛豫的过程，进行极化，达到M0，有了一个宏观磁化矢量，可观测量。然后我们用个脉冲这个序列加一个脉冲射频场，脉冲射频的有90度，一串180度，那么如果这个时候我没有测量的话，你就发现90的脉冲以后有FID，然后在180的脉冲以后就有一串的回波的信号，回波串。然后我们再来，测完了以后我们再来这个过程，又是磁化，又是测量，所以你最后看到的仪器给你显示的就是这样一个B0磁场，然后磁化，然后有一个脉冲射频场，然后再采集，然后再这样循环下去。</p><p>T2这个信号是逐渐衰减的，第一个180度后，第一个回波串信号幅度是大的，它逐渐衰减。其实在过程中一个就是一个相散和聚相的问题，聚相信号最大化，为什么第二个回波串它的能量要减小了？<br>T2本质上叫自旋-自旋弛豫时间，实际上它反映的是两个自旋之间或者自旋-自旋之间的很多的这种相互作用，这种相互作用它除了聚相的过程，除了相位重聚的过程以外，它本身由于自旋和自旋的相互作用，它会消耗能量，所以它是一个衰减的过程。它最后要恢复到没有了，在轴上它慢慢就没有了。<br>所以，T2是一个样品的重要的物理性质，它是一个它的物理特性，需要我们只是把T2通过这个方式把它测量出来。这个方式它之所以这个衰减，它是由它的物理性质来决定的，是自旋-自旋相互作用消耗能量来决定的，正是它我们想要研究它的一个特性。<br>相当于每一次聚相和散相他自身的能量已经衰减。</p><p>等待时间是不是就相当于必须要恢复最大的M0？</p><script type="math/tex; mode=display">T_w = 3 * T_1 → 95% M_0</script><script type="math/tex; mode=display">T_w = 5 * T_1 → 99.7% M_0</script><p>磁场的非均匀性：B0的非均匀性 + 介质本身的非均匀性。在B0的作用下，岩石的多孔介质本身会产生一个背景梯度磁场，而且你B0越大，B0背景梯度磁场就越大。</p><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E5%8E%9F%E7%90%86%E7%BB%8F%E5%85%B8%E5%85%AD%E5%BC%A0%E5%9B%BE-4.png" class="" title="核磁共振测井"><p>多孔介质中的问题：<br>第一：液体那么到了孔隙里面的时候，这是孔隙的表面，它就会形成一个在表面上面形成一层膜，行非常重要。水流液体的分子，它本身会快速的扩散。里面的水中心部位的水，它的水分就快速的扩散。那么到了瓶子的表面的时候，这个界面上面就表面，那么它会形成一个这样的简单的膜，这个膜不是简单的膜，这个膜可能很复杂，这个膜的科学表面的科学无处不在。<br>1、表面是什么？表面还有很多胶接物，对吧？那么它有大量的这种非成对的电子，很多的非水性类的电子，这些非成对的电子的存在就导致了它会跟你相互作用，甚至会发生化学反应。<br>2、表面润湿性问题？<br>3、表面粗糙程度不同？表面的粗糙度可能也不一样，海岸线一样的，很粗糙不光滑的。因为在扩散的过程，在中间扩散的时候，它是布朗运动描述，可是一旦到定界面相接触的时候，他就会慢下来，碰撞淹没。所以这里面就有一个过渡带在这个地方，我们叫分子的表面层。</p><p>自由状态的液体，它有一个本身的T2，也有一个本身T1，但是你一旦液体放在孔隙里面，饱和到孔隙里面以后，它所测量出来的T2、T1，就跟原来完全不一样了。<br>在孔隙介质里面以后的T1、T2与饱和在孔隙介质本身的T1、T2很不一样。</p><script type="math/tex; mode=display">\frac{1}{T_1} = \frac{1}{T_{1 bulk}} + \rho_1 \frac{S}{V}</script><script type="math/tex; mode=display">\frac{1}{T_2} = \frac{1}{T_{2 bulk}} + \rho_2 \frac{S}{V} + \frac{ D (\gamma G T_E)^2 }{12}</script><p>T1不仅和自身有关，还跟比表面(S/V)有关，纵向弛豫强度(ρ1)有关<br>T2不仅与自身有关，还跟比表面(S/V)有关，横向弛豫强度(ρ1)有关，还有扩散项有关。</p><p>又有别的扩散弛豫理论，孔隙介质里面的弛豫跟孔隙的直径和扩散系数的比值又有关系。在这个条件它会分分快扩散区域，慢扩散区域，中等扩散区域。</p><p>如果说在常规里面，那几个假设可能还存在的话，成立的话，那么在页岩油气以及非常规里面，那几个假设可能就不成立了，假设条件不成立了，所以还能不能用？</p><p>应用1：孔径测量</p><p>当测量的T1和T2跟孔径对应的时候，T1忽略了自由弛豫，T2忽略了自由弛豫和扩散弛豫，才有了这样一个持续时间和孔径分布的一个对应关系，或者说一个相关性。因为这个量的量纲就是孔径，那么这样才有了这样一个定性的这样一个关系。<br>当然也可以定量，定量就是假设你知道这个的情况下，知道它的纵向持续强度和横向持续强度，你就可以来做定量。</p><p>那么如果成立这个时候，我们才有可能这样，一个是多指数，一个是统计，我们可以做孔径上的标定。<br>那么我可以说在常规游戏的时候，这个没有太多的问题，但是在页岩油气的时候这个一定有问题。我们可能没有这样的信噪比和分辨率，去表证页岩的孔径。</p><p>应用2：T2分布</p><p>在第一个应用假设存在的情况下，T2的大小跟孔径有了对应关系，所以需要分布可以把它标定成孔径的分布。一个简单的物理的原理就是说小孔隙流体不容易流动，大孔隙的流体容易流动。又假设孔隙与孔隙之间是连通的，注意死孔不是这样不存在，假设孔隙孔隙是连通的条件下，那么大孔是可流动的，小孔是没法流动的。那些就是碎屑岩，分选的比较好，并且也胶结作用不是很大的一些条件下面，这种状态他才成立这个东西。</p><p>那么如果存在这个的话，我们说我们在物理上就可以说这一部分是可动，另一部分是不可动。</p><p>那么既然在概念上有这两个，我们就想象可能有一个截止值。它首先截止值并不是它的T2截止值，这个截止值首先从物理上来说，它应该是一个孔径的截止值，当我们把孔径分布上面，一部分是束缚的，一部分是可动的，分开的时候，我们假设有一个孔径的介质，由于核磁的T2分布能够标定为孔径分布，所以我们对应的就有一个T2的截止值，这是有点野蛮粗暴，但是管用。它简单它管用，因为测井里面经常会遇到特别多的影响因素，你要每个都考虑的话，你是考虑不清楚的，那就抓主要的东西，管用。<br>T2截止值是表象，孔径截止值是本质。</p><p>所以我们现在假设孔径都是这种一个颗粒这样堆积起来的，那么它的T2分布就是代表一个孔径的分布。<br>对核磁来说，应该它的反应孔径的覆盖范围是目前所有的技术里面比较比较大的范围，而且是一个比较准的范围，而且不损害样品。</p><p>那么假设成立的情况下，这个时候我们就可以通过实验的方法来获得一个确定值，用来确定它的流体的赋存状态用，来获取我们非常重要的束缚水信息。所以一直就发展了一套标准实验流程和方法。</p><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E5%8E%9F%E7%90%86%E7%BB%8F%E5%85%B8%E5%85%AD%E5%BC%A0%E5%9B%BE-5.png" class="" title="核磁共振测井"><p>有标准的流程，也有假设。<br>标准的流程：<br>第一步，把样品100%饱和水的时候，做一个核磁共振CPMG测量，然后做一个反演，得到它的T2分布，然后对应的相当于一个孔径的分布。<br>第二步，我们把这个岩样脱水，注意用离心机脱水，当然你也可以用别的方法脱水，然后脱完水以后，把那些自由水或者自由流体给它离心掉了，或者给它脱掉了，我们认为这个样品里面剩下来的都是束缚水。然后重复测量。<br>注意这是一个非常重要的概念，我们认为。谁认为？做实验的人认为。这个说法对不对？对也不对，这个自由裁定权太大，问题出在这儿。<br>第三步，两个放在一块，然后来求一个100%饱和的时候的一个孔隙度，然后只有束缚水时候一个记录束缚水孔隙度。自由水和束缚水相交的地方就是所谓的T2截止值，代表的是孔径截止值。</p><p>所以它有标准的规范流程，可是有一点它没有规范流程，你认为他是舒服水。</p><p>关键是你怎么去控制它，我认为这个里面就是舒服水？<br>那离心机的离心的速度，它可能会设置不一样，高了就把你们本来就束缚的也把它离心出来了，低了他还有可能所谓的在自由水，它也变成了在束缚水里面的。</p><p>斯伦贝谢提出相对截止值，放弃绝对的截止值。</p><p>测井的渗透率计算他是个估算的问题，是吧？我们并不直接的测量，我们是通过一些间接的量去估算的渗透率，那么估算的过程中间，它就会有很多的影响因素在这里，这些影响因素决定了我们究竟得到一个什么样的渗透率。</p><p>孔隙度、束缚水、渗透率是岩石物理特性，但是在流体的时候是另外的概念。流体储存的空间以及流体本身的性质和每一种流体的含量，这些对我们来说当然非常重要。</p><p>在单个孔里面，如果流体它们是混相的话，你是分不出来的，你是分不出来几个峰的，他们就是一个峰混在一块的。但是当我们中间有油，比方说我们假设中间可能也是个油珠，在哪个地方也可能跟他有认识性关系的等等。这个时候因为油和水它们不稳，像是它如果是溶解油油的话，你可能也看不出来。当他们两个不混相的时候，在他们的界面就有一层膜，他们互相不能扩散，不能扩散的情况下，这个时候他们就是两个分开的弛豫机制起作用。这个时候我们测量回波算，你就可以看到这个峰代表的是界面上的弛豫，这个峰是代表水包油的弛豫。所以这个时候就有了这样一个你看很明显的一个一个区分。</p><p>更进一步，假设这是股价，这个是骨架，这是孔隙是吧？孔隙里面有一个油珠，随着油的减少，你看水的峰，油的峰都在发生相应的变化。</p><p>对这两个问题，第一个问题只是我们我们是可以去识别油气了，油水了，可以把它们分开了。<br>第二个怎么分开，它是一个怎么样的变化的过程？<br>你看是非常明确的一个物理的概念，物理的意义在这里面了，你可以看到要把它分析出来，这分析出来将来对我们应用是有用的。</p><p>孔隙表面峰随着孔隙表面的水的增加，你看它的T2在变大，它不光是峰值在变化，T2也在变化。因为它跟界面的相互作用在变化。</p><p>核磁不管是谱学还是成像还是弛豫，它最重要的一点就是说几乎所有的表象都能够对应原理，这是核磁一个分析测试手段受到大家欢迎和青睐的重要的原因之一。当我们说不清楚的时候，是因为我们对样品缺少相关的理论缺少认识。</p><p>在孔隙里面的时候，然后在孔隙里面的时候怎么来区分流体的赋存状态，然后怎么样去求取相应的参数，然后当流体里面有不同的流体的时候，那么它的弛豫测量值上面又有什么表现？这是我们应用的基础。</p><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E5%8E%9F%E7%90%86%E7%BB%8F%E5%85%B8%E5%85%AD%E5%BC%A0%E5%9B%BE-6.png" class="" title="核磁共振测井"><p>因为一维里面尽管好像有，但是通常情况下分不出来。更多的时候是分不出来的，叠在一块，叠在一块你就要用新技术是吧？用就要认识它的三维里面是什么样子，然后就要有发展二维的是吧？轻轻自由中等粘度的油稠油天然气分别通过二维怎么把它能够更好的区分开来是吧？</p><p>定域谱的这些技术，一个像素里面，那么它的t2分布是什么样子？</p><h2 id="来源《核磁共振测井原理与应用》书籍截图"><a href="#来源《核磁共振测井原理与应用》书籍截图" class="headerlink" title="来源《核磁共振测井原理与应用》书籍截图"></a>来源《核磁共振测井原理与应用》书籍截图</h2><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%A0%B8%E8%87%AA%E6%97%8B.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E5%8E%9F%E5%AD%90%E6%A0%B8%E8%BF%9B%E5%8A%A8.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E5%AE%8F%E8%A7%82%E7%A3%81%E5%8C%96%E7%9F%A2%E9%87%8F.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/T1%E5%BC%9B%E8%B1%AB.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E8%83%BD%E7%BA%A7%E8%B7%83%E8%BF%81.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E6%89%B3%E8%BD%AC%E8%A7%92.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E8%87%AA%E7%94%B1%E6%84%9F%E5%BA%94%E8%A1%B0%E5%87%8F%E4%BF%A1%E5%8F%B7.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E8%87%AA%E6%97%8B%E5%9B%9E%E6%B3%A2%E4%BF%A1%E5%8F%B7.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/CPMG%E8%84%89%E5%86%B2%E5%BA%8F%E5%88%97.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/T2%E5%BC%9B%E8%B1%AB.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/CPMG%E8%84%89%E5%86%B2%E5%BA%8F%E5%88%97-2.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E5%AD%94%E9%9A%99%E6%B5%81%E4%BD%93%E5%BC%9B%E8%B1%AB.png" class="" title="核磁共振测井"><img src="/2024/05/14/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E6%B5%8B%E4%BA%95/%E5%AD%94%E9%9A%99%E6%B5%81%E4%BD%93%E5%BC%9B%E8%B1%AB-2.png" class="" title="核磁共振测井"><h1 id="NMR数据采集"><a href="#NMR数据采集" class="headerlink" title="NMR数据采集"></a>NMR数据采集</h1><p>测井仪器除了探测FID和CPMG自旋回波信号，还需要将这些信号进行数字化、存储和处理。典型的CPMG自旋回波串通常有成百上千的回波，将整个回波曲线数字化难度太大，故而只需测量和存储自旋回波信号的峰值即可。信号的记录方式有两种：<code>单道检测法</code>和<code>正交检测法</code>。</p><p><code>单道检测法</code>：从样品来的射频信号送入单个的相敏检测器中以识别样品信号频率与射频脉冲载频的频率(即为测量值设定的平均拉莫尔频率)之间的差异。由于单个相敏检测器只能得到频率差异的大小，而不能确定相位差，所以在NMR测井中不能利用单道检测法。</p><p><code>正交检测法</code>：输入的采样信号送到两个同样的相敏检测器中，这两个检测器的参考信号相差90°，合成声频信号被放大，然后通过同样的低通滤波器，经多元模数转换器数字化并存储在单独的数据存储区中。正交傅里叶变换以与单道检测法相同的方式产生实数谱和虚数谱，并可区分出相对与射频载频的正频率和负频率。</p><p>为了消除振铃和基线偏移的影响，测井作业中总是成对采集T2的CPMG测量值，该方法称为交叉相位对(PAPs)。第一个回波串的采集利用常规设置，而在第二组中发射脉冲的相位改变了180°，从而使得自旋回波具有负振幅，这两组CPMG信号被称为正相位和负相位，结合形成PAPs，用以消除振铃和基线偏移的影响。</p><h2 id="相位旋转"><a href="#相位旋转" class="headerlink" title="相位旋转"></a>相位旋转</h2><p>记录数据的两个通道通常称为X通道和Y通道。这两个通道的信号表达式如下：</p><script type="math/tex; mode=display">X_{j}=S_{j}\cos \varphi+\varepsilon_{j}^{X}</script><script type="math/tex; mode=display">Y_j=S_j sin\varphi +\varepsilon_j^Y</script><p>式中，<script type="math/tex">X_j</script> 和 <script type="math/tex">Y_j</script> 分别是 X 和 Y 通道的第j个回波的幅度，<script type="math/tex">S_j</script>是回波的实际幅度，<script type="math/tex">\varphi</script>为相位角，<script type="math/tex">\varepsilon_{j}^{X}</script> 和 <script type="math/tex">\varepsilon_j^Y</script> 分别是 X 和 Y 通道的第 j 个回波的噪声。如果将回波串中的所有回波相加(回波个数为n)，可以得到：</p><script type="math/tex; mode=display">\sum_{j=1}^nX_j=\left[\sum_{j=1}^nS_j\right]cos\varphi+\left[\sum_{j=1}^n\varepsilon_j^X\right]</script><script type="math/tex; mode=display">\sum_{j=1}^nY_j=\left[\sum_{j=1}^nS_j\right]sin\varphi+\left[\sum_{j=1}^n\varepsilon_j^Y\right]</script><p>因为噪声是随机的，取正负值的概率相等。如果去平均噪声进行计算，上述方程等式右边的第二项将最终趋近于零。将两式相比，则可以得到相位角公式：</p><script type="math/tex; mode=display">\varphi=\arctan\left[\sum_{j=1}^{n}Y_{j}/\sum_{j=1}^{n}X_{j}\right]</script><p>通过上式得到相位角后，便可利用下面的等式进行坐标轴的旋转，从而得到信号道和噪声道。</p><script type="math/tex; mode=display">C_{signal}(j)=X_jcos\varphi+Y_jsin\varphi=S_j+(\varepsilon_j^Xcos\varphi+\varepsilon_j^Ysin\varphi)</script><script type="math/tex; mode=display">C_{noise}(j)=-X_jsin\varphi+Y_jcos\varphi=(-\varepsilon_j^Xsin\varphi+\varepsilon_j^Ycos\varphi)</script><h1 id="NMR数据处理"><a href="#NMR数据处理" class="headerlink" title="NMR数据处理"></a>NMR数据处理</h1><p>核磁共振测井技术是一种间接测量技术，它所采集到的原始数据是岩石孔隙中所含流体弛豫信号的叠加，即自旋回波信号，必须采用现代数学方法对回波信号进行反演得到T2谱才能进一步应用，并且反演结果直接影响后续储层参数计算和流体识别评价的准确性。</p><p>假设有n个回波，每个回波在时间 <script type="math/tex">t_i</script> 处的幅度测量值为 <script type="math/tex">g_i</script>，并假设有m个预先选择的驰豫时间 <script type="math/tex">T_j</script> 在对数刻度上等间隔分布。回波幅度 <script type="math/tex">g_i</script> 为  <script type="math/tex">t_i</script> 时刻测量的系统磁化强度 <script type="math/tex">M(t_i)</script> ，并经由t=0时的磁化强度 <script type="math/tex">M_0</script> 的归一化。回波测量值 <script type="math/tex">g_i</script> 的方程表达式如下：</p><script type="math/tex; mode=display">g_i=\frac{M(t_i)}{M_0}=\sum_{j=1}^ma_jf_je^{-t_i/T_j}+\varepsilon_i</script><p>其中，i=1,…,n。<script type="math/tex">f_j</script>是T2驰豫时间为 <script type="math/tex">T_j</script> 的孔隙度的部分孔隙度，在NMR测井中，回波幅度往往采用孔隙度单位(p.u.)。<script type="math/tex">a_j</script>为极化因子，它的方程如下：</p><script type="math/tex; mode=display">a_{j}=1-e^{-TW/T_{1j}}</script><p>通常等待时间很长，往往是T1驰豫时间的三倍，在这种情况下，因子<script type="math/tex">a_j</script>j为1。</p><p>因为所有的<script type="math/tex">e^{-t_i/T_j}</script>都已知，求解<script type="math/tex">f_j</script>就是一个线性反演问题。可以用最小二乘法拟合来使下面的求和最小化，即：</p><script type="math/tex; mode=display">\min\left\{\varphi(f)=\sum_{i=1}^{n}\frac{1}{\delta_{i}^{2}}\left[\sum_{j=1}^{m}f_{j}e^{-t_{i}/T_{j}}-g_{i}\right]\right\}</script><p>其中，<script type="math/tex">δ_i</script> 是第i个回波 <script type="math/tex">g_i</script>的测量误差。</p><h2 id="多指数反演算法"><a href="#多指数反演算法" class="headerlink" title="多指数反演算法"></a>多指数反演算法</h2><p>一维核磁共振的一般响应方程为：</p><script type="math/tex; mode=display">\mathrm{b(t)}=\int f(T_i)(c_1-c_2\cdot\exp{(-t/T_i)})dT_i+\varepsilon</script><p>其中，i=1,2。当i=1时表示T1信号，<script type="math/tex">c_1</script>=1，<script type="math/tex">c_2</script>=1表示饱和恢复法，若<script type="math/tex">c_1</script>=1，<script type="math/tex">c_2</script>=2表示反转恢复法；当i=2时表示T2信号，这时<script type="math/tex">c_1</script>=0，<script type="math/tex">c_2</script>=-1。</p><p>上式的离散形式为：</p><script type="math/tex; mode=display">b_k=\sum_{T_{i,min}}^{T_{i,max}}f(T_{i,j})\left[c_1-c_2\cdot exp^{-\frac{t_k}{T_{i,j}}}\right]+\varepsilon_k</script><p>其中，j=1,···n, n为预选的驰豫分量的个数；k=1,···m, m为回波个数，<script type="math/tex">t_k</script>为采集时间（通常为回波间隔的整数倍）；<script type="math/tex">b_k</script>为回波信号幅度；<script type="math/tex">T_{i,j}</script>为<script type="math/tex">T_i</script>预选的第 j个弛豫时间分量；<script type="math/tex">ε_k</script>为测量噪声；<script type="math/tex">f(T_{i,j})</script> 为弛豫时间<script type="math/tex">T_{i,j}</script>的幅度。</p><p>求解上述方程，实际上是解第一类Fredholm积分方程，这是一个非适定问题，即在误差允许的条件下，存在不同的<script type="math/tex">f(T_{i,j})</script>驰豫时间分布函数都能相当好地拟合原始回波衰减曲线。</p><h2 id="SVD算法"><a href="#SVD算法" class="headerlink" title="SVD算法"></a>SVD算法</h2><p>SVD算法基于如下分解定理：对任意的矩阵<script type="math/tex">A_{m×n}</script>，都可以分解为正交矩阵<script type="math/tex">U_{m×m}</script>，非负对角矩阵<script type="math/tex">W_{m×n}</script>以及正交矩阵<script type="math/tex">V_{n×n}</script>的转置的乘积，即</p><script type="math/tex; mode=display">A_{m\times n}=U_{m\times m}\cdot[\mathrm{diag}(w_j)]_{m\times n}\cdot V_{n\times n}^T</script><p>对角元素<script type="math/tex">w_1>w_2>···>w_m≥0</script>，w称为矩阵A的奇异值。</p><p>对于如下的多指数衰减T2模型，有</p><script type="math/tex; mode=display">y=M·f</script><p>其中<script type="math/tex">y=(y_1,y_2,···,y_n)^T</script>为测量的自旋回波衰减信号，<script type="math/tex">M=[m_{ij}]_{n×m}=[exp(-t_i/T_{2j})]_(n×m)；f=(f_1,f_2,···,f_n)^T</script>为驰豫时间<script type="math/tex">T_{2j}</script>对应的各点的幅度值，<script type="math/tex">T_{2j}</script>为预先指定的T2时间分布系列，典型的取法为在<script type="math/tex">(T_{2min},T_{2max})</script>区间内对数均匀的选取m个点，也可采用2的幂指数布点、线性均匀布点等方式。若矩阵的条件数为无穷大，则该矩阵奇异；若矩阵的条件数太大，即其倒数超出了机器的浮点精度，则称该矩阵为病态的矩阵。采用SVD分解法来求解上式，系数矩阵<script type="math/tex">M_{m×n}=U_{m×m}·[diag(w_j)]_{m×n}·V_{n×n}^T</script>，则上式最小二乘意义下的解为：</p><script type="math/tex; mode=display">f=V\cdot[diag(\frac{1}{\mathrm{w_{1}}},\frac{1}{\mathrm{w_{2}}},\cdots,\frac{SNR}{\mathrm{w_{1}}},0,\cdots,0)]\cdot(U^{T}\cdot\mathrm{y})</script><p>这里给出了矩阵条件数小于等于SNR的限制，避免了解的不确定性。其中SNR为从测量数据中估算出的信噪比。SNR定义为第一个回波的幅度值除以误差矢量r的标准差σ。</p><h2 id="T2谱非负限制性的实现"><a href="#T2谱非负限制性的实现" class="headerlink" title="T2谱非负限制性的实现"></a>T2谱非负限制性的实现</h2><p>按照如上SVD算法对矩阵M进行奇异值分解后，根据信噪比计算出截断值为<script type="math/tex">SNR/w_1</script> ，对分解得到的非负矩阵<script type="math/tex">W_{m×n}</script>求逆，因为对角元素按其角标增大而减小，故求逆后，对角元素随其角标增大而增大，寻找到恰好比截断值小的对角元素的角标i，对i其后的对角元素赋值为0，对角标为i的元素则进行重新赋值：</p><script type="math/tex; mode=display">W(i,i)=\frac{1}{w_{i}}*[(w_{i-1}-\frac{SNR}{w_{1}})/(w_{i-1}-\frac{1}{w_{i}})]</script><p>将重新赋值的对角矩阵 <script type="math/tex">W=diag(1/w_1 ,1/w_2 ,···,W(i,i),0,···,0)</script> 代入到f的式子中，若求得f的结果中存在负值，则记下负值的角标，并删除矩阵M中对应的列，删掉T2布点中对应的角标的值。将删列的矩阵M再进行SVD分解…，重复循环直到计算得到f中不再有负值存在。<br>缺点是会破坏T2分布的连续性，造成T2谱的畸形。</p><h2 id="BRD算法"><a href="#BRD算法" class="headerlink" title="BRD算法"></a>BRD算法</h2><p>首先给定如下的目标函数：</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^n[y_1-\sum_{j=1}^m(f_j\cdot m_{ij})]^2+\lambda\cdot\sum_{j=1}^mf_j^2=||y-Mf||^2+\lambda||f||^2</script><p>这里<script type="math/tex">M=[m_{ij}]=[exp^{(-t_i/T_2j )}]</script>，λ为平滑因子。</p><p>对幅度f=(f_1,f_1,···,f_m)^T的第k分量求极值并令其等于0，则有：</p><script type="math/tex; mode=display">\frac{\partial\chi^2}{\partial f_k}=-2\sum_{i=1}^n\left[y_i-\sum_{j=1}^mf_j\cdot m_{ij}\right]\cdot m_{ik}+2\lambda\cdot f_k=0</script><p>交换求和顺序，并移项整理，可得：</p><script type="math/tex; mode=display">\sum_{i=1}^m[f_j\cdot\sum_{j=1}^nm_{ik}\cdot m_{ij}]+\lambda\cdot f_k=\sum_{j=1}^mm_{ik}\cdot y_k</script><p>很容易验证，k=1,2, ···,m的m个等式组成的方程组满足：</p><script type="math/tex; mode=display">(M^T\mathrm{M})\cdot\mathrm{f}+\lambda I_{m\times m}\cdot f=M^T\cdot y</script><p>上式中<script type="math/tex">I_{m×m}</script>为m×m单位矩阵。我们对方程y=M·f 做如下线性变换，令</p><script type="math/tex; mode=display">f=\mathrm{M}^T\cdot c</script><p>未知变量<script type="math/tex">c=(c_1,c_1,···,c_n)^T</script>为n×1维的，而不是m×1，采用上述变换将m维T2域空间的解变换到n维时域空间来求解。则有：</p><script type="math/tex; mode=display">M^T\cdot(M^T\text{M}+\lambda I_{n\times n})\cdot\text{c}=M^T\cdot\text{y}</script><p>则原问题的解f可以通过求解方程：</p><script type="math/tex; mode=display">(M^T\text{M}+\lambda I_{n\times n})\cdot\text{c}=\text{y}</script><p>的解c，再通过线性变换<script type="math/tex">f=M^T·c</script>回代而获得，选择合适的λ，以保证矩<script type="math/tex">(M^T M+λI_{n×n} )·c</script>的可逆，则我们就可以很容易的求得方程的最小二乘解：</p><script type="math/tex; mode=display">f=M^T\cdot(\mathrm{M}^T\mathrm{M}+\lambda\mathrm{I}(\mathrm{n}\times\mathrm{n})\cdot)^{-1}\cdot y</script><p>比较理想的平滑因子为：</p><script type="math/tex; mode=display">\lambda=\frac{\sqrt n\cdot\sigma}{||c||}</script><h2 id="T2谱非负限制性的实现-1"><a href="#T2谱非负限制性的实现-1" class="headerlink" title="T2谱非负限制性的实现"></a>T2谱非负限制性的实现</h2><p>按照如上BRD算法对模平滑函数（惩罚函数）进行T2域空间变换到时域空间后，可得初始值<script type="math/tex">c_1=y/(M^T M+λI_{n×n})</script>，继而可得<script type="math/tex">c_k=M^T·c_1</script>，若<script type="math/tex">c_{ki}>0</script>则<script type="math/tex">f_i=c_{ki}</script>（若<script type="math/tex">c_{ki}≤0</script>则<script type="math/tex">f_i=0</script>），并将<script type="math/tex">c_{ki}>0</script>对应的矩阵M的相应列提取出来赋给新的矩阵A，然后用新矩阵A替代旧矩阵M，做以上循环求取新的<script type="math/tex">c_2</script>，<script type="math/tex">c_k</script>值。循环退出的条件有三：一，矩阵A为空即所有<script type="math/tex">c_k</script>均小于等于0；二，小于误差项即<script type="math/tex">c_2-c_1<1e-8</script>；三，超出规定的迭代次数。</p><p>基于以上迭代方法的惩罚函数有三类：模平滑、斜率平滑及曲率平滑。这些平滑方法中惩罚项的作用是压制未知函数f的振荡性。</p>]]></content>
    
    
    <summary type="html">核磁共振测井</summary>
    
    
    
    <category term="核磁共振" scheme="http://hibiscidai.com/categories/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF/"/>
    
    
    <category term="核磁共振原理" scheme="http://hibiscidai.com/tags/%E6%A0%B8%E7%A3%81%E5%85%B1%E6%8C%AF%E5%8E%9F%E7%90%86/"/>
    
    <category term="测井方法" scheme="http://hibiscidai.com/tags/%E6%B5%8B%E4%BA%95%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>人人共享的机器学习</title>
    <link href="http://hibiscidai.com/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://hibiscidai.com/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-12-19T01:00:00.000Z</published>
    <updated>2023-12-20T02:58:52.000Z</updated>
    
    <content type="html"><![CDATA[<img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.png" class="" title="人人共享的机器学习"><p>人人共享的机器学习</p><p>原文链接：<a href="https://vas3k.com/blog/machine_learning/">Machine Learning for Everyone</a></p><span id="more"></span><p>[TOC]</p><h1 id="人人共享的机器学习"><a href="#人人共享的机器学习" class="headerlink" title="人人共享的机器学习"></a>人人共享的机器学习</h1><p>Machine Learning is like sex in high school. Everyone is talking about it, a few know what to do, and only your teacher is doing it. If you ever tried to read articles about machine learning on the Internet, most likely you stumbled upon two types of them: thick academic trilogies filled with theorems (I couldn’t even get through half of one) or fishy fairytales about artificial intelligence, data-science magic, and jobs of the future.<br>机器学习就像高中时的性爱。每个人都在谈论它，少数人知道该做什么，只有你的老师在做。如果你试图在互联网上阅读关于机器学习的文章，很可能你偶然发现了两种类型的文章：充满定理的厚厚的学术三部曲（我甚至读不完一半），或者关于人工智能、数据科学魔法和未来工作的可疑童话。</p><p>I decided to write a post I’ve been wishing existed for a long time. A simple introduction for those who always wanted to understand machine learning. Only real-world problems, practical solutions, simple language, and no high-level theorems. One and for everyone. Whether you are a programmer or a manager.<br>我决定写一篇我一直希望存在已久的帖子。对于那些一直想了解机器学习的人来说，这是一个简单的介绍。只有现实世界的问题，实用的解决方案，简单的语言，没有高级定理。一个，为每个人。无论你是程序员还是经理。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1.jpg" class="" title="人人共享的机器学习-1"><h1 id="Why-do-we-want-machines-to-learn-我们为什么要让机器学习？"><a href="#Why-do-we-want-machines-to-learn-我们为什么要让机器学习？" class="headerlink" title="Why do we want machines to learn? 我们为什么要让机器学习？"></a>Why do we want machines to learn? 我们为什么要让机器学习？</h1><p>This is Billy. Billy wants to buy a car. He tries to calculate how much he needs to save monthly for that. He went over dozens of ads on the internet and learned that new cars are around $20,000, used year-old ones are $19,000, 2-year old are $18,000 and so on.<br>这是比利。比利想买一辆车。他试着计算每月需要存多少钱。他浏览了互联网上的几十个广告，了解到新车价格约为2万美元，二手车价格为1.9万美元，2年车价格为1.8万美元，以此类推。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.jpg" class="" title="人人共享的机器学习-2"><p>Billy, our brilliant analytic, starts seeing a pattern: so, the car price depends on its age and drops $1,000 every year, but won’t get lower than $10,000.<br>Billy，我们出色的分析师，开始看到一个模式：所以，汽车价格取决于它的年龄，每年下降1000美元，但不会低于10000美元。</p><p>In machine learning terms, Billy invented regression – he predicted a value (price) based on known historical data. People do it all the time, when trying to estimate a reasonable cost for a used iPhone on eBay or figure out how many ribs to buy for a BBQ party. 200 grams per person? 500?<br>在机器学习方面，Billy发明了回归——他根据已知的历史数据预测价值（价格）。人们总是这样做，试图在易趣上估计一部二手iPhone的合理成本，或者计算出烧烤派对要买多少排骨。每人200克？500？</p><p>Yeah, it would be nice to have a simple formula for every problem in the world. Especially, for a BBQ party. Unfortunately, it’s impossible.<br>是的，如果能为世界上的每一个问题都有一个简单的公式，那就太好了。尤其是烧烤派对。不幸的是，这是不可能的。</p><p>Let’s get back to cars. The problem is, they have different manufacturing dates, dozens of options, technical condition, seasonal demand spikes, and god only knows how many more hidden factors. An average Billy can’t keep all that data in his head while calculating the price. Me too.<br>让我们回到汽车上。问题是，它们有不同的生产日期、几十种选择、技术条件、季节性需求激增，天知道还有多少隐藏因素。一个普通的比利在计算价格时无法将所有这些数据都记在脑子里。我也是。</p><p>People are dumb and lazy – we need robots to do the maths for them. So, let’s go the computational way here. Let’s provide the machine some data and ask it to find all hidden patterns related to price.<br>人们又笨又懒——我们需要机器人为他们计算。所以，让我们从计算的角度来看。让我们向机器提供一些数据，并要求它找到所有与价格相关的隐藏模式。</p><p>Aaaand it works. The most exciting thing is that the machine copes with this task much better than a real person does when carefully analyzing all the dependencies in their mind.<br>Aaaan而且有效。最令人兴奋的是，当仔细分析他们脑海中的所有依赖关系时，机器比真人更好地处理这项任务。</p><p>That was the birth of machine learning.<br>这就是机器学习的诞生。</p><h1 id="Three-components-of-machine-learning-机器学习的三个组成部分"><a href="#Three-components-of-machine-learning-机器学习的三个组成部分" class="headerlink" title="Three components of machine learning 机器学习的三个组成部分"></a>Three components of machine learning 机器学习的三个组成部分</h1><p>Without all the AI-bullshit, the only goal of machine learning is to predict results based on incoming data. That’s it. All ML tasks can be represented this way, or it’s not an ML problem from the beginning.<br>如果没有人工智能的废话，机器学习的唯一目标就是根据输入的数据预测结果。就是这样。所有ML任务都可以用这种方式表示，或者从一开始就不是ML问题。</p><p>The greater variety in the samples you have, the easier it is to find relevant patterns and predict the result. Therefore, we need three components to teach the machine:<br>样本的种类越多，就越容易找到相关的模式并预测结果。因此，我们需要三个组件来教机器：</p><p><strong>Data</strong> Want to detect spam? Get samples of spam messages. Want to forecast stocks? Find the price history. Want to find out user preferences? Parse their activities on Facebook (no, Mark, stop collecting it, enough!). The more diverse the data, the better the result. Tens of thousands of rows is the bare minimum for the desperate ones.<br><strong>数据</strong>想要检测垃圾邮件吗？获取垃圾邮件的示例。想预测股票吗？查找价格历史记录。想了解用户偏好吗？分析他们在Facebook上的活动（不，马克，停止收集，够了！）。数据越多样化，结果越好。对于绝望的人来说，数以万计的争吵是最低限度的。</p><p>There are two main ways to get the data — <strong>manual and automatic</strong>. Manually collected data contains far fewer errors but takes more time to collect — that makes it more expensive in general.<br>获取数据主要有两种方式——<strong>手动</strong>和<strong>自动</strong>。手动收集的数据包含的错误要少得多，但需要更多的时间来收集，这通常会使数据的成本更高。</p><p>Automatic approach is cheaper — you’re gathering everything you can find and hope for the best.<br>自动方法更便宜——你正在收集你能找到的一切，并希望一切都好。</p><p>Some smart asses like Google use their own customers to label data for them for free. Remember ReCaptcha which forces you to “Select all street signs”? That’s exactly what they’re doing. Free labour! Nice. In their place, I’d start to show captcha more and more. Oh, wait…<br>一些像谷歌这样的聪明人利用自己的客户免费为他们标记数据。还记得ReCaptcha强制你“选择所有路标”吗？这正是他们正在做的。免费劳动力！美好的在他们的位置上，我会越来越多地向captcha展示。哦，等等。。。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3.jpg" class="" title="人人共享的机器学习-3"><p>It’s extremely tough to collect a good collection of data (usually called a dataset). They are so important that companies may even reveal their algorithms, but rarely datasets.<br>收集一个好的数据集（通常称为数据集）是非常困难的。它们是如此重要，以至于公司甚至可以公布他们的算法，但很少公布数据集。</p><p><strong>Features</strong> Also known as parameters or variables. Those could be car mileage, user’s gender, stock price, word frequency in the text. In other words, these are the factors for a machine to look at.<br><strong>特征</strong>也称为参数或变量。这些可能是汽车里程、用户性别、股价、文本中的词频。换句话说，这些都是机器需要考虑的因素。</p><p>When data stored in tables it’s simple — features are column names. But what are they if you have 100 Gb of cat pics? We cannot consider each pixel as a feature. That’s why selecting the right features usually takes way longer than all the other ML parts. That’s also the main source of errors. Meatbags are always subjective. They choose only features they like or find “more important”. Please, avoid being human.<br>当数据存储在表中时，这很简单——特性就是列名。但是，如果你有100 Gb的猫照片，它们是什么？我们不能将每个像素视为一个特征。这就是为什么选择正确的特性通常比所有其他ML部分花费更长的时间。这也是错误的主要来源。肉包总是主观的。他们只选择自己喜欢或觉得“更重要”的功能。请不要做人。</p><p><strong>Algorithms</strong> Most obvious part. Any problem can be solved differently. The method you choose affects the precision, performance, and size of the final model. There is one important nuance though: if the data is crappy, even the best algorithm won’t help. Sometimes it’s referred as “garbage in – garbage out”. So don’t pay too much attention to the percentage of accuracy, try to acquire more data first.<br><strong>算法</strong>最明显的部分。任何问题都可以用不同的方式解决。选择的方法会影响最终模型的精度、性能和大小。不过，有一个重要的细微差别：如果数据很糟糕，即使是最好的算法也无济于事。有时它被称为“垃圾进-垃圾出”。因此，不要过于关注准确率，尽量先获取更多数据。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4.jpg" class="" title="人人共享的机器学习-4"><h1 id="Learning-vs-Intelligence-学习与智力"><a href="#Learning-vs-Intelligence-学习与智力" class="headerlink" title="Learning vs Intelligence 学习与智力"></a>Learning vs Intelligence 学习与智力</h1><p>Once I saw an article titled “Will neural networks replace machine learning?” on some hipster media website. These media guys always call any shitty linear regression at least artificial intelligence, almost SkyNet. Here is a simple picture to show who is who.<br>有一次，我在一些时髦的媒体网站上看到一篇题为“神经网络会取代机器学习吗？”的文章。这些媒体人总是把任何糟糕的线性回归称为人工智能，几乎是天网。这里有一张简单的图片来展示谁是谁。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5.jpg" class="" title="人人共享的机器学习-5"><p><strong>Artificial intelligence</strong> is the name of a whole knowledge field, similar to biology or chemistry.<br><strong>人工智能</strong>是整个知识领域的名称，类似于生物学或化学。</p><p><strong>Machine Learning</strong> is a part of artificial intelligence. An important part, but not the only one.<br><strong>机器学习</strong>是人工智能的一部分。一个重要的部分，但不是唯一的。</p><p><strong>Neural Networks</strong> are one of machine learning types. A popular one, but there are other good guys in the class.<br><strong>神经网络</strong>是机器学习类型之一。很受欢迎，但班上还有其他好人。</p><p><strong>Deep Learning</strong> is a modern method of building, training, and using neural networks. Basically, it’s a new architecture. Nowadays in practice, no one separates deep learning from the “ordinary networks”. We even use the same libraries for them. To not look like a dumbass, it’s better just name the type of network and avoid buzzwords.<br><strong>深度学习</strong>是一种构建、训练和使用神经网络的现代方法。基本上，这是一个新的体系结构。如今，在实践中，没有人将深度学习与“普通网络”分开。我们甚至为它们使用相同的库。为了不让自己看起来像个傻瓜，最好只是说出网络的类型，避免使用流行语。</p><p>The general rule is to compare things on the same level. That’s why the phrase “will neural nets replace machine learning” sounds like “will the wheels replace cars”. Dear media, it’s compromising your reputation a lot.<br>一般的规则是在同一水平上进行比较。这就是为什么“神经网络会取代机器学习吗”这句话听起来像“车轮会取代汽车吗”。亲爱的媒体，这会大大损害你的声誉。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Machine can</th><th style="text-align:center">Machine cannot</th></tr></thead><tbody><tr><td style="text-align:center">Forecast</td><td style="text-align:center">Create something new</td></tr><tr><td style="text-align:center">Memorize</td><td style="text-align:center">Get smart really fast</td></tr><tr><td style="text-align:center">Reproduce</td><td style="text-align:center">Go beyond their task</td></tr><tr><td style="text-align:center">Choose best item</td><td style="text-align:center">Kill all humans</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center">机器可以</th><th style="text-align:center">机器不能</th></tr></thead><tbody><tr><td style="text-align:center">预测</td><td style="text-align:center">创造新的东西</td></tr><tr><td style="text-align:center">记忆</td><td style="text-align:center">快速变得聪明</td></tr><tr><td style="text-align:center">复制</td><td style="text-align:center">超越他们的任务</td></tr><tr><td style="text-align:center">选择最好的物品</td><td style="text-align:center">杀死所有人类</td></tr></tbody></table></div><h1 id="The-map-of-the-machine-learning-world-机器学习世界地图"><a href="#The-map-of-the-machine-learning-world-机器学习世界地图" class="headerlink" title="The map of the machine learning world 机器学习世界地图"></a>The map of the machine learning world 机器学习世界地图</h1><p>If you are too lazy for long reads, take a look at the picture below to get some understanding.<br>如果你太懒了，不适合长时间阅读，看看下面的图片，了解一下。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6.jpg" class="" title="人人共享的机器学习-6"><p>Always important to remember — there is never a sole way to solve a problem in the machine learning world. There are always several algorithms that fit, and you have to choose which one fits better. Everything can be solved with a neural network, of course, but who will pay for all these GeForces?<br>永远重要的是要记住——在机器学习的世界里，解决问题从来没有唯一的方法。总有几种算法适合，你必须选择哪一种更适合。当然，一切都可以用神经网络解决，但谁来为所有这些GeForces买单？</p><p>Let’s start with a basic overview. Nowadays there are four main directions in machine learning.<br>让我们从一个基本概述开始。如今，机器学习有四个主要方向。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7.jpg" class="" title="人人共享的机器学习-7"><h1 id="Part-1-Classical-Machine-Learning-第1部分。经典机器学习"><a href="#Part-1-Classical-Machine-Learning-第1部分。经典机器学习" class="headerlink" title="Part 1. Classical Machine Learning 第1部分。经典机器学习"></a>Part 1. Classical Machine Learning 第1部分。经典机器学习</h1><p>The first methods came from pure statistics in the ‘50s. They solved formal math tasks — searching for patterns in numbers, evaluating the proximity of data points, and calculating vectors’ directions.<br>第一种方法来自50年代的纯统计学。他们解决了正式的数学任务——搜索数字中的模式，评估数据点的接近度，以及计算矢量的方向。</p><p>Nowadays, half of the Internet is working on these algorithms. When you see a list of articles to “read next” or your bank blocks your card at random gas station in the middle of nowhere, most likely it’s the work of one of those little guys.<br>如今，一半的互联网都在研究这些算法。当你看到一张“下一步要读”的文章清单，或者你的银行在一个不知名的加油站挡住了你的卡，很可能是其中一个小家伙的工作。</p><p>Big tech companies are huge fans of neural networks. Obviously. For them, 2% accuracy is an additional 2 billion in revenue. But when you are small, it doesn’t make sense. I heard stories of the teams spending a year on a new recommendation algorithm for their e-commerce website, before discovering that 99% of traffic came from search engines. Their algorithms were useless. Most users didn’t even open the main page.<br>大型科技公司是神经网络的超级粉丝。明显地对他们来说，2%的准确率意味着额外的20亿收入。但当你很小的时候，这就没有意义了。我听说这些团队花了一年时间为他们的电子商务网站开发一种新的推荐算法，然后发现99%的流量来自搜索引擎。他们的算法毫无用处。大多数用户甚至没有打开主页。</p><p>Despite the popularity, classical approaches are so natural that you could easily explain them to a toddler. They are like basic arithmetic — we use it every day, without even thinking.<br>尽管很受欢迎，但经典的方法是如此自然，你可以很容易地向蹒跚学步的孩子解释它们。它们就像基本的算术——我们每天都在使用它，甚至不用思考。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-8.jpg" class="" title="人人共享的机器学习-8"><h2 id="1-1-Supervised-Learning-1-1监督学习"><a href="#1-1-Supervised-Learning-1-1监督学习" class="headerlink" title="1.1 Supervised Learning 1.1监督学习"></a>1.1 Supervised Learning 1.1监督学习</h2><p>Classical machine learning is often divided into two categories – <strong>Supervised</strong> and <strong>Unsupervised Learning</strong>.<br>经典的机器学习通常分为两类——<strong>有监督学习</strong>和<strong>无监督学习</strong>。</p><p>In the first case, the machine has a “supervisor” or a “teacher” who gives the machine all the answers, like whether it’s a cat in the picture or a dog. The teacher has already divided (labeled) the data into cats and dogs, and the machine is using these examples to learn. One by one. Dog by cat.<br>在第一种情况下，机器有一个“主管”或“老师”，他给机器所有的答案，比如照片中的猫还是狗。老师已经将数据分为猫和狗，机器正在使用这些例子进行学习。一个接一个。一只狗一只猫。</p><p>Unsupervised learning means the machine is left on its own with a pile of animal photos and a task to find out who’s who. Data is not labeled, there’s no teacher, the machine is trying to find any patterns on its own. We’ll talk about these methods below.<br>无监督学习意味着机器只剩下一堆动物照片和一项找出谁是谁的任务。数据没有标签，没有老师，机器正试图自己找到任何模式。我们将在下面讨论这些方法。</p><p>Clearly, the machine will learn faster with a teacher, so it’s more commonly used in real-life tasks. There are two types of such tasks: <strong>classification – an object’s category prediction, and regression – prediction of a specific point on a numeric axis</strong>.<br>很明显，这台机器在老师的指导下会学得更快，所以它更常用于现实生活中的任务。这类任务有两种类型：<strong>分类——对象的类别预测，回归——数字轴上特定点的预测</strong>。</p><h3 id="Classification-分类"><a href="#Classification-分类" class="headerlink" title="Classification 分类"></a>Classification 分类</h3><p>“Splits objects based at one of the attributes known beforehand. Separate socks by based on color, documents based on language, music by genre”<br>根据事先已知的一个属性拆分对象。根据颜色、根据语言和音乐类型划分袜子</p><p>Today used for:</p><ul><li>Spam filtering</li><li>Language detection</li><li>A search of similar documents</li><li>Sentiment analysis</li><li>Recognition of handwritten characters and numbers</li><li>Fraud detection</li></ul><p>今天用于：</p><ul><li>垃圾邮件过滤</li><li>语言检测</li><li>搜索类似文档</li><li>情绪分析</li><li>识别手写字符和数字</li><li>欺诈检测</li></ul><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-9.jpg" class="" title="人人共享的机器学习-9"><p>Popular algorithms: Naive Bayes, Decision Tree, Logistic Regression, K-Nearest Neighbours, Support Vector Machine<br>流行算法：朴素贝叶斯、决策树、逻辑回归、K近邻、支持向量机</p><p><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">朴素贝叶斯</a><br><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">决策树</a><br><a href="https://en.wikipedia.org/wiki/Logistic_regression">逻辑回归</a><br><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">K近邻</a><br><a href="https://en.wikipedia.org/wiki/Support_vector_machine">支持向量机</a></p><p>Machine learning is about classifying things, mostly. The machine here is like a baby learning to sort toys: here’s a robot, here’s a car, here’s a robo-car… Oh, wait. Error! Error!<br>机器学习主要是对事物进行分类。这里的机器就像一个婴儿在学习分类玩具：这是一个机器人，这是一辆汽车，这是机器人汽车。。。哦，等等。错误错误</p><p>In classification, you always need a teacher. The data should be labeled with features so the machine could assign the classes based on them. Everything could be classified — users based on interests (as algorithmic feeds do), articles based on language and topic (that’s important for search engines), music based on genre (Spotify playlists), and even your emails.<br>在分类方面，你总是需要一位老师。数据应该标有特征，这样机器就可以根据这些特征分配类。一切都可以分类——用户基于兴趣（就像算法提要一样），文章基于语言和主题（这对搜索引擎很重要），音乐基于流派（Spotify播放列表），甚至你的电子邮件。</p><p>In spam filtering the Naive Bayes algorithm was widely used. The machine counts the number of “viagra” mentions in spam and normal mail, then it multiplies both probabilities using the Bayes equation, sums the results and yay, we have Machine Learning.<br>Naive Bayes算法在垃圾邮件过滤中得到了广泛的应用。该机器计算垃圾邮件和普通邮件中提到“伟哥”的次数，然后使用贝叶斯方程乘以这两种概率，对结果求和，是的，我们有机器学习。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-10.jpg" class="" title="人人共享的机器学习-10"><p>Later, spammers learned how to deal with Bayesian filters by adding lots of “good” words at the end of the email. Ironically, the method was called Bayesian poisoning. Naive Bayes went down in history as the most elegant and first practically useful one, but now other algorithms are used for spam filtering.<br>后来，垃圾邮件发送者通过在电子邮件末尾添加大量“好”字，学会了如何处理贝叶斯过滤器。具有讽刺意味的是，这种方法被称为贝叶斯中毒。Naive Bayes作为最优雅、最早实用的算法而载入史册，但现在其他算法也被用于垃圾邮件过滤。</p><p><a href="https://en.wikipedia.org/wiki/Bayesian_poisoning">Bayesian poisoning</a></p><p>Here’s another practical example of classification. Let’s say you need some money on credit. How will the bank know if you’ll pay it back or not? There’s no way to know for sure. But the bank has lots of profiles of people who took money before. They have data about age, education, occupation and salary and – most importantly – the fact of paying the money back. Or not.<br>这是另一个实用的分类示例。比方说你需要一些贷款。银行怎么知道你是否会还钱？没有办法确定。但该银行有很多以前拿钱的人的档案。他们有关于年龄、教育、职业和工资的数据，最重要的是，还有还钱的事实。或者不。</p><p>Using this data, we can teach the machine to find the patterns and get the answer. There’s no issue with getting an answer. The issue is that the bank can’t blindly trust the machine answer. What if there’s a system failure, hacker attack or a quick fix from a drunk senior.<br>使用这些数据，我们可以教机器找到模式并得到答案。得到答案没有问题。问题是，银行不能盲目相信机器的答案。如果出现系统故障、黑客攻击或醉酒的学长快速修复，该怎么办。</p><p>To deal with it, we have Decision Trees. All the data automatically divided to yes/no questions. They could sound a bit weird from a human perspective, e.g., whether the creditor earns more than $128.12? Though, the machine comes up with such questions to split the data best at each step.<br>为了解决这个问题，我们有决策树。所有数据自动划分为是/否问题。从人类的角度来看，这听起来可能有点奇怪，例如，债权人的收入是否超过128.12美元？不过，这台机器会提出这样的问题，以便在每一步都能最好地分割数据。</p><p><a href="https://www.youtube.com/watch?v=eKD5gxPPeY0">Decision Trees</a></p><p>That’s how a tree is made. The higher the branch — the broader the question. Any analyst can take it and explain afterward. He may not understand it, but explain easily! (typical analyst)<br>树就是这样造的。分支越高，问题就越广泛。任何分析师都可以接受并在事后解释。他可能听不懂，但很容易解释！（典型分析员）</p><p>Decision trees are widely used in high responsibility spheres: diagnostics, medicine, and finances.<br>决策树广泛应用于高责任领域：诊断、医学和金融。</p><blockquote><p>The two most popular algorithms for forming the trees are CART and C4.5.<br>用于形成树的两种最流行的算法是CART和C4.5</p></blockquote><p><a href="https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29">CART</a><br><a href="https://en.wikipedia.org/wiki/C4.5_algorithm">C4.5</a></p><p>Pure decision trees are rarely used today. However, they often set the basis for large systems, and their ensembles even work better than neural networks. We’ll talk about that later.<br>今天很少使用纯决策树。然而，它们通常为大型系统奠定基础，它们的集成甚至比神经网络工作得更好。我们稍后再谈。</p><blockquote><p>When you google something, that’s precisely the bunch of dumb trees which are looking for a range of answers for you. Search engines love them because they’re fast.<br>当你在谷歌上搜索某个东西时，那正是一群愚蠢的树在为你寻找一系列答案。搜索引擎喜欢它们，因为它们很快。</p></blockquote><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-11.jpg" class="" title="人人共享的机器学习-11"><p><em>Support Vector Machines (SVM)</em> is rightfully the most popular method of classical classification. It was used to classify everything in existence: plants by appearance in photos, documents by categories, etc.<br><em>支持向量机（SVM）</em>是最流行的经典分类方法。它被用来对现存的一切进行分类：植物按照片中的外观分类，文件按类别分类等等。</p><p>The idea behind SVM is simple – it’s trying to draw two lines between your data points with the largest margin between them. Look at the picture:<br>SVM背后的想法很简单——它试图在数据点之间画两条线，并在它们之间留有最大的余量。请看图片：</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-12.jpg" class="" title="人人共享的机器学习-12"><p>There’s one very useful side of the classification — anomaly detection. When a feature does not fit any of the classes, we highlight it. Now that’s used in medicine — on MRIs, computers highlight all the suspicious areas or deviations of the test. Stock markets use it to detect abnormal behaviour of traders to find the insiders. When teaching the computer the right things, we automatically teach it what things are wrong.<br>分类有一个非常有用的方面——异常检测。当一个功能不适合任何类别时，我们会突出显示它。现在它被用于医学——在核磁共振成像上，计算机会突出显示测试的所有可疑区域或偏差。股票市场利用它来检测交易员的异常行为，以找到内部人士。当教计算机正确的东西时，我们会自动教它什么是错误的。</p><p>Today, neural networks are more frequently used for classification. Well, that’s what they were created for.<br>如今，神经网络更频繁地用于分类。好吧，这就是他们被创造的目的。</p><p>The rule of thumb is the more complex the data, the more complex the algorithm. For text, numbers, and tables, I’d choose the classical approach. The models are smaller there, they learn faster and work more clearly. For pictures, video and all other complicated big data things, I’d definitely look at neural networks.<br>经验法则是数据越复杂，算法就越复杂。对于文本、数字和表格，我会选择经典的方法。那里的模型更小，学习更快，工作更清晰。对于图片、视频和所有其他复杂的大数据，我肯定会研究神经网络。</p><p>Just five years ago you could find a face classifier built on SVM. Today it’s easier to choose from hundreds of pre-trained networks. Nothing has changed for spam filters, though. They are still written with SVM. And there’s no good reason to switch from it anywhere.<br>就在五年前，你还可以找到一个基于SVM的人脸分类器。如今，从数百个经过预训练的网络中进行选择变得更加容易。不过，垃圾邮件过滤器没有任何变化。它们仍然是用SVM编写的。而且没有充分的理由从任何地方切换。</p><p>Even my website has SVM-based spam detection in comments<br>甚至我的网站在评论中也有基于SVM的垃圾邮件检测</p><h3 id="Regression-回归"><a href="#Regression-回归" class="headerlink" title="Regression 回归"></a>Regression 回归</h3><p>“Draw a line through these dots. Yep, that’s the machine learning”<br>“在这些点之间划一条线。是的，这就是机器学习”</p><p>Today this is used for:</p><ul><li>Stock price forecasts 股票价格预测</li><li>Demand and sales volume analysis 需求和销售量分析</li><li>Medical diagnosis 医学诊断</li><li>Any number-time correlations 任意数字时间相关性</li></ul><p>Popular algorithms are Linear and Polynomial regressions.<br>常用的算法是线性回归和多项式回归。</p><p><a href="https://en.wikipedia.org/wiki/Linear_regression">Linear</a></p><p><a href="https://en.wikipedia.org/wiki/Polynomial_regression">Polynomial</a></p><p>Regression is basically classification where we forecast a number instead of category. Examples are car price by its mileage, traffic by time of the day, demand volume by growth of the company etc. Regression is perfect when something depends on time.<br>回归基本上是分类，我们预测一个数字而不是类别。例如，按里程计算的汽车价格、按时间计算的交通量、按公司增长计算的需求量等。当某些事情取决于时间时，回归是完美的。</p><p>Everyone who works with finance and analysis loves regression. It’s even built-in to Excel. And it’s super smooth inside — the machine simply tries to draw a line that indicates average correlation. Though, unlike a person with a pen and a whiteboard, machine does so with mathematical accuracy, calculating the average interval to every dot.<br>每个从事金融和分析工作的人都喜欢回归。它甚至内置在Excel中。它的内部非常平滑——机器只是试图画一条线来表示平均相关性。不过，和拿着笔和白板的人不同，这台机器能准确地计算出每个点的平均间隔。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-13.jpg" class="" title="人人共享的机器学习-13"><p>When the line is straight — it’s a linear regression, when it’s curved – polynomial. These are two major types of regression. The other ones are more exotic. Logistic regression is a black sheep in the flock. Don’t let it trick you, as it’s a classification method, not regression.<br>当直线是直线时——它是线性回归，当它是曲线时——多项式。这是两种主要的回归类型。其他的更具异国情调。逻辑回归是羊群中的害群之马。不要让它欺骗你，因为这是一种分类方法，而不是回归。</p><p>It’s okay to mess with regression and classification, though. Many classifiers turn into regression after some tuning. We can not only define the class of the object but memorize how close it is. Here comes a regression.<br>不过，搞砸回归和分类是可以的。许多分类器经过一些调整后会变成回归。我们不仅可以定义对象的类，还可以记住它的接近程度。</p><p>If you want to get deeper into this, check these series: Machine Learning for Humans. I really love and recommend it!<br>如果你想更深入地了解这一点，请查看以下系列：面向人类的机器学习。我真的很喜欢并推荐它！</p><p><a href="https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab">Machine Learning for Humans</a></p><h2 id="1-2-Unsupervised-learning-1-2无监督学习"><a href="#1-2-Unsupervised-learning-1-2无监督学习" class="headerlink" title="1.2 Unsupervised learning 1.2无监督学习"></a>1.2 Unsupervised learning 1.2无监督学习</h2><p>Unsupervised was invented a bit later, in the ‘90s. It is used less often, but sometimes we simply have no choice.<br>无监督是在90年代发明的。它的使用频率较低，但有时我们别无选择。</p><p>Labeled data is luxury. But what if I want to create, let’s say, a bus classifier? Should I manually take photos of million fucking buses on the streets and label each of them? No way, that will take a lifetime, and I still have so many games not played on my Steam account.<br>标记数据是一种奢侈。但是，如果我想创建一个总线分类器呢？我应该手动拍摄一百万辆他妈的公交车在街上的照片，并给每辆贴上标签吗？不可能，那将需要一辈子的时间，而且我的Steam帐户上还有很多游戏没有玩。</p><p>There’s a little hope for capitalism in this case. Thanks to social stratification, we have millions of cheap workers and services like Mechanical Turk who are ready to complete your task for $0.05. And that’s how things usually get done here.<br>在这种情况下，资本主义还有一点希望。由于社会分层，我们有数百万像机械土耳其人这样的廉价工人和服务，他们准备以0.05美元的价格完成您的任务。这里通常就是这样做的。</p><p><a href="https://www.mturk.com/">Mechanical Turk</a></p><p>Or you can try to use unsupervised learning. But I can’t remember any good practical application for it, though. It’s usually useful for exploratory data analysis but not as the main algorithm. Specially trained meatbag with Oxford degree feeds the machine with a ton of garbage and watches it. Are there any clusters? No. Any visible relations? No. Well, continue then. You wanted to work in data science, right?<br>或者你可以尝试使用无监督学习。但我不记得它有什么好的实际应用。它通常用于探索性数据分析，但不是主要的算法。受过专门训练的牛津学位的肉包往机器里倒一吨垃圾，然后看着它。有集群吗？没有。有明显的关系吗？不，那就继续。你想从事数据科学工作，对吧？</p><p><a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">exploratory data analysis</a></p><h3 id="Clustering-聚类"><a href="#Clustering-聚类" class="headerlink" title="Clustering 聚类"></a>Clustering 聚类</h3><p>“Divides objects based on unknown features. Machine chooses the best way”“根据未知功能划分对象。机器选择最佳方式”</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-14.jpg" class="" title="人人共享的机器学习-14"><p>Nowadays used:</p><ul><li>For market segmentation (types of customers, loyalty) 针对市场细分（客户类型、忠诚度）</li><li>To merge close points on a map 合并地图上的闭合点的步骤</li><li>For image compression 用于图像压缩</li><li>To analyze and label new data 分析和标记新数据</li><li>To detect abnormal behavior 检测异常行为</li></ul><p>Popular algorithms: K-means_clustering, Mean-Shift, DBSCAN</p><p><a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means_clustering</a><br><a href="https://en.wikipedia.org/wiki/Mean_shift">Mean-Shift</a><br><a href="https://en.wikipedia.org/wiki/DBSCAN">DBSCAN</a></p><p>Clustering is a classification with no predefined classes. It’s like dividing socks by color when you don’t remember all the colors you have. Clustering algorithm trying to find similar (by some features) objects and merge them in a cluster. Those who have lots of similar features are joined in one class. With some algorithms, you even can specify the exact number of clusters you want.<br>聚类是一种没有预定义类的分类。这就像当你不记得所有的颜色时，按颜色划分袜子。聚类算法试图找到相似的（通过某些特征）对象并将它们合并到一个聚类中。那些有许多相似特征的人被加入到一个类中。使用某些算法，您甚至可以指定所需的簇的确切数量。</p><p>An excellent example of clustering — markers on web maps. When you’re looking for all vegan restaurants around, the clustering engine groups them to blobs with a number. Otherwise, your browser would freeze, trying to draw all three million vegan restaurants in that hipster downtown.<br>聚类的一个很好的例子——网络地图上的标记。当你在寻找周围所有的纯素食餐厅时，集群引擎会用一个数字将它们分组。否则，你的浏览器就会冻结，试图吸引市中心时髦人群中的300万家纯素食餐厅。</p><p>Apple Photos and Google Photos use more complex clustering. They’re looking for faces in photos to create albums of your friends. The app doesn’t know how many friends you have and how they look, but it’s trying to find the common facial features. Typical clustering.<br>苹果照片和谷歌照片使用更复杂的聚类。他们正在照片中查找人脸，以创建你朋友的相册。该应用程序不知道你有多少朋友，他们看起来怎么样，但它正在努力寻找常见的面部特征。典型的集群。</p><p>Another popular issue is image compression. When saving the image to PNG you can set the palette, let’s say, to 32 colors. It means clustering will find all the “reddish” pixels, calculate the “average red” and set it for all the red pixels. Fewer colors — lower file size — profit!<br>另一个流行的问题是图像压缩。将图像保存为PNG时，可以将调色板设置为32种颜色。这意味着聚类将找到所有“红色”像素，计算“平均红色”，并将其设置为所有红色像素。更少的颜色—更低的文件大小—利润！</p><p>However, you may have problems with colors like Cyan-like colors. Is it green or blue? Here comes the K-Means algorithm.<br>但是，您可能对青色等颜色有问题-比如颜色。它是绿色的还是蓝色的？K-Means算法来了。</p><p><a href="https://www.youtube.com/watch?v=_aWzGGNrcic&amp;ab_channel=VictorLavrenko">K-Means</a></p><p>It randomly sets 32 color dots in the palette. Now, those are centroids. The remaining points are marked as assigned to the nearest centroid. Thus, we get kind of galaxies around these 32 colors. Then we’re moving the centroid to the center of its galaxy and repeat that until centroids stop moving.<br>它在调色板中随机设置32个色点。现在，这些是质心。剩余的点被标记为已指定给最近的质心。因此，我们得到了围绕这32种颜色的星系。然后我们将质心移动到其星系的中心，并重复这一过程，直到质心停止移动。</p><p>All done. Clusters defined, stable, and there are exactly 32 of them. Here is a more real-world explanation:<br>全部完成。集群是定义的，是稳定的，并且正好有32个集群。以下是一个更真实的解释：</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-15.jpg" class="" title="人人共享的机器学习-15"><p>Searching for the centroids is convenient. Though, in real life clusters not always circles. Let’s imagine you’re a geologist. And you need to find some similar minerals on the map. In that case, the clusters can be weirdly shaped and even nested. Also, you don’t even know how many of them to expect. 10? 100?<br>搜索质心很方便。然而，在现实生活中，集群并不总是圆形的。让我们想象一下你是一名地质学家。你需要在地图上找到一些类似的矿物。在这种情况下，簇的形状可能很奇怪，甚至可以嵌套。此外，你甚至不知道他们中有多少值得期待。10？100？</p><p>K-means does not fit here, but DBSCAN can be helpful. Let’s say, our dots are people at the town square. Find any three people standing close to each other and ask them to hold hands. Then, tell them to start grabbing hands of those neighbors they can reach. And so on, and so on until no one else can take anyone’s hand. That’s our first cluster. Repeat the process until everyone is clustered. Done.<br>K-means不适合这里，但DBSCAN可能会有所帮助。比方说，我们的圆点是城镇广场上的人。找任意三个人站得很近，让他们手牵着手。然后，告诉他们开始与他们能接触到的邻居握手。等等，等等，直到没有人能握住任何人的手。这是我们的第一个集群。重复此过程，直到所有人都聚集在一起。完成。</p><p><a href="https://towardsdatascience.com/how-dbscan-works-and-why-should-i-use-it-443b4a191c80?gi=3a98260adc51">DBSCAN</a></p><blockquote><p>A nice bonus: a person who has no one to hold hands with — is an anomaly.<br>一个很好的好处是：一个没有人可以牵手的人——是一种反常现象。</p></blockquote><p>It all looks cool in motion:<br>这一切看起来都很酷：</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-16.gif" class="" title="人人共享的机器学习-16"><blockquote><p>Interested in clustering? Check out this piece The 5 Clustering Algorithms Data Scientists Need to Know<br>对集群感兴趣吗？看看这篇文章数据科学家需要知道的5种聚类算法</p></blockquote><p><a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">The 5 Clustering Algorithms Data Scientists Need to Know</a></p><p>Just like classification, clustering could be used to detect anomalies. User behaves abnormally after signing up? Let the machine ban him temporarily and create a ticket for the support to check it. Maybe it’s a bot. We don’t even need to know what “normal behavior” is, we just upload all user actions to our model and let the machine decide if it’s a “typical” user or not.<br>就像分类一样，聚类也可以用来检测异常。用户注册后行为异常？让机器暂时禁止他，并创建一个票证供支持人员检查。也许这是一个机器人。我们甚至不需要知道什么是“正常行为”，我们只需将所有用户操作上传到我们的模型中，让机器决定它是否是“典型”用户。</p><p>This approach doesn’t work that well compared to the classification one, but it never hurts to try.<br>与分类方法相比，这种方法效果不太好，但尝试一下也不会有什么坏处。</p><h3 id="Dimensionality-Reduction-Generalization-降维（泛化）"><a href="#Dimensionality-Reduction-Generalization-降维（泛化）" class="headerlink" title="Dimensionality Reduction (Generalization) 降维（泛化）"></a>Dimensionality Reduction (Generalization) 降维（泛化）</h3><p>“Assembles specific features into more high-level ones”<br>“将特定功能汇编成更高级的功能”</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-17.jpg" class="" title="人人共享的机器学习-17"><p>Nowadays is used for:</p><ul><li>Recommender systems (★) 推荐系统 </li><li>Beautiful visualizations 美丽的可视化</li><li>Topic modeling and similar document search 主题建模和类似文档搜索</li><li>Fake image analysis 假图像分析</li><li>Risk management 风险管理</li></ul><p>Popular algorithms: Principal Component Analysis (PCA), Singular Value Decomposition (SVD), Latent Dirichlet allocation (LDA), Latent Semantic Analysis (LSA, pLSA, GLSA), t-SNE (for visualization)<br>流行算法：主成分分析（PCA）、奇异值分解（SVD）、潜在狄利克雷分配（LDA）、潜在语义分析（LSA、pLSA、GLSA）、t-SNE（用于可视化）</p><p><a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis-PAC</a><br><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular Value Decomposition-SVD</a><br><a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet allocation-LDA</a><br><a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis">Latent Semantic Analysis-LSA/pLSA/GLSA</a><br><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a></p><p>Previously these methods were used by hardcore data scientists, who had to find “something interesting” in huge piles of numbers. When Excel charts didn’t help, they forced machines to do the pattern-finding. That’s how they got Dimension Reduction or Feature Learning methods.<br>以前，这些方法是由核心数据科学家使用的，他们必须在大量数据中找到“有趣的东西”。当Excel图表没有帮助时，他们强迫机器进行模式查找。这就是他们获得降维或特征学习方法的原因。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-18.gif" class="" title="人人共享的机器学习-18"><p>Projecting 2D-data to a line (PCA)将2D数据投影到直线（PCA）</p><p>It is always more convenient for people to use abstractions, not a bunch of fragmented features. For example, we can merge all dogs with triangle ears, long noses, and big tails to a nice abstraction — “shepherd”. Yes, we’re losing some information about the specific shepherds, but the new abstraction is much more useful for naming and explaining purposes. As a bonus, such “abstracted” models learn faster, overfit less and use a lower number of features.<br>人们总是更方便地使用抽象，而不是一堆碎片化的特性。例如，我们可以将所有三角形耳朵、长鼻和大尾巴的狗合并为一个很好的抽象概念——“牧羊人”。是的，我们正在丢失一些关于特定牧羊人的信息，但新的抽象对于命名和解释目的更有用。额外的好处是，这种“抽象”模型学习速度更快，过拟合更少，使用的特征数量更少。</p><p>These algorithms became an amazing tool for <strong>Topic Modeling</strong>. We can abstract from specific words to their meanings. This is what Latent semantic analysis (LSA) does. It is based on how frequently you see the word on the exact topic. Like, there are more tech terms in tech articles, for sure. The names of politicians are mostly found in political news, etc. 这些算法成为<strong>主题建模</strong>的一个惊人工具。我们可以从特定的单词中抽象出它们的含义。这就是潜在语义分析（LSA）的作用。这取决于你在确切的主题上看到这个词的频率。比如，科技文章中肯定有更多的科技术语。政治家的名字大多出现在政治新闻等中。</p><p><a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent semantic analysis</a></p><p>Yes, we can just make clusters from all the words at the articles, but we will lose all the important connections (for example the same meaning of battery and accumulator in different documents). LSA will handle it properly, that’s why its called “latent semantic”. 是的，我们可以根据文章中的所有单词进行聚类，但我们将失去所有重要的连接（例如，不同文档中电池和蓄电池的含义相同）。LSA会正确处理它，这就是为什么它被称为“潜在语义”。</p><p>So we need to connect the words and documents into one feature to keep these latent connections — it turns out that Singular decomposition (SVD) nails this task, revealing useful topic clusters from seen-together words. 因此，我们需要将单词和文档连接到一个功能中，以保持这些潜在的连接——事实证明，奇异分解（SVD）固定了这项任务，从一起看到的单词中揭示了有用的主题集群。</p><p><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition"> Singular decomposition</a></p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-19.jpg" class="" title="人人共享的机器学习-19"><p><strong>Recommender Systems and Collaborative Filtering</strong> is another super-popular use of the dimensionality reduction method. Seems like if you use it to abstract user ratings, you get a great system to recommend movies, music, games and whatever you want.<br><strong>推荐系统和协作过滤</strong>是降维方法的另一个非常流行的用途。似乎如果你用它来抽象用户评分，你会得到一个很好的系统来推荐电影、音乐、游戏和任何你想要的东西。</p><blockquote><p>Here I can recommend my favorite book “Programming Collective Intelligence”. It was my bedside book while studying at university!<br>在这里我可以推荐我最喜欢的书“编程集体智能”。这是我在大学学习时的床边书！</p></blockquote><p><a href="https://www.oreilly.com/library/view/programming-collective-intelligence/9780596529321/">Programming Collective Intelligence</a></p><p>It’s barely possible to fully understand this machine abstraction, but it’s possible to see some correlations on a closer look. Some of them correlate with user’s age — kids play Minecraft and watch cartoons more; others correlate with movie genre or user hobbies.<br>几乎不可能完全理解这种机器抽象，但可以仔细观察一些相关性。其中一些与用户的年龄有关——孩子们玩《我的世界》，看动画片的次数更多；其他则与电影类型或用户爱好相关。</p><p>Machines get these high-level concepts even without understanding them, based only on knowledge of user ratings. Nicely done, Mr.Computer. Now we can write a thesis on why bearded lumberjacks love My Little Pony.<br>机器甚至在不了解这些高级概念的情况下，仅基于用户评级的知识，就可以获得这些概念。干得好，电脑先生。现在我们可以写一篇关于为什么留胡子的伐木工喜欢我的小马的论文了。</p><h3 id="Association-rule-learning-关联规则学习"><a href="#Association-rule-learning-关联规则学习" class="headerlink" title="Association rule learning 关联规则学习"></a>Association rule learning 关联规则学习</h3><p>“Look for patterns in the orders’ stream”<br>“在订单流中查找模式”</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-20.jpg" class="" title="人人共享的机器学习-20"><p>Nowadays is used:</p><ul><li>To forecast sales and discounts 预测销售额和折扣</li><li>To analyze goods bought together 分析一起购买的商品</li><li>To place the products on the shelves 把产品放在货架上</li><li>To analyze web surfing patterns 分析网络冲浪模式</li></ul><p>Popular algorithms: Apriori, Euclat, FP-growth<br>流行算法：Apriori、Euclat、FP growth</p><p><a href="https://en.wikipedia.org/wiki/Association_rule_learning#Algorithms">Apriori、Euclat、FP growth</a></p><p>This includes all the methods to analyze shopping carts, automate marketing strategy, and other event-related tasks. When you have a sequence of something and want to find patterns in it — try these thingys.<br>这包括分析购物车、自动化营销策略和其他与事件相关的任务的所有方法。当你有一系列的东西并想在其中找到模式时，试试这些东西。</p><p>Say, a customer takes a six-pack of beers and goes to the checkout. Should we place peanuts on the way? How often do people buy them together? Yes, it probably works for beer and peanuts, but what other sequences can we predict? Can a small change in the arrangement of goods lead to a significant increase in profits?<br>比如说，一位顾客拿了六包啤酒去结账。我们应该在路上放花生吗？人们多久一起买一次？是的，它可能适用于啤酒和花生，但我们还能预测其他哪些序列？货物排列上的一个小变化能导致利润的显著增加吗？</p><p>Same goes for e-commerce. The task is even more interesting there — what is the customer going to buy next time?<br>电子商务也是如此。这里的任务更有趣——顾客下次要买什么？</p><p>No idea why rule-learning seems to be the least elaborated upon category of machine learning. Classical methods are based on a head-on look through all the bought goods using trees or sets. Algorithms can only search for patterns, but cannot generalize or reproduce those on new examples.<br>不知道为什么规则学习似乎是机器学习中阐述最少的一类。传统的方法是基于使用树或集合对所有购买的商品进行正面查看。算法只能搜索模式，但不能在新的例子中推广或复制这些模式。</p><p>In the real world, every big retailer builds their own proprietary solution, so nooo revolutions here for you. The highest level of tech here — recommender systems. Though, I may be not aware of a breakthrough in the area. Let me know in the comments if you have something to share.<br>在现实世界中，每个大型零售商都会构建自己的专有解决方案，所以这里没有革命。这里的最高技术水平——推荐系统。不过，我可能不知道在这方面有什么突破。如果你有什么要分享的，请在评论中告诉我。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-21.jpg" class="" title="人人共享的机器学习-21"><h1 id="Part-2-Reinforcement-Learning-第2部分。强化学习"><a href="#Part-2-Reinforcement-Learning-第2部分。强化学习" class="headerlink" title="Part 2. Reinforcement Learning 第2部分。强化学习"></a>Part 2. Reinforcement Learning 第2部分。强化学习</h1><p>“Throw a robot into a maze and let it find an exit”<br>“把一个机器人扔进迷宫，让它找到出口”</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-22.jpg" class="" title="人人共享的机器学习-22"><p>Nowadays used for:</p><ul><li>Self-driving cars 自动驾驶汽车</li><li>Robot vacuums 机器人吸尘器</li><li>Games 游戏</li><li>Automating trading 自动化交易</li><li>Enterprise resource management 企业资源管理 </li></ul><p>Popular algorithms: Q-Learning, SARSA, DQN, A3C, Genetic algorithm<br>热门算法：Q-Learning、SARSA、DQN、A3C、遗传算法</p><p><a href="https://en.wikipedia.org/wiki/Q-learning">Q-Learning</a><br><a href="https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action">SARSA</a><br><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2">A3C</a><br><a href="https://en.wikipedia.org/wiki/Genetic_algorithm">遗传算法</a></p><p>Finally, we get to something looks like real artificial intelligence. In lots of articles reinforcement learning is placed somewhere in between of supervised and unsupervised learning. They have nothing in common! Is this because of the name?<br>最后，我们看到了一些看起来像真正的人工智能的东西。在许多文章中，强化学习被置于有监督和无监督学习之间。他们没有共同点！这是因为这个名字吗？</p><p>Reinforcement learning is used in cases when your problem is not related to data at all, but you have an environment to live in. Like a video game world or a city for self-driving car.<br>强化学习用于当你的问题与数据无关，但你有一个可以生活的环境时。比如电子游戏世界或自动驾驶汽车的城市。</p><p><a href="https://www.youtube.com/watch?v=qv6UVOQ0F44&amp;ab_channel=SethBling">Neural network plays Mario</a></p><p>Knowledge of all the road rules in the world will not teach the autopilot how to drive on the roads. Regardless of how much data we collect, we still can’t foresee all the possible situations. This is why its goal is to <strong>minimize error, not to predict all the moves</strong>.<br>了解世界上所有的道路规则并不能教会自动驾驶仪如何在道路上行驶。不管我们收集了多少数据，我们仍然无法预见所有可能的情况。这就是为什么它的目标是最大限度地<strong>减少误差，而不是预测所有的移动</strong>。</p><p>Surviving in an environment is a core idea of reinforcement learning. Throw poor little robot into real life, punish it for errors and reward it for right deeds. Same way we teach our kids, right?<br>在环境中生存是强化学习的核心理念。把可怜的小机器人丢进现实生活中，惩罚它的错误，奖励它的正确行为。和我们教孩子的方式一样，对吧？</p><p>More effective way here — to build a virtual city and let self-driving car to learn all its tricks there first. That’s exactly how we train auto-pilots right now. Create a virtual city based on a real map, populate with pedestrians and let the car learn to kill as few people as possible. When the robot is reasonably confident in this artificial GTA, it’s freed to test in the real streets. Fun!<br>更有效的方法是建立一个虚拟城市，让自动驾驶汽车先在那里学习它的所有技巧。这正是我们现在训练自动驾驶的方式。在真实地图的基础上创建一个虚拟城市，挤满行人，让汽车学会尽可能少地杀人。当机器人对这种人造GTA相当有信心时，它就可以自由地在真实的街道上进行测试。享乐</p><p>There may be two different approaches — <strong>Model-Based and Model-Free</strong>.<br>可能有两种不同的方法——<strong>基于模型和无模型</strong>。</p><p>Model-Based means that car needs to memorize a map or its parts. That’s a pretty outdated approach since it’s impossible for the poor self-driving car to memorize the whole planet.<br>基于模型意味着汽车需要记住地图或其部件。这是一种相当过时的方法，因为糟糕的自动驾驶汽车不可能记住整个星球。</p><p>In Model-Free learning, the car doesn’t memorize every movement but tries to generalize situations and act rationally while obtaining a maximum reward.<br>在无模型学习中，汽车不会记住每一个动作，而是试图概括情况并理性行事，同时获得最大的奖励。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-23.jpg" class="" title="人人共享的机器学习-23"><p>how machines behave in case of fire？<br>classical programming：i counted all the scenarios, and now you have to take off your underwear and make a rope of it<br>meching learning：according to my statistics, people die in 6% of fires.So I recommend you to die now.<br>reinforcement learning：just run for your freaking life AAAA!!!<br>发生火灾时机器如何表现？<br>经典编程：我统计了所有的场景，现在你必须脱掉你的内衣，用它做一根绳子<br>机械学习：根据我的统计，6%的人死于火灾。所以我建议你现在就去死。<br>强化学习：为你该死的生活奔跑吧AAAA！！！</p><p>Remember the news about AI beating a top player at the game of Go? Despite shortly before this it being proved that the number of combinations in this game is greater than the number of atoms in the universe.<br>还记得人工智能在围棋比赛中击败顶尖棋手的新闻吗？尽管在此之前不久，已经证明了这个游戏中组合的数量大于宇宙中原子的数量。</p><p><a href="https://www.wired.com/2016/01/in-a-huge-breakthrough-googles-ai-beats-a-top-player-at-the-game-of-go/">AI beating a top player at the game of Go</a><br><a href="https://www.vice.com/en/article/vv7ejx/after-2500-years-a-chinese-gaming-mystery-is-solved">proved</a></p><p>This means the machine could not remember all the combinations and thereby win Go (as it did chess). At each turn, it simply chose the best move for each situation, and it did well enough to outplay a human meatbag.<br>这意味着机器无法记住所有组合，从而赢得围棋（就像下棋一样）。在每一个转弯处，它都会简单地为每种情况选择最好的动作，而且它做得足够好，胜过了人类的肉袋子。</p><p>This approach is a core concept behind Q-learning and its derivatives (SARSA &amp; DQN). ‘Q’ in the name stands for “Quality” as a robot learns to perform the most “qualitative” action in each situation and all the situations are memorized as a simple markovian process.<br>这种方法是Q学习及其衍生物（SARSA和DQN）背后的核心概念。”名称中的Q代表“质量”，因为机器人学会在每种情况下执行最“定性”的动作，所有情况都被记忆为一个简单的马尔可夫过程。 </p><p><a href="https://www.youtube.com/watch?v=aCEvtRtNO-M&amp;ab_channel=SirajRaval">Q-learning</a><br><a href="https://en.wikipedia.org/wiki/Markov_chain">markovian process</a></p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-24.jpg" class="" title="人人共享的机器学习-24"><p>Such a machine can test billions of situations in a virtual environment, remembering which solutions led to greater reward. But how can it distinguish previously seen situations from a completely new one? If a self-driving car is at a road crossing and the traffic light turns green — does it mean it can go now? What if there’s an ambulance rushing through a street nearby?<br>这样的机器可以在虚拟环境中测试数十亿种情况，记住哪些解决方案会带来更大的回报。但是，它如何区分以前看到的情况和全新的情况呢？如果一辆自动驾驶汽车在十字路口，红绿灯变绿，这是否意味着它现在可以行驶？如果有一辆救护车在附近的街道上飞驰怎么办？</p><p>The answer today is “no one knows”. There’s no easy answer. Researchers are constantly searching for it but meanwhile only finding workarounds. Some would hardcode all the situations manually that let them solve exceptional cases, like the trolley problem. Others would go deep and let neural networks do the job of figuring it out. This led us to the evolution of Q-learning called Deep Q-Network (DQN). But they are not a silver bullet either.<br>今天的答案是“没人知道”。没有简单的答案。研究人员一直在寻找它，但同时只找到变通办法。有些人会手动对所有情况进行硬编码，以解决特殊情况，如手推车问题。其他人会深入研究，让神经网络来解决这个问题。这导致了Q学习的发展，称为深度Q网络（DQN）。但它们也不是灵丹妙药。</p><p><a href="https://en.wikipedia.org/wiki/Trolley_problem">trolley problem</a></p><p>Reinforcement Learning for an average person would look like a real artificial intelligence. Because it makes you think wow, this machine is making decisions in real life situations! This topic is hyped right now, it’s advancing with incredible pace and intersecting with a neural network to clean your floor more accurately. Amazing world of technologies!<br>强化学习对于一个普通人来说就像一个真正的人工智能。因为它让你觉得哇，这台机器是在现实生活中做出决定的！这个话题现在被炒得沸沸扬扬，它以令人难以置信的速度前进，并与神经网络交叉，更准确地清洁你的地板。技术的奇妙世界！</p><p>Off-topic. When I was a student, genetic algorithms (link has cool visualization) were really popular. This is about throwing a bunch of robots into a single environment and making them try reaching the goal until they die. Then we pick the best ones, cross them, mutate some genes and rerun the simulation. After a few milliard years, we will get an intelligent creature. Probably. Evolution at its finest.<br>脱离主题。当我还是一个学生的时候，遗传算法（链接有很酷的可视化）真的很流行。这是关于把一群机器人扔到一个单一的环境中，让它们尝试达到目标，直到死亡。然后我们挑选最好的，将它们交叉，使一些基因发生突变，然后重新运行模拟。几百万年后，我们将得到一个聪明的生物。可能进化到了极致。</p><p><a href="https://rednuht.org/genetic_walkers/">genetic algorithms</a><br><a href="https://mathworld.wolfram.com/Milliard.html">milliard</a></p><p>Genetic algorithms are considered as part of reinforcement learning and they have the most important feature proved by decade-long practice: no one gives a shit about them.<br>遗传算法被认为是强化学习的一部分，经过十年的实践证明，它们有一个最重要的特点：没有人在乎它们。</p><p>Humanity still couldn’t come up with a task where those would be more effective than other methods. But they are great for student experiments and let people get their university supervisors excited about “artificial intelligence” without too much labour. And youtube would love it as well.<br>人类仍然无法想出一个比其他方法更有效的任务。但它们非常适合学生实验，让人们不用太多劳动就能让大学主管对“人工智能”感到兴奋。youtube也会喜欢的。 </p><h1 id="Part-3-Ensemble-Methods-第3部分。集成方法"><a href="#Part-3-Ensemble-Methods-第3部分。集成方法" class="headerlink" title="Part 3. Ensemble Methods 第3部分。集成方法"></a>Part 3. Ensemble Methods 第3部分。集成方法</h1><p>“Bunch of stupid trees learning to correct errors of each other”<br> “一群愚蠢的树学会互相纠正错误”</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-25.jpg" class="" title="人人共享的机器学习-25"><p>Nowadays is used for:</p><ul><li>Everything that fits classical algorithm approaches (but works better) 所有符合经典算法方法的东西（但效果更好）</li><li>Search systems (★) 搜索系统</li><li>Computer vision 计算机视觉</li><li>Object detection 物体检测</li></ul><p>Popular algorithms: Random Forest, Gradient Boosting<br>流行算法：随机森林，梯度增强</p><p><a href="https://en.wikipedia.org/wiki/Random_forest">Random Forest</a><br><a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient Boosting</a></p><p>It’s time for modern, grown-up methods. Ensembles and neural networks are two main fighters paving our path to a singularity. Today they are producing the most accurate results and are widely used in production.<br>是时候采用现代、成熟的方法了。集合和神经网络是为我们通往奇点铺平道路的两个主要斗士。如今，它们正在产生最准确的结果，并在生产中广泛使用。</p><p>However, the neural networks got all the hype today, while the words like “boosting” or “bagging” are scarce hipsters on TechCrunch.<br>然而，神经网络今天得到了所有的炒作，而像“助推”或“装袋”这样的词在TechCrunch上很少出现。</p><p>Despite all the effectiveness the idea behind these is overly simple. If you take a bunch of inefficient algorithms and force them to correct each other’s mistakes, the overall quality of a system will be higher than even the best individual algorithms.<br>尽管如此，这些背后的想法还是过于简单。如果你采用一堆低效的算法，强迫它们互相纠正错误，那么系统的整体质量甚至会高于最好的单个算法。</p><p>You’ll get even better results if you take the most unstable algorithms that are predicting completely different results on small noise in input data. Like Regression and Decision Trees. These algorithms are so sensitive to even a single outlier in input data to have models go mad.<br>如果你采用最不稳定的算法，在输入数据中的小噪声上预测完全不同的结果，你会得到更好的结果。比如回归树和决策树。这些算法对输入数据中的单个异常值都非常敏感，以至于模型都会发疯。</p><p>In fact, this is what we need.<br>事实上，这正是我们所需要的。</p><p>We can use any algorithm we know to create an ensemble. Just throw a bunch of classifiers, spice it up with regression and don’t forget to measure accuracy. From my experience: don’t even try a Bayes or kNN here. Although “dumb”, they are really stable. That’s boring and predictable. Like your ex.<br>我们可以使用我们所知道的任何算法来创建集合。只需抛出一堆分类器，用回归来增加趣味性，别忘了测量准确性。根据我的经验：甚至不要在这里尝试贝叶斯或kNN。虽然“笨”，但他们确实很稳定。这很无聊，而且是可以预测的。就像你的前任。</p><p>Instead, there are three battle-tested methods to create ensembles.<br>相反，有三种经过战斗考验的方法来创建乐团。</p><p><strong>Stacking</strong> Output of several parallel models is passed as input to the last one which makes a final decision. Like that girl who asks her girlfriends whether to meet with you in order to make the final decision herself.<br>几个并行模型的<strong>叠加</strong>输出作为输入传递给最后一个模型，后者做出最终决定。就像那个女孩问她的女朋友是否和你见面，以便自己做出最终决定。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-26.jpg" class="" title="人人共享的机器学习-26"><p>Emphasis here on the word “different”. Mixing the same algorithms on the same data would make no sense. The choice of algorithms is completely up to you. However, for final decision-making model, regression is usually a good choice.<br>这里强调“不同”这个词。在相同的数据上混合使用相同的算法是没有意义的。算法的选择完全取决于你。然而，对于最终的决策模型，回归通常是一个不错的选择。</p><p>Based on my experience stacking is less popular in practice, because two other methods are giving better accuracy.<br>根据我的经验，堆叠在实践中不太受欢迎，因为另外两种方法提供了更好的准确性。</p><p><strong>Bagging</strong> aka Bootstrap AGGregatING. Use the same algorithm but train it on different subsets of original data. In the end — just average answers.<br><strong>装袋</strong>又称Bootstrap聚集。使用相同的算法，但在原始数据的不同子集上进行训练。最后——只是一般的答案。</p><p>Data in random subsets may repeat. For example, from a set like “1-2-3” we can get subsets like “2-2-3”, “1-2-2”, “3-1-2” and so on. We use these new datasets to teach the same algorithm several times and then predict the final answer via simple majority voting.<br>随机子集中的数据可能重复。例如，从“1-2-3”这样的集合中，我们可以得到“2-2-3”、“1-2-2”、“3-1-2”等子集。我们使用这些新的数据集多次教授相同的算法，然后通过简单多数投票预测最终答案。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-27.jpg" class="" title="人人共享的机器学习-27"><p>The most famous example of bagging is the Random Forest algorithm, which is simply bagging on the decision trees (which were illustrated above). When you open your phone’s camera app and see it drawing boxes around people’s faces — it’s probably the results of Random Forest work. Neural networks would be too slow to run real-time yet bagging is ideal given it can calculate trees on all the shaders of a video card or on these new fancy ML processors.<br>装袋最著名的例子是随机森林算法，它只是在决策树上装袋（如上所述）。当你打开手机的相机应用程序，看到它在人们的脸上画方框时，这可能是随机森林工作的结果。神经网络运行速度太慢，无法实时运行，但考虑到它可以在视频卡的所有着色器或这些新的高级ML处理器上计算树，因此装袋是理想的。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-28.jpg" class="" title="人人共享的机器学习-28"><p><a href="https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&amp;ab_channel=StatQuestwithJoshStarmer">Random Forest</a></p><p>In some tasks, the ability of the Random Forest to run in parallel is more important than a small loss in accuracy to the boosting, for example. Especially in real-time processing. There is always a trade-off.<br>例如，在某些任务中，随机森林并行运行的能力比助推精度的小损失更重要。尤其是在实时处理中。总有一种权衡。</p><p><strong>Boosting</strong> Algorithms are trained one by one sequentially. Each subsequent one paying most of its attention to data points that were mispredicted by the previous one. Repeat until you are happy.<br><strong>Boosting</strong>算法是按顺序逐个训练的。随后的每一个都将大部分注意力放在前一个预测错误的数据点上。重复，直到你感到高兴。</p><p>Same as in bagging, we use subsets of our data but this time they are not randomly generated. Now, in each subsample we take a part of the data the previous algorithm failed to process. Thus, we make a new algorithm learn to fix the errors of the previous one.<br>与装袋一样，我们使用数据的子集，但这次它们不是随机生成的。现在，在每个子样本中，我们获取先前算法未能处理的部分数据。因此，我们使一个新的算法学习修复前一个算法的错误。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-29.jpg" class="" title="人人共享的机器学习-29"><p>The main advantage here — a very high, even illegal in some countries precision of classification that all cool kids can envy. The cons were already called out — it doesn’t parallelize. But it’s still faster than neural networks. It’s like a race between a dump truck and a racecar. The truck can do more, but if you want to go fast — take a car.<br>这里的主要优势是——分类精度非常高，在一些国家甚至是非法的，所有酷孩子都会羡慕。缺点已经被调用了——它没有并行化。但它仍然比神经网络更快。这就像一场自卸车和赛车之间的比赛。卡车可以做得更多，但如果你想走得快，那就开车吧。</p><p>If you want a real example of boosting — open Facebook or Google and start typing in a search query. Can you hear an army of trees roaring and smashing together to sort results by relevancy? That’s because they are using boosting.<br>如果你想要一个真正的提升示例——打开Facebook或谷歌，开始输入搜索查询。你能听到一大群树咆哮着砸在一起，根据相关性对结果进行排序吗？这是因为他们在使用助推。</p><blockquote><p>Nowadays there are three popular tools for boosting, you can read a comparative report in CatBoost vs. LightGBM vs. XGBoost<br>现在有三种流行的助推工具，你可以阅读CatBoost与LightGBM与XGBoost的比较报告</p></blockquote><p><a href="https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db">CatBoost vs. LightGBM vs. XGBoost</a></p><h1 id="Part-4-Neural-Networks-and-Deep-Leaning-第4部分。神经网络与深度学习"><a href="#Part-4-Neural-Networks-and-Deep-Leaning-第4部分。神经网络与深度学习" class="headerlink" title="Part 4. Neural Networks and Deep Leaning 第4部分。神经网络与深度学习"></a>Part 4. Neural Networks and Deep Leaning 第4部分。神经网络与深度学习</h1><p>“We have a thousand-layer network, dozens of video cards, but still no idea where to use it. Let’s generate cat pics!”<br> “我们有一个千层网络，几十个视频卡，但仍然不知道在哪里使用。让我们生成猫的照片！”</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-30.jpg" class="" title="人人共享的机器学习-30"><p>Used today for:</p><ul><li>Replacement of all algorithms above 替换上述所有算法</li><li>Object identification on photos and videos 照片和视频上的对象识别</li><li>Speech recognition and synthesis 语音识别与合成 </li><li>Image processing, style transfer 图像处理、风格转换</li><li>Machine translation 机器翻译</li></ul><p>Popular architectures: Perceptron, Convolutional Network (CNN), Recurrent Networks (RNN), Autoencoders<br>流行架构：感知器、卷积网络（CNN）、递归网络（RNN）、自动编码器</p><p><a href="https://en.wikipedia.org/wiki/Perceptron">Perceptron</a><br><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Network-CNN</a><br><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Networks-RNN</a><br><a href="https://en.wikipedia.org/wiki/Autoencoder">Autoencoders</a></p><p>If no one has ever tried to explain neural networks to you using “human brain” analogies, you’re happy. Tell me your secret. But first, let me explain it the way I like.<br>如果从来没有人试图用“人脑”的类比来向你解释神经网络，你会很高兴。告诉我你的秘密。但首先，让我用我喜欢的方式解释一下。</p><p>Any neural network is basically a collection of <strong>neurons</strong> and <strong>connections</strong> between them. <strong>Neuron</strong> is a function with a bunch of inputs and one output. Its task is to take all numbers from its input, perform a function on them and send the result to the output.<br>任何神经网络基本上都是<strong>神经元</strong>及<strong>其之间连接</strong>的集合。<strong>神经元</strong>是一个具有一组输入和一个输出的函数。它的任务是从输入中获取所有数字，对它们执行函数，并将结果发送到输出。</p><p>Here is an example of a simple but useful in real life neuron: sum up all numbers from the inputs and if that sum is bigger than N — give 1 as a result. Otherwise — zero.<br>这里有一个简单但在现实生活中有用的神经元的例子：将输入的所有数字相加，如果总和大于N，则给出1。否则——零。</p><p>Connections are like channels between neurons. They connect outputs of one neuron with the inputs of another so they can send digits to each other. Each connection has only one parameter — weight. It’s like a connection strength for a signal. When the number 10 passes through a connection with a weight 0.5 it turns into 5.<br>连接就像神经元之间的通道。它们将一个神经元的输出与另一个神经元输入连接起来，以便相互发送数字。每个连接只有一个参数——权重。这就像信号的连接强度。当数字10通过一个具有0.5重量的连接时，它变成了5。</p><p>These weights tell the neuron to respond more to one input and less to another. Weights are adjusted when training — that’s how the network learns. Basically, that’s all there is to it.<br>这些权重告诉神经元对一个输入的反应更多，对另一个输入反应更少。训练时会调整权重——这就是网络学习的方式。基本上，这就是它的全部。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-31.jpg" class="" title="人人共享的机器学习-31"><p>To prevent the network from falling into anarchy, the neurons are linked by layers, not randomly. Within a layer neurons are not connected, but they are connected to neurons of the next and previous layers. Data in the network goes strictly in one direction — from the inputs of the first layer to the outputs of the last.<br>为了防止网络陷入无政府状态，神经元是分层连接的，而不是随机连接的。在一层内，神经元不相连，但它们与下一层和前一层的神经元相连。网络中的数据严格地朝着一个方向发展——从第一层的输入到最后一层的输出。</p><p>If you throw in a sufficient number of layers and put the weights correctly, you will get the following: by applying to the input, say, the image of handwritten digit 4, black pixels activate the associated neurons, they activate the next layers, and so on and on, until it finally lights up the exit in charge of the four. The result is achieved.<br>如果你放入足够多的层并正确放置权重，你会得到以下结果：通过应用于输入，比如手写数字4的图像，黑色像素激活相关的神经元，它们激活下一层，以此类推，直到它最终照亮负责这四层的出口。取得了效果。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-32.jpg" class="" title="人人共享的机器学习-32"><p>When doing real-life programming nobody is writing neurons and connections. Instead, everything is represented as matrices and calculated based on matrix multiplication for better performance. My favourite video on this and its sequel below describe the whole process in an easily digestible way using the example of recognizing hand-written digits. Watch them if you want to figure this out.<br>在进行真实编程时，没有人在编写神经元和连接。相反，所有内容都表示为矩阵，并基于矩阵乘法进行计算，以获得更好的性能。我最喜欢的这段视频及其后续视频以识别手写数字为例，以易于理解的方式描述了整个过程。如果你想弄清楚这一点，请注意他们。</p><p><a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;ab_channel=3Blue1Brown">但是什么是神经网络？ |第一章深入学习</a></p><blockquote><p>A network that has multiple layers that have connections between every neuron is called a perceptron (MLP) and considered the simplest architecture for a novice. I didn’t see it used for solving tasks in production.<br>一个具有多层且每个神经元之间都有连接的网络被称为感知器（MLP），被认为是新手最简单的架构。我没有看到它被用于解决生产中的任务。</p></blockquote><p><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">perceptron</a></p><p>After we constructed a network, our task is to assign proper ways so neurons will react correctly to incoming signals. Now is the time to remember that we have data that is samples of ‘inputs’ and proper ‘outputs’. We will be showing our network a drawing of the same digit 4 and tell it ‘adapt your weights so whenever you see this input your output would emit 4’.<br>在我们构建了一个网络后，我们的任务是分配正确的方式，使神经元对传入的信号做出正确的反应。现在是时候记住，我们拥有的数据是“输入”和正确“输出”的样本了。我们将向我们的网络展示同一数字4的图形，并告诉它“调整你的权重，这样每当你看到这个输入时，你的输出就会发出4”。</p><p>To start with, all weights are assigned randomly. After we show it a digit it emits a random answer because the weights are not correct yet, and we compare how much this result differs from the right one. Then we start traversing network backward from outputs to inputs and tell every neuron ‘hey, you did activate here but you did a terrible job and everything went south from here downwards, let’s keep less attention to this connection and more of that one, mkay?’.<br>首先，所有权重都是随机分配的。在我们给它显示一个数字后，它会发出一个随机答案，因为权重还不正确，我们比较这个结果与正确的结果有多大差异。然后，我们开始从输出到输入反向遍历网络，并告诉每个神经元“嘿，你确实在这里激活了，但你做得很糟糕，一切都从这里向下发展，让我们少关注这个连接，多关注那个连接，mkay？”。</p><p>After hundreds of thousands of such cycles of ‘infer-check-punish’, there is a hope that the weights are corrected and act as intended. The science name for this approach is <strong>Backpropagation</strong>, or a ‘method of backpropagating an error’. Funny thing it took twenty years to come up with this method. Before this we still taught neural networks somehow.<br>经过数十万次这样的“推断-检查-惩罚”循环，有希望纠正权重并按预期行事。这种方法的科学名称是<strong>反向传播</strong>，或“反向传播错误的方法”。有趣的是，花了二十年的时间才想出这个方法。在此之前，我们仍然以某种方式教授神经网络。</p><p><a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation</a></p><p>My second favorite vid is describing this process in depth, but it’s still very accessible.<br>我第二喜欢的视频是深入描述这个过程，但它仍然很容易理解。</p><p><a href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;ab_channel=3Blue1Brown">梯度下降，神经网络如何学习|深度学习，第2章</a></p><p><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></p><p>A well trained neural network can fake the work of any of the algorithms described in this chapter (and frequently works more precisely). This universality is what made them widely popular. Finally we have an architecture of human brain they said we just need to assemble lots of layers and teach them on any possible data they hoped. Then the first AI winter started, then it thawed, and then another wave of disappointment hit.<br>经过良好训练的神经网络可以伪造本章中描述的任何算法的工作（而且通常工作得更精确）。这种普遍性使它们广受欢迎。最后，我们有了人类大脑的架构，他们说我们只需要组装很多层，并根据他们希望的任何可能的数据教授他们。然后第一个人工智能的冬天开始了，然后它解冻了，然后又一波失望袭来。</p><p><a href="https://en.wikipedia.org/wiki/AI_winter">AI winter</a></p><p>It turned out networks with a large number of layers required computation power unimaginable at that time. Nowadays any gamer PC with geforces outperforms the datacenters of that time. So people didn’t have any hope then to acquire computation power like that and neural networks were a huge bummer.<br>事实证明，具有大量层的网络需要当时难以想象的计算能力。如今，任何一台拥有geforces的游戏PC都胜过当时的数据中心。因此，当时人们对获得这样的计算能力没有任何希望，而神经网络是一个巨大的障碍。</p><p>And then ten years ago deep learning rose.<br>十年前，深度学习兴起了。</p><blockquote><p>There’s a nice Timeline of machine learning describing the rollercoaster of hopes &amp; waves of pessimism.<br>有一个很好的机器学习时间表，描述了希望和悲观情绪的过山车。</p></blockquote><p><a href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning">Timeline of machine learning</a></p><p>In 2012 convolutional neural networks acquired an overwhelming victory in ImageNet competition that made the world suddenly remember about methods of deep learning described in the ancient 90s. Now we have video cards!<br>2012年，卷积神经网络在ImageNet竞争中取得了压倒性的胜利，这让世界突然想起了90年代描述的深度学习方法。现在我们有了视频卡！</p><p><a href="https://habr.com/en/articles/183380/">overwhelming victory in ImageNet competition</a></p><p>Differences of deep learning from classical neural networks were in new methods of training that could handle bigger networks. Nowadays only theoretics would try to divide which learning to consider deep and not so deep. And we, as practitioners are using popular ‘deep’ libraries like Keras, TensorFlow &amp; PyTorch even when we build a mini-network with five layers. Just because it’s better suited than all the tools that came before. And we just call them neural networks.<br>深度学习与经典神经网络的不同之处在于可以处理更大网络的新训练方法。如今，只有理论家才会试图划分哪门学问应该考虑得很深和不那么深。作为从业者，我们正在使用流行的“深度”库，如Keras、TensorFlow和PyTorch，即使我们构建了一个有五层的迷你网络。只是因为它比以前的所有工具都更适合。我们称之为神经网络。</p><p><a href="https://keras.io/">Keras</a><br><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a><br><a href="https://pytorch.org/">PyTorch</a></p><p>I’ll tell about two main kinds nowadays.<br>我现在要讲两种主要的。</p><h2 id="Convolutional-Neural-Networks-CNN-卷积神经网络（CNN）"><a href="#Convolutional-Neural-Networks-CNN-卷积神经网络（CNN）" class="headerlink" title="Convolutional Neural Networks (CNN) 卷积神经网络（CNN）"></a>Convolutional Neural Networks (CNN) 卷积神经网络（CNN）</h2><p>Convolutional neural networks are all the rage right now. They are used to search for objects on photos and in videos, face recognition, style transfer, generating and enhancing images, creating effects like slow-mo and improving image quality. Nowadays CNNs are used in all the cases that involve pictures and video s. Even in your iPhone several of these networks are going through your nudes to detect objects in those. If there is something to detect, heh.<br>卷积神经网络现在风靡一时。它们用于搜索照片和视频中的对象、人脸识别、风格转换、生成和增强图像、创建慢动作等效果并提高图像质量。如今，所有涉及图片和视频的案例都使用了细胞神经网络。即使在你的iPhone中，其中几个网络也会通过你的裸体来检测其中的物体。如果有什么要探测的，呵呵。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-33.jpg" class="" title="人人共享的机器学习-33"><blockquote><p>Image above is a result produced by Detectron that was recently open-sourced by Facebook<br>上图是Detectron最近由Facebook开源的结果</p></blockquote><p><a href="https://github.com/facebookresearch/Detectron">Detectron</a></p><p>A problem with images was always the difficulty of extracting features out of them. You can split text by sentences, lookup words’ attributes in specialized vocabularies, etc. But images had to be labeled manually to teach the machine where cat ears or tails were in this specific image. This approach got the name ‘handcrafting features’ and used to be used almost by everyone.<br>图像的一个问题总是难以从中提取特征。你可以按句子分割文本，在专业词汇表中查找单词的属性等。但图像必须手动标记，才能告诉机器猫耳朵或尾巴在这个特定图像中的位置。这种方法被称为“手工制作功能”，过去几乎每个人都在使用。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-34.jpg" class="" title="人人共享的机器学习-34"><p>There are lots of issues with the handcrafting.<br>手工制作有很多问题。</p><p>First of all, if a cat had its ears down or turned away from the camera: you are in trouble, the neural network won’t see a thing.<br>首先，如果一只猫低着耳朵或转身离开相机：你有麻烦了，神经网络什么都看不见。</p><p>Secondly, try naming on the spot 10 different features that distinguish cats from other animals. I for one couldn’t do it, but when I see a black blob rushing past me at night — even if I only see it in the corner of my eye — I would definitely tell a cat from a rat. Because people don’t look only at ear form or leg count and account lots of different features they don’t even think about. And thus cannot explain it to the machine.<br>其次，试着当场命名10种不同的特征，将猫与其他动物区分开来。就我而言，我做不到，但当我在晚上看到一个黑色的斑点从我身边掠过时——即使我只是从眼角看到它——我肯定会分辨猫和老鼠。因为人们不仅仅看耳朵的形状或腿的数量，还考虑了很多他们甚至没有想过的不同特征。因此无法向机器解释。</p><p>So it means the machine needs to learn such features on its own, building on top of basic lines. We’ll do the following: first, we divide the whole image into 8x8 pixel blocks and assign to each a type of dominant line – either horizontal [-], vertical [|] or one of the diagonals [/]. It can also be that several would be highly visible — this happens and we are not always absolutely confident.<br>因此，这意味着机器需要在基本线的基础上自行学习这些功能。我们将执行以下操作：首先，我们将整个图像划分为8x8像素的块，并为每个块分配一种类型的主导线——水平[-]、垂直[|]或其中一条对角线[/]。也可能有几个会非常明显——这种情况发生了，我们并不总是绝对有信心。</p><p>Output would be several tables of sticks that are in fact the simplest features representing objects edges on the image. They are images on their own but built out of sticks. So we can once again take a block of 8x8 and see how they match together. And again and again…<br>输出将是几张棍子表，这些棍子实际上是表示图像上对象边缘的最简单特征。它们是自己的图像，但却是用棍子做成的。所以我们可以再次取一个8x8的块，看看它们是如何匹配在一起的。一次又一次…</p><p>This operation is called convolution, which gave the name for the method. Convolution can be represented as a layer of a neural network, because each neuron can act as any function.<br>这个操作被称为卷积，它为该方法命名。卷积可以表示为神经网络的一层，因为每个神经元都可以作为任何函数。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-35.jpg" class="" title="人人共享的机器学习-35"><p>When we feed our neural network with lots of photos of cats it automatically assigns bigger weights to those combinations of sticks it saw the most frequently. It doesn’t care whether it was a straight line of a cat’s back or a geometrically complicated object like a cat’s face, something will be highly activating.<br>当我们给神经网络提供大量猫的照片时，它会自动为它最频繁看到的棍子组合分配更大的权重。它不在乎它是猫背部的一条直线，还是像猫脸这样几何复杂的物体，有些东西会非常活跃。</p><p>As the output, we would put a simple perceptron which will look at the most activated combinations and based on that differentiate cats from dogs.<br>作为输出，我们将放置一个简单的感知器，它将观察最活跃的组合，并以此为基础区分猫和狗。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-36.jpg" class="" title="人人共享的机器学习-36"><p>The beauty of this idea is that we have a neural net that searches for the most distinctive features of the objects on its own. We don’t need to pick them manually. We can feed it any amount of images of any object just by googling billions of images with it and our net will create feature maps from sticks and learn to differentiate any object on its own.<br>这个想法的美妙之处在于，我们有一个神经网络，可以自己搜索物体最独特的特征。我们不需要手动挑选它们。我们只需在谷歌上搜索数十亿张图像，就可以向它提供任何物体的任何数量的图像，我们的网络将从棒中创建特征图，并学会独立区分任何物体。</p><p>For this I even have a handy unfunny joke:<br>对此，我甚至有一个不好笑的笑话：</p><blockquote><p>Give your neural net a fish and it will be able to detect fish for the rest of its life. Give your neural net a fishing rod and it will be able to detect fishing rods for the rest of its life…<br>给你的神经网络一条鱼，它将能够在余生中检测到鱼。给你的神经网络一根鱼竿，它将能够在余生中检测鱼竿…</p></blockquote><h2 id="Recurrent-Neural-Networks-RNN-递归神经网络"><a href="#Recurrent-Neural-Networks-RNN-递归神经网络" class="headerlink" title="Recurrent Neural Networks (RNN) 递归神经网络"></a>Recurrent Neural Networks (RNN) 递归神经网络</h2><p>The second most popular architecture today. Recurrent networks gave us useful things like neural machine translation (here is my post about it), speech recognition and voice synthesis in smart assistants. RNNs are the best for sequential data like voice, text or music.<br>当今第二流行的建筑。递归网络为我们提供了一些有用的东西，比如神经机器翻译（这是我的帖子）、语音识别和智能助手中的语音合成。RNN最适用于语音、文本或音乐等顺序数据。</p><p><a href="https://vas3k.com/blog/machine_translation/">Machine Translation-From the Cold War to Deep Learning</a></p><p>Remember Microsoft Sam, the old-school speech synthesizer from Windows XP? That funny guy builds words letter by letter, trying to glue them up together. Now, look at Amazon Alexa or Assistant from Google. They don’t only say the words clearly, they even place the right accents!<br>还记得微软的Sam吗？那是Windows XP的老式语音合成器？那个有趣的家伙一个字母一个字母地造单词，试图把它们粘在一起。现在，看看亚马逊Alexa或谷歌助手。他们不仅把单词说得很清楚，他们甚至用正确的口音！</p><p><a href="https://www.youtube.com/watch?v=NG-LATBZNBs&amp;ab_channel=SomethingUnreal">Neural Net is trying to speak</a></p><p>All because modern voice assistants are trained to speak not letter by letter, but on whole phrases at once. We can take a bunch of voiced texts and train a neural network to generate an audio-sequence closest to the original speech.<br>这一切都是因为现代语音助理被训练成不是一个字母一个字母地说话，而是同时说出整个短语。我们可以取一堆有声文本，训练神经网络生成最接近原始语音的音频序列。</p><p>In other words, we use text as input and its audio as the desired output. We ask a neural network to generate some audio for the given text, then compare it with the original, correct errors and try to get as close as possible to ideal.<br>换句话说，我们使用文本作为输入，使用它的音频作为所需的输出。我们要求神经网络为给定的文本生成一些音频，然后将其与原始文本进行比较，纠正错误，并尽可能接近理想。</p><p>Sounds like a classical learning process. Even a perceptron is suitable for this. But how should we define its outputs? Firing one particular output for each possible phrase is not an option — obviously.<br>听起来像是一个经典的学习过程。即使是感知器也适用于此。但我们应该如何定义其产出？显然，为每个可能的短语触发一个特定的输出不是一种选择。</p><p>Here we’ll be helped by the fact that text, speech or music are sequences. They consist of consecutive units like syllables. They all sound unique but depend on previous ones. Lose this connection and you get dubstep.<br>在这里，文本、语音或音乐都是序列这一事实将对我们有所帮助。它们像音节一样由连续的单元组成。它们听起来都很独特，但依赖于以前的版本。失去此连接，您将获得dubstep。</p><p>We can train the perceptron to generate these unique sounds, but how will it remember previous answers? So the idea is to add memory to each neuron and use it as an additional input on the next run. A neuron could make a note for itself - hey, we had a vowel here, the next sound should sound higher (it’s a very simplified example).<br>我们可以训练感知器来产生这些独特的声音，但它将如何记住以前的答案？因此，我们的想法是为每个神经元添加内存，并在下一次运行时将其用作额外的输入。神经元可以为自己做一个音符——嘿，我们这里有一个元音，下一个声音应该听起来更高（这是一个非常简单的例子）。</p><p>That’s how recurrent networks appeared. 循环网络就是这样出现的。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-37.jpg" class="" title="人人共享的机器学习-37"><p>This approach had one huge problem - when all neurons remembered their past results, the number of connections in the network became so huge that it was technically impossible to adjust all the weights.<br>这种方法有一个巨大的问题——当所有神经元都记得他们过去的结果时，网络中的连接数量变得如此巨大，以至于从技术上讲不可能调整所有的权重。</p><p>When a neural network can’t forget, it can’t learn new things (people have the same flaw).<br>当神经网络无法忘记时，它就无法学习新事物（人们也有同样的缺陷）。</p><p>The first decision was simple: limit the neuron memory. Let’s say, to memorize no more than 5 recent results. But it broke the whole idea.<br>第一个决定很简单：限制神经元的记忆。比方说，记住不超过5个最近的结果。但它打破了整个想法。</p><p>A much better approach came later: to use special cells, similar to computer memory. Each cell can record a number, read it or reset it. They were called long and short-term memory (LSTM) cells.<br>后来出现了一种更好的方法：使用类似于计算机内存的特殊单元。每个细胞都可以记录、读取或重置一个数字。它们被称为长短期记忆（LSTM）细胞。</p><p>Now, when a neuron needs to set a reminder, it puts a flag in that cell. Like “it was a consonant in a word, next time use different pronunciation rules”. When the flag is no longer needed, the cells are reset, leaving only the “long-term” connections of the classical perceptron. In other words, the network is trained not only to learn weights but also to set these reminders.<br>现在，当神经元需要设置提醒时，它会在该细胞中设置一个标志。就像“它是一个单词中的一个辅音，下次使用不同的发音规则”。当不再需要标志时，单元被重置，只留下经典感知器的“长期”连接。换句话说，网络不仅被训练来学习权重，还被训练来设置这些提醒。</p><p>Simple, but it works!<br>很简单，但它有效！</p><p>You can take speech samples from anywhere. BuzzFeed, for example, took Obama’s speeches and trained a neural network to imitate his voice. As you see, audio synthesis is already a simple task. Video still has issues, but it’s a question of time.<br>你可以从任何地方采集语音样本。例如，BuzzFeed采用了奥巴马的演讲，并训练了一个神经网络来模仿他的声音。正如您所看到的，音频合成已经是一项简单的任务。视频仍然有问题，但这是时间问题。</p><p><a href="https://www.youtube.com/watch?v=cQ54GDm1eL0&amp;embeds_referring_euri=https%3A%2F%2Fvas3k.com%2F&amp;source_ve_path=Mjg2NjY&amp;feature=emb_logo">CNN + RNN = Fake Obama</a></p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-38.webp" class="" title="人人共享的机器学习-38"><p>There are many more network architectures in the wild. I recommend a good article called Neural Network Zoo, where almost all types of neural networks are collected and briefly explained.<br>现在有更多的网络体系结构。我推荐一篇名为《神经网络动物园》的好文章，其中收集并简要解释了几乎所有类型的神经网络。</p><p><a href="http://www.asimovinstitute.org/neural-network-zoo/">Neural Network Zoo</a></p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-39.png" class="" title="人人共享的机器学习-39"><h1 id="The-End-when-the-war-with-the-machines-结局：什么时候和机器开战？"><a href="#The-End-when-the-war-with-the-machines-结局：什么时候和机器开战？" class="headerlink" title="The End: when the war with the machines? 结局：什么时候和机器开战？"></a>The End: when the war with the machines? 结局：什么时候和机器开战？</h1><p>The main problem here is that the question “when will the machines become smarter than us and enslave everyone?” is initially wrong. There are too many hidden conditions in it.<br>这里的主要问题是，“机器什么时候会变得比我们更聪明并奴役所有人？”这个问题最初是错误的。其中有太多的隐藏条件。</p><p>We say “become smarter than us” like we mean that <strong>there is a certain unified scale of intelligence</strong>. The top of which is a human, dogs are a bit lower, and stupid pigeons are hanging around at the very bottom.<br>我们说“变得比我们更聪明”，就像我们的意思是<strong>智力有一定的统一尺度一样</strong>。它的顶部是一个人，狗有点低，愚蠢的鸽子在最底部徘徊。</p><p>That’s wrong.<br>这是错误的。</p><p>If this were the case, every human must beat animals in everything but it’s not true. The average squirrel can remember a thousand hidden places with nuts — I can’t even remember where are my keys.<br>如果是这样的话，每个人都必须在任何事情上打败动物，但事实并非如此。一般松鼠都能记住一千个藏着坚果的地方——我甚至记不起我的钥匙在哪里了。</p><p>So intelligence is a set of different skills, not a single measurable value? Or is remembering nuts stashed locations not included in intelligence?<br>所以智力是一套不同的技能，而不是一个单一的可测量的价值？还是情报中没有包括记住坚果的藏匿地点？</p><p>An even more interesting question for me - <strong>why do we believe that the human brain possibilities are limited</strong>? There are many popular graphs on the Internet, where the technological progress is drawn as an exponent and the human possibilities are constant. But is it?<br>对我来说，还有一个更有趣的问题——<strong>为什么我们认为人脑的可能性是有限的</strong>？互联网上有许多流行的图表，其中技术进步被画成指数，人类的可能性是恒定的。但是是吗？</p><p>Ok, multiply 1680 by 950 right now in your mind. I know you won’t even try, lazy bastards. But give you a calculator — you’ll do it in two seconds. Does this mean that the calculator just expanded the capabilities of your brain?<br>好的，现在在你的脑海中用1680乘以950。我知道你甚至不会尝试，懒惰的混蛋。但给你一个计算器，你两秒钟就能算出。这是否意味着计算器只是扩展了你大脑的功能？</p><p>If yes, can I continue to expand them with other machines? Like, use notes in my phone to not to remember a shitload of data? Oh, seems like I’m doing it right now. I’m expanding the capabilities of my brain with the machines.<br>如果是，我可以继续用其他机器扩展它们吗？比如，用我手机里的笔记来不记住大量数据？哦，看来我现在正在做。我正在用机器扩展我大脑的能力。</p><p>Think about it. Thanks for reading.<br>想想看。谢谢你的阅读。</p><img src="/2023/12/19/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E4%BA%BA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-40.png" class="" title="人人共享的机器学习-40">]]></content>
    
    
    <summary type="html">人人共享的机器学习</summary>
    
    
    
    <category term="机器学习" scheme="http://hibiscidai.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习笔记" scheme="http://hibiscidai.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="机器学习" scheme="http://hibiscidai.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
