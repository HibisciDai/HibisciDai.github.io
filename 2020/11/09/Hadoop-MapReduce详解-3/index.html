<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Hadoop-MapReduce详解-3"><meta name="keywords" content="学习笔记,Hadoop,大数据"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>Hadoop-MapReduce详解-3 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3"><span class="toc-number">1.</span> <span class="toc-text">Hadoop-MapReduce详解-3</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC4%E7%AB%A0-Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.</span> <span class="toc-text">第4章 Hadoop数据压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">压缩概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MR%E6%94%AF%E6%8C%81%E7%9A%84%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81"><span class="toc-number">2.2.</span> <span class="toc-text">MR支持的压缩编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E6%96%B9%E5%BC%8F%E9%80%89%E6%8B%A9"><span class="toc-number">2.3.</span> <span class="toc-text">压缩方式选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Gzip%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.3.1.</span> <span class="toc-text">Gzip压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bzip2%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.3.2.</span> <span class="toc-text">Bzip2压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lzo%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.3.3.</span> <span class="toc-text">Lzo压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Snappy%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.3.4.</span> <span class="toc-text">Snappy压缩</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9"><span class="toc-number">2.4.</span> <span class="toc-text">压缩位置选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.4.1.</span> <span class="toc-text">输入端采用压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mapper%E8%BE%93%E5%87%BA%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.4.2.</span> <span class="toc-text">Mapper输出采用压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reducer%E8%BE%93%E5%87%BA%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.4.3.</span> <span class="toc-text">Reducer输出采用压缩</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-number">2.5.</span> <span class="toc-text">压缩参数配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E5%AE%9E%E6%93%8D%E6%A1%88%E4%BE%8B"><span class="toc-number">2.6.</span> <span class="toc-text">压缩实操案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%92%8C%E8%A7%A3%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.6.1.</span> <span class="toc-text">数据流的压缩和解压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">2.6.2.</span> <span class="toc-text">测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Map%E8%BE%93%E5%87%BA%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.6.3.</span> <span class="toc-text">Map输出端采用压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reduce%E8%BE%93%E5%87%BA%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.6.4.</span> <span class="toc-text">Reduce输出端采用压缩</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC5%E7%AB%A0-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">第5章 Yarn资源调度器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Yarn%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">Yarn基本架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Yarn%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">3.2.</span> <span class="toc-text">Yarn工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E5%85%A8%E8%BF%87%E7%A8%8B"><span class="toc-number">3.3.</span> <span class="toc-text">作业提交全过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Yarn%E9%98%B6%E6%AE%B5"><span class="toc-number">3.3.1.</span> <span class="toc-text">Yarn阶段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapReduce%E9%98%B6%E6%AE%B5"><span class="toc-number">3.3.2.</span> <span class="toc-text">MapReduce阶段</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">3.4.</span> <span class="toc-text">资源调度器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%88%E8%BF%9B%E5%85%88%E5%87%BA%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88FIFO%EF%BC%89"><span class="toc-number">3.4.1.</span> <span class="toc-text">先进先出调度器（FIFO）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88Capacity-Scheduler%EF%BC%89"><span class="toc-number">3.4.2.</span> <span class="toc-text">容量调度器（Capacity Scheduler）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88Fair-Scheduler%EF%BC%89"><span class="toc-number">3.4.3.</span> <span class="toc-text">公平调度器（Fair Scheduler）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C"><span class="toc-number">3.5.</span> <span class="toc-text">任务的推测执行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC6%E7%AB%A0-Hadoop%E4%BC%81%E4%B8%9A%E4%BC%98%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">第6章 Hadoop企业优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce%E8%B7%91%E5%BE%97%E6%85%A2%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="toc-number">4.1.</span> <span class="toc-text">MapReduce跑得慢的原因</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">MapReduce优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5"><span class="toc-number">4.2.1.</span> <span class="toc-text">数据输入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Map%E9%98%B6%E6%AE%B5"><span class="toc-number">4.2.2.</span> <span class="toc-text">Map阶段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reduce%E9%98%B6%E6%AE%B5"><span class="toc-number">4.2.3.</span> <span class="toc-text">Reduce阶段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#IO%E4%BC%A0%E8%BE%93"><span class="toc-number">4.2.4.</span> <span class="toc-text">IO传输</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E9%97%AE%E9%A2%98"><span class="toc-number">4.2.5.</span> <span class="toc-text">数据倾斜问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%8E%B0%E8%B1%A1"><span class="toc-number">4.2.5.1.</span> <span class="toc-text">数据倾斜现象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%8F%E5%B0%91%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.5.2.</span> <span class="toc-text">减少数据倾斜的方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B0%83%E4%BC%98%E5%8F%82%E6%95%B0"><span class="toc-number">4.2.6.</span> <span class="toc-text">常用的调优参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B5%84%E6%BA%90%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0"><span class="toc-number">4.2.6.1.</span> <span class="toc-text">资源相关参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%B9%E9%94%99%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0-MapReduce%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96"><span class="toc-number">4.2.6.2.</span> <span class="toc-text">容错相关参数(MapReduce性能优化)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">HDFS小文件优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%B0%8F%E6%96%87%E4%BB%B6%E5%BC%8A%E7%AB%AF"><span class="toc-number">4.3.1.</span> <span class="toc-text">HDFS小文件弊端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%B0%8F%E6%96%87%E4%BB%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">4.3.2.</span> <span class="toc-text">HDFS小文件解决方案</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC7%E7%AB%A0-MapReduce%E6%89%A9%E5%B1%95%E6%A1%88%E4%BE%8B"><span class="toc-number">5.</span> <span class="toc-text">第7章 MapReduce扩展案例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E6%A1%88%E4%BE%8B%EF%BC%88%E5%A4%9Ajob%E4%B8%B2%E8%81%94%EF%BC%89"><span class="toc-number">5.1.</span> <span class="toc-text">倒排索引案例（多job串联）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-number">5.1.1.</span> <span class="toc-text">需求分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.1.2.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%A4%84%E7%90%86"><span class="toc-number">5.1.2.1.</span> <span class="toc-text">第一次处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%A4%84%E7%90%86"><span class="toc-number">5.1.2.2.</span> <span class="toc-text">第二次处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TopN%E6%A1%88%E4%BE%8B"><span class="toc-number">5.2.</span> <span class="toc-text">TopN案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-1"><span class="toc-number">5.2.1.</span> <span class="toc-text">需求分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82"><span class="toc-number">5.2.1.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">5.2.1.2.</span> <span class="toc-text">输入数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E6%95%B0%E6%8D%AE"><span class="toc-number">5.2.1.3.</span> <span class="toc-text">输出数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">5.2.2.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%BE%E5%8D%9A%E5%AE%A2%E5%85%B1%E5%90%8C%E7%B2%89%E4%B8%9D%E6%A1%88%E4%BE%8B"><span class="toc-number">5.3.</span> <span class="toc-text">找博客共同粉丝案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-2"><span class="toc-number">5.3.1.</span> <span class="toc-text">需求分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-1"><span class="toc-number">5.3.1.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5-1"><span class="toc-number">5.3.1.2.</span> <span class="toc-text">数据输入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90"><span class="toc-number">5.3.1.3.</span> <span class="toc-text">流程分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">5.3.2.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC8%E7%AB%A0-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">6.</span> <span class="toc-text">第8章 常见错误及解决方案</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">222</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">76</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">29</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">Hadoop-MapReduce详解-3</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-09</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hadoop/">Hadoop</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">11.4k</span><span class="post-meta__separator">|</span><span>阅读时长: 47 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2020/11/09/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3.png" class="" title="Hadoop-MapReduce详解-3">
<p>Hadoop-MapReduce详解-3</p>
<span id="more"></span>
<p>[TOC]</p>
<h1 id="Hadoop-MapReduce详解-3"><a href="#Hadoop-MapReduce详解-3" class="headerlink" title="Hadoop-MapReduce详解-3"></a>Hadoop-MapReduce详解-3</h1><h1 id="第4章-Hadoop数据压缩"><a href="#第4章-Hadoop数据压缩" class="headerlink" title="第4章 Hadoop数据压缩"></a>第4章 Hadoop数据压缩</h1><h2 id="压缩概述"><a href="#压缩概述" class="headerlink" title="压缩概述"></a>压缩概述</h2><p>压缩技术能够有效减少底层存储系统(HDFS)读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时, I/O操作、网络数据传输、Shufle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此,使用数据压缩显得非常重要。</p>
<p>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。</p>
<p>压缩是提高Hadoop运行效率的一种优化策略。</p>
<p>通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度。</p>
<blockquote>
<p>注意:采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能。</p>
</blockquote>
<ul>
<li>压缩基本原则:</li>
</ul>
<p>(1) 运算密集型的job，少用压缩（涉及大量公式，算法）<br>(2) IO密集型的job，多用压缩（数据传输）</p>
<h2 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">压缩格式</th>
<th style="text-align:center">hadoop自带？</th>
<th style="text-align:center">算法</th>
<th style="text-align:center">文件扩展名</th>
<th style="text-align:center">是否可切分</th>
<th style="text-align:center">换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">DEFLATE</td>
<td style="text-align:center">是，直接使用</td>
<td style="text-align:center">DEFLATE</td>
<td style="text-align:center">.deflate</td>
<td style="text-align:center">否</td>
<td style="text-align:center">和文本处理一样，不需要修改</td>
</tr>
<tr>
<td style="text-align:center">Gzip</td>
<td style="text-align:center">是，直接使用</td>
<td style="text-align:center">DEFLATE</td>
<td style="text-align:center">.gz</td>
<td style="text-align:center">否</td>
<td style="text-align:center">和文本处理一样，不需要修改</td>
</tr>
<tr>
<td style="text-align:center">bzip2</td>
<td style="text-align:center">是，直接使用</td>
<td style="text-align:center">bzip2</td>
<td style="text-align:center">.bz2</td>
<td style="text-align:center">是</td>
<td style="text-align:center">和文本处理一样，不需要修改</td>
</tr>
<tr>
<td style="text-align:center">LZO</td>
<td style="text-align:center">否，需要安装</td>
<td style="text-align:center">LZO</td>
<td style="text-align:center">.lzo</td>
<td style="text-align:center">是</td>
<td style="text-align:center">需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td style="text-align:center">Snappy</td>
<td style="text-align:center">否，需要安装</td>
<td style="text-align:center">Snappy</td>
<td style="text-align:center">.snappy</td>
<td style="text-align:center">否</td>
<td style="text-align:center">和文本处理一样，不需要修改</td>
</tr>
</tbody>
</table>
</div>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">压缩格式</th>
<th style="text-align:center">对应的编码/解码器</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">DEFLATE</td>
<td style="text-align:center">org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td style="text-align:center">gzip</td>
<td style="text-align:center">org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td style="text-align:center">bzip2</td>
<td style="text-align:center">org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td style="text-align:center">LZO</td>
<td style="text-align:center">com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td style="text-align:center">Snappy</td>
<td style="text-align:center">org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody>
</table>
</div>
<p>压缩性能的比较</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">压缩算法</th>
<th style="text-align:center">原始文件大小</th>
<th style="text-align:center">压缩文件大小</th>
<th style="text-align:center">压缩速度</th>
<th style="text-align:center">解压速度</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">gzip</td>
<td style="text-align:center">8.3GB</td>
<td style="text-align:center">1.8GB</td>
<td style="text-align:center">17.5MB/s</td>
<td style="text-align:center">58MB/s</td>
</tr>
<tr>
<td style="text-align:center">bzip2</td>
<td style="text-align:center">8.3GB</td>
<td style="text-align:center">1.1GB</td>
<td style="text-align:center">2.4MB/s</td>
<td style="text-align:center">9.5MB/s</td>
</tr>
<tr>
<td style="text-align:center">LZO</td>
<td style="text-align:center">8.3GB</td>
<td style="text-align:center">2.9GB</td>
<td style="text-align:center">49.3MB/s</td>
<td style="text-align:center">74.6MB/s</td>
</tr>
</tbody>
</table>
</div>
<p><a target="_blank" rel="noopener" href="https://google.github.io/snappy/">Google/Snappy</a><br><a target="_blank" rel="noopener" href="https://github.com/google/snappy">github/Snappy</a></p>
<h2 id="压缩方式选择"><a href="#压缩方式选择" class="headerlink" title="压缩方式选择"></a>压缩方式选择</h2><h3 id="Gzip压缩"><a href="#Gzip压缩" class="headerlink" title="Gzip压缩"></a>Gzip压缩</h3><ul>
<li>优点</li>
</ul>
<p>压缩率比较高，而且压缩解压速度也比较快;<br>Hadoop本身支持, 在应用中处理Gzip格式的文件就和直接处理文本一样;<br>大部分Linux系统都自带Gzip命令，使用方便。</p>
<ul>
<li>缺点</li>
</ul>
<p>不支持Split。</p>
<ul>
<li>应用场景</li>
</ul>
<p>当每个文件压缩之后在130M以内的(1个块大小内) ，都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。</p>
<h3 id="Bzip2压缩"><a href="#Bzip2压缩" class="headerlink" title="Bzip2压缩"></a>Bzip2压缩</h3><ul>
<li>优点</li>
</ul>
<p>支持Split；<br>具有很高的压缩率，比Gzip压缩率都高；<br>Hadoop本身自带，使用方便。</p>
<ul>
<li>缺点</li>
</ul>
<p>压缩/解压速度慢。</p>
<ul>
<li>应用场景</li>
</ul>
<p>适合对速度要求不高，但需要较高的压缩率的时候；<br>或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况（历史数据备份）；<br>或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split，而且兼容之前的应用程序的情况。</p>
<h3 id="Lzo压缩"><a href="#Lzo压缩" class="headerlink" title="Lzo压缩"></a>Lzo压缩</h3><ul>
<li>优点</li>
</ul>
<p>压缩/解压速度也比较快，合理的压缩率；<br>支持Split，是Hadoop中最流行的压缩格式；<br>可以在Linux系统下安装Izop命令，使用方便。</p>
<ul>
<li>缺点</li>
</ul>
<p>压缩率比Gzip要低一些;<br>Hadoop本身不支持，需要安装；<br>在应用中对Lzo格式的文件需要做一些特殊处理(为了支持Split需要建索引，还需要指定InputFormat为Lzo格式)</p>
<ul>
<li>应用场景</li>
</ul>
<p>一个很大的文本文件，压缩之后还大于200M以，上的可以考虑，而且单个文件越大，Lzo优点越越明显。</p>
<h3 id="Snappy压缩"><a href="#Snappy压缩" class="headerlink" title="Snappy压缩"></a>Snappy压缩</h3><ul>
<li>优点</li>
</ul>
<p>高速压缩速度和合理的压缩率。</p>
<ul>
<li>缺点</li>
</ul>
<p>不支持Split；<br>压缩率比Gzip要低；<br>Hadoop本身不支持, 需要安装。</p>
<ul>
<li>应用场景</li>
</ul>
<p>当NapReduce作业的Map输出的数据比较大的时候， 作为Map到Redunce的中间数据的压缩格式；<br>或者作为一个MapRednce作业的输出和另外一个MapReduce作业的输入。</p>
<h2 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h2><h3 id="输入端采用压缩"><a href="#输入端采用压缩" class="headerlink" title="输入端采用压缩"></a>输入端采用压缩</h3><p>在有大量数据并计划重复处理的情况下，应该考虑对输入进行压缩。然而，你无须显示指定使用的编解码方式。<br>Hadoop自动检查文件扩展名，如果扩展名能够匹配，就会用恰当的编解码方式对文件进行压缩和解压。<br>否则，Hadoop就不会使用任何编解码器。</p>
<h3 id="Mapper输出采用压缩"><a href="#Mapper输出采用压缩" class="headerlink" title="Mapper输出采用压缩"></a>Mapper输出采用压缩</h3><p>当Map任务输出的中间数据量很大时，应考虑在此阶段采用压缩技术。这能显著改善内部数据Shufne过程，而Shffle过程在Hadoop处理过程中是资源肖耗最多的环节。<br>如果发现数据量大造成网络传输缓慢，应该考虑使用压缩技术。<br>可用于压缩Mapper 输出的快速编解码器包括LZO或者Snappy。</p>
<blockquote>
<p>注: LZO是供Hadoop压缩数据用的通用压缩编解码器。其设计目标是达到与硬盘读取速度相当的压缩速度，因此速度是优先考虑的因素，而不是压缩率。与Gzip编解码器相比，它的压缩速度是Gzip的5倍，而解压速度是Gzp的2倍。同一个文件用LZO压缩后比用Gzip压缩后大50%，但比压缩前小25%~ 50%。这对改善性能非常有利。Map阶段完成时间快4倍。</p>
</blockquote>
<h3 id="Reducer输出采用压缩"><a href="#Reducer输出采用压缩" class="headerlink" title="Reducer输出采用压缩"></a>Reducer输出采用压缩</h3><p>在此阶段启用压缩技术能够减少要存储的数据量，因此降低所需的磁盘空间。当MapReduce作业形成作业链条时，因为第二个作业的输入也已压缩，所以启用压缩同样有效。</p>
<h2 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h2><p>要在Hadoop中启用压缩，可以配置如下参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">io.compression.codecs（在core-site.xml中配置）</td>
<td style="text-align:left">org.apache.hadoop.io.compress.DefaultCodec,  org.apache.hadoop.io.compress.GzipCodec,  org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td style="text-align:left">mapreduce.map.output.compress（在mapred-site.xml中配置）</td>
<td style="text-align:left">false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td style="text-align:left">mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td>
<td style="text-align:left">org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td style="text-align:left">mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td>
<td style="text-align:left">false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td style="text-align:left">mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td>
<td style="text-align:left">org.apache.hadoop.io.compress.DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td style="text-align:left">mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）</td>
<td style="text-align:left">RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody>
</table>
</div>
<h2 id="压缩实操案例"><a href="#压缩实操案例" class="headerlink" title="压缩实操案例"></a>压缩实操案例</h2><h3 id="数据流的压缩和解压缩"><a href="#数据流的压缩和解压缩" class="headerlink" title="数据流的压缩和解压缩"></a>数据流的压缩和解压缩</h3><p>CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据。</p>
<p>要想对正在被写入一个输出流的数据进行压缩，我们可以使用<code>reateOutputStream(OutputStreamout)</code>方法创建一个<code>CompressionOutputStream</code>,将其以压缩格式写入底层的流。</p>
<p>相反，要想对从输入流读取而来的数据进行解压缩，则调用<code>createInputStream(InputStreamin)</code>函数，从而获得一个<code>CompressionInputStream</code>, 从而从底层的流读取未压缩的数据。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">DEFLATE</th>
<th style="text-align:center">org.apache.hadoop.io.compress.DefaultCodec</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">gzip</td>
<td style="text-align:center">org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td style="text-align:center">bzip2</td>
<td style="text-align:center">org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
</tbody>
</table>
</div>
<p><code>TestCompress.java</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.compress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodecFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ReflectionUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestCompress</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line"><span class="comment">//        compress(&quot;f:/IDEAWS/dashju/compressinput/hello.txt&quot;, &quot;org.apache.hadoop.io.compress.BZip2Codec&quot;);</span></span><br><span class="line"><span class="comment">//        compress(&quot;f:/IDEAWS/dashju/compressinput/hello.txt&quot;, &quot;org.apache.hadoop.io.compress.GzipCodec&quot;);</span></span><br><span class="line"><span class="comment">//        compress(&quot;f:/IDEAWS/dashju/compressinput/hello.txt&quot;, &quot;org.apache.hadoop.io.compress.DefaultCodec&quot;);</span></span><br><span class="line"></span><br><span class="line">        decompress(<span class="string">&quot;f:/IDEAWS/dashju/compressinput/hello.txt.bz2&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 压缩</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">compress</span><span class="params">(String filename, String method)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">// 1 获取输入流</span></span><br><span class="line">        <span class="type">FileInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(filename));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 获取输出流</span></span><br><span class="line">        <span class="comment">// 反射获取流</span></span><br><span class="line">        <span class="type">Class</span> <span class="variable">codeClass</span> <span class="operator">=</span> Class.forName(method);</span><br><span class="line">        <span class="type">CompressionCodec</span> <span class="variable">codec</span> <span class="operator">=</span> (CompressionCodec) ReflectionUtils.newInstance(codeClass, <span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line"></span><br><span class="line">        <span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(filename + codec.getDefaultExtension()));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 包装输出流</span></span><br><span class="line">        <span class="type">CompressionOutputStream</span> <span class="variable">cos</span> <span class="operator">=</span> codec.createOutputStream(fos);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 流的对拷</span></span><br><span class="line">        IOUtils.copyBytes(fis, cos, <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">5</span>, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(cos);</span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解压缩</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">decompress</span><span class="params">(String filename)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 1校验是否可以解压缩</span></span><br><span class="line">        <span class="type">CompressionCodecFactory</span> <span class="variable">compressionCodecFactory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CompressionCodecFactory</span>(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line">        <span class="type">CompressionCodec</span> <span class="variable">compressionCodec</span> <span class="operator">=</span> compressionCodecFactory.getCodec(<span class="keyword">new</span> <span class="title class_">Path</span>(filename));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="literal">null</span> == compressionCodec) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;不能解压缩&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 获取输入流</span></span><br><span class="line">        <span class="type">FileInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(filename));</span><br><span class="line">        <span class="type">CompressionInputStream</span> <span class="variable">cis</span> <span class="operator">=</span> compressionCodec.createInputStream(fis);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 获取输出流</span></span><br><span class="line">        <span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(filename + <span class="string">&quot;decode&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 流的对拷</span></span><br><span class="line">        IOUtils.copyBytes(cis, fos, <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">5</span>, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">        IOUtils.closeStream(cis);</span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Map输出端采用压缩"><a href="#Map输出端采用压缩" class="headerlink" title="Map输出端采用压缩"></a>Map输出端采用压缩</h3><p>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。</p>
<p>不影响输出结果，只是提高IO效率。</p>
<p>在wordcount案例中对<code>WordCountDriver.java</code>修改配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line"><span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line">job.setJarByClass(WordCountDriver.class);</span><br><span class="line">job.setMapperClass(WordCountMapper.class);</span><br><span class="line">job.setReducerClass(WordCountReducer.class);</span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"><span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">System.exit(result ? <span class="number">1</span> : <span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<h3 id="Reduce输出端采用压缩"><a href="#Reduce输出端采用压缩" class="headerlink" title="Reduce输出端采用压缩"></a>Reduce输出端采用压缩</h3><p>最终输出文件为压缩文件。</p>
<p>在wordcount案例中对<code>WordCountDriver.java</code>修改配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"><span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line">job.setJarByClass(WordCountDriver.class);</span><br><span class="line">job.setMapperClass(WordCountMapper.class);</span><br><span class="line">job.setReducerClass(WordCountReducer.class);</span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); </span><br><span class="line"></span><br><span class="line"><span class="comment">//修改map端输出和reduce输出，采用不一样的格式，最终输出格式由reduce端输出决定</span></span><br><span class="line"><span class="comment">//FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); </span></span><br><span class="line"><span class="comment">//FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); </span></span><br><span class="line"></span><br><span class="line"><span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">System.exit(result?<span class="number">1</span>:<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<h1 id="第5章-Yarn资源调度器"><a href="#第5章-Yarn资源调度器" class="headerlink" title="第5章 Yarn资源调度器"></a>第5章 Yarn资源调度器</h1><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p>
<h2 id="Yarn基本架构"><a href="#Yarn基本架构" class="headerlink" title="Yarn基本架构"></a>Yarn基本架构</h2><p>YARN主要由<code>ResourceManager</code>、<code>NodeManager</code>、<code>ApplicationMaster</code>和<code>Container</code>等组件构成。</p>
<img src="/2020/11/09/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3/Yarn%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.png" class="" title="Yarn基本架构">
<h2 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h2><img src="/2020/11/09/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3/Yarn%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" class="" title="Yarn工作机制">
<p>（1）MR程序提交到客户端所在的节点。<br>（2）YarnRunner向ResourceManager申请一个Application。<br>（3）RM将该应用程序的资源路径返回给YarnRunner。<br>（4）该程序将运行所需资源提交到HDFS上。<br>（5）程序资源提交完毕后，申请运行mrAppMaster。<br>（6）RM将用户的请求初始化成一个Task。<br>（7）其中一个NodeManager领取到Task任务。<br>（8）该NodeManager创建容器Container，并产生MRAppmaster。<br>（9）Container从HDFS上拷贝资源到本地。<br>（10）MRAppmaster向RM 申请运行MapTask资源。<br>（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。<br>（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。<br>（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。<br>（14）ReduceTask向MapTask获取相应分区的数据。<br>（15）程序运行完毕后，MR会向RM申请注销自己。</p>
<h2 id="作业提交全过程"><a href="#作业提交全过程" class="headerlink" title="作业提交全过程"></a>作业提交全过程</h2><h3 id="Yarn阶段"><a href="#Yarn阶段" class="headerlink" title="Yarn阶段"></a>Yarn阶段</h3><p>作业提交全过程详解</p>
<p>（1）作业提交</p>
<p>第1步：Client调用<code>job.waitForCompletion</code>方法，向整个集群提交MapReduce作业。<br>第2步：Client向RM申请一个作业id。<br>第3步：RM给Client返回该job资源的提交路径和作业id。<br>第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。<br>第5步：Client提交完资源后，向RM申请运行MrAppMaster。</p>
<p>（2）作业初始化</p>
<p>第6步：当RM收到Client的请求后，将该job添加到容量调度器中。<br>第7步：某一个空闲的NM领取到该Job。<br>第8步：该NM创建Container，并产生MRAppmaster。<br>第9步：下载Client提交的资源到本地。</p>
<p>（3）任务分配</p>
<p>第10步：MrAppMaster向RM申请运行多个MapTask任务资源。<br>第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<p>（4）任务运行</p>
<p>第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。<br>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。<br>第14步：ReduceTask向MapTask获取相应分区的数据。<br>第15步：程序运行完毕后，MR会向RM申请注销自己。</p>
<p>（5）进度和状态更新</p>
<p>YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过<code>mapreduce.client.progressmonitor.pollinterval</code>设置)向应用管理器请求进度更新, 展示给用户。</p>
<p>（6）作业完成</p>
<p>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用<code>waitForCompletion()</code>来检查作业是否完成。时间间隔可以通过<code>mapreduce.client.completion.pollinterval</code>来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p>
<h3 id="MapReduce阶段"><a href="#MapReduce阶段" class="headerlink" title="MapReduce阶段"></a>MapReduce阶段</h3><img src="/2020/11/09/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3/MapReduce%E9%98%B6%E6%AE%B5.png" class="" title="MapReduce阶段">
<h2 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h2><p>目前，Hadoop作业调度器主要有三种：<code>FIFO</code>、<code>Capacity Scheduler</code>和<code>Fair Scheduler</code>。<br>Hadoop2.7.2默认的资源调度器是Capacity Scheduler。</p>
<p>具体设置详见：<code>yarn-default.xml</code>文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="先进先出调度器（FIFO）"><a href="#先进先出调度器（FIFO）" class="headerlink" title="先进先出调度器（FIFO）"></a>先进先出调度器（FIFO）</h3><img src="/2020/11/09/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3/%E5%85%88%E8%BF%9B%E5%85%88%E5%87%BA%E8%B0%83%E5%BA%A6%E5%99%A8.png" class="" title="先进先出调度器">
<h3 id="容量调度器（Capacity-Scheduler）"><a href="#容量调度器（Capacity-Scheduler）" class="headerlink" title="容量调度器（Capacity Scheduler）"></a>容量调度器（Capacity Scheduler）</h3><p>为多个FIFO调度器组合，并发度较高。</p>
<img src="/2020/11/09/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3/%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8.png" class="" title="容量调度器">
<p>1、支持多个队列，每个队列可配置一定的资源量，每个队列采用FIFO调度策略。</p>
<p>2、为了防止同一个用户的作业独占队列中的资源，该调庶器会对同一用户提交的作业所占资源虽进行限定。</p>
<p>3、首先，计算每个队列中正在运行的任务数与其应该分得的计算资原之间的比值，选择一个该比值最小的队列——最闲的。</p>
<p>4、其次，按照作业优先级和提交时间顺字，同时考虑用户资源量限制和内存限制对队列内任务排序。</p>
<p>5、三个队列同时按照任务的先后顺序依次执行，比如，jobl1、 job21和jb3l分别排在队列最前面，先运行，也是并行运行。</p>
<h3 id="公平调度器（Fair-Scheduler）"><a href="#公平调度器（Fair-Scheduler）" class="headerlink" title="公平调度器（Fair Scheduler）"></a>公平调度器（Fair Scheduler）</h3><p>缺额排序，多个FIFO调度器组合。</p>
<img src="/2020/11/09/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3/%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8.png" class="" title="公平调度器">
<p>支持多队列多用户，每个队列中的资源量可以配置，同一队列中的作业公平共享队列中所有资源。</p>
<p>比如有三个队列: queueA、 queueB和queueC， 每个队列中的job按照优先级分配资源，优先级越高分配的资源越多，但是每个job都会分配到资源以确保公平。</p>
<p>在资源有限的情况下，每个job理想情兄下获得的计算资源与实际获得的计算资源存在一种差距，这个差距就叫做<code>缺额</code>。</p>
<p>在同一个队列中，job的资源缺额越大，越先获得资源优先执行。作业是按照缺额的高低来先后执行的，而且可以看到上图有多个作业同时运行。</p>
<h2 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h2><p>1．作业完成时间取决于最慢的任务完成时间</p>
<p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p>
<blockquote>
<p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p>
</blockquote>
<p>2．推测执行机制</p>
<p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</p>
<ol>
<li>执行推测任务的前提条件</li>
</ol>
<p>（1）每个Task只能有一个备份任务<br>（2）当前Job已完成的Task必须不小于0.05（5%）<br>（3）开启推测执行参数设置。<code>mapred-site.xml</code>文件中默认是打开的。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol>
<li>不能启用推测执行机制情况</li>
</ol>
<p>（1）任务间存在严重的负载倾斜；<br>（2）特殊任务，比如任务向数据库中写数据。</p>
<ol>
<li>算法原理</li>
</ol>
<p>假设某一时刻，任务T的执行进度为<code>progress</code>，则可通过一定的算法推测出该任务的最终完成时刻<code>estimateEndTime</code>。另一方面，如果此刻为该任务启动一个备份任务，则可推断出它可能的完成时刻<code>estimateEndTime2</code>；于是可得出以下几个公式:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">estimatedRunTime = (currentTimestamp - taskStartTime) / progress</span><br><span class="line">推测运行时间(60s) = (当前时刻(6) - 任务启动时刻(0)) / 任务运行比例(10%)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">estimateEndTime = estimatedRunTime + taskStartTime</span><br><span class="line">推测执行完时刻(60) = 推测运行时间(60s) + 任务启动时刻(0)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">estimateEndTime2 = currentTimestamp + averageRunTime</span><br><span class="line">备份任务推测完成时刻(16) = 当前时刻(6) + 运行完成任务的平均时间(10s)</span><br></pre></td></tr></table></figure>
<p>（1）MR总是选择(<code>estimateEndTime - estimateEndTime2</code>)差值最大的任务，并为之启动备份任务。</p>
<p>（2）为了防止大量任务同时启动备份任务造成的资源浪费，MR为每个作业设置了同时启动的备份任务数目上限。</p>
<p>（3）推测执行机制实际上采用了经典的优化算法：以空间换时间，它同时启动多个相同任务处理相同的数据，并让这些任务竞争以缩短数据处理时间。显然，这种方法需要占用更多的计算资源。在集群资源紧缺的情况下，应合理使用该机制，争取在多用少量资源的情况下，减少作业的计算时间。</p>
<h1 id="第6章-Hadoop企业优化"><a href="#第6章-Hadoop企业优化" class="headerlink" title="第6章 Hadoop企业优化"></a>第6章 Hadoop企业优化</h1><h2 id="MapReduce跑得慢的原因"><a href="#MapReduce跑得慢的原因" class="headerlink" title="MapReduce跑得慢的原因"></a>MapReduce跑得慢的原因</h2><p>MapReduce程序效率的瓶颈在于两点</p>
<ul>
<li>计算机性能</li>
</ul>
<p>CPU、内存、磁盘健康、网络</p>
<ul>
<li>1/O 操作优化</li>
</ul>
<p>（1）数据倾斜；<br>（2）Map和Reduce数设置不合理；<br>（3）Map运行时间太长，导致Reduce等待过久；<br>（4）小文件过多；<br>（5）大量的不可分块的超大文件；<br>（6）Spill次数过多；<br>（7）Merge次数过多。</p>
<h2 id="MapReduce优化方法"><a href="#MapReduce优化方法" class="headerlink" title="MapReduce优化方法"></a>MapReduce优化方法</h2><p>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce 阶段、Io传输、数据倾斜问题和常用的调优参数。</p>
<h3 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h3><p>（1）合并小文件在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。</p>
<p>（2）采用<code>CombineTextInputFormat</code>来作为输入，解决输入端大小文件场景。</p>
<h3 id="Map阶段"><a href="#Map阶段" class="headerlink" title="Map阶段"></a>Map阶段</h3><p>（1）减少<code>溢写(Spill)次数</code></p>
<p>通过调整<code>io.sort.mb</code>（环形缓冲区）及<code>sort.spill.percent</code>（到达多少开始溢写）参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO。</p>
<p>（2）减少<code>合并(Merge)次数</code></p>
<p>通过调整<code>io.sort.factor</code>参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。</p>
<p>（3）在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O。</p>
<h3 id="Reduce阶段"><a href="#Reduce阶段" class="headerlink" title="Reduce阶段"></a>Reduce阶段</h3><p>（1）合理设置Map和Reduce数</p>
<p>两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误。</p>
<p>（2）设置Map、Reduce共存</p>
<p>调整<code>slowstart.completedmaps</code>参数， 使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。</p>
<p>（3）规避使用Reduce</p>
<p>因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。</p>
<p>（4）合理设置Reduce端的Buffer</p>
<p>默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffen和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一 分数据可以直接输送到Reduce，从而减少IO开销：<code>mapreduce.reduce.input.bufer.percent</code>，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整。</p>
<h3 id="IO传输"><a href="#IO传输" class="headerlink" title="IO传输"></a>IO传输</h3><p>（1）采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器。</p>
<p>（2）使用SequenceFile二进制文件。</p>
<h3 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h3><h4 id="数据倾斜现象"><a href="#数据倾斜现象" class="headerlink" title="数据倾斜现象"></a>数据倾斜现象</h4><p>数据频率倾斜：某一个区域的数据要远远大于其他区域。</p>
<p>数据大小倾斜：部分记录的大小远远大于平均值。</p>
<h4 id="减少数据倾斜的方法"><a href="#减少数据倾斜的方法" class="headerlink" title="减少数据倾斜的方法"></a>减少数据倾斜的方法</h4><ul>
<li>方法1：抽样和范围分区</li>
</ul>
<p>可以通过对原始数据进行抽样得到的结果集来预设分区边界值。</p>
<ul>
<li>方法2：自定义分区</li>
</ul>
<p>基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业司汇较多。那么就可以自定义分区将这这些专业司汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例。</p>
<ul>
<li>方法3：Combine</li>
</ul>
<p>使用Combine可以大量地减小数据倾斜。在可能的情况下，Cobine的目的就是聚合并精简数据。</p>
<ul>
<li>方法4：采用Map Join，尽量避免Reduce Join。</li>
</ul>
<h3 id="常用的调优参数"><a href="#常用的调优参数" class="headerlink" title="常用的调优参数"></a>常用的调优参数</h3><h4 id="资源相关参数"><a href="#资源相关参数" class="headerlink" title="资源相关参数"></a>资源相关参数</h4><p>（1）以下参数是在用户自己的MR应用程序中配置就可以生效（<code>mapred-default.xml</code>）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">mapreduce.map.memory.mb</td>
<td>一个MapTask可使用的资源上限（单位：MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。（开发中2-4G）</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.reduce.memory.mb</td>
<td>一个ReduceTask可使用的资源上限（单位：MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.map.cpu.vcores</td>
<td>每个MapTask可使用的最多cpu core数目，默认值：1。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.reduce.cpu.vcores</td>
<td>每个ReduceTask可使用的最多cpu core数目，默认值：1。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.reduce.shuffle.parallelcopies</td>
<td>每个Reduce去Map中取数据的并行数，默认值：5。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.reduce.shuffle.merge.percent</td>
<td>Buffer中的数据达到多少比例开始写入磁盘，默认值：0.66。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.reduce.shuffle.input.buffer.percent</td>
<td>Buffer大小占Reduce可用内存的比例，默认值：0.7。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.reduce.input.buffer.percent</td>
<td>指定多少比例的内存用来存放Buffer中的数据，默认值：0。</td>
</tr>
</tbody>
</table>
</div>
<p>（2）应该在YARN启动之前就配置在服务器的配置文件中才能生效（<code>yarn-default.xml</code>）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">yarn.scheduler.minimum-allocation-mb</td>
<td>给应用程序Container分配的最小内存，默认值：1024。</td>
</tr>
<tr>
<td style="text-align:center">yarn.scheduler.maximum-allocation-mb</td>
<td>给应用程序Container分配的最大内存，默认值：8192。</td>
</tr>
<tr>
<td style="text-align:center">yarn.scheduler.minimum-allocation-vcores</td>
<td>每个Container申请的最小CPU核数，默认值：1。</td>
</tr>
<tr>
<td style="text-align:center">yarn.scheduler.maximum-allocation-vcores</td>
<td>每个Container申请的最大CPU核数，默认值：32。</td>
</tr>
<tr>
<td style="text-align:center">yarn.nodemanager.resource.memory-mb</td>
<td>给Containers分配的最大物理内存，默认值：8192。</td>
</tr>
</tbody>
</table>
</div>
<p>（3）Shuffle性能优化的关键参数，应在YARN启动之前就配置好（<code>mapred-default.xml</code>）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">mapreduce.task.io.sort.mb</td>
<td>Shuffle的环形缓冲区大小，默认值：100M。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.map.sort.spill.percent</td>
<td>环形缓冲区溢出的阈值，默认值：80%。</td>
</tr>
</tbody>
</table>
</div>
<h4 id="容错相关参数-MapReduce性能优化"><a href="#容错相关参数-MapReduce性能优化" class="headerlink" title="容错相关参数(MapReduce性能优化)"></a>容错相关参数(MapReduce性能优化)</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">mapreduce.map.maxattempts</td>
<td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.reduce.maxattempts</td>
<td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td style="text-align:center">mapreduce.task.timeout</td>
<td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是<code>AttemptID:attempt_14267829456721_123456_m_000224_0  Timed out after 300 secsContainer killed by the ApplicationMaster.</code>。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="HDFS小文件优化方法"><a href="#HDFS小文件优化方法" class="headerlink" title="HDFS小文件优化方法"></a>HDFS小文件优化方法</h2><h3 id="HDFS小文件弊端"><a href="#HDFS小文件弊端" class="headerlink" title="HDFS小文件弊端"></a>HDFS小文件弊端</h3><p>HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p>
<h3 id="HDFS小文件解决方案"><a href="#HDFS小文件解决方案" class="headerlink" title="HDFS小文件解决方案"></a>HDFS小文件解决方案</h3><p>小文件的优化无非以下几种方式：</p>
<p>（1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。<br>（2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。<br>（3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。</p>
<ul>
<li><ol>
<li>Hadoop Archive</li>
</ol>
</li>
</ul>
<p>是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了NameNode的内存使用。</p>
<ul>
<li><ol>
<li>SequenceFile</li>
</ol>
</li>
</ul>
<p>SequenceFile由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。</p>
<ul>
<li><ol>
<li>CombineFileInputFormat</li>
</ol>
</li>
</ul>
<p>CombineFileInputFormat是一种新的InputFomat， 用于将多个文件台并成一个单独的Split，另外，它会考虑数据的存储位置。</p>
<ul>
<li><ol>
<li>开启JVM重用</li>
</ol>
</li>
</ul>
<p>对于大量小文件Job，可以开启JVM重用会减少45%运行时间。</p>
<p>JVM重用原理：一个Map运行在一个JVM上，开启重用的话，该Map在JVM上运行完毕后，JVM继续运行其他Map。</p>
<p>具体设置: <code>mapreduce.job.jvm.numtasks</code>（JVM线程池）值在10-20之间。</p>
<h1 id="第7章-MapReduce扩展案例"><a href="#第7章-MapReduce扩展案例" class="headerlink" title="第7章 MapReduce扩展案例"></a>第7章 MapReduce扩展案例</h1><h2 id="倒排索引案例（多job串联）"><a href="#倒排索引案例（多job串联）" class="headerlink" title="倒排索引案例（多job串联）"></a>倒排索引案例（多job串联）</h2><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><ul>
<li>需求</li>
</ul>
<p>有大量的文本（文档、网页），需要建立搜索索引。</p>
<ul>
<li>数据输入</li>
</ul>
<p><code>a.txt</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atguigu pingping</span><br><span class="line">atguigu ss</span><br><span class="line">atguigu ss</span><br></pre></td></tr></table></figure>
<p><code>b.txt</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atguigu pingping</span><br><span class="line">atguigu pingping</span><br><span class="line">atguigu ss</span><br></pre></td></tr></table></figure>
<p><code>c.txt</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">atguigu ss</span><br><span class="line">atguigu pingping</span><br></pre></td></tr></table></figure>
<ul>
<li>期望输出</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atguigu	c.txt--&gt;2	b.txt--&gt;2	a.txt--&gt;3	</span><br><span class="line">pingping	c.txt--&gt;1	b.txt--&gt;3	a.txt--&gt;1	</span><br><span class="line">ss	c.txt--&gt;1	b.txt--&gt;1	a.txt--&gt;2	</span><br></pre></td></tr></table></figure>
<img src="/2020/11/09/Hadoop-MapReduce%E8%AF%A6%E8%A7%A3-3/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E6%A1%88%E4%BE%8B-1.png" class="" title="倒排索引案例-1">
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="第一次处理"><a href="#第一次处理" class="headerlink" title="第一次处理"></a>第一次处理</h4><p>（1）第一次处理，编写OneIndexMapper类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.index;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">OneIndexMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    String name;</span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//获取文件名称</span></span><br><span class="line">        <span class="type">FileSplit</span> <span class="variable">fileSplit</span> <span class="operator">=</span> (FileSplit)context.getInputSplit();</span><br><span class="line">        name = fileSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] fields = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String word : fields) &#123;</span><br><span class="line">            <span class="comment">// 3 拼接</span></span><br><span class="line">            k.set(word + <span class="string">&quot;--&quot;</span> + name);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 4 写出</span></span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（2）第一次处理，编写OneIndexReducer类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.index;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">OneIndexReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        v.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（3）第一次处理，编写OneIndexDriver类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.index;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">OneIndexDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;f:/IDEAWS/dashju/oneindexinput&quot;</span>, <span class="string">&quot;f:/IDEAWS/dashju/oneindexoutput&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(OneIndexDriver.class);</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(OneIndexMapper.class);</span><br><span class="line">        job.setReducerClass(OneIndexReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（4）查看第一次输出结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">atguigu--a.txt	3</span><br><span class="line">atguigu--b.txt	3</span><br><span class="line">atguigu--c.txt	2</span><br><span class="line">pingping--a.txt	1</span><br><span class="line">pingping--b.txt	2</span><br><span class="line">pingping--c.txt	1</span><br><span class="line">ss--a.txt	2</span><br><span class="line">ss--b.txt	1</span><br><span class="line">ss--c.txt	1</span><br></pre></td></tr></table></figure>
<h4 id="第二次处理"><a href="#第二次处理" class="headerlink" title="第二次处理"></a>第二次处理</h4><p>（1）第二次处理，编写TwoIndexMapper类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.index;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TwoIndexMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, Text&gt; &#123;</span><br><span class="line">    <span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="type">Text</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 用“--“切割</span></span><br><span class="line">        String[] fields = line.split(<span class="string">&quot;--&quot;</span>);</span><br><span class="line"></span><br><span class="line">        k.set(fields[<span class="number">0</span>]);</span><br><span class="line">        v.set(fields[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 输出</span></span><br><span class="line">        context.write(k, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（2）第二次处理，编写TwoIndexReducer类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.index;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TwoIndexReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, Text, Text, Text&gt; &#123;</span><br><span class="line">    <span class="type">Text</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        <span class="comment">// 1 拼接</span></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            sb.append(value.toString().replace(<span class="string">&quot;\t&quot;</span>, <span class="string">&quot;--&gt;&quot;</span>) + <span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        v.set(sb.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（3）第二次处理，编写TwoIndexDriver类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.index;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TwoIndexDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;f:/IDEAWS/dashju/twoindexinput&quot;</span>, <span class="string">&quot;f:/IDEAWS/dashju/twoindexoutput&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">config</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(config);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TwoIndexDriver.class);</span><br><span class="line">        job.setMapperClass(TwoIndexMapper.class);</span><br><span class="line">        job.setReducerClass(TwoIndexReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（4）第二次查看最终结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atguigu	c.txt--&gt;2	b.txt--&gt;3	a.txt--&gt;3	</span><br><span class="line">pingping	c.txt--&gt;1	b.txt--&gt;2	a.txt--&gt;1	</span><br><span class="line">ss	c.txt--&gt;1	b.txt--&gt;1	a.txt--&gt;2	</span><br></pre></td></tr></table></figure>
<h2 id="TopN案例"><a href="#TopN案例" class="headerlink" title="TopN案例"></a>TopN案例</h2><h3 id="需求分析-1"><a href="#需求分析-1" class="headerlink" title="需求分析"></a>需求分析</h3><h4 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h4><p>对需求序列化输出结果进行加工，输出流量使用量在前10的用户信息。</p>
<h4 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h4><p>手机号，上行流量，下行流量，总流量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">13470253144	180	180	360</span><br><span class="line">13509468723	7335	110349	117684</span><br><span class="line">13560439638	918	4938	5856</span><br><span class="line">13568436656	3597	25635	29232</span><br><span class="line">13590439668	1116	954	2070</span><br><span class="line">13630577991	6960	690	7650</span><br><span class="line">13682846555	1938	2910	4848</span><br><span class="line">13729199489	240	0	240</span><br><span class="line">13736230513	2481	24681	27162</span><br><span class="line">13768778790	120	120	240</span><br><span class="line">13846544121	264	0	264</span><br><span class="line">13956435636	132	1512	1644</span><br><span class="line">13966251146	240	0	240</span><br><span class="line">13975057813	11058	48243	59301</span><br><span class="line">13992314666	3008	3720	6728</span><br><span class="line">15043685818	3659	3538	7197</span><br><span class="line">15910133277	3156	2936	6092</span><br><span class="line">15959002129	1938	180	2118</span><br><span class="line">18271575951	1527	2106	3633</span><br><span class="line">18390173782	9531	2412	11943</span><br><span class="line">84188413	4116	1432	5548</span><br></pre></td></tr></table></figure>
<h4 id="输出数据"><a href="#输出数据" class="headerlink" title="输出数据"></a>输出数据</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">13509468723	7335	110349	117684</span><br><span class="line">13975057813	11058	48243	59301</span><br><span class="line">13568436656	3597	25635	29232</span><br><span class="line">13736230513	2481	24681	27162</span><br><span class="line">18390173782	9531	2412	11943</span><br><span class="line">13630577991	6960	690	7650</span><br><span class="line">15043685818	3659	3538	7197</span><br><span class="line">13992314666	3008	3720	6728</span><br><span class="line">15910133277	3156	2936	6092</span><br><span class="line">13560439638	918	4938	5856</span><br></pre></td></tr></table></figure>
<h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><p>（1）编写FlowBean类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.topN;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">(<span class="type">long</span> upFlow, <span class="type">long</span> downFlow, <span class="type">long</span> sumFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="built_in">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setUpFlow</span><span class="params">(<span class="type">long</span> upFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDownFlow</span><span class="params">(<span class="type">long</span> downFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">(<span class="type">long</span> sumFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getUpFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getDownFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">set</span><span class="params">(<span class="type">long</span> downFlow2, <span class="type">long</span> upFlow2)</span> &#123;</span><br><span class="line">        downFlow = downFlow2;</span><br><span class="line">        upFlow = upFlow2;</span><br><span class="line">        sumFlow = downFlow2 + upFlow2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line">        <span class="type">int</span> result;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.getSumFlow()) &#123;</span><br><span class="line">            result = -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.getSumFlow()) &#123;</span><br><span class="line">            result = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            result = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        upFlow = in.readLong();</span><br><span class="line">        downFlow = in.readLong();</span><br><span class="line">        sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（2）编写TopNMapper类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.topN;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.TreeMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TopNMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> TreeMap&lt;FlowBean, Text&gt; flowMap = <span class="keyword">new</span> <span class="title class_">TreeMap</span>&lt;FlowBean, Text&gt;();</span><br><span class="line">    <span class="keyword">private</span> FlowBean kBean;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        kBean = <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">        <span class="type">Text</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 数据封装</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phoneNum</span> <span class="operator">=</span> fields[<span class="number">0</span>];</span><br><span class="line">        <span class="type">Long</span> <span class="variable">upFlow</span> <span class="operator">=</span> Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">        <span class="type">Long</span> <span class="variable">downFlow</span> <span class="operator">=</span> Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">        <span class="type">Long</span> <span class="variable">sumFlow</span> <span class="operator">=</span> Long.parseLong(fields[<span class="number">3</span>]);</span><br><span class="line"></span><br><span class="line">        v.set(phoneNum);</span><br><span class="line"></span><br><span class="line">        kBean.setUpFlow(upFlow);</span><br><span class="line">        kBean.setDownFlow(downFlow);</span><br><span class="line">        kBean.setSumFlow(sumFlow);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 向TreeMap中添加数据</span></span><br><span class="line">        flowMap.put(kBean, v);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 限制TreeMap的数据量，超过10条就删除掉流量最小的一条数据</span></span><br><span class="line">        <span class="keyword">if</span> (flowMap.size() &gt; <span class="number">10</span>) &#123;</span><br><span class="line">            flowMap.remove(flowMap.lastKey());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 6 遍历treeMap集合，输出数据</span></span><br><span class="line">        Iterator&lt;FlowBean&gt; bean = flowMap.keySet().iterator();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (bean.hasNext()) &#123;</span><br><span class="line">            <span class="type">FlowBean</span> <span class="variable">k</span> <span class="operator">=</span> bean.next();</span><br><span class="line">            context.write(k, flowMap.get(k));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（3）编写TopNReducer类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.topN;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.TreeMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TopNReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;FlowBean, Text, Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="comment">// 定义一个TreeMap作为存储数据的容器（天然按key排序）</span></span><br><span class="line">    TreeMap&lt;FlowBean, Text&gt; flowMap = <span class="keyword">new</span> <span class="title class_">TreeMap</span>&lt;FlowBean, Text&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            <span class="type">FlowBean</span> <span class="variable">bean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">            bean.set(key.getDownFlow(), key.getUpFlow());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 1 向treeMap集合中添加数据</span></span><br><span class="line">            flowMap.put(bean, <span class="keyword">new</span> <span class="title class_">Text</span>(value));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2 限制TreeMap数据量，超过10条就删除掉流量最小的一条数据</span></span><br><span class="line">            <span class="keyword">if</span> (flowMap.size() &gt; <span class="number">10</span>) &#123;</span><br><span class="line">                flowMap.remove(flowMap.firstKey());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 3 遍历集合，输出数据</span></span><br><span class="line">        Iterator&lt;FlowBean&gt; it = flowMap.keySet().iterator();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">            <span class="type">FlowBean</span> <span class="variable">v</span> <span class="operator">=</span> it.next();</span><br><span class="line"></span><br><span class="line">            context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(flowMap.get(v)), v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（4）编写TopNDriver类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.topN;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TopNDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, IOException, ClassNotFoundException &#123;</span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;f:/IDEAWS/dashju/topNinput&quot;</span>, <span class="string">&quot;f:/IDEAWS/dashju/topNoutput&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取配置信息，或者job对象实例</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 指定本程序的jar包所在的本地路径</span></span><br><span class="line">        job.setJarByClass(TopNDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 指定本业务job要使用的mapper/Reducer业务类</span></span><br><span class="line">        job.setMapperClass(TopNMapper.class);</span><br><span class="line">        job.setReducerClass(TopNReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 指定mapper输出数据的kv类型</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 指定最终输出的数据的kv类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 指定job的输入原始文件所在目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="找博客共同粉丝案例"><a href="#找博客共同粉丝案例" class="headerlink" title="找博客共同粉丝案例"></a>找博客共同粉丝案例</h2><h3 id="需求分析-2"><a href="#需求分析-2" class="headerlink" title="需求分析"></a>需求分析</h3><h4 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h4><p>以下是博客的粉丝列表数据，冒号前是一个用户，冒号后是该用户的所有粉丝（数据中的粉丝关系是单向的）<br>求出哪些人两两之间有共同粉丝，及他俩粉的谁？</p>
<h4 id="数据输入-1"><a href="#数据输入-1" class="headerlink" title="数据输入"></a>数据输入</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:F,A,D,I</span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J</span><br></pre></td></tr></table></figure>
<h4 id="流程分析"><a href="#流程分析" class="headerlink" title="流程分析"></a>流程分析</h4><p>先求出A、B、C、….等是谁的粉丝<br>第一次输出结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A	I,K,C,B,G,F,H,O,D,</span><br><span class="line">B	A,F,J,E,</span><br><span class="line">C	A,E,B,H,F,G,K,</span><br><span class="line">D	G,C,K,A,L,F,E,H,</span><br><span class="line">E	G,M,L,H,A,F,B,D,</span><br><span class="line">F	L,M,D,C,G,A,</span><br><span class="line">G	M,</span><br><span class="line">H	O,</span><br><span class="line">I	O,C,</span><br><span class="line">J	O,</span><br><span class="line">K	B,</span><br><span class="line">L	D,E,</span><br><span class="line">M	E,F,</span><br><span class="line">O	A,H,I,J,F,</span><br></pre></td></tr></table></figure>
<p>第二次输出结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">A-B	E C </span><br><span class="line">A-C	D F </span><br><span class="line">A-D	E F </span><br><span class="line">A-E	D B C </span><br><span class="line">A-F	O B C D E </span><br><span class="line">A-G	F E C D </span><br><span class="line">A-H	E C D O </span><br><span class="line">A-I	O </span><br><span class="line">A-J	O B </span><br><span class="line">A-K	D C </span><br><span class="line">A-L	F E D </span><br><span class="line">A-M	E F </span><br><span class="line">B-C	A </span><br><span class="line">B-D	A E </span><br><span class="line">B-E	C </span><br><span class="line">B-F	E A C </span><br><span class="line">B-G	C E A </span><br><span class="line">B-H	A E C </span><br><span class="line">B-I	A </span><br><span class="line">B-K	C A </span><br><span class="line">B-L	E </span><br><span class="line">B-M	E </span><br><span class="line">B-O	A </span><br><span class="line">C-D	A F </span><br><span class="line">C-E	D </span><br><span class="line">C-F	D A </span><br><span class="line">C-G	D F A </span><br><span class="line">C-H	D A </span><br><span class="line">C-I	A </span><br><span class="line">C-K	A D </span><br><span class="line">C-L	D F </span><br><span class="line">C-M	F </span><br><span class="line">C-O	I A </span><br><span class="line">D-E	L </span><br><span class="line">D-F	A E </span><br><span class="line">D-G	E A F </span><br><span class="line">D-H	A E </span><br><span class="line">D-I	A </span><br><span class="line">D-K	A </span><br><span class="line">D-L	E F </span><br><span class="line">D-M	F E </span><br><span class="line">D-O	A </span><br><span class="line">E-F	D M C B </span><br><span class="line">E-G	C D </span><br><span class="line">E-H	C D </span><br><span class="line">E-J	B </span><br><span class="line">E-K	C D </span><br><span class="line">E-L	D </span><br><span class="line">F-G	D C A E </span><br><span class="line">F-H	A D O E C </span><br><span class="line">F-I	O A </span><br><span class="line">F-J	B O </span><br><span class="line">F-K	D C A </span><br><span class="line">F-L	E D </span><br><span class="line">F-M	E </span><br><span class="line">F-O	A </span><br><span class="line">G-H	D C E A </span><br><span class="line">G-I	A </span><br><span class="line">G-K	D A C </span><br><span class="line">G-L	D F E </span><br><span class="line">G-M	E F </span><br><span class="line">G-O	A </span><br><span class="line">H-I	O A </span><br><span class="line">H-J	O </span><br><span class="line">H-K	A C D </span><br><span class="line">H-L	D E </span><br><span class="line">H-M	E </span><br><span class="line">H-O	A </span><br><span class="line">I-J	O </span><br><span class="line">I-K	A </span><br><span class="line">I-O	A </span><br><span class="line">K-L	D </span><br><span class="line">K-O	A </span><br><span class="line">L-M	E F</span><br></pre></td></tr></table></figure>
<h3 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h3><ul>
<li>（1）第一次Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.friends;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShareFriendsOneMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1 获取一行 A:B,C,D,F,E,O</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] fields = line.split(<span class="string">&quot;:&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 获取博主和粉丝</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">person</span> <span class="operator">=</span> fields[<span class="number">0</span>];  <span class="comment">//A</span></span><br><span class="line">        String[] friends = fields[<span class="number">1</span>].split(<span class="string">&quot;,&quot;</span>);    <span class="comment">//B,C,D,F,E,O</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 写出</span></span><br><span class="line">        <span class="keyword">for</span> (String friend : friends) &#123;</span><br><span class="line">            <span class="comment">// 输出&lt;粉丝，博主&gt;</span></span><br><span class="line">            <span class="comment">// &lt;(B,C,D,F,E,O),A&gt; -&gt; &lt;B,A&gt; &lt;C,A&gt; &lt;D,A&gt; &lt;F,A&gt; &lt;E,A&gt; &lt;O,A&gt; | &lt;粉丝，博主&gt;</span></span><br><span class="line">            context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(friend), <span class="keyword">new</span> <span class="title class_">Text</span>(person));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>（2）第一次Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.friends;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShareFriendsOneReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, Text, Text, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// &lt;(B,C,D,F,E,O),A&gt; -&gt; &lt;B,A&gt; &lt;C,A&gt; &lt;D,A&gt; &lt;F,A&gt; &lt;E,A&gt; &lt;O,A&gt; | &lt;粉丝，博主&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="type">StringBuffer</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuffer</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 拼接</span></span><br><span class="line">        <span class="comment">// 每个粉丝后边拼接所关注的博主</span></span><br><span class="line">        <span class="keyword">for</span> (Text person : values) &#123;</span><br><span class="line">            sb.append(person).append(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, <span class="keyword">new</span> <span class="title class_">Text</span>(sb.toString()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>（3）第一次Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.friends;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShareFriendsOneDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;F:/IDEAWS/dashju/ShareFriendsinput1&quot;</span>, <span class="string">&quot;F:/IDEAWS/dashju/ShareFriendsoutput1&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取job对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 指定jar包运行的路径</span></span><br><span class="line">        job.setJarByClass(ShareFriendsOneDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 指定map/reduce使用的类</span></span><br><span class="line">        job.setMapperClass(ShareFriendsOneMapper.class);</span><br><span class="line">        job.setReducerClass(ShareFriendsOneReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 指定map输出的数据类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 指定最终输出的数据类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 指定job的输入原始所在目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>（4）第二次Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.friends;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShareFriendsTwoMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 粉丝，博主，博主，博主</span></span><br><span class="line">        <span class="comment">// A	I,K,C,B,G,F,H,O,D,</span></span><br><span class="line">        <span class="comment">// B	A,F,J,E,</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] fans_posters = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">fans</span> <span class="operator">=</span> fans_posters[<span class="number">0</span>];  <span class="comment">// A</span></span><br><span class="line">        String[] posters = fans_posters[<span class="number">1</span>].split(<span class="string">&quot;,&quot;</span>);  <span class="comment">// I,K,C,B,G,F,H,O,D,</span></span><br><span class="line"></span><br><span class="line">        Arrays.sort(posters);   <span class="comment">// B,C,D,F,G,H,I,K,O</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 共同粉丝迭代</span></span><br><span class="line">        <span class="comment">// 对于相同的博主-博主，依次写出</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; posters.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">1</span>; j &lt; posters.length; j++) &#123;</span><br><span class="line">                <span class="comment">// 发出&lt;博主-博主,粉丝&gt;，相同的&quot;博主-博主&quot;对的所有好友就会到同一个reduce中去。</span></span><br><span class="line">                context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(posters[i] + <span class="string">&quot;-&quot;</span> + posters[j]), <span class="keyword">new</span> <span class="title class_">Text</span>(fans));</span><br><span class="line">                <span class="comment">// (B-B,A) (B-C,A) (B-D,A)....</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>（5）第二次Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.friends;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShareFriendsTwoReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, Text, Text, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// &lt;(博主-博主),粉丝&gt;</span></span><br><span class="line">        <span class="comment">// (B-B,A) (B-C,A) (B-D,A)....</span></span><br><span class="line"></span><br><span class="line">        <span class="type">StringBuffer</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuffer</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 汇总</span></span><br><span class="line">        <span class="comment">// 以(博主-博主)为key，粉丝作为value</span></span><br><span class="line">        <span class="keyword">for</span> (Text fans : values) &#123;</span><br><span class="line">            sb.append(fans).append(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, <span class="keyword">new</span> <span class="title class_">Text</span>(sb.toString()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>（6）第二次Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mr.friends;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShareFriendsTwoDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;F:/IDEAWS/dashju/ShareFriendsoutput1&quot;</span>, <span class="string">&quot;F:/IDEAWS/dashju/ShareFriendsoutput2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取job对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 指定jar包运行的路径</span></span><br><span class="line">        job.setJarByClass(ShareFriendsTwoDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 指定map/reduce使用的类</span></span><br><span class="line">        job.setMapperClass(ShareFriendsTwoMapper.class);</span><br><span class="line">        job.setReducerClass(ShareFriendsTwoReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 指定map输出的数据类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 指定最终输出的数据类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 指定job的输入原始所在目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="第8章-常见错误及解决方案"><a href="#第8章-常见错误及解决方案" class="headerlink" title="第8章 常见错误及解决方案"></a>第8章 常见错误及解决方案</h1><p>1）导包容易出错。尤其Text和CombineTextInputFormat。</p>
<p>2）Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable.  报的错误是类型转换异常。</p>
<p>3）java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)，说明Partition和ReduceTask个数没对上，调整ReduceTask个数。</p>
<p>4）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。</p>
<p>5）在Windows环境编译的jar包导入到Linux环境中运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/ /user/atguigu/output</span><br></pre></td></tr></table></figure>
<p>报如下错误：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0</span><br></pre></td></tr></table></figure>
<p>原因是Windows环境用的jdk1.7，Linux环境用的jdk1.8。</p>
<p>解决方案：统一jdk版本。</p>
<p>6）缓存pd.txt小文件案例中，报找不到pd.txt文件</p>
<p>原因：大部分为路径书写错误。还有就是要检查pd.txt.txt的问题。还有个别电脑写相对路径找不到pd.txt，可以修改为绝对路径。</p>
<p>7）报类型转换异常。</p>
<p>通常都是在驱动函数中设置Map输出和最终输出时编写错误。<br>Map输出的key如果没有排序，也会报类型转换异常。</p>
<p>8）集群中运行wc.jar时出现了无法获得输入文件。</p>
<p>原因：WordCount案例的输入文件不能放用HDFS集群的根目录。</p>
<p>9）出现了如下相关异常</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z</span><br><span class="line">	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:609)</span><br><span class="line">	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)</span><br><span class="line">java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.</span><br><span class="line">	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:356)</span><br><span class="line">	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:371)</span><br><span class="line">	at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:364)</span><br></pre></td></tr></table></figure>
<p>解决方案：拷贝hadoop.dll文件到Windows目录C:\Windows\System32。个别电脑还需要修改Hadoop源码。</p>
<p>10）自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">		<span class="keyword">if</span> (atguigufos != <span class="literal">null</span>) &#123;</span><br><span class="line">			atguigufos.close();</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (otherfos != <span class="literal">null</span>) &#123;</span><br><span class="line">			otherfos.close();</span><br><span class="line">		&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2020/11/09/Hadoop-MapReduce详解-3/">http://hibiscidai.com/2020/11/09/Hadoop-MapReduce详解-3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/12/24/%E5%B0%8F%E7%B1%B33%E8%B7%AF%E7%94%B1%E5%99%A8%E5%88%B7%E5%8D%8E%E7%A1%95RT-N56U%E5%9B%BA%E4%BB%B6/"><i class="fa fa-chevron-left">  </i><span>小米3路由器刷华硕RT-N56U固件</span></a></div><div class="next-post pull-right"><a href="/2020/11/05/pertrel%E5%9C%B0%E8%B4%A8%E5%BB%BA%E6%A8%A1-%E5%AE%9E%E6%93%8D/"><span>pertrel地质建模-实操</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://xn--mesr8b36x.agency/#/register?code=R5RS1JHy">好用、实惠、稳定的梯子,点击这里<img src="https://hibiscidai.com/gallery/adv_1.png" width="728" height="90"></a></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'ccecdd8a44b8307064ce',
  clientSecret: '0a2f712808b434da5c06f04e17da4be2ac0d7cc5',
  repo: 'HibisciDai.github.io',
  owner: 'HibisciDai',
  admin: 'HibisciDai',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2023 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>