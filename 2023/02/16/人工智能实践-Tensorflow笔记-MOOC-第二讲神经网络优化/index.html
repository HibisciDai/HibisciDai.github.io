<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化"><meta name="keywords" content="学习笔记,Tensorflow,TensorflowMOOC"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-number">2.</span> <span class="toc-text">预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-where-%E5%88%A4%E6%96%AD"><span class="toc-number">2.1.</span> <span class="toc-text">tf.where() | 判断</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#np-random-RandomState-rand-%E9%9A%8F%E6%9C%BA%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">np.random.RandomState.rand() | 随机数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#np-vstack-%E6%8B%BC%E6%8E%A5%E5%8F%A0%E5%8A%A0"><span class="toc-number">2.3.</span> <span class="toc-text">np.vstack() | 拼接叠加</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#np-mgrid-ravel-c-%E7%BD%91%E6%A0%BC%E5%9D%90%E6%A0%87%E7%82%B9"><span class="toc-number">2.4.</span> <span class="toc-text">np.mgrid .ravel .c | 网格坐标点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#np-mgrid"><span class="toc-number">2.4.1.</span> <span class="toc-text">np.mgrid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#x-ravel"><span class="toc-number">2.4.2.</span> <span class="toc-text">x.ravel()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#np-c"><span class="toc-number">2.4.3.</span> <span class="toc-text">np.c</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">3.</span> <span class="toc-text">复杂度学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">3.1.</span> <span class="toc-text">时间空间复杂度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90"><span class="toc-number">3.1.1.</span> <span class="toc-text">空间复杂度分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90"><span class="toc-number">3.1.2.</span> <span class="toc-text">时间复杂度分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">3.2.</span> <span class="toc-text">学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E6%95%B0%E8%A1%B0%E5%87%8F%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">3.2.1.</span> <span class="toc-text">指数衰减学习率</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%A7%80%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">优秀的激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%BE%93%E5%87%BA%E5%80%BC%E7%9A%84%E8%8C%83%E5%9B%B4"><span class="toc-number">4.2.</span> <span class="toc-text">激活函数输出值的范围</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">4.3.</span> <span class="toc-text">常用激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">4.3.1.</span> <span class="toc-text">Sigmoid函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sigmoid%E5%87%BD%E6%95%B0API"><span class="toc-number">4.3.1.1.</span> <span class="toc-text">Sigmoid函数API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh%E5%87%BD%E6%95%B0"><span class="toc-number">4.3.2.</span> <span class="toc-text">Tanh函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Tanh%E5%87%BD%E6%95%B0API"><span class="toc-number">4.3.2.1.</span> <span class="toc-text">Tanh函数API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Relu%E5%87%BD%E6%95%B0"><span class="toc-number">4.3.3.</span> <span class="toc-text">Relu函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Relu%E5%87%BD%E6%95%B0API"><span class="toc-number">4.3.3.1.</span> <span class="toc-text">Relu函数API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Leaky-Relu%E5%87%BD%E6%95%B0"><span class="toc-number">4.3.4.</span> <span class="toc-text">Leaky Relu函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Leaky-Relu%E5%87%BD%E6%95%B0API"><span class="toc-number">4.3.4.1.</span> <span class="toc-text">Leaky Relu函数API</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AEMSE"><span class="toc-number">5.1.</span> <span class="toc-text">均方误差MSE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MSE-API"><span class="toc-number">5.1.1.</span> <span class="toc-text">MSE-API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MSE%E6%A1%88%E4%BE%8B"><span class="toc-number">5.1.2.</span> <span class="toc-text">MSE案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">自定义损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%A1%88%E4%BE%8B"><span class="toc-number">5.2.1.</span> <span class="toc-text">自定义损失函数案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-number">5.3.</span> <span class="toc-text">交叉熵</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%A1%88%E4%BE%8B"><span class="toc-number">5.3.1.</span> <span class="toc-text">交叉熵损失函数案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5-API"><span class="toc-number">5.3.2.</span> <span class="toc-text">交叉熵-API</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#softmax-API"><span class="toc-number">5.3.2.1.</span> <span class="toc-text">softmax-API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-keras-losses-categorical-crossentropy"><span class="toc-number">5.3.2.2.</span> <span class="toc-text">tf.keras.losses.categorical_crossentropy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-nn-softmax-cross-entropy-with-logits"><span class="toc-number">5.3.2.3.</span> <span class="toc-text">tf.nn.softmax_cross_entropy_with_logits</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-nn-sparse-softmax-cross-entropy-with-logits"><span class="toc-number">5.3.2.4.</span> <span class="toc-text">tf.nn.sparse_softmax_cross_entropy_with_logits</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E4%B8%8E%E4%BA%A4%E5%8F%89%E7%86%B5%E7%BB%93%E5%90%88"><span class="toc-number">5.3.3.</span> <span class="toc-text">softmax与交叉熵结合</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BC%93%E8%A7%A3%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">6.</span> <span class="toc-text">缓解过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">6.1.</span> <span class="toc-text">欠拟合的解决方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">6.2.</span> <span class="toc-text">过拟合的解决方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E7%BC%93%E8%A7%A3%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">6.3.</span> <span class="toc-text">正则化缓解过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#loss-w-%E7%9A%84%E8%AE%A1%E7%AE%97%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95"><span class="toc-number">6.3.1.</span> <span class="toc-text">loss(w)的计算可以使用两种方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96%E8%AE%A1%E7%AE%97W%E8%BF%87%E7%A8%8B"><span class="toc-number">6.3.2.</span> <span class="toc-text">L2正则化计算W过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">7.</span> <span class="toc-text">优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-%E6%97%A0momentum"><span class="toc-number">7.1.</span> <span class="toc-text">SGD随机梯度下降(无momentum)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SGDM-%E5%90%ABmomentum%E7%9A%84SGD"><span class="toc-number">7.2.</span> <span class="toc-text">SGDM(含momentum的SGD)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD-with-Nesterov-Acceleration"><span class="toc-number">7.3.</span> <span class="toc-text">SGD with Nesterov Acceleration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adagrad"><span class="toc-number">7.4.</span> <span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSProp"><span class="toc-number">7.5.</span> <span class="toc-text">RMSProp</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AdaDelta"><span class="toc-number">7.6.</span> <span class="toc-text">AdaDelta</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam"><span class="toc-number">7.7.</span> <span class="toc-text">Adam</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93"><span class="toc-number">7.8.</span> <span class="toc-text">五种优化器对比总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E6%9D%A5%E6%BA%90"><span class="toc-number">7.8.1.</span> <span class="toc-text">各种优化器来源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Visualizing-Optimization-Algos"><span class="toc-number">7.8.1.1.</span> <span class="toc-text">Visualizing Optimization Algos</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD"><span class="toc-number">7.8.2.</span> <span class="toc-text">SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGDM"><span class="toc-number">7.8.3.</span> <span class="toc-text">SGDM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adagrad-1"><span class="toc-number">7.8.4.</span> <span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSProp-1"><span class="toc-number">7.8.5.</span> <span class="toc-text">RMSProp</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam-1"><span class="toc-number">7.8.6.</span> <span class="toc-text">Adam</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96API"><span class="toc-number">8.</span> <span class="toc-text">其他API</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-cast"><span class="toc-number">8.1.</span> <span class="toc-text">tf.cast</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-random-normal"><span class="toc-number">8.2.</span> <span class="toc-text">tf.random.normal</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-where"><span class="toc-number">8.3.</span> <span class="toc-text">tf.where</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">219</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">75</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">29</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-02-16</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">20.7k</span><span class="post-meta__separator">|</span><span>阅读时长: 90 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96.png" class="" title="人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化">
<p>人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化</p>
<span id="more"></span>
<p>[TOC]</p>
<h1 id="人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化"><a href="#人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化" class="headerlink" title="人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化"></a>人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化</h1><h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><h2 id="tf-where-判断"><a href="#tf-where-判断" class="headerlink" title="tf.where() | 判断"></a>tf.where() | 判断</h2><ul>
<li>条件语句真返回A，条件语句假返回B</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.where(条件语句, 真返回A, 假返回B)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">b=tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">c=tf.where(tf.greater(a, b), a, b) <span class="comment"># 若a&gt;b,返回a对应位置的元素，否则返回b对应位置的元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;c:&quot;</span>, c)</span><br></pre></td></tr></table></figure>
<p>运行结果：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c: tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)</span><br></pre></td></tr></table></figure></p>
<h2 id="np-random-RandomState-rand-随机数"><a href="#np-random-RandomState-rand-随机数" class="headerlink" title="np.random.RandomState.rand() | 随机数"></a>np.random.RandomState.rand() | 随机数</h2><ul>
<li>返回一个[0, 1)之间的随机数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.RandomState.rand(维度)	<span class="comment">#维度为空，返回标量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">rdm = np.random.RandomState(seed = <span class="number">1</span>) <span class="comment"># seed=常数每次生成随机数相同</span></span><br><span class="line">a = rdm.rand() <span class="comment"># 返回一个随机标量</span></span><br><span class="line">b = rdm.rand(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># 返回维度为2行3列随机数矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a:&quot;</span>, a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b:&quot;</span>, b)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a: 0.417022004702574</span><br><span class="line">b: [[7.20324493e-01 1.14374817e-04 3.02332573e-01]</span><br><span class="line">	[1.46755891e-01 9.23385948e-02 1.86260211e-01]]</span><br></pre></td></tr></table></figure>
<h2 id="np-vstack-拼接叠加"><a href="#np-vstack-拼接叠加" class="headerlink" title="np.vstack() | 拼接叠加"></a>np.vstack() | 拼接叠加</h2><ul>
<li>将两个数组按垂直方向叠加</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.vstack(数组<span class="number">1</span>, 数组<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">c = np.vstack((a, b))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;c:\n&quot;</span>, c)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c:</span><br><span class="line">[[1 2 3]</span><br><span class="line">[4 5 6]]</span><br></pre></td></tr></table></figure>
<h2 id="np-mgrid-ravel-c-网格坐标点"><a href="#np-mgrid-ravel-c-网格坐标点" class="headerlink" title="np.mgrid .ravel .c | 网格坐标点"></a>np.mgrid .ravel .c | 网格坐标点</h2><p>通常配合使用，可以生成网格坐标点</p>
<h3 id="np-mgrid"><a href="#np-mgrid" class="headerlink" title="np.mgrid"></a>np.mgrid</h3><ul>
<li>返回若干组纬度相同的等差数组</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.mgrid[起始值 : 结束值 : 步长 , 起始值 : 结束值 : 步长 , … ]</span><br><span class="line"><span class="comment"># [起始值, 结束值)</span></span><br></pre></td></tr></table></figure>
<h3 id="x-ravel"><a href="#x-ravel" class="headerlink" title="x.ravel()"></a>x.ravel()</h3><ul>
<li>将x变为一维数组</li>
</ul>
<p>相当于把变量拉直</p>
<h3 id="np-c"><a href="#np-c" class="headerlink" title="np.c"></a>np.c</h3><ul>
<li>使返回的间隔数值点配对</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.c_[数组<span class="number">1</span>, 数组<span class="number">2</span>, ...]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x, y = np.mgrid [<span class="number">1</span>:<span class="number">3</span>:<span class="number">1</span>, <span class="number">2</span>:<span class="number">4</span>:<span class="number">0.5</span>]</span><br><span class="line"><span class="comment"># 第一个等差数组返回 1., 2.</span></span><br><span class="line"><span class="comment"># 第二个等差数组返回 2., 2.5, 3., 3.5</span></span><br><span class="line"><span class="comment"># 为了保证纬度相同, 以数多的为准, 所以返回两行四列</span></span><br><span class="line"></span><br><span class="line">grid = np.c_[x.ravel(), y.ravel()]</span><br><span class="line"><span class="comment"># 讲x和y配对输出</span></span><br><span class="line"><span class="comment"># x = 1. 时</span></span><br><span class="line"><span class="comment"># y = 2., 2.5, 3., 3.5</span></span><br><span class="line"><span class="comment"># [1. 2.] [1. 2.5] [1. 3.] [1. 3.5]</span></span><br><span class="line"><span class="comment"># x = 2. 时</span></span><br><span class="line"><span class="comment"># y = 2., 2.5, 3., 3.5</span></span><br><span class="line"><span class="comment"># [2. 2.] [2. 2.5] [2. 3.] [2. 3.5]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:&quot;</span>, x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:&quot;</span>, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;grid:\n&#x27;</span>, grid)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = [[1. 1. 1. 1.]</span><br><span class="line">	[2. 2. 2. 2.]]</span><br><span class="line">y = [[2. 2.5 3. 3.5]</span><br><span class="line">	[2. 2.5 3. 3.5]]</span><br><span class="line"></span><br><span class="line">grid:</span><br><span class="line">[[1. 2. ]</span><br><span class="line">[1. 2.5]</span><br><span class="line">[1. 3. ]</span><br><span class="line">[1. 3.5]</span><br><span class="line">[2. 2. ]</span><br><span class="line">[2. 2.5]</span><br><span class="line">[2. 3. ]</span><br><span class="line">[2. 3.5]]</span><br></pre></td></tr></table></figure>
<h1 id="复杂度学习率"><a href="#复杂度学习率" class="headerlink" title="复杂度学习率"></a>复杂度学习率</h1><p><strong>空间复杂度</strong>用神经网络层数和神经网络中待优化参数的个数表示</p>
<p><strong>时间复杂度</strong>可以用神经网络中乘加运算的次数表示</p>
<p>计算神经网络层数时，只统计具有运算能力的层</p>
<p>输入层仅仅把数据传输过来，没有运算，统计网络层数时候，不算输入层</p>
<p>输入层和输出层之间，所有层都叫做隐藏层</p>
<p>神经网络的层数是n个隐藏层的层数加上1个输出层</p>
<h2 id="时间空间复杂度"><a href="#时间空间复杂度" class="headerlink" title="时间空间复杂度"></a>时间空间复杂度</h2><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%A1%88%E4%BE%8B.png" class="" title="网络结构案例">
<p>输入层有三个节点</p>
<p>隐藏层只有一层，有四个节点</p>
<p>输出层有两个节点</p>
<p>这个网络共有两层神经网络，分别是隐藏层和输出层</p>
<p>参数的个数是所有w和b的总数</p>
<p>第一层参数是三行四列个w加上4个偏置项b，每个神经元有一个偏置项b，有3 * 4 + 4 = 16 个参数</p>
<p>第二层参数是四行两列个w加上2个偏置项b，每个神经元有一个偏置项b，有4 * 2 + 2 = 10 个参数</p>
<p>每个具有计算能力的神经元小球，都要收集前一层的每一个输入特征乘以各自线上的权重w，再加上这个神经元的偏置项b。</p>
<h3 id="空间复杂度分析"><a href="#空间复杂度分析" class="headerlink" title="空间复杂度分析"></a>空间复杂度分析</h3><p>层数 = 隐藏层的层数 + 1个输出层 = 2</p>
<p>总参数 = 总w + 总b = 26<br>    第一层 = 3×4+4 = 16<br>    第二层 = 4×2+2 = 10</p>
<h3 id="时间复杂度分析"><a href="#时间复杂度分析" class="headerlink" title="时间复杂度分析"></a>时间复杂度分析</h3><p>乘加运算次数 = 20<br>    第一层 = 3×4 = 12<br>    第二层 = 4×2 = 8</p>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><script type="math/tex; mode=display">w_{t+1} = w_{t} - lr * \frac{\partial loss}{\partial w_{t}}</script><p>$ w_{t+1} $ : 更新后的参数</p>
<p>$ w_{t} $ : 当前参数</p>
<p>$ lr $ : 学习率</p>
<p>$ \frac{\partial loss}{\partial w_{t}} $ ：损失函数的梯度（偏导数）</p>
<p>损失函数：<script type="math/tex">loss = (w+1)^{2}</script></p>
<p>$ \frac{\partial loss}{\partial w_{t}} = 2w+2 $</p>
<p>参数w初始化为5，学习率为0.2，则<br>1次    参数w=5    5-0.2×(2×5+2)=2.6<br>2次    参数w=2.6    2.6-0.2×(2×2.6+2)=1.16<br>3次    参数w=1.16    1.16-0.2×(2×1.16+2)=0.296<br>4次    参数w=0.296<br>…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))</span><br><span class="line">lr = <span class="number">0.2</span></span><br><span class="line">epoch = <span class="number">40</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构到grads框起了梯度的计算过程。</span></span><br><span class="line">        loss = tf.square(w + <span class="number">1</span>)</span><br><span class="line">    grads = tape.gradient(loss, w)  <span class="comment"># .gradient函数告知谁对谁求导</span></span><br><span class="line"></span><br><span class="line">    w.assign_sub(lr * grads)  <span class="comment"># .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After %s epoch,w is %f,loss is %f&quot;</span> % (epoch, w.numpy(), loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程</span></span><br><span class="line"><span class="comment"># 最终目的：找到 loss 最小 即 w = -1 的最优参数w</span></span><br></pre></td></tr></table></figure>
<p>输出<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">After 0 epoch,w is 2.600000,loss is 36.000000</span><br><span class="line">After 1 epoch,w is 1.160000,loss is 12.959999</span><br><span class="line">After 2 epoch,w is 0.296000,loss is 4.665599</span><br><span class="line">After 3 epoch,w is -0.222400,loss is 1.679616</span><br><span class="line">After 4 epoch,w is -0.533440,loss is 0.604662</span><br><span class="line">After 5 epoch,w is -0.720064,loss is 0.217678</span><br><span class="line">After 6 epoch,w is -0.832038,loss is 0.078364</span><br><span class="line">After 7 epoch,w is -0.899223,loss is 0.028211</span><br><span class="line">After 8 epoch,w is -0.939534,loss is 0.010156</span><br><span class="line">After 9 epoch,w is -0.963720,loss is 0.003656</span><br><span class="line">After 10 epoch,w is -0.978232,loss is 0.001316</span><br><span class="line">After 11 epoch,w is -0.986939,loss is 0.000474</span><br><span class="line">After 12 epoch,w is -0.992164,loss is 0.000171</span><br><span class="line">After 13 epoch,w is -0.995298,loss is 0.000061</span><br><span class="line">After 14 epoch,w is -0.997179,loss is 0.000022</span><br><span class="line">After 15 epoch,w is -0.998307,loss is 0.000008</span><br><span class="line">After 16 epoch,w is -0.998984,loss is 0.000003</span><br><span class="line">After 17 epoch,w is -0.999391,loss is 0.000001</span><br><span class="line">After 18 epoch,w is -0.999634,loss is 0.000000</span><br><span class="line">After 19 epoch,w is -0.999781,loss is 0.000000</span><br><span class="line">After 20 epoch,w is -0.999868,loss is 0.000000</span><br><span class="line">After 21 epoch,w is -0.999921,loss is 0.000000</span><br><span class="line">After 22 epoch,w is -0.999953,loss is 0.000000</span><br><span class="line">After 23 epoch,w is -0.999972,loss is 0.000000</span><br><span class="line">After 24 epoch,w is -0.999983,loss is 0.000000</span><br><span class="line">After 25 epoch,w is -0.999990,loss is 0.000000</span><br><span class="line">After 26 epoch,w is -0.999994,loss is 0.000000</span><br><span class="line">After 27 epoch,w is -0.999996,loss is 0.000000</span><br><span class="line">After 28 epoch,w is -0.999998,loss is 0.000000</span><br><span class="line">After 29 epoch,w is -0.999999,loss is 0.000000</span><br><span class="line">After 30 epoch,w is -0.999999,loss is 0.000000</span><br><span class="line">After 31 epoch,w is -1.000000,loss is 0.000000</span><br><span class="line">After 32 epoch,w is -1.000000,loss is 0.000000</span><br><span class="line">After 33 epoch,w is -1.000000,loss is 0.000000</span><br><span class="line">After 34 epoch,w is -1.000000,loss is 0.000000</span><br><span class="line">After 35 epoch,w is -1.000000,loss is 0.000000</span><br><span class="line">After 36 epoch,w is -1.000000,loss is 0.000000</span><br><span class="line">After 37 epoch,w is -1.000000,loss is 0.000000</span><br><span class="line">After 38 epoch,w is -1.000000,loss is 0.000000</span><br><span class="line">After 39 epoch,w is -1.000000,loss is 0.000000</span><br></pre></td></tr></table></figure></p>
<p>lr=0.001过慢</p>
<p>输出：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">After 0 epoch,w is 4.988000,loss is 36.000000</span><br><span class="line">After 1 epoch,w is 4.976024,loss is 35.856144</span><br><span class="line">After 2 epoch,w is 4.964072,loss is 35.712864</span><br><span class="line">After 3 epoch,w is 4.952144,loss is 35.570156</span><br><span class="line">After 4 epoch,w is 4.940240,loss is 35.428020</span><br><span class="line">After 5 epoch,w is 4.928360,loss is 35.286449</span><br><span class="line">After 6 epoch,w is 4.916503,loss is 35.145447</span><br><span class="line">After 7 epoch,w is 4.904670,loss is 35.005009</span><br><span class="line">After 8 epoch,w is 4.892860,loss is 34.865124</span><br><span class="line">After 9 epoch,w is 4.881075,loss is 34.725803</span><br><span class="line">After 10 epoch,w is 4.869313,loss is 34.587044</span><br><span class="line">After 11 epoch,w is 4.857574,loss is 34.448833</span><br><span class="line">After 12 epoch,w is 4.845859,loss is 34.311172</span><br><span class="line">After 13 epoch,w is 4.834167,loss is 34.174068</span><br><span class="line">After 14 epoch,w is 4.822499,loss is 34.037510</span><br><span class="line">After 15 epoch,w is 4.810854,loss is 33.901497</span><br><span class="line">After 16 epoch,w is 4.799233,loss is 33.766029</span><br><span class="line">After 17 epoch,w is 4.787634,loss is 33.631104</span><br><span class="line">After 18 epoch,w is 4.776059,loss is 33.496712</span><br><span class="line">After 19 epoch,w is 4.764507,loss is 33.362858</span><br><span class="line">After 20 epoch,w is 4.752978,loss is 33.229538</span><br><span class="line">After 21 epoch,w is 4.741472,loss is 33.096756</span><br><span class="line">After 22 epoch,w is 4.729989,loss is 32.964497</span><br><span class="line">After 23 epoch,w is 4.718529,loss is 32.832775</span><br><span class="line">After 24 epoch,w is 4.707092,loss is 32.701576</span><br><span class="line">After 25 epoch,w is 4.695678,loss is 32.570904</span><br><span class="line">After 26 epoch,w is 4.684287,loss is 32.440750</span><br><span class="line">After 27 epoch,w is 4.672918,loss is 32.311119</span><br><span class="line">After 28 epoch,w is 4.661572,loss is 32.182003</span><br><span class="line">After 29 epoch,w is 4.650249,loss is 32.053402</span><br><span class="line">After 30 epoch,w is 4.638949,loss is 31.925320</span><br><span class="line">After 31 epoch,w is 4.627671,loss is 31.797745</span><br><span class="line">After 32 epoch,w is 4.616416,loss is 31.670683</span><br><span class="line">After 33 epoch,w is 4.605183,loss is 31.544128</span><br><span class="line">After 34 epoch,w is 4.593973,loss is 31.418077</span><br><span class="line">After 35 epoch,w is 4.582785,loss is 31.292530</span><br><span class="line">After 36 epoch,w is 4.571619,loss is 31.167484</span><br><span class="line">After 37 epoch,w is 4.560476,loss is 31.042938</span><br><span class="line">After 38 epoch,w is 4.549355,loss is 30.918892</span><br><span class="line">After 39 epoch,w is 4.538256,loss is 30.795341</span><br></pre></td></tr></table></figure>
<p>lr=0.999不收敛</p>
<p>输出：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">After 0 epoch,w is -6.988000,loss is 36.000000</span><br><span class="line">After 1 epoch,w is 4.976024,loss is 35.856144</span><br><span class="line">After 2 epoch,w is -6.964072,loss is 35.712860</span><br><span class="line">After 3 epoch,w is 4.952145,loss is 35.570156</span><br><span class="line">After 4 epoch,w is -6.940241,loss is 35.428024</span><br><span class="line">After 5 epoch,w is 4.928361,loss is 35.286461</span><br><span class="line">After 6 epoch,w is -6.916504,loss is 35.145462</span><br><span class="line">After 7 epoch,w is 4.904671,loss is 35.005020</span><br><span class="line">After 8 epoch,w is -6.892861,loss is 34.865135</span><br><span class="line">After 9 epoch,w is 4.881076,loss is 34.725815</span><br><span class="line">After 10 epoch,w is -6.869314,loss is 34.587051</span><br><span class="line">After 11 epoch,w is 4.857575,loss is 34.448849</span><br><span class="line">After 12 epoch,w is -6.845860,loss is 34.311192</span><br><span class="line">After 13 epoch,w is 4.834168,loss is 34.174084</span><br><span class="line">After 14 epoch,w is -6.822500,loss is 34.037521</span><br><span class="line">After 15 epoch,w is 4.810855,loss is 33.901508</span><br><span class="line">After 16 epoch,w is -6.799233,loss is 33.766033</span><br><span class="line">After 17 epoch,w is 4.787635,loss is 33.631107</span><br><span class="line">After 18 epoch,w is -6.776060,loss is 33.496716</span><br><span class="line">After 19 epoch,w is 4.764508,loss is 33.362869</span><br><span class="line">After 20 epoch,w is -6.752979,loss is 33.229557</span><br><span class="line">After 21 epoch,w is 4.741473,loss is 33.096771</span><br><span class="line">After 22 epoch,w is -6.729990,loss is 32.964516</span><br><span class="line">After 23 epoch,w is 4.718530,loss is 32.832787</span><br><span class="line">After 24 epoch,w is -6.707093,loss is 32.701580</span><br><span class="line">After 25 epoch,w is 4.695680,loss is 32.570911</span><br><span class="line">After 26 epoch,w is -6.684288,loss is 32.440765</span><br><span class="line">After 27 epoch,w is 4.672919,loss is 32.311131</span><br><span class="line">After 28 epoch,w is -6.661573,loss is 32.182014</span><br><span class="line">After 29 epoch,w is 4.650250,loss is 32.053413</span><br><span class="line">After 30 epoch,w is -6.638950,loss is 31.925329</span><br><span class="line">After 31 epoch,w is 4.627672,loss is 31.797762</span><br><span class="line">After 32 epoch,w is -6.616417,loss is 31.670694</span><br><span class="line">After 33 epoch,w is 4.605185,loss is 31.544140</span><br><span class="line">After 34 epoch,w is -6.593974,loss is 31.418095</span><br><span class="line">After 35 epoch,w is 4.582787,loss is 31.292547</span><br><span class="line">After 36 epoch,w is -6.571621,loss is 31.167505</span><br><span class="line">After 37 epoch,w is 4.560478,loss is 31.042959</span><br><span class="line">After 38 epoch,w is -6.549357,loss is 30.918919</span><br><span class="line">After 39 epoch,w is 4.538259,loss is 30.795368</span><br></pre></td></tr></table></figure>
<h3 id="指数衰减学习率"><a href="#指数衰减学习率" class="headerlink" title="指数衰减学习率"></a>指数衰减学习率</h3><p>可以先用较大的学习率，快速得到较优解，然后逐步减小学习率，使模型在训练后期稳定。</p>
<p><strong><script type="math/tex">指数衰减学习率 = 初始学习率 × 学习率衰减率^{（ 当前轮数 / 多少轮衰减一次 ）}</script></strong></p>
<p><code>初始学习率</code>、<code>学习率衰减率</code>、<code>多少轮衰减一次</code>：超参数</p>
<p><code>当前轮数</code>：变量、计数器，可以用当前跌倒了多少次数据集，也就是epoch数值表示；也可以用当前一共迭代了多少次batch也就是golobal_step表示</p>
<p><code>多少轮衰减一次</code>：迭代多少次数据集，或者迭代多少次batch更新一次学习率；决定了学习率更新的频率</p>
<p>指数衰减学习率的计算一般写在for循环中</p>
<p>在上个代码中添加指数衰减学习率后</p>
<p>使得学习率根据迭代的轮数指数衰减了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">epoch = <span class="number">40</span></span><br><span class="line"></span><br><span class="line">LR_BASE = <span class="number">0.2</span>  <span class="comment"># 最初学习率</span></span><br><span class="line">LR_DECAY = <span class="number">0.99</span>  <span class="comment"># 学习率衰减率</span></span><br><span class="line">LR_STEP = <span class="number">1</span>  <span class="comment"># 喂入多少轮BATCH_SIZE后，更新一次学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环100次迭代。</span></span><br><span class="line">    lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP)	<span class="comment">#指数衰减学习率</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构到grads框起了梯度的计算过程。</span></span><br><span class="line">        loss = tf.square(w + <span class="number">1</span>)</span><br><span class="line">    grads = tape.gradient(loss, w)  <span class="comment"># .gradient函数告知谁对谁求导</span></span><br><span class="line"></span><br><span class="line">    w.assign_sub(lr * grads)  <span class="comment"># .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After %s epoch,w is %f,loss is %f,lr is %f&quot;</span> % (epoch, w.numpy(), loss, lr))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">After 0 epoch,w is 2.600000,loss is 36.000000,lr is 0.200000</span><br><span class="line">After 1 epoch,w is 1.174400,loss is 12.959999,lr is 0.198000</span><br><span class="line">After 2 epoch,w is 0.321948,loss is 4.728015,lr is 0.196020</span><br><span class="line">After 3 epoch,w is -0.191126,loss is 1.747547,lr is 0.194060</span><br><span class="line">After 4 epoch,w is -0.501926,loss is 0.654277,lr is 0.192119</span><br><span class="line">After 5 epoch,w is -0.691392,loss is 0.248077,lr is 0.190198</span><br><span class="line">After 6 epoch,w is -0.807611,loss is 0.095239,lr is 0.188296</span><br><span class="line">After 7 epoch,w is -0.879339,loss is 0.037014,lr is 0.186413</span><br><span class="line">After 8 epoch,w is -0.923874,loss is 0.014559,lr is 0.184549</span><br><span class="line">After 9 epoch,w is -0.951691,loss is 0.005795,lr is 0.182703</span><br><span class="line">After 10 epoch,w is -0.969167,loss is 0.002334,lr is 0.180876</span><br><span class="line">After 11 epoch,w is -0.980209,loss is 0.000951,lr is 0.179068</span><br><span class="line">After 12 epoch,w is -0.987226,loss is 0.000392,lr is 0.177277</span><br><span class="line">After 13 epoch,w is -0.991710,loss is 0.000163,lr is 0.175504</span><br><span class="line">After 14 epoch,w is -0.994591,loss is 0.000069,lr is 0.173749</span><br><span class="line">After 15 epoch,w is -0.996452,loss is 0.000029,lr is 0.172012</span><br><span class="line">After 16 epoch,w is -0.997660,loss is 0.000013,lr is 0.170292</span><br><span class="line">After 17 epoch,w is -0.998449,loss is 0.000005,lr is 0.168589</span><br><span class="line">After 18 epoch,w is -0.998967,loss is 0.000002,lr is 0.166903</span><br><span class="line">After 19 epoch,w is -0.999308,loss is 0.000001,lr is 0.165234</span><br><span class="line">After 20 epoch,w is -0.999535,loss is 0.000000,lr is 0.163581</span><br><span class="line">After 21 epoch,w is -0.999685,loss is 0.000000,lr is 0.161946</span><br><span class="line">After 22 epoch,w is -0.999786,loss is 0.000000,lr is 0.160326</span><br><span class="line">After 23 epoch,w is -0.999854,loss is 0.000000,lr is 0.158723</span><br><span class="line">After 24 epoch,w is -0.999900,loss is 0.000000,lr is 0.157136</span><br><span class="line">After 25 epoch,w is -0.999931,loss is 0.000000,lr is 0.155564</span><br><span class="line">After 26 epoch,w is -0.999952,loss is 0.000000,lr is 0.154009</span><br><span class="line">After 27 epoch,w is -0.999967,loss is 0.000000,lr is 0.152469</span><br><span class="line">After 28 epoch,w is -0.999977,loss is 0.000000,lr is 0.150944</span><br><span class="line">After 29 epoch,w is -0.999984,loss is 0.000000,lr is 0.149434</span><br><span class="line">After 30 epoch,w is -0.999989,loss is 0.000000,lr is 0.147940</span><br><span class="line">After 31 epoch,w is -0.999992,loss is 0.000000,lr is 0.146461</span><br><span class="line">After 32 epoch,w is -0.999994,loss is 0.000000,lr is 0.144996</span><br><span class="line">After 33 epoch,w is -0.999996,loss is 0.000000,lr is 0.143546</span><br><span class="line">After 34 epoch,w is -0.999997,loss is 0.000000,lr is 0.142111</span><br><span class="line">After 35 epoch,w is -0.999998,loss is 0.000000,lr is 0.140690</span><br><span class="line">After 36 epoch,w is -0.999999,loss is 0.000000,lr is 0.139283</span><br><span class="line">After 37 epoch,w is -0.999999,loss is 0.000000,lr is 0.137890</span><br><span class="line">After 38 epoch,w is -0.999999,loss is 0.000000,lr is 0.136511</span><br><span class="line">After 39 epoch,w is -0.999999,loss is 0.000000,lr is 0.135146</span><br></pre></td></tr></table></figure>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>鸢尾花的神经网络模型如下图所示，其激活函数是线性函数，即使增加网络层数，依旧为线性，模型的表达能力不够。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png" class="" title="鸢尾花神经网络激活函数">
<script type="math/tex; mode=display">y = x * w + b</script><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/MP%E6%A8%A1%E5%9E%8B%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png" class="" title="MP模型激活函数">
<script type="math/tex; mode=display">y = f ( x * w + b )</script><p>相较于线性网络结构，多了一个非线性函数，叫做激活函数，提升了模型的表达力。</p>
<p>使得网络不再是简单的线性组合，可以随着层数的增加提升表达能力。</p>
<h2 id="优秀的激活函数"><a href="#优秀的激活函数" class="headerlink" title="优秀的激活函数"></a>优秀的激活函数</h2><p>• 非线性： 激活函数非线性时，多层神经网络可逼近所有函数</p>
<p>• 可微性： 优化器大多用梯度下降更新参数</p>
<p>• 单调性： 当激活函数是单调的，能保证单层网络的损失函数是凸函数</p>
<p>• 近似恒等性： f(x)≈x当参数初始化为随机小值时，神经网络更稳定</p>
<h2 id="激活函数输出值的范围"><a href="#激活函数输出值的范围" class="headerlink" title="激活函数输出值的范围"></a>激活函数输出值的范围</h2><p>• 激活函数输出为有限值时，权重对特征的影响会更显著，基于梯度的优化方法更稳定</p>
<p>• 激活函数输出为无限值时，参数的初始值对模型的影响非常大，建议调小学习率</p>
<h2 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h2><h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sigmoid(x)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">f(x) = \frac{1}{1 + e^{-x}}</script><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/sigmoid%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png" class="" title="sigmoid函数图像">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/sigmoid%E5%AF%BC%E6%95%B0%E5%9B%BE%E5%83%8F.png" class="" title="sigmoid导数图像">
<p>Sigmoid函数把输入值变换到0~1之间输出</p>
<p>输入非常大的负数，为0；输入非常大的整数，为1。相当于归一化操作。</p>
<p>深层神经网络更新参数时，需要从输出层到输入层逐层进行链式求导。但Sigmoid函数的倒数输出是0~0.25之间的小数。会出现多个0到0.25之间的连续相乘，结果将趋近于0，产生梯度消失，使得参数无法继续更新。我们希望输入每层神经网络的特征是以0为均值的小数值。但是如果Sigmoid激活函数后的数据都是正数，会使得收敛变慢。另外，Sigmoid函数存在幂运算计算复杂度过大，训练时间长。</p>
<p>特点：</p>
<p>（1）易造成梯度消失</p>
<p>（2）输出非0均值，收敛慢</p>
<p>（3）幂运算复杂，训练时间长</p>
<h4 id="Sigmoid函数API"><a href="#Sigmoid函数API" class="headerlink" title="Sigmoid函数API"></a>Sigmoid函数API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.math.sigmoid(</span><br><span class="line">	x, name = <span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：计算x每一个元素的Sigmoid值</li>
<li>等价API：<code>tf.nn.sigmoid, tf.sigmoid</code></li>
<li>参数：<br>  <code>x</code> : 张量x</li>
<li>返回：与x shape相同的张量</li>
<li>案例：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], )</span><br><span class="line"><span class="built_in">print</span>(tf.math.sigmoid(x))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.7310586</span> <span class="number">0.880797</span> <span class="number">0.95257413</span>], shape=(<span class="number">3</span>,), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 等价实现</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span>/(<span class="number">1</span>+tf.math.exp(-x)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.7310586</span> <span class="number">0.880797</span> <span class="number">0.95257413</span>], shape=(<span class="number">3</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.math.tanh(x)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">f(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}</script><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/tanh%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png" class="" title="tanh函数图像">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/tanh%E5%AF%BC%E6%95%B0%E5%9B%BE%E5%83%8F.png" class="" title="tanh导数图像">
<p>特点：</p>
<p>（1）输出是0均值</p>
<p>（2）易造成梯度消失</p>
<p>（3）幂运算复杂，训练时间长</p>
<h4 id="Tanh函数API"><a href="#Tanh函数API" class="headerlink" title="Tanh函数API"></a>Tanh函数API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.math.tanh(</span><br><span class="line">	x, name = <span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：计算x每一个元素的Tanh值</li>
<li>等价API：<code>tf.nn.tanh, tf.tanh</code></li>
<li>参数：<br>  <code>x</code> : 张量x</li>
<li>返回：与x shape相同的张量</li>
<li>案例：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([-<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>), -<span class="number">5</span>, -<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">1.2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)])</span><br><span class="line"><span class="built_in">print</span>(tf.math.tanh(x))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([-<span class="number">1.</span> -<span class="number">0.99990916</span> -<span class="number">0.46211717</span> <span class="number">0.7615942</span> <span class="number">0.8336547</span> <span class="number">0.9640276</span></span><br><span class="line"><span class="number">0.9950547</span> <span class="number">1.</span>], shape=(<span class="number">8</span>,), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价实现</span></span><br><span class="line"><span class="built_in">print</span>((tf.math.exp(x)-tf.math.exp(-x))/(tf.math.exp(x)+tf.math.exp(-x)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([nan -<span class="number">0.9999091</span> -<span class="number">0.46211714</span> <span class="number">0.7615942</span> <span class="number">0.83365464</span> <span class="number">0.9640275</span></span><br><span class="line"><span class="number">0.9950547</span> nan], shape=(<span class="number">8</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="Relu函数"><a href="#Relu函数" class="headerlink" title="Relu函数"></a>Relu函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu(x)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">f(x) = max(x, 0)  = \left\{ \begin{array}{rcl} 0 & x<0 \\ x & x>=0 \end{array} \right.</script><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/relu%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png" class="" title="relu函数图像">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/relu%E5%AF%BC%E6%95%B0%E5%9B%BE%E5%83%8F.png" class="" title="relu导数图像">
<p>优点：</p>
<p>（1） 解决了梯度消失问题 (在正区间)</p>
<p>（2） 只需判断输入是否大于0，计算速度快</p>
<p>（3） 收敛速度远快于sigmoid和tanh</p>
<p>缺点：</p>
<p>（1） 输出非0均值，收敛慢</p>
<p>（2） Dead RelU问题：某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。送入激活函数的输入特征是负数时，激活函数输出是0，反向传播得到的梯度是0，参数无法更新，神经元死亡。神经元死亡的原因是，经过relu函数的负数特征过多，可以改进随机初始化，避免过多的负数特征送入relu函数。可以通过设置更小的学习率，减少参数分布的巨大变化，避免训练中产生过多负数特征进入relu函数。</p>
<h4 id="Relu函数API"><a href="#Relu函数API" class="headerlink" title="Relu函数API"></a>Relu函数API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu(</span><br><span class="line">	features, name = <span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：计算修正线性值(rectified linear)：max(features, 0).</li>
<li>参数：<br>  features：张量</li>
<li>返回：与features shape相同的张量</li>
<li>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.nn.relu([-<span class="number">2.</span>, <span class="number">0.</span>, -<span class="number">0.</span>, <span class="number">3.</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.</span> <span class="number">0.</span> -<span class="number">0.</span> <span class="number">3.</span>], shape=(<span class="number">4</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="Leaky-Relu函数"><a href="#Leaky-Relu函数" class="headerlink" title="Leaky Relu函数"></a>Leaky Relu函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.leaky_relu(x)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">f(x) = max(\alpha x, x) = \left\{ \begin{array}{rcl} \alpha x & x<0 \\ x & x>=0 \end{array} \right.</script><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/leakyrelu%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png" class="" title="leakyrelu函数图像">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/leakyrelu%E5%AF%BC%E6%95%B0%E5%9B%BE%E5%83%8F.png" class="" title="leakyrelu导数图像">
<p>Leaky Relu函数是为了解决relu负区间为0，引起神经元死亡问题而设计的，Leaky Relu负区间引入了一个固定的斜率α，使得Leaky Relu负区间不再恒等于0。</p>
<p>理论上来讲， Leaky Relu有Relu的所有优点，外加不会有Dead Relu问题，但是在实际操作当中，并没有完全证明Leaky Relu总是好于Relu，选择Relu作为激活函数的网络会更多。</p>
<p>对于初学者的建议：</p>
<ul>
<li>首选relu激活函数；</li>
<li>学习率设置较小值；</li>
<li>输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布；</li>
<li>初始参数中心化，即让随机生成的参数满足以0为均值， $ \sqrt{\frac{2}{当前层输入特征个数}} $ 为标准差的正态分布。</li>
</ul>
<h4 id="Leaky-Relu函数API"><a href="#Leaky-Relu函数API" class="headerlink" title="Leaky Relu函数API"></a>Leaky Relu函数API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.leaky_relu(</span><br><span class="line">	features, alpha=<span class="number">0.2</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：计算 Leaky Relu值</li>
<li>参数：<br>  features：张量<br>  alpha：x&lt;0时的斜率值</li>
<li>返回：与features shape相同的张量</li>
<li>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.nn.leaky_relu([-<span class="number">2.</span>, <span class="number">0.</span>, -<span class="number">0.</span>, <span class="number">3.</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([-<span class="number">0.4</span> <span class="number">0.</span> -<span class="number">0.</span> <span class="number">3.</span>], shape=(<span class="number">4</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>损失函数(loss)：向前传播计算出的预测值(y)与已知答案( y_ )的差距</p>
<p>神经网络的优化目标就是找到某套参数，使得计算出来的结果y和已知标准答案 y_ 无限接近，也就是他们的差距loss值最小。</p>
<p>主流loss有三种计算方法：均方误差、自定义和交叉熵。</p>
<h2 id="均方误差MSE"><a href="#均方误差MSE" class="headerlink" title="均方误差MSE"></a>均方误差MSE</h2><script type="math/tex; mode=display">MSE(y_{\_}, y) = \frac{\sum_{i=1}^{n}(y-y_{\_})^{2} }{n}</script><p>Tensorflow是这样实现均方误差损失函数计算的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_mse = tf.reduce_mean(tf.square(y, y_))</span><br></pre></td></tr></table></figure>
<h3 id="MSE-API"><a href="#MSE-API" class="headerlink" title="MSE-API"></a>MSE-API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.losses.MSE(</span><br><span class="line">	y_true, y_pred</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：计算<code>y_true</code>和<code>y_pred</code>的均方误差</li>
<li>例子：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y_true = tf.constant([<span class="number">0.5</span>, <span class="number">0.8</span>])</span><br><span class="line">y_pred = tf.constant([<span class="number">1.0</span>, <span class="number">1.0</span>])</span><br><span class="line"><span class="built_in">print</span>(tf.keras.losses.MSE(y_true, y_pred))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor(<span class="number">0.145</span>, shape=(), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价实现</span></span><br><span class="line"><span class="built_in">print</span>(tf.reduce_mean(tf.square(y_true - y_pred)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor(<span class="number">0.145</span>, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="MSE案例"><a href="#MSE案例" class="headerlink" title="MSE案例"></a>MSE案例</h3><p>预测酸奶日销量y， x1、 x2是影响日销量的因素。<br>建模前，应预先采集的数据有：每日x1、 x2和销量y<em>（即已知答案，最佳情况：产量=销量）<br>拟造数据集 X,Y</em>： y_ = x1 + x2 噪声： -0.05 ~ +0.05 拟合可以预测销量的函数</p>
<p>缺少数据集自己造，一层神经网络，预测酸奶的日销量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(seed=SEED)  <span class="comment"># 生成[0,1)之间的随机数</span></span><br><span class="line">x = rdm.rand(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line">y_ = [[x1 + x2 + (rdm.rand() / <span class="number">10.0</span> - <span class="number">0.05</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> x]  <span class="comment"># 生成噪声[0,1)/10=[0,0.1); [0,0.1)-0.05=[-0.05,0.05)</span></span><br><span class="line">x = tf.cast(x, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">epoch = <span class="number">15000</span></span><br><span class="line">lr = <span class="number">0.002</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = tf.matmul(x, w1)</span><br><span class="line">        loss_mse = tf.reduce_mean(tf.square(y_ - y))</span><br><span class="line"></span><br><span class="line">    grads = tape.gradient(loss_mse, w1)</span><br><span class="line">    w1.assign_sub(lr * grads)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;After %d training steps,w1 is &quot;</span> % (epoch))</span><br><span class="line">        <span class="built_in">print</span>(w1.numpy(), <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Final w1 is: &quot;</span>, w1.numpy())</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">After 0 training steps,w1 is </span><br><span class="line">[[-0.8096241]</span><br><span class="line"> [ 1.4855157]] </span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">After 3000 training steps,w1 is </span><br><span class="line">[[0.61725086]</span><br><span class="line"> [1.332841  ]] </span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">After 6000 training steps,w1 is </span><br><span class="line">[[0.88665503]</span><br><span class="line"> [1.098054  ]] </span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">After 10000 training steps,w1 is </span><br><span class="line">[[0.9801975]</span><br><span class="line"> [1.0159837]] </span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">After 12000 training steps,w1 is </span><br><span class="line">[[0.9933934]</span><br><span class="line"> [1.0044063]] </span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Final w1 is:  [[1.0009792]</span><br><span class="line"> [0.9977485]]</span><br></pre></td></tr></table></figure>
<p>两个参数正向着1趋近，最后得到神经网络的参数是接近1的。</p>
<p>最终拟合结果：<script type="math/tex">y = 1.00 × x_{1} + 1.00 × x_{2}</script></p>
<p>结果和制造数据集的公式一致。</p>
<h2 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h2><p>使用均方误差使用损失函数，默认认为销量预测的多了或者少了，损失是一样的。</p>
<p>而真实情况是，商品销量预测多了，损失的是成本；预测少了，损失的是利润。</p>
<p>利润和成本往往不相等，则MSE产生的loss不能使得利益最大化。</p>
<p>使用自定义损失函数，用自定义损失函数计算每一个预测结果y与标准答案 y_ 产生的损失累积和。</p>
<script type="math/tex; mode=display">loss(y_{\_}, y)  = \sum_{n} f({y_{\_},y})</script><p>y_ ：标准答案数据集<br>y：预测答案计算出的</p>
<p>可以把损失定义为一个分段函数，如果预测的结果<code>y</code>小于标准答案 <code>y_</code> ，则预测的<code>y</code>少了，损失的是利润。</p>
<script type="math/tex; mode=display">f({y_{\_},y}) = \left\{\begin{array}{rcl} PROFIT × (y_{\_} - y) & y < y_{\_} & 预测的y少了，损失利润(PROFIT) \\ COST × (y - y_{\_}) & y>= y_{\_} & 预测的y多了，损失成本(COST) \end{array} \right.</script><p>自己写出一个损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_zdy = tf.reduce_sum(tf.where(tf.greater(y, y_), COST * (y-y_), PROFIT * (y_-y)))</span><br></pre></td></tr></table></figure>
<p>如：预测酸奶销量，酸奶成本（COST） 1元，酸奶利润（PROFIT） 99元。预测少了损失利润99元，大于预测多了损失成本1元。预测少了损失大，希望生成的预测函数往多了预测。</p>
<h3 id="自定义损失函数案例"><a href="#自定义损失函数案例" class="headerlink" title="自定义损失函数案例"></a>自定义损失函数案例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line">COST = <span class="number">1</span></span><br><span class="line">PROFIT = <span class="number">99</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">x = rdm.rand(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line">y_ = [[x1 + x2 + (rdm.rand() / <span class="number">10.0</span> - <span class="number">0.05</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> x]  <span class="comment"># 生成噪声[0,1)/10=[0,0.1); [0,0.1)-0.05=[-0.05,0.05)</span></span><br><span class="line">x = tf.cast(x, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">epoch = <span class="number">10000</span></span><br><span class="line">lr = <span class="number">0.002</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = tf.matmul(x, w1)</span><br><span class="line">        loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * COST, (y_ - y) * PROFIT))</span><br><span class="line"></span><br><span class="line">    grads = tape.gradient(loss, w1)</span><br><span class="line">    w1.assign_sub(lr * grads)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;After %d training steps,w1 is &quot;</span> % (epoch))</span><br><span class="line">        <span class="built_in">print</span>(w1.numpy(), <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Final w1 is: &quot;</span>, w1.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义损失函数</span></span><br><span class="line"><span class="comment"># 酸奶成本1元， 酸奶利润99元</span></span><br><span class="line"><span class="comment"># 成本很低，利润很高，人们希望多预测些，生成模型系数大于1，往多了预测</span></span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">After 0 training steps,w1 is </span><br><span class="line">[[2.0855923]</span><br><span class="line"> [3.8476257]] </span><br><span class="line"></span><br><span class="line">After 500 training steps,w1 is </span><br><span class="line">[[1.1830753]</span><br><span class="line"> [1.1627482]] </span><br><span class="line"></span><br><span class="line">After 1000 training steps,w1 is </span><br><span class="line">[[1.1526372]</span><br><span class="line"> [1.0175619]] </span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">After 6000 training steps,w1 is </span><br><span class="line">[[1.1528853]</span><br><span class="line"> [1.1765157]] </span><br><span class="line"></span><br><span class="line">... </span><br><span class="line"></span><br><span class="line">After 9500 training steps,w1 is </span><br><span class="line">[[1.1611756]</span><br><span class="line"> [1.0651482]] </span><br><span class="line"></span><br><span class="line">Final w1 is:  [[1.1626335]</span><br><span class="line"> [1.1191947]]</span><br></pre></td></tr></table></figure>
<p>当cost=1，profit=99时，两个参数都大于1，都大于用均方误差做损失函数时的系数，模型在尽量往多了预测。</p>
<p>最终拟合结果：<script type="math/tex">y = 1.16 × x_{1} + 1.12 × x_{2}</script></p>
<p>当cost=99，profit=1时，结果两个参数均小于1，模型在尽量往少了预测。</p>
<p>最终拟合结果：<script type="math/tex">y = 0.92 × x_{1} + 0.92 × x_{2}</script></p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>交叉熵损失函数CE (Cross Entropy)：表征两个概率分布之间的距离。</p>
<p>交叉熵越大，两个概率分布越远；交叉熵越小，两个概率分布越近。</p>
<script type="math/tex; mode=display">H(y_{\_}, y) = - \sum y_{\_} * ln y</script><p><code>y_</code> : 真实结果的概率分布<br><code>y</code>：预测结果的概率分布</p>
<p>通过交叉熵的值可以判断哪一个预测结果和标准答案更接近。</p>
<h3 id="交叉熵损失函数案例"><a href="#交叉熵损失函数案例" class="headerlink" title="交叉熵损失函数案例"></a>交叉熵损失函数案例</h3><p>二分类 已知答案 <code>y_ = (1, 0)</code> 预测 <code>y1=(0.6, 0.4)</code> 和 <code>y2=(0.8, 0.2)</code> 哪个更接近标准答案？</p>
<script type="math/tex; mode=display">H_{1} ((1,0),(0.6,0.4))  =  -(1*ln0.6 + 0*ln0.4) ≈ -(-0.511 + 0) = 0.511</script><script type="math/tex; mode=display">H_{2} ((1,0),(0.8,0.2))  =  -(1*ln0.8 + 0*ln0.2) ≈ -(-0.223 + 0) = 0.223</script><p>因为H1 &gt; H2，所以y2预测更准</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.losses.categorical_crossentropy(y_, y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss_ce1=tf.losses.categorical_crossentropy([<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0.6</span>, <span class="number">0.4</span>])</span><br><span class="line">loss_ce2=tf.losses.categorical_crossentropy([<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss_ce1:&quot;</span>, loss_ce1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss_ce2:&quot;</span>, loss_ce2)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_ce1: tf.Tensor(0.5108256, shape=(), dtype=float32)</span><br><span class="line">loss_ce2: tf.Tensor(0.22314353, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="交叉熵-API"><a href="#交叉熵-API" class="headerlink" title="交叉熵-API"></a>交叉熵-API</h3><h4 id="softmax-API"><a href="#softmax-API" class="headerlink" title="softmax-API"></a>softmax-API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax(</span><br><span class="line">	logits, axis=<span class="literal">None</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：计算softmax激活值</li>
<li>等价API：<code>tf.math.softmax</code></li>
<li>参数：<br>  logits：张量<br>  axis：计算softmax所在的维度，默认为-1，即最后一个维度</li>
<li>返回：与logits shape相同的张量</li>
<li>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logits = tf.constant([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(tf.nn.softmax(logits))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.26538792</span> <span class="number">0.7213992</span> <span class="number">0.01321289</span>], shape=(<span class="number">3</span>,), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价实现</span></span><br><span class="line"><span class="built_in">print</span>(tf.exp(logits) / tf.reduce_sum(tf.exp(logits)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.26538792</span> <span class="number">0.72139925</span> <span class="number">0.01321289</span>], shape=(<span class="number">3</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>
<h4 id="tf-keras-losses-categorical-crossentropy"><a href="#tf-keras-losses-categorical-crossentropy" class="headerlink" title="tf.keras.losses.categorical_crossentropy"></a>tf.keras.losses.categorical_crossentropy</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.losses.categorical_crossentropy(</span><br><span class="line">	y_true, y_pred, from_logits=<span class="literal">False</span>, label_smoothing=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：计算交叉熵</li>
<li>等价API：<code>tf.losses.categorical_crossentropy</code></li>
<li>参数：<br>  <code>y_true</code>：真实值<br>  <code>y_pred</code>：预测值<br>  <code>from_logits</code>：y_pred是否为logits张量<br>  <code>label_smoothing</code>：[0, 1]之间的小数</li>
<li>返回：交叉熵损失值</li>
<li>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">y_true = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">y_pred1 = [<span class="number">0.5</span>, <span class="number">0.4</span>, <span class="number">0.1</span>]</span><br><span class="line">y_pred2 = [<span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]</span><br><span class="line"><span class="built_in">print</span>(tf.keras.losses.categorical_crossentropy(y_true, y_pred1))</span><br><span class="line"><span class="built_in">print</span>(tf.keras.losses.categorical_crossentropy(y_true, y_pred2))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor(<span class="number">0.6931472</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.22314353</span>, shape=(), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价实现</span></span><br><span class="line"><span class="built_in">print</span>(-tf.reduce_sum(y_true * tf.math.log(y_pred1)))</span><br><span class="line"><span class="built_in">print</span>(-tf.reduce_sum(y_true * tf.math.log(y_pred2)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor(<span class="number">0.6931472</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.22314353</span>, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<h4 id="tf-nn-softmax-cross-entropy-with-logits"><a href="#tf-nn-softmax-cross-entropy-with-logits" class="headerlink" title="tf.nn.softmax_cross_entropy_with_logits"></a>tf.nn.softmax_cross_entropy_with_logits</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">	labels, logits, axis=-<span class="number">1</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在机器学习中，对于多分类问题，把未经softmax归一化的向量值称为logits。logits经过softmax<br>层后，输出服从概率分布的向量。</p>
</blockquote>
<ul>
<li>功能：logits经过softmax后，与labels进行交叉熵计算</li>
<li>参数：<br>  <code>labels</code>：:在类别这一维度上，每个向量应服从有效的概率分布。例如，在<code>labels</code>的<code>shape</code>为<code>[batch_size, num_classes]</code>的情况下，<code>labels[i]</code>应服从概率分布。<br>  <code>logits</code>：每个类别的激活值，通常是线性层的输出. 激活值需要经过softmax归一化。<br>  <code>axis</code>: 类别所在维度，默认是-1，即最后一个维度。</li>
<li>返回：softmax交叉熵损失值。</li>
<li>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">labels = [[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>]]</span><br><span class="line">logits = [[<span class="number">4.0</span>, <span class="number">2.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">5.0</span>, <span class="number">1.0</span>]]</span><br><span class="line"><span class="built_in">print</span>(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.16984604</span> <span class="number">0.02474492</span>], shape=(<span class="number">2</span>,), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价实现</span></span><br><span class="line"><span class="built_in">print</span>(-tf.reduce_sum(labels * tf.math.log(tf.nn.softmax(logits)), axis=<span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.16984606</span> <span class="number">0.02474495</span>], shape=(<span class="number">2</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>
<h4 id="tf-nn-sparse-softmax-cross-entropy-with-logits"><a href="#tf-nn-sparse-softmax-cross-entropy-with-logits" class="headerlink" title="tf.nn.sparse_softmax_cross_entropy_with_logits"></a>tf.nn.sparse_softmax_cross_entropy_with_logits</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">	labels, logits, name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：labels经过one-hot编码，logits经过softmax，两者进行交叉熵计算。通常labels的shape为<code>[batch_size]</code>，logits的shape为<code>[batch_size, num_classes]</code>。sparse可理解为对labels进行稀疏化处理(即进行one-hot编码)</li>
<li>参数：<br>  <code>labels</code>：标签的索引值<br>  <code>logits</code>：每个类别的激活值，通常是线性层的输出。激活值需要经过softmax归一化。</li>
<li>返回：softmax交叉熵损失值</li>
<li>例子：（下例中先对labels进行one-hot编码为[[1,0,0], [0,1,0]]，logits经过softmax变为[[0.844，<br>0.114，0.042], [0.007, 0.976, 0.018]]，两者再进行交叉熵运算）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">labels = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">logits = [[<span class="number">4.0</span>, <span class="number">2.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">5.0</span>, <span class="number">1.0</span>]]</span><br><span class="line"><span class="built_in">print</span>(tf.nn.sparse_softmax_cross_entropy_with_logits(labels1, logits))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.16984604</span> <span class="number">0.02474492</span>], shape=(<span class="number">2</span>,), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价实现</span></span><br><span class="line"><span class="built_in">print</span>(-tf.reduce_sum(tf.one_hot(labels, tf.shape(logits)[<span class="number">1</span>]) * tf.math.log(tf.nn.softmax(logits)), axis=<span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">0.16984606</span> <span class="number">0.02474495</span>], shape=(<span class="number">2</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="softmax与交叉熵结合"><a href="#softmax与交叉熵结合" class="headerlink" title="softmax与交叉熵结合"></a>softmax与交叉熵结合</h3><p>输出先过softmax函数，再计算 y 与 y_ 的交叉熵损失函数。</p>
<p>同时计算概率分布和交叉熵的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(y_， y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax与交叉熵损失函数的结合</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y_ = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">y = np.array([[<span class="number">12</span>, <span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">10</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">6.5</span>, <span class="number">1.2</span>], [<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>]])</span><br><span class="line">y_pro = tf.nn.softmax(y)</span><br><span class="line">loss_ce1 = tf.losses.categorical_crossentropy(y_,y_pro)</span><br><span class="line">loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;分步计算的结果:\n&#x27;</span>, loss_ce1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;结合计算的结果:\n&#x27;</span>, loss_ce2)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">分步计算的结果:</span><br><span class="line"> tf.Tensor(</span><br><span class="line">[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00</span><br><span class="line"> 5.49852354e-02], shape=(5,), dtype=float64)</span><br><span class="line">结合计算的结果:</span><br><span class="line"> tf.Tensor(</span><br><span class="line">[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00</span><br><span class="line"> 5.49852354e-02], shape=(5,), dtype=float64)</span><br></pre></td></tr></table></figure>
<h1 id="缓解过拟合"><a href="#缓解过拟合" class="headerlink" title="缓解过拟合"></a>缓解过拟合</h1><ul>
<li>欠拟合：对现有数据集学习的不够彻底</li>
<li>过拟合：模型对当前数据拟合得太好了，但对未出现的新数据难以做出正确的判断，模型缺少泛化能力</li>
</ul>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/%E8%BF%87%E6%8B%9F%E5%90%88%E5%9B%BE%E7%A4%BA.png" class="" title="过拟合图示">
<h2 id="欠拟合的解决方法"><a href="#欠拟合的解决方法" class="headerlink" title="欠拟合的解决方法"></a>欠拟合的解决方法</h2><ul>
<li>增加输入特征项</li>
<li>增加网络参数</li>
<li>减少正则化参数</li>
</ul>
<h2 id="过拟合的解决方法"><a href="#过拟合的解决方法" class="headerlink" title="过拟合的解决方法"></a>过拟合的解决方法</h2><ul>
<li>数据清洗</li>
<li>增大训练集</li>
<li>采用正则化</li>
<li>增大正则化参数</li>
</ul>
<h2 id="正则化缓解过拟合"><a href="#正则化缓解过拟合" class="headerlink" title="正则化缓解过拟合"></a>正则化缓解过拟合</h2><p>正则化在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练数据的噪声（一般不正则化b）</p>
<p>使用正则化后，损失函数loss变成两部分的和。</p>
<p>第一部分是以前求得的loss值，描述了预测结果和正确结果之间的差距，比如交叉熵，均方误差等。</p>
<p>第二部分是参数的权重，REGULARIZER给出参数w在总loss中的比例，正则化的权重。</p>
<script type="math/tex; mode=display">loss = loss(y 与y_{\_}) + REGULARIZER * loss(w)</script><h3 id="loss-w-的计算可以使用两种方法"><a href="#loss-w-的计算可以使用两种方法" class="headerlink" title="loss(w)的计算可以使用两种方法"></a>loss(w)的计算可以使用两种方法</h3><p>一种是对所有参数的w的绝对值求和，L1正则化。</p>
<p>L1正则化大概率会使很多参数变为零，因此该方法可通过稀疏参数，即减少参数的数量，降低复杂度。</p>
<script type="math/tex; mode=display">loss_{L1}(w) = \sum_{i} | w_{i} |</script><p>二种是对所有参数的w平方求和，L2正则化。</p>
<p>L2正则化会使参数很接近零但不为零，因此该方法可通过减小参数值的大小降低复杂度。可以有效缓解数据集中由于噪声引起的过拟合。</p>
<script type="math/tex; mode=display">loss_{L2}(w) = \sum_{i} | w_{i}^{2} |</script><h3 id="L2正则化计算W过程"><a href="#L2正则化计算W过程" class="headerlink" title="L2正则化计算W过程"></a>L2正则化计算W过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment"># 记录梯度信息</span></span><br><span class="line">	h1 = tf.matmul(x_train, w1) + bl <span class="comment">#记录神经网络乘加运算</span></span><br><span class="line">	h1 = tf.nn.relu(h1)</span><br><span class="line">	y = tf.matmul(h1, w2) + b2</span><br><span class="line">	</span><br><span class="line">	<span class="comment">#采用均方误差损失函数mse=mean(sum(y-out)^2)</span></span><br><span class="line">	loss_mse = tf.reduce_mean(tf.square(y_train - y))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#添加12正则化</span></span><br><span class="line">    loss_regularization = []</span><br><span class="line">    loss_reqularization.append(tf.nn<span class="number">.12</span>_loss(w1))</span><br><span class="line">    loss_regularization.append(tf.nn<span class="number">.12</span>_loss(w2))</span><br><span class="line">    loss_regularization = tf.reduce_sum(loss_regularization) </span><br><span class="line">    loss = loss_mse + <span class="number">0.03</span> * loss_reqularization <span class="comment">#REGULARIZER = 0.03</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算loss对各个参数的梯度</span></span><br><span class="line">variables = [w1, b1, w2, b2]</span><br><span class="line">grads = tape.gradient(loss, variables)</span><br></pre></td></tr></table></figure>
<p>测试数据</p>
<p>输入数据：</p>
<p>x1和x2是输入特征，y_c是标签</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">x1,x2,y_c</span><br><span class="line">-0.416757847,-0.056266827,1</span><br><span class="line">-2.136196096,1.640270808,0</span><br><span class="line">-1.793435585,-0.841747366,0</span><br><span class="line">0.502881417,-1.245288087,1</span><br><span class="line">-1.057952219,-0.909007615,1</span><br><span class="line">0.551454045,2.292208013,0</span><br><span class="line">0.041539393,-1.117925445,1</span><br><span class="line">0.539058321,-0.5961597,1</span><br><span class="line">-0.019130497,1.17500122,1</span><br><span class="line">-0.747870949,0.009025251,1</span><br><span class="line">-0.878107893,-0.15643417,1</span><br><span class="line">0.256570452,-0.988779049,1</span><br><span class="line">-0.338821966,-0.236184031,1</span><br><span class="line">-0.637655012,-1.187612286,1</span><br><span class="line">-1.421217227,-0.153495196,0</span><br><span class="line">-0.26905696,2.231366789,0</span><br><span class="line">-2.434767577,0.112726505,0</span><br><span class="line">0.370444537,1.359633863,1</span><br><span class="line">0.501857207,-0.844213704,1</span><br><span class="line">9.76E-06,0.542352572,1</span><br><span class="line">-0.313508197,0.771011738,1</span><br><span class="line">-1.868090655,1.731184666,0</span><br><span class="line">1.467678011,-0.335677339,0</span><br><span class="line">0.61134078,0.047970592,1</span><br><span class="line">-0.829135289,0.087710218,1</span><br><span class="line">1.000365887,-0.381092518,1</span><br><span class="line">-0.375669423,-0.074470763,1</span><br><span class="line">0.43349633,1.27837923,1</span><br></pre></td></tr></table></figure>
<p>x1和x2作为横纵坐标可视化，标签为1是红色，标签为0是蓝色</p>
<p>先用神经网络拟合出输入特征x1、x2与标签的函数关系，生成网格覆盖这些点。</p>
<p>讲这些网格的交点也就是横纵坐标作为输入送入训练好的神经网络，神经网络会输出一个值，我们要区分输出偏向1还是偏向0。</p>
<p>可以把神经网络输出的预测值为0.5的线标出颜色。也就是红蓝点的区分线。</p>
<p><code>p29_regularizationfree.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据/标签 生成x_train y_train</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;dot.csv&#x27;</span>)</span><br><span class="line">x_data = np.array(df[[<span class="string">&#x27;x1&#x27;</span>, <span class="string">&#x27;x2&#x27;</span>]])</span><br><span class="line">y_data = np.array(df[<span class="string">&#x27;y_c&#x27;</span>])</span><br><span class="line"></span><br><span class="line">x_train = np.vstack(x_data).reshape(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">y_train = np.vstack(y_data).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Y_c = [[<span class="string">&#x27;red&#x27;</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">&#x27;blue&#x27;</span>] <span class="keyword">for</span> y <span class="keyword">in</span> y_train]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型问题报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">y_train = tf.cast(y_train, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数切分传入的张量的第一个维度，生成相应的数据集，使输入特征和标签值一一对应</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，输入层为2个神经元，隐藏层为11个神经元，1层隐藏层，输出层为1个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()保证参数可训练</span></span><br><span class="line">w1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">11</span>]), dtype=tf.float32)</span><br><span class="line">b1 = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=[<span class="number">11</span>]))</span><br><span class="line"></span><br><span class="line">w2 = tf.Variable(tf.random.normal([<span class="number">11</span>, <span class="number">1</span>]), dtype=tf.float32)</span><br><span class="line">b2 = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line">epoch = <span class="number">400</span>  <span class="comment"># 循环轮数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 记录梯度信息</span></span><br><span class="line"></span><br><span class="line">            h1 = tf.matmul(x_train, w1) + b1  <span class="comment"># 记录神经网络乘加运算</span></span><br><span class="line">            h1 = tf.nn.relu(h1)</span><br><span class="line">            y = tf.matmul(h1, w2) + b2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_train - y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        variables = [w1, b1, w2, b2]</span><br><span class="line">        grads = tape.gradient(loss, variables)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现梯度更新</span></span><br><span class="line">        <span class="comment"># w1 = w1 - lr * w1_grad tape.gradient是自动求导结果与[w1, b1, w2, b2] 索引为0，1，2，3 </span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])</span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line">        w2.assign_sub(lr * grads[<span class="number">2</span>])</span><br><span class="line">        b2.assign_sub(lr * grads[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每20个epoch，打印loss信息</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch:&#x27;</span>, epoch, <span class="string">&#x27;loss:&#x27;</span>, <span class="built_in">float</span>(loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测部分</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*******predict*******&quot;</span>)</span><br><span class="line"><span class="comment"># xx在-3到3之间以步长为0.01，yy在-3到3之间以步长0.01,生成间隔数值点</span></span><br><span class="line">xx, yy = np.mgrid[-<span class="number">3</span>:<span class="number">3</span>:<span class="number">.1</span>, -<span class="number">3</span>:<span class="number">3</span>:<span class="number">.1</span>]</span><br><span class="line"><span class="comment"># 将xx , yy拉直，并合并配对为二维张量，生成二维坐标点</span></span><br><span class="line">grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">grid = tf.cast(grid, tf.float32)</span><br><span class="line"><span class="comment"># 将网格坐标点喂入神经网络，进行预测，probs为输出</span></span><br><span class="line">probs = []</span><br><span class="line"><span class="keyword">for</span> x_test <span class="keyword">in</span> grid:</span><br><span class="line">    <span class="comment"># 使用训练好的参数进行预测</span></span><br><span class="line">    h1 = tf.matmul([x_test], w1) + b1</span><br><span class="line">    h1 = tf.nn.relu(h1)</span><br><span class="line">    y = tf.matmul(h1, w2) + b2  <span class="comment"># y为预测结果</span></span><br><span class="line">    probs.append(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取第0列给x1，取第1列给x2</span></span><br><span class="line">x1 = x_data[:, <span class="number">0</span>]</span><br><span class="line">x2 = x_data[:, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># probs的shape调整成xx的样子</span></span><br><span class="line">probs = np.array(probs).reshape(xx.shape)</span><br><span class="line">plt.scatter(x1, x2, color=np.squeeze(Y_c)) <span class="comment">#squeeze去掉纬度是1的纬度,相当于去掉[[&#x27;red&#x27;],[&#x27;&#x27;blue]],内层括号变为[&#x27;red&#x27;,&#x27;blue&#x27;]</span></span><br><span class="line"><span class="comment"># 把坐标xx yy和对应的值probs放入contour&lt;[‘kɑntʊr]&gt;函数，给probs值为0.5的所有点上色  plt点show后 显示的是红蓝点的分界线</span></span><br><span class="line">plt.contour(xx, yy, probs, levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入红蓝点，画出分割线，不包含正则化</span></span><br><span class="line"><span class="comment"># 不清楚的数据，建议print出来查看 </span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">epoch: 0 loss: 0.8860995173454285</span><br><span class="line">epoch: 20 loss: 0.07788623124361038</span><br><span class="line">epoch: 40 loss: 0.05887502059340477</span><br><span class="line">epoch: 60 loss: 0.043619509786367416</span><br><span class="line">epoch: 80 loss: 0.03502746298909187</span><br><span class="line">epoch: 100 loss: 0.03013235330581665</span><br><span class="line">epoch: 120 loss: 0.027722788974642754</span><br><span class="line">epoch: 140 loss: 0.026683270931243896</span><br><span class="line">epoch: 160 loss: 0.026026234030723572</span><br><span class="line">epoch: 180 loss: 0.025885531678795815</span><br><span class="line">epoch: 200 loss: 0.025877559557557106</span><br><span class="line">epoch: 220 loss: 0.02594858966767788</span><br><span class="line">epoch: 240 loss: 0.026042431592941284</span><br><span class="line">epoch: 260 loss: 0.026121510192751884</span><br><span class="line">epoch: 280 loss: 0.026135003194212914</span><br><span class="line">epoch: 300 loss: 0.026035090908408165</span><br><span class="line">epoch: 320 loss: 0.02597750537097454</span><br><span class="line">epoch: 340 loss: 0.025903111323714256</span><br><span class="line">epoch: 360 loss: 0.025866234675049782</span><br><span class="line">epoch: 380 loss: 0.02591524086892605</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/L2%E6%AD%A3%E5%88%99%E5%8C%96%E8%AE%A1%E7%AE%97W%E8%BF%87%E7%A8%8B%E7%BB%93%E6%9E%9C1.png" class="" title="L2正则化计算W过程结果1">
<p>轮廓不够平滑，存在过拟合现象。</p>
<p>加入L2正则化：</p>
<p><code>p29_regularizationcontain.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据/标签 生成x_train y_train</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;dot.csv&#x27;</span>)</span><br><span class="line">x_data = np.array(df[[<span class="string">&#x27;x1&#x27;</span>, <span class="string">&#x27;x2&#x27;</span>]])</span><br><span class="line">y_data = np.array(df[<span class="string">&#x27;y_c&#x27;</span>])</span><br><span class="line"></span><br><span class="line">x_train = x_data</span><br><span class="line">y_train = y_data.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Y_c = [[<span class="string">&#x27;red&#x27;</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">&#x27;blue&#x27;</span>] <span class="keyword">for</span> y <span class="keyword">in</span> y_train]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型问题报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">y_train = tf.cast(y_train, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数切分传入的张量的第一个维度，生成相应的数据集，使输入特征和标签值一一对应</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，输入层为4个神经元，隐藏层为32个神经元，2层隐藏层，输出层为3个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()保证参数可训练</span></span><br><span class="line">w1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">11</span>]), dtype=tf.float32)</span><br><span class="line">b1 = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=[<span class="number">11</span>]))</span><br><span class="line"></span><br><span class="line">w2 = tf.Variable(tf.random.normal([<span class="number">11</span>, <span class="number">1</span>]), dtype=tf.float32)</span><br><span class="line">b2 = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span>  <span class="comment"># 学习率为</span></span><br><span class="line">epoch = <span class="number">400</span>  <span class="comment"># 循环轮数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 记录梯度信息</span></span><br><span class="line"></span><br><span class="line">            h1 = tf.matmul(x_train, w1) + b1  <span class="comment"># 记录神经网络乘加运算</span></span><br><span class="line">            h1 = tf.nn.relu(h1)</span><br><span class="line">            y = tf.matmul(h1, w2) + b2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_mse = tf.reduce_mean(tf.square(y_train - y))</span><br><span class="line">            <span class="comment"># 添加l2正则化</span></span><br><span class="line">            loss_regularization = []</span><br><span class="line">            <span class="comment"># tf.nn.l2_loss(w)=sum(w ** 2) / 2</span></span><br><span class="line">            loss_regularization.append(tf.nn.l2_loss(w1))</span><br><span class="line">            loss_regularization.append(tf.nn.l2_loss(w2))</span><br><span class="line">            <span class="comment"># 求和</span></span><br><span class="line">            <span class="comment"># 例：x=tf.constant(([1,1,1],[1,1,1]))</span></span><br><span class="line">            <span class="comment"># tf.reduce_sum(x)</span></span><br><span class="line">            <span class="comment"># &gt;&gt;&gt;6</span></span><br><span class="line">            <span class="comment"># loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))</span></span><br><span class="line">            loss_regularization = tf.reduce_sum(loss_regularization)</span><br><span class="line">            loss = loss_mse + <span class="number">0.03</span> * loss_regularization <span class="comment">#REGULARIZER = 0.03</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        variables = [w1, b1, w2, b2]</span><br><span class="line">        grads = tape.gradient(loss, variables)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现梯度更新</span></span><br><span class="line">        <span class="comment"># w1 = w1 - lr * w1_grad</span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])</span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line">        w2.assign_sub(lr * grads[<span class="number">2</span>])</span><br><span class="line">        b2.assign_sub(lr * grads[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每200个epoch，打印loss信息</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch:&#x27;</span>, epoch, <span class="string">&#x27;loss:&#x27;</span>, <span class="built_in">float</span>(loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测部分</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*******predict*******&quot;</span>)</span><br><span class="line"><span class="comment"># xx在-3到3之间以步长为0.01，yy在-3到3之间以步长0.01,生成间隔数值点</span></span><br><span class="line">xx, yy = np.mgrid[-<span class="number">3</span>:<span class="number">3</span>:<span class="number">.1</span>, -<span class="number">3</span>:<span class="number">3</span>:<span class="number">.1</span>]</span><br><span class="line"><span class="comment"># 将xx, yy拉直，并合并配对为二维张量，生成二维坐标点</span></span><br><span class="line">grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">grid = tf.cast(grid, tf.float32)</span><br><span class="line"><span class="comment"># 将网格坐标点喂入神经网络，进行预测，probs为输出</span></span><br><span class="line">probs = []</span><br><span class="line"><span class="keyword">for</span> x_predict <span class="keyword">in</span> grid:</span><br><span class="line">    <span class="comment"># 使用训练好的参数进行预测</span></span><br><span class="line">    h1 = tf.matmul([x_predict], w1) + b1</span><br><span class="line">    h1 = tf.nn.relu(h1)</span><br><span class="line">    y = tf.matmul(h1, w2) + b2  <span class="comment"># y为预测结果</span></span><br><span class="line">    probs.append(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取第0列给x1，取第1列给x2</span></span><br><span class="line">x1 = x_data[:, <span class="number">0</span>]</span><br><span class="line">x2 = x_data[:, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># probs的shape调整成xx的样子</span></span><br><span class="line">probs = np.array(probs).reshape(xx.shape)</span><br><span class="line">plt.scatter(x1, x2, color=np.squeeze(Y_c))</span><br><span class="line"><span class="comment"># 把坐标xx yy和对应的值probs放入contour&lt;[‘kɑntʊr]&gt;函数，给probs值为0.5的所有点上色  plt点show后 显示的是红蓝点的分界线</span></span><br><span class="line">plt.contour(xx, yy, probs, levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入红蓝点，画出分割线，包含正则化</span></span><br><span class="line"><span class="comment"># 不清楚的数据，建议print出来查看 </span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">epoch: 0 loss: 1.187268853187561</span><br><span class="line">epoch: 20 loss: 0.4623664319515228</span><br><span class="line">epoch: 40 loss: 0.3910042941570282</span><br><span class="line">epoch: 60 loss: 0.3409908711910248</span><br><span class="line">epoch: 80 loss: 0.2887084484100342</span><br><span class="line">epoch: 100 loss: 0.25472864508628845</span><br><span class="line">epoch: 120 loss: 0.22871072590351105</span><br><span class="line">epoch: 140 loss: 0.20734436810016632</span><br><span class="line">epoch: 160 loss: 0.18896348774433136</span><br><span class="line">epoch: 180 loss: 0.173292875289917</span><br><span class="line">epoch: 200 loss: 0.15965603291988373</span><br><span class="line">epoch: 220 loss: 0.14779284596443176</span><br><span class="line">epoch: 240 loss: 0.137494295835495</span><br><span class="line">epoch: 260 loss: 0.12853200733661652</span><br><span class="line">epoch: 280 loss: 0.12084188312292099</span><br><span class="line">epoch: 300 loss: 0.11422470211982727</span><br><span class="line">epoch: 320 loss: 0.1085757240653038</span><br><span class="line">epoch: 340 loss: 0.10376542061567307</span><br><span class="line">epoch: 360 loss: 0.09959365427494049</span><br><span class="line">epoch: 380 loss: 0.09608905762434006</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/L2%E6%AD%A3%E5%88%99%E5%8C%96%E8%AE%A1%E7%AE%97W%E8%BF%87%E7%A8%8B%E7%BB%93%E6%9E%9C2.png" class="" title="L2正则化计算W过程结果2">
<p>加入L2正则化后的曲线更平缓，有效缓解了过拟合。</p>
<h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>优化算法可以分成一阶优化和二阶优化算法，其中一阶优化就是指的梯度算法及其变种，而二阶优化一般是用二阶导数（Hessian 矩阵）来计算，如牛顿法，由于需要计算Hessian阵和其逆矩阵，计算量较大，因此没有流行开来。这里主要总结一阶优化的各种梯度下降方法。  </p>
<p>深度学习优化算法经历了SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam这样的发展历程。  </p>
<p>当网络模型固定后，不同参数选取对模型的表达力影响很大。更新模型参数的过程，仿佛是教一个孩子认知世界，达到学龄的孩子，脑神经元的结构、规模是相似的，他们都具备了学习的潜力。但是不同的引导方法，会让孩子具备不同的能力，达到不同的高度。优化器就是引导神经网络更新参数的工具。</p>
<p>本节介绍五种常用的神经网络参数优化器。</p>
<p><strong>定义：待优化参数w，损失函数loss，学习率lr，每次迭代一个batch，t表示当前batch迭代的总次数：</strong></p>
<p>数据集中的数据是以batch为单位，批量喂入网络，每个batch通常包含$2^{n}$个数据，<code>t</code>表示当前batch迭代的总次数。</p>
<p>更新参数分四步完成：</p>
<ol>
<li><p>计算<code>t</code>时刻损失函数关于当前参数的梯度：$ g<em>{t} = \nabla loss = \frac{\partial loss}{\partial (w</em>{t})} $</p>
</li>
<li><p>计算<code>t</code>时刻一阶动量 $ m<em>{t} $ 和二阶动量 $ V</em>{t} $</p>
<blockquote>
<p>一阶动量：与梯度相关的函数 $ m<em>{t} = \phi(g</em>{1}, g<em>{2},…, g</em>{t}) $</p>
<p>二阶动量：与梯度平方相关的函数 $ V<em>{t} = \psi(g</em>{1}, g<em>{2},…, g</em>{t}) $</p>
</blockquote>
</li>
<li><p>计算t时刻下降梯度： $ \eta<em>{t} = lr·m</em>{t} / \sqrt{V_{t}} $</p>
</li>
<li><p>计算<code>t+1</code>时刻参数：$ w<em>{t+1} = w</em>{t} - \eta<em>{t} = w</em>{t} - lr · m<em>{t}/\sqrt{V</em>{t}} $</p>
</li>
</ol>
<p>对于步骤3、4对于各个算法都一样，主要区别在1、2上</p>
<h2 id="SGD随机梯度下降-无momentum"><a href="#SGD随机梯度下降-无momentum" class="headerlink" title="SGD随机梯度下降(无momentum)"></a>SGD随机梯度下降(无momentum)</h2><p>没有动量的概念。</p>
<p>最大的缺点是下降速度慢，可能会在沟壑的两边持续震荡，停留在一个局部最优点。</p>
<p>$ m<em>{t} = g</em>{t} $</p>
<p>$ V_{t} = 1 $</p>
<p>$ \eta<em>{t} =  lr·m</em>{t} / \sqrt{V<em>{t}} = lr · g</em>{t} $</p>
<script type="math/tex; mode=display">w_{t+1} = w_{t} -  \eta_{t} = w_{t} - lr · m_{t} / \sqrt{V_{t}} = w_{t} -  lr · g_{t}</script><script type="math/tex; mode=display">w_{t+1} = w_{t} - lr × \frac{\partial loss}{\partial (w_{t})}</script><p>对于单层网络的书写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sgd</span></span><br><span class="line">w1.assign_sub(lr * grads[<span class="number">0</span>])	<span class="comment">#参数w1自更新</span></span><br><span class="line">b1.assign_sub(lr * grads[<span class="number">1</span>])	<span class="comment">#参数b自更新</span></span><br></pre></td></tr></table></figure>
<p>相较于<code>p45_iris.py</code>相比，只改动了四处</p>
<p><code>p32_sgd.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time  <span class="comment">##1##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据，分别为输入特征和标签</span></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）</span></span><br><span class="line"><span class="comment"># seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）</span></span><br><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed，保证输入特征和标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行</span></span><br><span class="line">x_train = x_data[:-<span class="number">30</span>]</span><br><span class="line">y_train = y_data[:-<span class="number">30</span>]</span><br><span class="line">x_test = x_data[-<span class="number">30</span>:]</span><br><span class="line">y_test = y_data[-<span class="number">30</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()标记参数可训练</span></span><br><span class="line"><span class="comment"># 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span>  <span class="comment"># 学习率为0.1</span></span><br><span class="line">train_loss_results = []  <span class="comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span></span><br><span class="line">test_acc = []  <span class="comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span></span><br><span class="line">epoch = <span class="number">500</span>  <span class="comment"># 循环500轮</span></span><br><span class="line">loss_all = <span class="number">0</span>  <span class="comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line">now_time = time.time()  <span class="comment">##2##</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 数据集级别的循环，每个epoch循环一次数据集</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):  <span class="comment"># batch级别的循环 ，每个step循环一个batch</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_all += loss.numpy()  <span class="comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad</span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])  <span class="comment"># 参数w1自更新</span></span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])  <span class="comment"># 参数b自更新</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个epoch，打印loss信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_all / <span class="number">4</span>))</span><br><span class="line">    train_loss_results.append(loss_all / <span class="number">4</span>)  <span class="comment"># 将4个step的loss求平均记录在此变量中</span></span><br><span class="line">    loss_all = <span class="number">0</span>  <span class="comment"># loss_all归零，为记录下一个epoch的loss做准备</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试部分</span></span><br><span class="line">    <span class="comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span></span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">        <span class="comment"># 使用更新后的参数进行预测</span></span><br><span class="line">        y = tf.matmul(x_test, w1) + b1</span><br><span class="line">        y = tf.nn.softmax(y)</span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  <span class="comment"># 返回y中最大值的索引，即预测的分类</span></span><br><span class="line">        <span class="comment"># 将pred转换为y_test的数据类型</span></span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)</span><br><span class="line">        <span class="comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span></span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">        correct = tf.reduce_sum(correct)</span><br><span class="line">        <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">        total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line">        <span class="comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span></span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 总的准确率等于total_correct/total_number</span></span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test_acc:&quot;</span>, acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--------------------------&quot;</span>)</span><br><span class="line">total_time = time.time() - now_time  <span class="comment">##3##</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total_time&quot;</span>, total_time)  <span class="comment">##4##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 loss 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Loss Function Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(train_loss_results, label=<span class="string">&quot;$Loss$&quot;</span>)  <span class="comment"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span></span><br><span class="line">plt.legend()  <span class="comment"># 画出曲线图标</span></span><br><span class="line">plt.show()  <span class="comment"># 画出图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 Accuracy 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy$&quot;</span>)  <span class="comment"># 逐点画出test_acc值并连线，连线图标是Accuracy</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本文件较 class1\p45_iris.py 仅添加四处时间记录  用 ##n## 标识</span></span><br><span class="line"><span class="comment"># 请将loss曲线、ACC曲线、total_time记录到 class2\优化器对比.docx  对比各优化器收敛情况</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 5.0308239459991455</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGD-loss.png" class="" title="SGD-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGD-acc.png" class="" title="SGD-acc">
<blockquote>
<p>pycharm导入包出现红线，使用conda命令行使用conda或者pip命令安装即可<br>如果红线消除但是运行提示<code>DLL load failed: 找不到指定的程序。</code><br>在pycharm中设置运行环境增加<code>PATH=C:\ProgramData\Anaconda3\Library\bin;</code></p>
</blockquote>
<h2 id="SGDM-含momentum的SGD"><a href="#SGDM-含momentum的SGD" class="headerlink" title="SGDM(含momentum的SGD)"></a>SGDM(含momentum的SGD)</h2><p>动量法是一种使梯度向量向相关方向加速变化，抑制震荡，最终实现加速收敛的方法。<br>(Momentum is a method that helps accelerate SGD in the right direction and dampens oscillations. It adds a fraction of the update vector of the past time step to the current update vector. The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions.)</p>
<p>为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM全称是SGD with Momentum，在SGD基础上引入了一阶动量：</p>
<script type="math/tex; mode=display">m_{t} = \beta · m_{t-1} + (1 - \beta) · g_{t}</script><p>一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近 $ 1/(1-\beta<em>{1}) $ 个时刻的梯度向量和的平均值。也就是说，<code>t</code>时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 $ \beta</em>{1} $ 的经验值为0.9，这就意味着下降方向主要偏向此前累积的下降方向，并略微偏向当前时刻的下降方向。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGD%E4%B8%8ESGDM%E5%8C%BA%E5%88%AB%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="SGD与SGDM区别示意图">
<p>在SGD基础上增加一阶动量。</p>
<p>$ m<em>{t} $ 公式表示各时刻梯度方向的指数滑动平均值，和SGD相比，一阶动量的公式多了 $ m</em>{t-1} $ 这一项， $ m_{t-1} $ 表示上一时刻的一阶动量，上一时刻的一阶动量占大头，$ \beta $ 是一个超参数，是个接近1的数值，经验值是0.9。</p>
<p>二阶动量在SGDM中仍是恒等于1的。</p>
<script type="math/tex; mode=display">m_{t} = \beta · m_{t-1} + (1 - \beta) · g_{t}</script><script type="math/tex; mode=display">V_{t} = 1</script><script type="math/tex; mode=display">\eta_{t}  = lr · m_{t} / \sqrt{V_{t}} = lr · m_{t} = lr · (\beta · m_{t-1} + (1 - \beta) · g_{t})</script><script type="math/tex; mode=display">w_{t+1} = w_{t} - \eta_{t} = w_{t} - lr · (\beta · m_{t-1} + (1 - \beta) · g_{t})</script><p>参数更新公式最重要的是把一阶动量和二阶动量计算出来</p>
<script type="math/tex; mode=display">m_{t} = \beta · m_{t-1} + (1 - \beta) · g_{t}</script><script type="math/tex; mode=display">V_{t} = 1</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m_w, m_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"><span class="comment">#sgd-momentun</span></span><br><span class="line">m_w = beta * m_w + (<span class="number">1</span> - beta) * grads [<span class="number">0</span>]</span><br><span class="line">m_b = beta * m_b + (<span class="number">1</span> - beta) * grads [<span class="number">1</span>]</span><br><span class="line">w1.assign_sub(lr * m_w)</span><br><span class="line">b1.assign_sub(lr * m_b)</span><br></pre></td></tr></table></figure>
<p><code>p34_sgdm.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time  <span class="comment">##1##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据，分别为输入特征和标签</span></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）</span></span><br><span class="line"><span class="comment"># seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）</span></span><br><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed，保证输入特征和标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行</span></span><br><span class="line">x_train = x_data[:-<span class="number">30</span>]</span><br><span class="line">y_train = y_data[:-<span class="number">30</span>]</span><br><span class="line">x_test = x_data[-<span class="number">30</span>:]</span><br><span class="line">y_test = y_data[-<span class="number">30</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()标记参数可训练</span></span><br><span class="line"><span class="comment"># 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span>  <span class="comment"># 学习率为0.1</span></span><br><span class="line">train_loss_results = []  <span class="comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span></span><br><span class="line">test_acc = []  <span class="comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span></span><br><span class="line">epoch = <span class="number">500</span>  <span class="comment"># 循环500轮</span></span><br><span class="line">loss_all = <span class="number">0</span>  <span class="comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line">m_w, m_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line">now_time = time.time()  <span class="comment">##2##</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 数据集级别的循环，每个epoch循环一次数据集</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):  <span class="comment"># batch级别的循环 ，每个step循环一个batch</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_all += loss.numpy()  <span class="comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"></span><br><span class="line">        <span class="comment">##########################################################################</span></span><br><span class="line">        <span class="comment"># sgd-momentun  </span></span><br><span class="line">        m_w = beta * m_w + (<span class="number">1</span> - beta) * grads[<span class="number">0</span>]</span><br><span class="line">        m_b = beta * m_b + (<span class="number">1</span> - beta) * grads[<span class="number">1</span>]</span><br><span class="line">        w1.assign_sub(lr * m_w)</span><br><span class="line">        b1.assign_sub(lr * m_b)</span><br><span class="line">    <span class="comment">##########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个epoch，打印loss信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_all / <span class="number">4</span>))</span><br><span class="line">    train_loss_results.append(loss_all / <span class="number">4</span>)  <span class="comment"># 将4个step的loss求平均记录在此变量中</span></span><br><span class="line">    loss_all = <span class="number">0</span>  <span class="comment"># loss_all归零，为记录下一个epoch的loss做准备</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试部分</span></span><br><span class="line">    <span class="comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span></span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">        <span class="comment"># 使用更新后的参数进行预测</span></span><br><span class="line">        y = tf.matmul(x_test, w1) + b1</span><br><span class="line">        y = tf.nn.softmax(y)</span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  <span class="comment"># 返回y中最大值的索引，即预测的分类</span></span><br><span class="line">        <span class="comment"># 将pred转换为y_test的数据类型</span></span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)</span><br><span class="line">        <span class="comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span></span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">        correct = tf.reduce_sum(correct)</span><br><span class="line">        <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">        total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line">        <span class="comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span></span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 总的准确率等于total_correct/total_number</span></span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test_acc:&quot;</span>, acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--------------------------&quot;</span>)</span><br><span class="line">total_time = time.time() - now_time  <span class="comment">##3##</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total_time&quot;</span>, total_time)  <span class="comment">##4##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 loss 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Loss Function Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(train_loss_results, label=<span class="string">&quot;$Loss$&quot;</span>)  <span class="comment"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span></span><br><span class="line">plt.legend()  <span class="comment"># 画出曲线图标</span></span><br><span class="line">plt.show()  <span class="comment"># 画出图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 Accuracy 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy$&quot;</span>)  <span class="comment"># 逐点画出test_acc值并连线，连线图标是Accuracy</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请将loss曲线、ACC曲线、total_time记录到 class2\优化器对比.docx  对比各优化器收敛情况</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 5.486814260482788</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGDM-loss.png" class="" title="SGDM-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGDM-acc.png" class="" title="SGDM-acc">
<h2 id="SGD-with-Nesterov-Acceleration"><a href="#SGD-with-Nesterov-Acceleration" class="headerlink" title="SGD with Nesterov Acceleration"></a>SGD with Nesterov Acceleration</h2><p>SGD 还有一个问题是会被困在一个局部最优点里。就像被一个小盆地周围的矮山挡住了视野，看不到更远的更深的沟壑。<br>NAG全称Nesterov Accelerated Gradient，是在SGD、SGDM的基础上的进一步改进，改进点在于步骤1。我们知道在时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG在步骤1不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，考虑这个新地方的梯度方向。此时的梯度就变成了：</p>
<script type="math/tex; mode=display">g_{t} = \nabla f(w_{t} - \alpha · m_{t-1})</script><p>我们用这个梯度带入 SGDM 中计算 的式子里去，然后再计算当前时刻应有的梯度并更新这一次的参数。</p>
<p>其基本思路如下图（转自Hinton的Lecture slides）：</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGD-NA.png" class="" title="SGD-NA">
<p>首先，按照原来的更新方向更新一步（棕色线），然后计算该新位置的梯度方向（红色线），然后用这个梯度方向修正最终的更新方向（绿色线）。上图中描述了两步的更新示意图，其中蓝色线是标准momentum更新路径。</p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>上述SGD算法一直存在一个超参数（Hyper-parameter），即学习率。超参数是训练前需要手动选择的参数，前缀”hyper”就是用于区别训练过程中可自动更新的参数。学习率可以理解为参数<code>w</code>沿着梯度<code>g</code>反方向变化的步长。</p>
<p>SGD对所有的参数使用统一的、固定的学习率，一个自然的想法是对每个参数设置不同的学习率，然而在大型网络中这是不切实际的。因此，为解决此问题，AdaGrad算法被提出，其做法是给学习率一个缩放比例，从而达到了自适应学习率的效果（Ada = Adaptive）。</p>
<p>其思想是：对于频繁更新的参数，不希望被单个样本影响太大，我们给它们很小的学习率；对于偶尔出现的参数，希望能多得到一些信息，我们给它较大的学习率。</p>
<p>那怎么样度量历史更新频率呢？为此引入二阶动量，即该维度上，所有梯度值的平方和：</p>
<script type="math/tex; mode=display">V_{t} = \sum_{\tau=1}^{t} g_{\tau}^{2}</script><p>回顾步骤 3 中的下降梯度：$ \eta<em>{t} = \alpha · m</em>{t} / \sqrt{V<em>{t}} $ 可视为 $  \eta</em>{t} = \frac{\alpha}{\sqrt{V<em>{t}}} · m</em>{t} $ ，即对学习率进行缩放。（一般为了防止分母为 0 ，会对二阶动量加一个平滑项，即$ \eta<em>{t} = \alpha · m</em>{t} / \sqrt{V_{t} + \varepsilon}  $， $ \varepsilon $是一个非常小的数。）</p>
<p>AdaGrad 在稀疏数据场景下表现最好。因为对于频繁出现的参数，学习率衰减得快；对于稀疏的参数，学习率衰减得更慢。然而在实际很多情况下，二阶动量呈单调递增，累计从训练开始的梯度，学习率会很快减至0，导致参数不再更新，训练过程提前结束。</p>
<p>在SGD基础上增加二阶动量，可以对模型中的每个参数分配自适应学习率了。</p>
<p>Adagrad的一阶动量和SGD的一阶动量一样，是当前的梯度。</p>
<p>二阶动量是从现在开始，梯度平方的累计和。</p>
<p>$ m<em>{t} = g</em>{t} $</p>
<p>$ V<em>{t} = \sum</em>{\tau = 1}^{t} = g_{\tau}^{2} $</p>
<p>$ \eta<em>{t} =  lr·m</em>{t} / \sqrt{V<em>{t}} = lr · g</em>{t} / (\sqrt{ \sum<em>{\tau = 1}^{t} = g</em>{\tau}^{2}}) $</p>
<script type="math/tex; mode=display">w_{t+1} = w_{t} -  \eta_{t} = w_{t} - lr · g_{t} / (\sqrt{ \sum_{\tau = 1}^{t} = g_{\tau}^{2}})</script><p>一阶动量mt是当前时刻的梯度</p>
<p>二阶动量是梯度平方的累计和</p>
<p>$ m<em>{t} = g</em>{t} $</p>
<p>$ V<em>{t} = \sum</em>{\tau = 1}^{t} = g_{\tau}^{2} $</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="comment"># adagrad</span></span><br><span class="line">v_ w += tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b += tf.square(grads[<span class="number">1</span>])</span><br><span class="line">W1.assign_sub(lr * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(lr * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure>
<p><code>p36_adagrad.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time  <span class="comment">##1##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据，分别为输入特征和标签</span></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）</span></span><br><span class="line"><span class="comment"># seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）</span></span><br><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed，保证输入特征和标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行</span></span><br><span class="line">x_train = x_data[:-<span class="number">30</span>]</span><br><span class="line">y_train = y_data[:-<span class="number">30</span>]</span><br><span class="line">x_test = x_data[-<span class="number">30</span>:]</span><br><span class="line">y_test = y_data[-<span class="number">30</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()标记参数可训练</span></span><br><span class="line"><span class="comment"># 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span>  <span class="comment"># 学习率为0.1</span></span><br><span class="line">train_loss_results = []  <span class="comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span></span><br><span class="line">test_acc = []  <span class="comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span></span><br><span class="line">epoch = <span class="number">500</span>  <span class="comment"># 循环500轮</span></span><br><span class="line">loss_all = <span class="number">0</span>  <span class="comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line">now_time = time.time()  <span class="comment">##2##</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 数据集级别的循环，每个epoch循环一次数据集</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):  <span class="comment"># batch级别的循环 ，每个step循环一个batch</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_all += loss.numpy()  <span class="comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"></span><br><span class="line">        <span class="comment">##########################################################################</span></span><br><span class="line">        <span class="comment"># adagrad</span></span><br><span class="line">        v_w += tf.square(grads[<span class="number">0</span>])</span><br><span class="line">        v_b += tf.square(grads[<span class="number">1</span>])</span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br><span class="line">    <span class="comment">##########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个epoch，打印loss信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_all / <span class="number">4</span>))</span><br><span class="line">    train_loss_results.append(loss_all / <span class="number">4</span>)  <span class="comment"># 将4个step的loss求平均记录在此变量中</span></span><br><span class="line">    loss_all = <span class="number">0</span>  <span class="comment"># loss_all归零，为记录下一个epoch的loss做准备</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试部分</span></span><br><span class="line">    <span class="comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span></span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">        <span class="comment"># 使用更新后的参数进行预测</span></span><br><span class="line">        y = tf.matmul(x_test, w1) + b1</span><br><span class="line">        y = tf.nn.softmax(y)</span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  <span class="comment"># 返回y中最大值的索引，即预测的分类</span></span><br><span class="line">        <span class="comment"># 将pred转换为y_test的数据类型</span></span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)</span><br><span class="line">        <span class="comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span></span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">        correct = tf.reduce_sum(correct)</span><br><span class="line">        <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">        total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line">        <span class="comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span></span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 总的准确率等于total_correct/total_number</span></span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test_acc:&quot;</span>, acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--------------------------&quot;</span>)</span><br><span class="line">total_time = time.time() - now_time  <span class="comment">##3##</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total_time&quot;</span>, total_time)  <span class="comment">##4##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 loss 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Loss Function Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(train_loss_results, label=<span class="string">&quot;$Loss$&quot;</span>)  <span class="comment"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span></span><br><span class="line">plt.legend()  <span class="comment"># 画出曲线图标</span></span><br><span class="line">plt.show()  <span class="comment"># 画出图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 Accuracy 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy$&quot;</span>)  <span class="comment"># 逐点画出test_acc值并连线，连线图标是Accuracy</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请将loss曲线、ACC曲线、total_time记录到 class2\优化器对比.docx  对比各优化器收敛情况</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 5.356388568878174</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/Adagrad-loss.png" class="" title="Adagrad-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/Adagrad-acc.png" class="" title="Adagrad-acc">
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp算法的全称叫 Root Mean Square Prop，是由Geoffrey E. Hinton提出的一种优化算法（Hinton的课件见下图）。由于 AdaGrad 的学习率衰减太过激进，考虑改变二阶动量的计算策略：不累计全部梯度，只关注过去某一窗口内的梯度。修改的思路很直接，前面我们说过，指数移动平均值大约是过去一段时间的平均值，反映“局部的”参数信息，因此我们用这个方法来计算二阶累积动量：</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/RMSProp-Hinton.png" class="" title="RMSProp-Hinton">
<p>在SGD基础上增加二阶动量</p>
<p>二阶动量v使用指数滑动平均值计算，表征的是过去一段时间的平均值</p>
<p>$ m<em>{t} = g</em>{t} $</p>
<p>$ V<em>{t} = \beta · V</em>{t-1} + (1 - \beta) · g_{t}^{2} $</p>
<p>$ \eta<em>{t} =  lr·m</em>{t} / \sqrt{V<em>{t}} = lr · g</em>{t} / (\sqrt{\beta · V<em>{t-1} + (1 - \beta) · g</em>{t}^{2}}) $</p>
<script type="math/tex; mode=display">w_{t+1} = w_{t} -  \eta_{t} = w_{t} - lr · g_{t} / (\sqrt{\beta · V_{t-1} + (1 - \beta) · g_{t}^{2}})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"><span class="comment"># RMSProp</span></span><br><span class="line">v_w = beta * v_w + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta * v_b + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(lr * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">bl.assign_sub(lr * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure>
<p><code>p38_rmsprop.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time  <span class="comment">##1##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据，分别为输入特征和标签</span></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）</span></span><br><span class="line"><span class="comment"># seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）</span></span><br><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed，保证输入特征和标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行</span></span><br><span class="line">x_train = x_data[:-<span class="number">30</span>]</span><br><span class="line">y_train = y_data[:-<span class="number">30</span>]</span><br><span class="line">x_test = x_data[-<span class="number">30</span>:]</span><br><span class="line">y_test = y_data[-<span class="number">30</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()标记参数可训练</span></span><br><span class="line"><span class="comment"># 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span>  <span class="comment"># 学习率为0.1</span></span><br><span class="line">train_loss_results = []  <span class="comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span></span><br><span class="line">test_acc = []  <span class="comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span></span><br><span class="line">epoch = <span class="number">500</span>  <span class="comment"># 循环500轮</span></span><br><span class="line">loss_all = <span class="number">0</span>  <span class="comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line">now_time = time.time()  <span class="comment">##2##</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 数据集级别的循环，每个epoch循环一次数据集</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):  <span class="comment"># batch级别的循环 ，每个step循环一个batch</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_all += loss.numpy()  <span class="comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"></span><br><span class="line">        <span class="comment">##########################################################################</span></span><br><span class="line">        <span class="comment"># rmsprop</span></span><br><span class="line">        v_w = beta * v_w + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">        v_b = beta * v_b + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br><span class="line">    <span class="comment">##########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个epoch，打印loss信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_all / <span class="number">4</span>))</span><br><span class="line">    train_loss_results.append(loss_all / <span class="number">4</span>)  <span class="comment"># 将4个step的loss求平均记录在此变量中</span></span><br><span class="line">    loss_all = <span class="number">0</span>  <span class="comment"># loss_all归零，为记录下一个epoch的loss做准备</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试部分</span></span><br><span class="line">    <span class="comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span></span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">        <span class="comment"># 使用更新后的参数进行预测</span></span><br><span class="line">        y = tf.matmul(x_test, w1) + b1</span><br><span class="line">        y = tf.nn.softmax(y)</span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  <span class="comment"># 返回y中最大值的索引，即预测的分类</span></span><br><span class="line">        <span class="comment"># 将pred转换为y_test的数据类型</span></span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)</span><br><span class="line">        <span class="comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span></span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">        correct = tf.reduce_sum(correct)</span><br><span class="line">        <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">        total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line">        <span class="comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span></span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 总的准确率等于total_correct/total_number</span></span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test_acc:&quot;</span>, acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--------------------------&quot;</span>)</span><br><span class="line">total_time = time.time() - now_time  <span class="comment">##3##</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total_time&quot;</span>, total_time)  <span class="comment">##4##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 loss 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Loss Function Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(train_loss_results, label=<span class="string">&quot;$Loss$&quot;</span>)  <span class="comment"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span></span><br><span class="line">plt.legend()  <span class="comment"># 画出曲线图标</span></span><br><span class="line">plt.show()  <span class="comment"># 画出图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 Accuracy 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy$&quot;</span>)  <span class="comment"># 逐点画出test_acc值并连线，连线图标是Accuracy</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请将loss曲线、ACC曲线、total_time记录到 class2\优化器对比.docx  对比各优化器收敛情况</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 5.9049718379974365</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/RMSProploss.png" class="" title="RMSProp-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/RMSPropacc.png" class="" title="RMSProp-acc">
<h2 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h2><p>为解决AdaGrad的学习率递减太快的问题，RMSProp和AdaDelta几乎同时独立被提出。</p>
<p>我们先看论文的AdaDelta算法，下图来自原论文：</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/AdaDelta1.png" class="" title="AdaDelta1">
<p>对于上图算法的一点解释， $ RMS[g]<em>{t} $ 是梯度$g$的均方根（Root Mean Square），<br>$ RMS[\Delta x]</em>{t-1} $是$ \Delta x $的均方根：</p>
<script type="math/tex; mode=display">RMS[g]_{t} = \sqrt{E[g^{2}]_{t}}</script><script type="math/tex; mode=display">RMS[\Delta x]_{t-1} = \sqrt{E [\Delta x^{2}]_{t-1}}</script><p>我们可以看到AdaDelta与RMSprop仅仅是分子项不同，为了与前面公式保持一致，在此用$\sqrt{U_{t}}$表示$\eta$的均方根：</p>
<script type="math/tex; mode=display">m_{t} = g_{t}</script><script type="math/tex; mode=display">V_{t} = \beta_{2} · V_{t-1} + (1 - \beta_{2}) · g_{t}^{2}</script><script type="math/tex; mode=display">\eta_{t} = \frac{\sqrt{U_{t-1}}}{\sqrt{V_{t}}} · m_{t} = \frac{\sqrt{U_{t-1}}}{\sqrt{\beta_{2}·V_{t-1}+(1-\beta_{2})·g_{t}^{2}}} · g_{t}</script><script type="math/tex; mode=display">U_{t} = \beta_{2} · U_{t-1} + (1 - \beta_{2}) · \eta_{t}^{2}</script><script type="math/tex; mode=display">w_{t+1} = w_{t} - \eta_{t}</script><p>我们可以看到AdaDelta与RMSprop仅仅是分子项不同，为了与前面公式保持一致，在此用<br>表示 的均方根：</p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AdaDelta</span></span><br><span class="line">beta = <span class="number">0.999</span></span><br><span class="line">v_w = beta * v_w + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta * v_b + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">delta_w = tf.sqrt(u_w) * grads[<span class="number">0</span>] / tf.sqrt(v_w)</span><br><span class="line">delta_b = tf.sqrt(u_b) * grads[<span class="number">1</span>] / tf.sqrt(v_b)</span><br><span class="line"></span><br><span class="line">u_w = beta * u_w + (<span class="number">1</span> - beta) * tf.square(delta_w)</span><br><span class="line">u_b = beta * u_b + (<span class="number">1</span> - beta) * tf.square(delta_b)</span><br><span class="line"></span><br><span class="line">w1.assign_sub(delta_w)</span><br><span class="line">b1.assign_sub(delta_b)</span><br></pre></td></tr></table></figure>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam名字来源是adaptive moment estimation。Our method is designed to combine the advantages of two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gradients, and RMSProp (Tieleman &amp; Hinton, 2012), which works well in on-line and non-stationary settings。也就是说，adam融合了Adagrad和RMSprop的思想。</p>
<p>谈到这里，Adam的出现就很自然而然了——它们是前述方法的集大成者。我们看到，SGDM在SGD基础上增加了一阶动量，AdaGrad、RMSProp和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量结合起来，再修正偏差，就是Adam了。</p>
<p>同时引入了SGDM一阶动量和RMSProp二阶动量，并在此基础上增加了两个修正项，把修正后的一阶动量和二阶动量带入参数更新公式。</p>
<p>SGDM一阶动量：</p>
<script type="math/tex; mode=display">m_{t} = \beta_{1} · m_{t-1} + (1 - \beta_{1}) · g_{t}</script><p>RMSProp二阶动量：</p>
<script type="math/tex; mode=display">V_{t} = \beta_{2} · V_{t-1} + (1 - \beta_{2}) · g_{t}^{2}</script><p>其中，参数经验值是$ \beta<em>{1} = 0.9 $ 和 $ \beta</em>{2} = 0.999 $</p>
<p>一阶动量和二阶动量都是按照指数移动平均值进行计算的。初始化 $ m<em>{0} = 0, V</em>{0} = 0 $ ，在初期，迭代得到的 $ m<em>{t} $ 和 $ V</em>{t} $ 会接近于0。我们可以通过对 $ m<em>{t} $ 和 $ V</em>{t} $ 进行偏差修正来解决这一问题：</p>
<p>修正一阶动量的偏差：$ \widehat{m<em>{t}} = \frac{m</em>{t}}{1 - \beta_{1}^{t}} $</p>
<p>修正二阶动量的偏差：$  \widehat{V<em>{t}} = \frac{V</em>{t}}{1 -  \beta_{2}^{t}} $</p>
<script type="math/tex; mode=display">\eta_{t} = lr · \widehat{m_{t}} / \sqrt{\widehat{V_{t}}} = lr · \frac{m_{t}}{1-\beta_{1}^{t}} / \sqrt{\frac{V_{t}}{1-\beta_{2}^{t}}}</script><script type="math/tex; mode=display">w_{t+1} = w_{t} - \eta_{t} = w_{t} - lr · \frac{m_{t}}{1-\beta_{1}^{t}} / \sqrt{\frac{V_{t}}{1-\beta_{2}^{t}}}</script><p>adam一阶动量是和含momentum的SGD一阶动量一样</p>
<p>二阶动量表达是和RMSProp的二阶动量表达式一样</p>
<p>$ \widehat{m<em>{t}} = \frac{m</em>{t}}{1 - \beta_{1}^{t}} $</p>
<p>$  \widehat{V<em>{t}} = \frac{V</em>{t}}{1 -  \beta_{2}^{t}} $</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">m_w, m_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta1, beta2 = <span class="number">0.9</span>, <span class="number">0.999</span></span><br><span class="line">delta_w, delta_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#adam</span></span><br><span class="line">m_w = beta1 * m_w + (<span class="number">1</span> - beta1) * grads[<span class="number">0</span>]</span><br><span class="line">m_b = beta1 * m_b + (<span class="number">1</span> - beta1) * grads[<span class="number">1</span>]</span><br><span class="line">v_w = beta2 * v_w + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta2 * v_b + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#global_step是从训练开始到当前时所经历的总batch数</span></span><br><span class="line">m_w_correction = m_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">m_b_correction = m_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">v_w_correction = v_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line">v_b_correction = v_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line"></span><br><span class="line">w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))</span><br><span class="line">b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))</span><br></pre></td></tr></table></figure>
<p><code>p40_adam.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time  <span class="comment">##1##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据，分别为输入特征和标签</span></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）</span></span><br><span class="line"><span class="comment"># seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）</span></span><br><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed，保证输入特征和标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行</span></span><br><span class="line">x_train = x_data[:-<span class="number">30</span>]</span><br><span class="line">y_train = y_data[:-<span class="number">30</span>]</span><br><span class="line">x_test = x_data[-<span class="number">30</span>:]</span><br><span class="line">y_test = y_data[-<span class="number">30</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()标记参数可训练</span></span><br><span class="line"><span class="comment"># 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span>  <span class="comment"># 学习率为0.1</span></span><br><span class="line">train_loss_results = []  <span class="comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span></span><br><span class="line">test_acc = []  <span class="comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span></span><br><span class="line">epoch = <span class="number">500</span>  <span class="comment"># 循环500轮</span></span><br><span class="line">loss_all = <span class="number">0</span>  <span class="comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line">m_w, m_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta1, beta2 = <span class="number">0.9</span>, <span class="number">0.999</span></span><br><span class="line">delta_w, delta_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line">now_time = time.time()  <span class="comment">##2##</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 数据集级别的循环，每个epoch循环一次数据集</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):  <span class="comment"># batch级别的循环 ，每个step循环一个batch</span></span><br><span class="line"> <span class="comment">##########################################################################       </span></span><br><span class="line">        global_step += <span class="number">1</span></span><br><span class="line"> <span class="comment">##########################################################################       </span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_all += loss.numpy()  <span class="comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line"> <span class="comment"># adam</span></span><br><span class="line">        m_w = beta1 * m_w + (<span class="number">1</span> - beta1) * grads[<span class="number">0</span>]</span><br><span class="line">        m_b = beta1 * m_b + (<span class="number">1</span> - beta1) * grads[<span class="number">1</span>]</span><br><span class="line">        v_w = beta2 * v_w + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">        v_b = beta2 * v_b + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        m_w_correction = m_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">        m_b_correction = m_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">        v_w_correction = v_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line">        v_b_correction = v_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line"></span><br><span class="line">        w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))</span><br><span class="line">        b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))</span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个epoch，打印loss信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_all / <span class="number">4</span>))</span><br><span class="line">    train_loss_results.append(loss_all / <span class="number">4</span>)  <span class="comment"># 将4个step的loss求平均记录在此变量中</span></span><br><span class="line">    loss_all = <span class="number">0</span>  <span class="comment"># loss_all归零，为记录下一个epoch的loss做准备</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试部分</span></span><br><span class="line">    <span class="comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span></span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">        <span class="comment"># 使用更新后的参数进行预测</span></span><br><span class="line">        y = tf.matmul(x_test, w1) + b1</span><br><span class="line">        y = tf.nn.softmax(y)</span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  <span class="comment"># 返回y中最大值的索引，即预测的分类</span></span><br><span class="line">        <span class="comment"># 将pred转换为y_test的数据类型</span></span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)</span><br><span class="line">        <span class="comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span></span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">        correct = tf.reduce_sum(correct)</span><br><span class="line">        <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">        total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line">        <span class="comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span></span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 总的准确率等于total_correct/total_number</span></span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test_acc:&quot;</span>, acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--------------------------&quot;</span>)</span><br><span class="line">total_time = time.time() - now_time  <span class="comment">##3##</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total_time&quot;</span>, total_time)  <span class="comment">##4##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 loss 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Loss Function Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(train_loss_results, label=<span class="string">&quot;$Loss$&quot;</span>)  <span class="comment"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span></span><br><span class="line">plt.legend()  <span class="comment"># 画出曲线图标</span></span><br><span class="line">plt.show()  <span class="comment"># 画出图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 Accuracy 曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy$&quot;</span>)  <span class="comment"># 逐点画出test_acc值并连线，连线图标是Accuracy</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请将loss曲线、ACC曲线、total_time记录到 class2\优化器对比.docx  对比各优化器收敛情况</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 6.299233913421631</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/Adam-loss.png" class="" title="Adam-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/Adam-acc.png" class="" title="Adam-acc">
<h2 id="五种优化器对比总结"><a href="#五种优化器对比总结" class="headerlink" title="五种优化器对比总结"></a>五种优化器对比总结</h2><h3 id="各种优化器来源"><a href="#各种优化器来源" class="headerlink" title="各种优化器来源"></a>各种优化器来源</h3><ul>
<li>SGD（1952）：<a target="_blank" rel="noopener" href="https://projecteuclid.org/euclid.aoms/1177729392（源自回答）">https://projecteuclid.org/euclid.aoms/1177729392（源自回答）</a></li>
<li>SGD with Momentum（1999）：<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0893608098001166">https://www.sciencedirect.com/science/article/abs/pii/S0893608098001166</a></li>
<li>SGD with Nesterov Acceleration（1983）：由Yurii Nesterov提出</li>
<li>AdaGrad（2011）: <a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a></li>
<li>RMSProp（2012）: <a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></li>
<li>AdaDelta（2012）: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1212.5701">https://arxiv.org/abs/1212.5701</a></li>
<li>Adam:（2014） <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a></li>
<li>对上述算法非常好的可视化：<a target="_blank" rel="noopener" href="https://imgur.com/a/Hqolp">https://imgur.com/a/Hqolp</a></li>
<li>可视化开源项目：<a target="_blank" rel="noopener" href="https://github.com/seanwu1105/neural-network-sandbox">https://github.com/seanwu1105/neural-network-sandbox</a></li>
</ul>
<h4 id="Visualizing-Optimization-Algos"><a href="#Visualizing-Optimization-Algos" class="headerlink" title="Visualizing Optimization Algos"></a>Visualizing Optimization Algos</h4><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/%E4%BC%98%E5%8C%96%E5%99%A8%E5%8F%AF%E8%A7%86%E5%8C%961.gif" class="" title="优化器可视化1">
<p>Algos without scaling based on gradient information really struggle to break symmetry here - SGD gets no where and Nesterov Accelerated Gradient / Momentum exhibits oscillations until they build up velocity in the optimization direction.<br>没有基于梯度信息进行缩放的算法在这里真的很难打破对称性——SGD没有位置，Nesterov加速梯度/动量表现出振荡，直到它们在优化方向上建立速度。</p>
<p>Algos that scale step size based on the gradient quickly break symmetry and begin descent.<br>基于梯度缩放步长的算法会迅速打破对称性并开始下降。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/%E4%BC%98%E5%8C%96%E5%99%A8%E5%8F%AF%E8%A7%86%E5%8C%962.gif" class="" title="优化器可视化2">
<p>Due to the large initial gradient, velocity based techniques shoot off and bounce around - adagrad almost goes unstable for the same reason.<br>由于初始梯度大，基于速度的技术在阿达格勒附近发射和反弹几乎会因为同样的原因而变得不稳定。</p>
<p>Algos that scale gradients/step sizes like adadelta and RMSProp proceed more like accelerated SGD and handle large gradients with more stability.<br>像adadelta和RMSProp这样缩放梯度/步长的算法更像加速SGD，并且更稳定地处理大梯度。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/%E4%BC%98%E5%8C%96%E5%99%A8%E5%8F%AF%E8%A7%86%E5%8C%963.gif" class="" title="优化器可视化3">
<p>Behavior around a saddle point.<br>鞍点周围的行为。</p>
<p>NAG/Momentum again like to explore around, almost taking a different path.<br>NAG/Momentum再次喜欢四处探索，几乎走上了一条不同的道路。</p>
<p>Adadelta/Adagrad/RMSProp proceed like accelerated SGD.<br>阿达德尔塔/阿达格拉德/RMSProp像加速SGD一样进行。</p>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><script type="math/tex; mode=display">m_{t} = \beta · m_{t-1} + (1 - \beta) · g_{t}</script><script type="math/tex; mode=display">V_{t} = 1</script><script type="math/tex; mode=display">\eta_{t}  = lr · m_{t} / \sqrt{V_{t}} = lr · m_{t} = lr · (\beta · m_{t-1} + (1 - \beta) · g_{t})</script><script type="math/tex; mode=display">w_{t+1} = w_{t} - \eta_{t} = w_{t} - lr · （\beta · m_{t-1} + (1 - \beta) · g_{t}）</script><p>参数更新公式最重要的是把一阶动量和二阶动量计算出来</p>
<script type="math/tex; mode=display">m_{t} = \beta · m_{t-1} + (1 - \beta) · g_{t}</script><script type="math/tex; mode=display">V_{t} = 1</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 5.486814260482788</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGDM-loss.png" class="" title="SGDM-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGDM-acc.png" class="" title="SGDM-acc">
<h3 id="SGDM"><a href="#SGDM" class="headerlink" title="SGDM"></a>SGDM</h3><script type="math/tex; mode=display">m_{t} = \beta · m_{t-1} + (1 - \beta) · g_{t}</script><script type="math/tex; mode=display">V_{t} = 1</script><script type="math/tex; mode=display">\eta_{t}  = lr · m_{t} / \sqrt{V_{t}} = lr · m_{t} = lr · (\beta · m_{t-1} + (1 - \beta) · g_{t})</script><script type="math/tex; mode=display">w_{t+1} = w_{t} - \eta_{t} = w_{t} - lr · （\beta · m_{t-1} + (1 - \beta) · g_{t}）</script><p>参数更新公式最重要的是把一阶动量和二阶动量计算出来</p>
<script type="math/tex; mode=display">m_{t} = \beta · m_{t-1} + (1 - \beta) · g_{t}</script><script type="math/tex; mode=display">V_{t} = 1</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 5.486814260482788</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGDM-loss.png" class="" title="SGDM-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/SGDM-acc.png" class="" title="SGDM-acc">
<h3 id="Adagrad-1"><a href="#Adagrad-1" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>$ m<em>{t} = g</em>{t} $</p>
<p>$ V<em>{t} = \sum</em>{\tau = 1}^{t} = g_{\tau}^{2} $</p>
<p>$ \eta<em>{t} =  lr·m</em>{t} / \sqrt{V<em>{t}} = lr · g</em>{t} / (\sqrt{ \sum<em>{\tau = 1}^{t} = g</em>{\tau}^{2}}) $</p>
<script type="math/tex; mode=display">w_{t+1} = w_{t} -  \eta_{t} = w_{t} - lr · g_{t} / (\sqrt{ \sum_{\tau = 1}^{t} = g_{\tau}^{2}})</script><p>一阶动量mt是当前时刻的梯度</p>
<p>二阶动量是梯度平方的累计和</p>
<p>$ m<em>{t} = g</em>{t} $</p>
<p>$ V<em>{t} = \sum</em>{\tau = 1}^{t} = g_{\tau}^{2} $</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 5.356388568878174</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/Adagrad-loss.png" class="" title="Adagrad-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/Adagrad-acc.png" class="" title="Adagrad-acc">
<h3 id="RMSProp-1"><a href="#RMSProp-1" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>$ m<em>{t} = g</em>{t} $</p>
<p>$ V<em>{t} = \beta · V</em>{t-1} + (1 - \beta) · g_{t}^{2} $</p>
<p>$ \eta<em>{t} =  lr·m</em>{t} / \sqrt{V<em>{t}} = lr · g</em>{t} / (\sqrt{\beta · V<em>{t-1} + (1 - \beta) · g</em>{t}^{2}}) $</p>
<script type="math/tex; mode=display">w_{t+1} = w_{t} -  \eta_{t} = w_{t} - lr · g_{t} / (\sqrt{\beta · V_{t-1} + (1 - \beta) · g_{t}^{2}})</script><p>$ m<em>{t} = g</em>{t} $</p>
<p>$ V<em>{t} = \beta · V</em>{t-1} + (1 - \beta) · g_{t}^{2} $</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 5.9049718379974365</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/RMSProploss.png" class="" title="RMSProp-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/RMSPropacc.png" class="" title="RMSProp-acc">
<h3 id="Adam-1"><a href="#Adam-1" class="headerlink" title="Adam"></a>Adam</h3><script type="math/tex; mode=display">m_{t} = \beta_{1} · m_{t-1} + (1 - \beta_{1}) · g_{t}</script><p>修正一阶动量的偏差：$ \widehat{m<em>{t}} = \frac{m</em>{t}}{1 - \beta_{1}^{t}} $</p>
<script type="math/tex; mode=display">V_{t} = \beta_{2} · V_{step-1} + (1 - \beta_{2}) · g_{t}^{2}</script><p>修正二阶动量的偏差：$  \widehat{V<em>{t}} = \frac{V</em>{t}}{1 -  \beta_{2}^{t}} $</p>
<script type="math/tex; mode=display">\eta_{t} = lr · \widehat{m_{t}} / \sqrt{\widehat{V_{t}}} = lr · \frac{m_{t}}{1-\beta_{1}^{t}} / \sqrt{\frac{V_{t}}{1-\beta_{2}^{t}}}</script><script type="math/tex; mode=display">w_{t+1} = w_{t} - \eta_{t} = w_{t} - lr · \frac{m_{t}}{1-\beta_{1}^{t}} / \sqrt{\frac{V_{t}}{1-\beta_{2}^{t}}}</script><p>adam一阶动量是和含momentum的SGD一阶动量一样</p>
<p>二阶动量表达是和RMSProp的二阶动量表达式一样</p>
<p>$ \widehat{m<em>{t}} = \frac{m</em>{t}}{1 - \beta_{1}^{t}} $</p>
<p>$  \widehat{V<em>{t}} = \frac{V</em>{t}}{1 -  \beta_{2}^{t}} $</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_time 6.299233913421631</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/Adam-loss.png" class="" title="Adam-loss">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/Adam-acc.png" class="" title="Adam-acc">
<h1 id="其他API"><a href="#其他API" class="headerlink" title="其他API"></a>其他API</h1><h2 id="tf-cast"><a href="#tf-cast" class="headerlink" title="tf.cast"></a>tf.cast</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.cast(</span><br><span class="line">	x, dtype, name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：转换数据（张量）类型。</li>
<li>参数：<br>  <code>x</code>：待转换的数据（张量）<br>  <code>dtype</code>：目标数据类型<br>  <code>name</code>：定义操作的名称（可选参数）</li>
<li>返回：数据类型为<code>dtype</code>，shape与x相同的张量</li>
<li>例子：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1.8</span>, <span class="number">2.2</span>], dtype=tf.float32)</span><br><span class="line"><span class="built_in">print</span>(tf.cast(x, tf.int32))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">1</span> <span class="number">2</span>], shape=(<span class="number">2</span>,), dtype=int32)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="tf-random-normal"><a href="#tf-random-normal" class="headerlink" title="tf.random.normal"></a>tf.random.normal</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.random.normal(</span><br><span class="line">	shape, mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.dtypes.float32, seed=<span class="literal">None</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：生成服从正态分布的随机值</li>
<li>参数：<br>  <code>x</code>：一维张量<br>  <code>mean</code>：正态分布的均值<br>  <code>stddev</code>：正态分布的方差</li>
<li>返回：满足指定shape并且服从正态分布的张量</li>
<li>例子：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.random.normal([<span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">7</span>, shape=(<span class="number">3</span>, <span class="number">5</span>), dtype=float32, numpy=</span><br><span class="line">array([[-<span class="number">0.3951666</span> , -<span class="number">0.06858674</span>, <span class="number">0.29626969</span>, <span class="number">0.8070933</span> , -<span class="number">0.81376624</span>],</span><br><span class="line">[ <span class="number">0.09532423</span>, -<span class="number">0.20840745</span>, <span class="number">0.37404788</span>, <span class="number">0.5459829</span> , <span class="number">0.17986278</span>],</span><br><span class="line">[-<span class="number">1.0439969</span> , -<span class="number">0.8826001</span> , <span class="number">0.7081867</span> , -<span class="number">0.40955627</span>, -<span class="number">2.6596873</span> ]],</span><br><span class="line">dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="tf-where"><a href="#tf-where" class="headerlink" title="tf.where"></a>tf.where</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.where(</span><br><span class="line">	condition, x=<span class="literal">None</span>, y=<span class="literal">None</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>功能：根据condition，取x或y中的值。如果为True，对应位置取x的值；如果为False，对应位置取y的值。</li>
<li>参数：<br>  <code>condition</code>：bool型张量<br>  <code>x</code>：与<code>y</code> shape相同的张量<br>  <code>y</code>：与<code>x</code> shape相同的张量</li>
<li>返回：shape与<code>x</code>相同的张量</li>
<li>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.where([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.Tensor([<span class="number">1</span> <span class="number">6</span> <span class="number">3</span> <span class="number">8</span>], shape=(<span class="number">4</span>,), dtype=int32)</span><br></pre></td></tr></table></figure></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2023/02/16/人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化/">http://hibiscidai.com/2023/02/16/人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/TensorflowMOOC/">TensorflowMOOC</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/zhifubaodashang.jpg"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/weixin.png"><div class="post-qr-code__desc">微信打赏</div></div></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><i class="fa fa-chevron-left">  </i><span>人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络</span></a></div><div class="next-post pull-right"><a href="/2023/02/04/%E5%AE%9E%E7%94%A8Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1MOOC-%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0tkinter%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"><span>实用Python程序设计MOOC-第十四章tkinter图形界面程序设计</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2023 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>