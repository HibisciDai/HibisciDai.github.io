<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="PyTorch-26H-2"><meta name="keywords" content="学习笔记,PyTorch"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>PyTorch-26H-2 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-26H-2"><span class="toc-number">1.</span> <span class="toc-text">PyTorch-26H-2</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter-1-%E2%80%93-PyTorch-Workflow-Fundamentals"><span class="toc-number">2.</span> <span class="toc-text">Chapter 1 – PyTorch Workflow Fundamentals</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#What-we%E2%80%99re-going-to-cover"><span class="toc-number">2.1.</span> <span class="toc-text">What we’re going to cover</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Where-can-you-get-help"><span class="toc-number">2.2.</span> <span class="toc-text">Where can you get help?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Data-preparing-and-loading"><span class="toc-number">2.3.</span> <span class="toc-text">1. Data (preparing and loading)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Split-data-into-training-and-test-sets"><span class="toc-number">2.3.1.</span> <span class="toc-text">Split data into training and test sets</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Build-model"><span class="toc-number">2.4.</span> <span class="toc-text">2. Build model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-model-building-essentials-PyTorch-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E8%A6%81%E7%82%B9"><span class="toc-number">2.4.1.</span> <span class="toc-text">PyTorch model building essentials PyTorch 模型构建要点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Checking-the-contents-of-a-PyTorch-model-%E6%A3%80%E6%9F%A5-PyTorch-%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%86%85%E5%AE%B9"><span class="toc-number">2.4.2.</span> <span class="toc-text">Checking the contents of a PyTorch model 检查 PyTorch 模型的内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Making-predictions-using-torch-inference-mode"><span class="toc-number">2.4.3.</span> <span class="toc-text">Making predictions using torch.inference_mode()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Train-model-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.5.</span> <span class="toc-text">3. Train model 训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-a-loss-function-and-optimizer-in-PyTorch-%E5%9C%A8-PyTorch-%E4%B8%AD%E5%88%9B%E5%BB%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.5.1.</span> <span class="toc-text">Creating a loss function and optimizer in PyTorch 在 PyTorch 中创建损失函数和优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-an-optimization-loop-in-PyTorch-%E5%9C%A8-PyTorch-%E4%B8%AD%E5%88%9B%E5%BB%BA%E4%BC%98%E5%8C%96%E5%BE%AA%E7%8E%AF"><span class="toc-number">2.5.2.</span> <span class="toc-text">Creating an optimization loop in PyTorch 在 PyTorch 中创建优化循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-training-loop-PyTorch-%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-number">2.5.3.</span> <span class="toc-text">PyTorch training loop PyTorch 训练循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-testing-loop-PyTorch-%E6%B5%8B%E8%AF%95%E5%BE%AA%E7%8E%AF"><span class="toc-number">2.5.4.</span> <span class="toc-text">PyTorch testing loop PyTorch 测试循环</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Making-predictions-with-a-trained-PyTorch-model-inference-%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84-PyTorch-%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%EF%BC%88%E6%8E%A8%E7%90%86%EF%BC%89"><span class="toc-number">2.6.</span> <span class="toc-text">4. Making predictions with a trained PyTorch model (inference) 使用训练好的 PyTorch 模型进行预测（推理）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Saving-and-loading-a-PyTorch-model-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD-PyTorch-%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.7.</span> <span class="toc-text">5. Saving and loading a PyTorch model 保存和加载 PyTorch 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Saving-a-PyTorch-model%E2%80%99s-state-dict-%E4%BF%9D%E5%AD%98-PyTorch-%E6%A8%A1%E5%9E%8B%E7%9A%84state-dict"><span class="toc-number">2.7.1.</span> <span class="toc-text">Saving a PyTorch model’s state_dict() 保存 PyTorch 模型的state_dict()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loading-a-saved-PyTorch-model%E2%80%99s-state-dict-%E5%8A%A0%E8%BD%BD%E5%B7%B2%E4%BF%9D%E5%AD%98%E7%9A%84-PyTorch-%E6%A8%A1%E5%9E%8B%E7%9A%84-state-dict"><span class="toc-number">2.7.2.</span> <span class="toc-text">Loading a saved PyTorch model’s state_dict() 加载已保存的 PyTorch 模型的 state_dict()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Putting-it-all-together"><span class="toc-number">2.8.</span> <span class="toc-text">6. Putting it all together</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-Data"><span class="toc-number">2.8.1.</span> <span class="toc-text">6.1 Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-Building-a-PyTorch-linear-model"><span class="toc-number">2.8.2.</span> <span class="toc-text">6.2 Building a PyTorch linear model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-Training"><span class="toc-number">2.8.3.</span> <span class="toc-text">6.3 Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-Making-predictions"><span class="toc-number">2.8.4.</span> <span class="toc-text">6.4 Making predictions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-Saving-and-loading-a-model-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.8.5.</span> <span class="toc-text">6.5 Saving and loading a model 保存和加载模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Exercises"><span class="toc-number">2.9.</span> <span class="toc-text">Exercises</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Extra-curriculum"><span class="toc-number">2.10.</span> <span class="toc-text">Extra-curriculum</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">244</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">88</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">33</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">PyTorch-26H-2</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2024-08-15</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/PyTorch/">PyTorch</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">11.9k</span><span class="post-meta__separator">|</span><span>阅读时长: 48 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2.png" class="" title="PyTorch-26H-2">
<p>PyTorch-26H-2</p>
<span id="more"></span>
<h1 id="PyTorch-26H-2"><a href="#PyTorch-26H-2" class="headerlink" title="PyTorch-26H-2"></a>PyTorch-26H-2</h1><p>主页：<a target="_blank" rel="noopener" href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p>
<p>youtub：<a target="_blank" rel="noopener" href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p>
<p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a target="_blank" rel="noopener" href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p>
<p>PyTorch documentation：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>
<h1 id="Chapter-1-–-PyTorch-Workflow-Fundamentals"><a href="#Chapter-1-–-PyTorch-Workflow-Fundamentals" class="headerlink" title="Chapter 1 – PyTorch Workflow Fundamentals"></a>Chapter 1 – PyTorch Workflow Fundamentals</h1><p>机器学习和深度学习的本质是从过去获取一些数据，建立一种算法（如神经网络）来发现其中的模式，并利用发现的模式来预测未来。</p>
<p>从一条直线开始，构建一个 PyTorch 模型来学习直线的模式并进行匹配。</p>
<h2 id="What-we’re-going-to-cover"><a href="#What-we’re-going-to-cover" class="headerlink" title="What we’re going to cover"></a>What we’re going to cover</h2><img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-1.png" class="" title="PyTorch-26H-2-1">
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">话题</th>
<th style="text-align:center">内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1. 准备数据</td>
<td style="text-align:center">数据可以是任何东西，但首先我们要创建一条简单的直线</td>
</tr>
<tr>
<td style="text-align:center">2. 建立模型</td>
<td style="text-align:center">在这里我们将创建一个模型来学习数据中的模式，我们还将选择一个损失函数、优化器并建立一个训练循环。</td>
</tr>
<tr>
<td style="text-align:center">3. 将模型拟合到数据（训练）</td>
<td style="text-align:center">我们有数据和模型，现在让我们让模型（尝试）在（训练）数据中寻找模式。</td>
</tr>
<tr>
<td style="text-align:center">4. 做出预测并评估模型（推理）</td>
<td style="text-align:center">我们的模型在数据中发现了模式，让我们将它的发现与实际（测试）数据进行比较。</td>
</tr>
<tr>
<td style="text-align:center">5. 保存和加载模型</td>
<td style="text-align:center">在其他地方使用模型，或者稍后再回来。</td>
</tr>
<tr>
<td style="text-align:center">6. 综合起来</td>
<td style="text-align:center">让我们把以上所有内容结合起来。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Where-can-you-get-help"><a href="#Where-can-you-get-help" class="headerlink" title="Where can you get help?"></a>Where can you get help?</h2><p>本课程的所有材料均可在 <a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning">GitHub</a> 上找到。</p>
<p>如果您遇到麻烦，您也可以在<a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning/discussions">讨论页面</a>上提问。</p>
<p>还有<a target="_blank" rel="noopener" href="https://discuss.pytorch.org/">PyTorch 开发者论坛</a>，这是一个对所有 PyTorch 相关事宜非常有用的地方。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">what_were_covering = &#123;<span class="number">1</span>: <span class="string">&quot;data (prepare and load)&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;build model&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;fitting the model to data (training)&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;making predictions and evaluating a model (inference)&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;saving and loading a model&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;putting it all together&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">what_were_covering = &#123;<span class="number">1</span>: <span class="string">&quot;数据（准备和加载）&quot;</span>,</span><br><span class="line">	<span class="number">2</span>: <span class="string">&quot;构建模型&quot;</span>,</span><br><span class="line">	<span class="number">3</span>: <span class="string">&quot;将模型与数据拟合（训练）&quot;</span>,</span><br><span class="line">	<span class="number">4</span>: <span class="string">&quot;进行预测和评估模型（推理）&quot;</span>,</span><br><span class="line">	<span class="number">5</span>: <span class="string">&quot;保存和加载模型&quot;</span>,</span><br><span class="line">	<span class="number">6</span>: <span class="string">&quot;将所有内容整合在一起&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>模块导入</p>
<p>我们将获得<code>torch</code>，<code>torch.nn</code>（nn代表神经网络，这个包包含在 PyTorch 中创建神经网络的构建块）和<code>matplotlib</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn <span class="comment"># nn contains all of PyTorch&#x27;s building blocks for neural networks</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check PyTorch version</span></span><br><span class="line">torch.__version__</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;2.4.1&#x27;</span><br></pre></td></tr></table></figure>
<h2 id="1-Data-preparing-and-loading"><a href="#1-Data-preparing-and-loading" class="headerlink" title="1. Data (preparing and loading)"></a>1. Data (preparing and loading)</h2><p>机器学习中的“数据”几乎可以是任何你能想象到的东西。数字表（比如一个大的 Excel 电子表格）、任何类型的图像、视频（YouTube 上有大量数据！）、歌曲或播客等音频文件、蛋白质结构、文本等等。</p>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-2.png" class="" title="PyTorch-26H-2-2">
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-3.png" class="" title="PyTorch-26H-2-3">
<p>机器学习是一个由两部分组成的游戏：</p>
<p>1、<strong>数据（无论它是什么）转换为数字（一种表示）。</strong><br>2、<strong>选择或建立一个模型来尽可能好地学习表示。</strong></p>
<p>有时一项和两项可以同时进行。但是如果没有数据怎么办？嗯，这就是我们现在的情况。没有数据。但我们可以创造一些。我们将数据创建为一条直线。</p>
<p>我们将使用 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Linear_regression">线性回归</a> 来创建具有已知参数（模型可以学习的东西）的数据，然后我们将使用 PyTorch 来查看是否可以构建模型来使用 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">梯度下降</a> 来估计这些参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create *known* parameters</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.02</span></span><br><span class="line">X = torch.arange(start, end, step).unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">y = weight * X + bias</span><br><span class="line"></span><br><span class="line">X[:<span class="number">10</span>], y[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0.0000],</span><br><span class="line">         [0.0200],</span><br><span class="line">         [0.0400],</span><br><span class="line">         [0.0600],</span><br><span class="line">         [0.0800],</span><br><span class="line">         [0.1000],</span><br><span class="line">         [0.1200],</span><br><span class="line">         [0.1400],</span><br><span class="line">         [0.1600],</span><br><span class="line">         [0.1800]]),</span><br><span class="line"> tensor([[0.3000],</span><br><span class="line">         [0.3140],</span><br><span class="line">         [0.3280],</span><br><span class="line">         [0.3420],</span><br><span class="line">         [0.3560],</span><br><span class="line">         [0.3700],</span><br><span class="line">         [0.3840],</span><br><span class="line">         [0.3980],</span><br><span class="line">         [0.4120],</span><br><span class="line">         [0.4260]]))</span><br></pre></td></tr></table></figure>
<p>开始构建一个可以学习X（特征）和y（标签）之间关系的模型。</p>
<h3 id="Split-data-into-training-and-test-sets"><a href="#Split-data-into-training-and-test-sets" class="headerlink" title="Split data into training and test sets"></a>Split data into training and test sets</h3><p>在建立模型之前，我们需要将其拆分。</p>
<p>机器学习项目中最重要的步骤之一是创建训练和测试集（必要时还要创建验证集）。</p>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-3_2.png" class="" title="PyTorch-26H-2-3_2">
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">分类</th>
<th style="text-align:center">目的</th>
<th style="text-align:center">总数据量</th>
<th style="text-align:center">使用频率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">训练集</td>
<td style="text-align:center">模型从这些数据中学习（例如您在学期期间学习的课程材料）。</td>
<td style="text-align:center">~60-80％</td>
<td style="text-align:center">Always</td>
</tr>
<tr>
<td style="text-align:center">验证集</td>
<td style="text-align:center">模型会根据这些数据进行调整（就像期末考试之前进行的模拟考试一样）。</td>
<td style="text-align:center">~10-20%</td>
<td style="text-align:center">Often but not always</td>
</tr>
<tr>
<td style="text-align:center">测试集</td>
<td style="text-align:center">模型会根据这些数据进行评估，以测试其所学到的知识（就像学期末参加的期末考试一样）。</td>
<td style="text-align:center">~10-20%</td>
<td style="text-align:center">Always</td>
</tr>
</tbody>
</table>
</div>
<p>只使用训练和测试集，这意味着我们将拥有一个数据集供我们的模型学习和评估。</p>
<p>通过分割X和Y张量来创建它们。</p>
<blockquote>
<p>处理真实数据时，此步骤通常在项目开始时完成（测试集应始终与所有其他数据分开）。我们希望我们的模型从训练数据中学习，然后在测试数据上对其进行评估，以了解它对未见过的示例的推广效果如何。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create train/test split</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X)) <span class="comment"># 80% of data used for training set, 20% for testing </span></span><br><span class="line">X_train, y_train = X[:train_split], y[:train_split]</span><br><span class="line">X_test, y_test = X[train_split:], y[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(X_train), len(y_train), len(X_test), len(y_test)</span><br></pre></td></tr></table></figure>
<p>40 个样本用于训练（X_train &amp; y_train）和 10 个样本用于测试（X_test &amp; y_test）。</p>
<p>创建的模型将尝试学习X_train &amp; y_train之间的关系，然后我们将评估它在 X_test 和 y_test 上的学习内容。</p>
<p>创建函数可视化数字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_predictions</span>(<span class="params">train_data=X_train, </span></span><br><span class="line"><span class="params">                     train_labels=y_train, </span></span><br><span class="line"><span class="params">                     test_data=X_test, </span></span><br><span class="line"><span class="params">                     test_labels=y_test, </span></span><br><span class="line"><span class="params">                     predictions=<span class="literal">None</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Plots training data, test data and compares predictions.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Plot training data in blue</span></span><br><span class="line">  plt.scatter(train_data, train_labels, c=<span class="string">&quot;b&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Training data&quot;</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Plot test data in green</span></span><br><span class="line">  plt.scatter(test_data, test_labels, c=<span class="string">&quot;g&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Testing data&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> predictions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># Plot the predictions in red (predictions were made on the test data)</span></span><br><span class="line">    plt.scatter(test_data, predictions, c=<span class="string">&quot;r&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Predictions&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Show the legend</span></span><br><span class="line">  plt.legend(prop=&#123;<span class="string">&quot;size&quot;</span>: <span class="number">14</span>&#125;)</span><br><span class="line"></span><br><span class="line">plot_predictions()</span><br></pre></td></tr></table></figure>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-4.png" class="" title="PyTorch-26H-2-4">
<h2 id="2-Build-model"><a href="#2-Build-model" class="headerlink" title="2. Build model"></a>2. Build model</h2><p>建立一个模型，使用蓝点来预测绿点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a Linear Regression model class</span></span><br><span class="line"><span class="comment"># 创建线性回归模型类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module): <span class="comment"># &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)PyTorch 中的几乎所有东西都是 nn.Module（可以将其视为神经网络乐高积木）</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="comment"># &lt;- start with random weights (this will get adjusted as the model learns)从随机权重开始（这将随着模型的学习而进行调整）</span></span><br><span class="line">                                                dtype=torch.<span class="built_in">float</span>), <span class="comment"># &lt;- PyTorch loves float32 by defaultPyTorch 默认喜欢 float32</span></span><br><span class="line">                                   requires_grad=<span class="literal">True</span>) <span class="comment"># &lt;- can we update this value with gradient descent?)我们可以用梯度下降来更新这个值吗？）</span></span><br><span class="line"></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="comment"># &lt;- start with random bias (this will get adjusted as the model learns)从随机偏差开始（这将随着模型的学习而进行调整）</span></span><br><span class="line">                                            dtype=torch.<span class="built_in">float</span>), <span class="comment"># &lt;- PyTorch loves float32 by default</span></span><br><span class="line">                                requires_grad=<span class="literal">True</span>) <span class="comment"># &lt;- can we update this value with gradient descent?))我们可以用梯度下降来更新这个值吗？）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward defines the computation in the model Forward 定义模型中的计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor: <span class="comment"># &lt;- &quot;x&quot; is the input data (e.g. training/testing features) “x”是输入数据（例如训练/测试特征）</span></span><br><span class="line">        <span class="keyword">return</span> self.weights * x + self.bias <span class="comment"># &lt;- this is the linear regression formula (y = m*x + b) 这是线性回归公式 (y = m*x + b)</span></span><br></pre></td></tr></table></figure>
<p>Start with random values (weight &amp; bias)<br>从随机值开始（权重和偏差）</p>
<p>Look at training data and adjust the random values to better represent (or get closer to) the ideal values (the weight &amp; bias values we used to create the data)<br>查看训练数据并调整随机值以更好地表示（或更接近）理想值（我们用于创建数据的权重和偏差值）</p>
<p>Through two main algorithms:<br>通过两种主要算法：</p>
<ol>
<li>Gradient descent：<a target="_blank" rel="noopener" href="https://youtu.be/IHZwWFHWa-w">https://youtu.be/IHZwWFHWa-w</a></li>
<li>梯度下降：<a target="_blank" rel="noopener" href="https://youtu.be/IHZwWFHWa-w">https://youtu.be/IHZwWFHWa-w</a></li>
<li>Backpropagation：<a target="_blank" rel="noopener" href="https://youtu.be/llg3gGewQ5U">https://youtu.be/llg3gGewQ5U</a></li>
<li>反向传播：<a target="_blank" rel="noopener" href="https://youtu.be/llg3gGewQ5U">https://youtu.be/llg3gGewQ5U</a></li>
</ol>
<p><a target="_blank" rel="noopener" href="https://realpython.com/python3-object-oriented-programming/">python3面向对象编程指南</a></p>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-4_2.png" class="" title="PyTorch-26H-2-4_2">
<p>Subclass nn.Module(this contains all the building blocks for neural networks)子类 nn.Module（包含神经网络的所有构建块）</p>
<p>Initialise model parameters to be used in various computations (these could be diMerent layers from torch.nn, single parameters, hard-coded values or functions)初始化用于各种计算的模型参数（这些参数可能是来自 torch.nn 的不同层、单个参数、硬编码值或函数）</p>
<p><code>requires_grad =True</code> means PyTorch will track the gradients of this speciLc parameter for use with torch.autograd and gradient descent (for many torch.nn modules, <code>requires_grad =True</code> is set by default)<code>require_grad =True</code> 表示 PyTorch 将跟踪此特定参数的梯度，以便与 torch.autograd 和梯度下降一起使用（对于许多 torch.nn 模块，<code>requires_grad =True</code> 是默认设置的）</p>
<p>Any subclass of nn.Module needs to override <code>forward()</code> (this deLnes the forward computation of the model)nn.Module 的任何子类都需要重写 <code>forward()</code>（这定义了模型的前向计算）</p>
<h3 id="PyTorch-model-building-essentials-PyTorch-模型构建要点"><a href="#PyTorch-model-building-essentials-PyTorch-模型构建要点" class="headerlink" title="PyTorch model building essentials PyTorch 模型构建要点"></a>PyTorch model building essentials PyTorch 模型构建要点</h3><p>PyTorch 有四个（大约）基本模块，<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a>、<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a>、<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code>torch.utils.data.Dataset</code></a>、<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html"><code>torch.utils.data.DataLoader</code></a>你可以用它们来创建几乎任何你能想到的神经网络。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">PyTorch模块</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">torch.nn</a></td>
<td style="text-align:center">包含计算图的所有构建块（本质上是以特定方式执行的一系列计算）。</td>
</tr>
<tr>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter">torch.nn.Parameter</a></td>
<td style="text-align:center">存储可以与 一起使用的张量nn.Module。如果requires_grad=True梯度（用于通过<a target="_blank" rel="noopener" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html">梯度下降</a>更新模型参数）是自动计算的，这通常被称为“autograd”。</td>
</tr>
<tr>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">torch.nn.Module</a></td>
<td style="text-align:center">所有神经网络模块的基类，神经网络的所有构建块都是子类。如果你在 PyTorch 中构建神经网络，你的模型应该是子类nn.Module。需要forward()实现一个方法。</td>
</tr>
<tr>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch.optim</a></td>
<td style="text-align:center">包含各种优化算法（这些算法告诉存储的模型参数nn.Parameter如何最好地改变以改善梯度下降并进而减少损失）。</td>
</tr>
<tr>
<td style="text-align:center">def forward()</td>
<td style="text-align:center">所有nn.Module子类都需要一种方法，它定义了传递给特定的数据（例如上面的线性回归公式）forward()将进行的计算。nn.Module</td>
</tr>
</tbody>
</table>
</div>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-4_3.png" class="" title="PyTorch-26H-2-4_3">
<p>PyTorch 神经网络中的几乎所有内容都来自<code>torch.nn</code>。</p>
<p><code>nn.Module</code>包含较大的构建块（层）<br><code>nn.Parameter</code>包含较小的参数，如权重和偏差（将它们放在一起形成<code>nn.Module(s)</code>）<br><code>forward()</code>告诉较大的块如何在 <code>nn.Module(s)</code> 内对输入（充满数据的张量）进行计算<br><code>torch.optim</code>包含关于如何改进参数<code>nn.Parameter</code>以更好地表示输入数据的优化方法</p>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-5.png" class="" title="PyTorch-26H-2-5">
<p>子类 <code>nn.Module</code>（包含神经网络的所有构建块）<br>初始化用于各种计算的<code>模型参数</code>（这些参数可能是来自torch.nn 的不同层、单个参数、硬编码值或函数）<br><code>require_grad=True</code> 表示 PyTorch 将跟踪此特定参数的梯度，以便与 <code>torch.autograd</code> 和梯度下降一起使用（对于许多 <code>torch.nn</code> 模块，<code>requires_grad=True</code> 是默认设置）<br><code>nn.Module</code> 的任何子类都需要重写 <code>forward()</code>（这定义了模型的前向计算）</p>
<p>通过子类化创建 PyTorch 模型的基本构建块<code>nn.Module</code>。对于子类化的对象<code>nn.Module</code>，<code>forward()</code>必须定义方法。</p>
<p>在 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/ptcheat.html">PyTorch Cheat Sheet</a> 中查看更多这些基本模块及其用例。</p>
<h3 id="Checking-the-contents-of-a-PyTorch-model-检查-PyTorch-模型的内容"><a href="#Checking-the-contents-of-a-PyTorch-model-检查-PyTorch-模型的内容" class="headerlink" title="Checking the contents of a PyTorch model 检查 PyTorch 模型的内容"></a>Checking the contents of a PyTorch model 检查 PyTorch 模型的内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set manual seed since nn.Parameter are randomly initialized由于 nn.Parameter 是随机初始化的，因此请设置手动种子</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))创建模型的实例（这是包含 nn.Parameter(s) 的 nn.Module 的子类）</span></span><br><span class="line">model_0 = LinearRegressionModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the nn.Parameter(s) within the nn.Module subclass we created检查我们创建的 nn.Module 子类中的 nn.Parameter(s)</span></span><br><span class="line"><span class="built_in">list</span>(model_0.parameters())</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[Parameter containing:</span><br><span class="line"> tensor([0.3367], requires_grad=True),</span><br><span class="line"> Parameter containing:</span><br><span class="line"> tensor([0.1288], requires_grad=True)]</span><br></pre></td></tr></table></figure>
<p>我们还可以使用 获取模型的状态（模型包含的内容）<code>.state_dict()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List named parameters </span></span><br><span class="line">model_0.state_dict()</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([(&#x27;weights&#x27;, tensor([0.3367])), (&#x27;bias&#x27;, tensor([0.1288]))])</span><br></pre></td></tr></table></figure>
<p>注意 <code>model_0.state_dict()</code> 中的权重和偏差的值是如何作为随机浮点张量出现的吗？<br>这是因为我们上面使用 <code>torch.randn()</code> 初始化了它们。<br>本质上，我们希望从随机参数开始，并让模型将它们更新为最适合我们数据的参数（我们在创建直线数据时设置的硬编码 <code>weight</code> 和 <code>bias</code>）。</p>
<p>尝试改变上面两个单元格的 <code>torch.manual_seed()</code> 值，看看权重和偏差值会发生什么变化。<br>因为我们的模型从随机值开始，所以现在它的预测能力较差。</p>
<h3 id="Making-predictions-using-torch-inference-mode"><a href="#Making-predictions-using-torch-inference-mode" class="headerlink" title="Making predictions using torch.inference_mode()"></a>Making predictions using <code>torch.inference_mode()</code></h3><p>将测试数据传递给它，<code>X_test</code> 看看它的预测有多接近 <code>y_test</code>。</p>
<p>将数据传递给模型时，它将通过模型的 <code>forward()</code> 方法并使用我们定义的计算产生结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions with model</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode(): </span><br><span class="line">    y_preds = model_0(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: in older PyTorch code you might also see torch.no_grad()</span></span><br><span class="line"><span class="comment"># with torch.no_grad():</span></span><br><span class="line"><span class="comment">#   y_preds = model_0(X_test)</span></span><br></pre></td></tr></table></figure>
<p>使用 <code>torch.inference_mode()</code> 作为<a target="_blank" rel="noopener" href="https://realpython.com/python-with-statement/">上下文管理器</a>（这就是 <code>torch.inference_mode()</code>: 的作用）来进行预测。</p>
<p>顾名思义，<code>torch.inference_mode()</code> 用于使用模型进行推理（做出预测）。</p>
<p><code>torch.inference_mode()</code> 关闭了许多功能（例如梯度跟踪，这对于训练是必需的，但对于推理不是必需的），以使前向传递（数据通过 forward() 方法）更快。</p>
<p>在较旧的 PyTorch 代码中，您可能还会看到 <code>torch.no_grad()</code> 用于推理。虽然 <code>torch.inference_mode()</code> 和 <code>torch.no_grad()</code> 的作用类似，但 <code>torch.inference_mode()</code> 较新，可能更快且更受欢迎。有关更多信息，请参阅 <a target="_blank" rel="noopener" href="https://twitter.com/PyTorch/status/1437838231505096708?s=20">Tweet from PyTorch</a> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the predictions</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of testing samples: <span class="subst">&#123;<span class="built_in">len</span>(X_test)&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of predictions made: <span class="subst">&#123;<span class="built_in">len</span>(y_preds)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted values:\n<span class="subst">&#123;y_preds&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Number of testing samples: 10</span><br><span class="line">Number of predictions made: 10</span><br><span class="line">Predicted values:</span><br><span class="line">tensor([[0.3982],</span><br><span class="line">        [0.4049],</span><br><span class="line">        [0.4116],</span><br><span class="line">        [0.4184],</span><br><span class="line">        [0.4251],</span><br><span class="line">        [0.4318],</span><br><span class="line">        [0.4386],</span><br><span class="line">        [0.4453],</span><br><span class="line">        [0.4520],</span><br><span class="line">        [0.4588]])</span><br></pre></td></tr></table></figure>
<p>请注意每个测试样本有一个预测值。</p>
<p>这是因为我们使用的数据类型。对于我们的直线，一个X值对应一个y值。</p>
<p>然而，机器学习模型非常灵活。可以将 100 个X值映射到一个、两个、三个或 10 个y值。这完全取决于正在处理的内容。</p>
<p>预测仍然是页面上的数字，使用<code>plot_predictions()</code>上面创建的函数将它们可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions(predictions=y_preds)</span><br></pre></td></tr></table></figure>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-6.png" class="" title="PyTorch-26H-2-6">
<p>对比预测结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_test - y_preds</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4618],</span><br><span class="line">        [0.4691],</span><br><span class="line">        [0.4764],</span><br><span class="line">        [0.4836],</span><br><span class="line">        [0.4909],</span><br><span class="line">        [0.4982],</span><br><span class="line">        [0.5054],</span><br><span class="line">        [0.5127],</span><br><span class="line">        [0.5200],</span><br><span class="line">        [0.5272]])</span><br></pre></td></tr></table></figure>
<p>使用随机参数做出预测，没有进行观察的结果，差距很大。</p>
<h2 id="3-Train-model-训练模型"><a href="#3-Train-model-训练模型" class="headerlink" title="3. Train model 训练模型"></a>3. Train model 训练模型</h2><p>模型正在使用随机参数进行计算进行预测，这基本上是猜测（随机）。</p>
<p>更新其内部参数（将参数称为模式），使用<code>weights</code>和<code>bias</code>随机设置的值<code>nn.Parameter()</code>，<code>torch.randn()</code>以便更好地表示数据。</p>
<p>可以对此进行硬编码（默认值weight=0.7和bias=0.3）</p>
<h3 id="Creating-a-loss-function-and-optimizer-in-PyTorch-在-PyTorch-中创建损失函数和优化器"><a href="#Creating-a-loss-function-and-optimizer-in-PyTorch-在-PyTorch-中创建损失函数和优化器" class="headerlink" title="Creating a loss function and optimizer in PyTorch 在 PyTorch 中创建损失函数和优化器"></a>Creating a loss function and optimizer in PyTorch 在 PyTorch 中创建损失函数和优化器</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">功能</th>
<th style="text-align:center">作用</th>
<th style="text-align:center">位置</th>
<th style="text-align:center">价值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">损失函数</td>
<td style="text-align:center">衡量模型预测与真实标签相比的误差程度。误差越低越好。</td>
<td style="text-align:center">内置损失函数<code>torch.nn</code></td>
<td style="text-align:center">回归问题平均绝对误差(MAE)<code>torch.nn.L1Loss()</code>,二元分类问题的二元交叉熵<code>torch.nn.BCELoss()</code></td>
</tr>
<tr>
<td style="text-align:center">优化器</td>
<td style="text-align:center">告诉模型如何更新其内部参数以最好的降低损失。</td>
<td style="text-align:center">优化函数实现 <code>torch.optim</code></td>
<td style="text-align:center">随机梯度下降 <code>torch.optim.SGD()</code> , Adam优化器<code>torch.optim.Adam()</code></td>
</tr>
</tbody>
</table>
</div>
<p>据处理的问题类型，将决定使用的损失函数和优化器。</p>
<p>经验：SGD（随机梯度下降）或 Adam 优化器，效果很好。用于回归问题（预测数字）的 MAE（平均绝对误差）损失函数或用于分类问题（预测一件事或另一件事）的二元交叉熵损失函数。</p>
<p>对于我们的问题，因为我们正在预测一个数字，所以我们使用 PyTorch 中的 MAE（位于 <code>torch.nn.L1Loss()</code> 下）作为我们的损失函数。</p>
<p>平均绝对误差 (MAE，在 PyTorch 中为：<code>torch.nn.L1Loss</code>) 测量两点（预测和标签）之间的绝对差异，然后对所有示例取平均值。</p>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-7.png" class="" title="PyTorch-26H-2-7">
<p>我们将使用 SGD，<code>torch.optim.SGD(params, lr)</code>，其中：</p>
<p><code>params</code> 是您想要优化的目标模型参数（例如我们之前随机设置的<code>weights</code>和<code>bias</code>）。<br><code>lr</code> 是您希望优化器更新参数的学习率，越高意味着优化器将尝试更大的更新（这些更新有时可能太大，优化器将无法工作），越低意味着优化器将尝试较小的更新（这些更新有时可能太小，优化器将花费太长时间才能找到理想值）。学习率被视为超参数（因为它是由机器学习工程师设置的）。学习率的常见起始值为 0.01、0.001、0.0001，但是，这些值也可以随着时间的推移进行调整（这称为<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">学习率调度</a>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the loss function 创建损失函数</span></span><br><span class="line">loss_fn = nn.L1Loss() <span class="comment"># MAE loss is same as L1Loss MAE 损失与 L1Loss 相同</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the optimizer 创建优化器</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_0.parameters(), <span class="comment"># parameters of target model to optimize 待优化目标模型参数</span></span><br><span class="line">                            lr=<span class="number">0.01</span>) <span class="comment"># learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))学习率（优化器在每一步应该改变多少参数，越高=越多（越不稳定），越低=越少（可能需要很长时间））</span></span><br></pre></td></tr></table></figure>
<h3 id="Creating-an-optimization-loop-in-PyTorch-在-PyTorch-中创建优化循环"><a href="#Creating-an-optimization-loop-in-PyTorch-在-PyTorch-中创建优化循环" class="headerlink" title="Creating an optimization loop in PyTorch 在 PyTorch 中创建优化循环"></a>Creating an optimization loop in PyTorch 在 PyTorch 中创建优化循环</h3><p>训练循环涉及模型 遍历训练数据并学习<code>features</code>和<code>labels</code>之间的关系。</p>
<p>测试循环涉及检查测试数据并评估模型在训练数据上学习到的模式的优劣（模型在训练期间永远不会看到测试数据）。</p>
<p>每个都称为一个“循环”，因为我们希望我们的模型查看（循环）每个数据集中的每个样本。</p>
<h3 id="PyTorch-training-loop-PyTorch-训练循环"><a href="#PyTorch-training-loop-PyTorch-训练循环" class="headerlink" title="PyTorch training loop PyTorch 训练循环"></a>PyTorch training loop PyTorch 训练循环</h3><p>训练步骤：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th style="text-align:center">步骤</th>
<th style="text-align:center">作用</th>
<th style="text-align:center">示例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">Forward pass</td>
<td style="text-align:center">该模型会遍历所有训练数据一次，并执行其 <code>forward()</code> 函数计算。</td>
<td style="text-align:center"><code>model(x_train)</code></td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">Calculate the loss</td>
<td style="text-align:center">将模型的输出（预测）与基本事实进行比较，并进行评估以查看其错误程度。</td>
<td style="text-align:center"><code>loss = loss_fn(y_pred, y_train)</code></td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">Zero gradients</td>
<td style="text-align:center">优化器的梯度设置为零（默认情况下是累积的），因此可以针对特定的训练步骤重新计算它们。</td>
<td style="text-align:center"><code>optimizer.zero_grad()</code></td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">Perform backpropagation on the loss（Loss backward）</td>
<td style="text-align:center">计算每个要更新的模型参数的损失梯度（每个参数的 <code>require_grad=True</code>）。这称为反向传播，因此是“向后”的。</td>
<td style="text-align:center"><code>loss.backward()</code></td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">Update the optimizer (gradient descent)</td>
<td style="text-align:center">使用 <code>require_grad=True</code> 来根据损失梯度更新参数，以改进它们。</td>
<td style="text-align:center"><code>optimizer.step()</code></td>
</tr>
</tbody>
</table>
</div>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-8.png" class="" title="PyTorch-26H-2-8">
<p>Pass the data through the model for a number of epochs (e.g. 100 for 100 passes of the data)<br>将数据通过模型传递若干个时期（例如，100 次数据传递为 100 个时期）</p>
<p>Pass the data through the model, this will perform the <code>forward()</code> method located within the model object<br>通过模型传递数据，这将执行位于模型对象内的 <code>forward()</code> 方法</p>
<p>Calculate the loss value (how wrong the model’s predictions are)<br>计算损失值（模型预测的错误程度）</p>
<p>Zero the optimizer gradients (they accumulate every epoch, zero them to start fresh each forward pass)<br>将优化器梯度归零（它们在每个时期都会累积，在每次前向传递时将它们归零以重新开始）</p>
<p>Perform backpropagation on the loss function (compute the gradient of every parameter with <code>requires_grad=True</code>)<br>对损失函数进行反向传播（使用 <code>require_grad=True</code> 计算每个参数的梯度）</p>
<p>Step the optimizer to update the model’s parameters with respect to the gradients calculated by <code>loss.backward()</code><br>让优化器根据 <code>loss.backward()</code> 计算出的梯度来更新模型的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">1</span></span><br><span class="line"><span class="comment"># Pass the data through the model for a number of epochs (e.g. 100)</span></span><br><span class="line"><span class="comment"># 将数据通过模型传递若干个时期（例如 100）</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs) :</span><br><span class="line">    <span class="comment"># Put model in training mode (this is the default state of a model)</span></span><br><span class="line">    <span class="comment"># 将模型置于训练模式（这是模型的默认状态）</span></span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. Forward pass on train data using the forward() method inside</span></span><br><span class="line">    <span class="comment"># 1. 使用内部的 forward() 方法向前传递训练数据</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate the Loss (how different are the model&#x27;s predictions to the true values</span></span><br><span class="line">    <span class="comment"># 2. 计算损失（模型的预测与真实值有多大差异</span></span><br><span class="line">    Loss = Loss_fn(y_pred, y_true)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. Zero the gradients of the optimizer (they accumulate by default)</span></span><br><span class="line">    <span class="comment"># 3. 将优化器的梯度归零（默认情况下它们会累积）</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. Perform backpropagation on the loss</span></span><br><span class="line">    <span class="comment"># 4. 对损失进行反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. Progress/step the optimizer ( gradient descent)</span></span><br><span class="line">    <span class="comment"># 5. 推进/步进优化器（梯度下降）</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>以上只是步骤排序或描述的一个例子。随着经验的积累，你会发现制作 PyTorch 训练循环可以非常灵活。</p>
</blockquote>
<p>训练循环歌曲:<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Nutpusq_AFw">The Unofficial PyTorch Optimization Loop Song</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">It&#x27;s train time!</span><br><span class="line">do the forward pass,</span><br><span class="line">calculate the loss,</span><br><span class="line">optimizer zero grad,</span><br><span class="line">losssss backwards!</span><br><span class="line"></span><br><span class="line">Optimizer step step step</span><br><span class="line"></span><br><span class="line">Let&#x27;s test now!</span><br><span class="line">with torch no grad:</span><br><span class="line">do the forward pass,</span><br><span class="line">calculate the loss,</span><br><span class="line">watch it go down down down!</span><br></pre></td></tr></table></figure>
<p>至于事物的顺序，以上是一个很好的默认顺序，但你可能会看到略有不同的顺序。一些经验法则：</p>
<ul>
<li>在对损失执行反向传播 <code>(loss.backward())</code> 之前，先计算损失 <code>(loss = ...)</code>。</li>
<li>在针对每个模型参数 <code>(loss.backward())</code> 计算损失的梯度之前，先将梯度归零 <code>(optimizer.zero_grad())</code>。</li>
<li>在对损失执行反向传播 <code>(loss.backward())</code> 之后，逐步执行优化器 <code>(optimizer.step())</code>。</li>
</ul>
<p>有关帮助理解反向传播和梯度下降幕后情况的资源，请参阅课外部分。</p>
<h3 id="PyTorch-testing-loop-PyTorch-测试循环"><a href="#PyTorch-testing-loop-PyTorch-测试循环" class="headerlink" title="PyTorch testing loop PyTorch 测试循环"></a>PyTorch testing loop PyTorch 测试循环</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th style="text-align:center">步骤</th>
<th style="text-align:center">作用</th>
<th style="text-align:center">例子</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">Forward pass</td>
<td style="text-align:center">该模型会遍历所有测试数据一次，并执行其 <code>forward()</code> 函数计算。</td>
<td style="text-align:center"><code>model(x_test)</code></td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">Calculate the loss</td>
<td style="text-align:center">将模型的输出（预测）与基本事实进行比较，并进行评估以查看其错误程度。</td>
<td style="text-align:center"><code>loss = loss_fn(y_pred, y_test)</code></td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">Calulate evaluation metrics (optional)</td>
<td style="text-align:center">除了损失值之外，您可能还想计算其他评估指标，例如测试集的准确性。</td>
<td style="text-align:center">Custom functions</td>
</tr>
</tbody>
</table>
</div>
<p>请注意，测试循环不包含执行反向传播（<code>loss.backward()</code>）或步进优化器（<code>optimizer.step()</code>），这是因为在测试期间模型中的任何参数都不会改变，它们已经被计算出来了。对于测试，我们只对通过模型的前向传递的输出感兴趣。</p>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-9.png" class="" title="PyTorch-26H-2-9">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup empty lists to keep track of model progress</span></span><br><span class="line"><span class="comment"># 设置空列表来跟踪模型进度</span></span><br><span class="line">epoch_count = []</span><br><span class="line">train_loss_values = []</span><br><span class="line">test_loss_values = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pass the data through the model for a number of epochs (e.g. 100) pochs):</span></span><br><span class="line"><span class="comment"># 将数据通过模型传递若干个时期（例如 100 个时期）：</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span> (epochs):</span><br><span class="line">    <span class="comment">### Training Loop code here ###</span></span><br><span class="line">    <span class="comment">### Testing starts ###</span></span><br><span class="line">    <span class="comment"># Put the model in evaluation mode </span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># Turn on inference mode context manager :</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        <span class="comment"># 1. Forward pass on test data</span></span><br><span class="line">        test_pred = model(X_test)</span><br><span class="line">        <span class="comment"># 2. Caculate loss on test data</span></span><br><span class="line">        test_loss = Loss_fn(test_pred, y_test) </span><br><span class="line"><span class="comment"># Print out what&#x27;s happening every 10 epochs</span></span><br><span class="line"><span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">    epoch_count.append(epoch)</span><br><span class="line">    train_Loss_values.append(loss)</span><br><span class="line">    test_loss_values.append(test_loss)</span><br><span class="line">    <span class="built_in">print</span>( <span class="string">f&quot; Epoch: <span class="subst">&#123;epoch&#125;</span>| MAE Train Loss: <span class="subst">&#123;loss&#125;</span>I MAE Test Loss: <span class="subst">&#123;test_loss&#125;</span></span></span><br></pre></td></tr></table></figure>
<p>Create empty lists for storing useful values (helpful for tracking model progress)<br>创建空列表来存储有用的值（有助于跟踪模型进度）</p>
<p>Tell the model we want to evaluate rather than train (this turns off functionality used for training but not evaluation)<br>告诉模型我们想要评估而不是训练（这会关闭用于训练但不用于评估的功能）</p>
<p>Turn on <code>torch.inference_mode()</code> context manager to disable functionality such as gradient tracking for inference (gradient tracking not needed for inference)<br>打开 <code>torch.inference_mode()</code> 上下文管理器以禁用推理的梯度跟踪等功能（推理不需要梯度跟踪）</p>
<p>Pass the test data through the model (this will call the model’s implemented <code>forward()</code> method)<br>通过模型传递测试数据（这将调用模型实现的 <code>forward()</code> 方法）</p>
<p>Calculate the test loss value (how wrong the model’s predictions are on the test dataset, lower is better)<br>计算测试损失值（模型对测试数据集的预测错误程度，越低越好）</p>
<p>Display information outputs for how the model is doing during training/testing every ~10 epochs (note: what gets printed out here can be adjusted for speciLc problems)<br>每~10 个时期显示模型在训练/测试过程中的运行情况的信息输出（注意：此处打印的内容可针对具体问题进行调整）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs (how many times the model will pass over the training data)</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty loss lists to track values</span></span><br><span class="line">train_loss_values = []</span><br><span class="line">test_loss_values = []</span><br><span class="line">epoch_count = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put model in training mode (this is the default state of a model)</span></span><br><span class="line">    model_0.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass on train data using the forward() method inside </span></span><br><span class="line">    y_pred = model_0(X_train)</span><br><span class="line">    <span class="comment"># print(y_pred)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate the loss (how different are our models predictions to the ground truth)</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Zero grad of the optimizer</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Progress the optimizer</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the model in evaluation mode</span></span><br><span class="line">    model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass on test data</span></span><br><span class="line">      test_pred = model_0(X_test)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 2. Caculate loss on test data</span></span><br><span class="line">      test_loss = loss_fn(test_pred, y_test.<span class="built_in">type</span>(torch.<span class="built_in">float</span>)) <span class="comment"># predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">      <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            epoch_count.append(epoch)</span><br><span class="line">            train_loss_values.append(loss.detach().numpy())</span><br><span class="line">            test_loss_values.append(test_loss.detach().numpy())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | MAE Train Loss: <span class="subst">&#123;loss&#125;</span> | MAE Test Loss: <span class="subst">&#123;test_loss&#125;</span> &quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 </span><br><span class="line">Epoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 </span><br><span class="line">Epoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 </span><br><span class="line">Epoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 </span><br><span class="line">Epoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 </span><br><span class="line">Epoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 </span><br><span class="line">Epoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 </span><br><span class="line">Epoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 </span><br><span class="line">Epoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 </span><br><span class="line">Epoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819</span><br></pre></td></tr></table></figure>
<p>查看损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the loss curves</span></span><br><span class="line">plt.plot(epoch_count, train_loss_values, label = <span class="string">&quot;Train loss&quot;</span>)</span><br><span class="line">plt.plot(epoch_count, test_loss_values, label = <span class="string">&quot;Test loss&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training and test loss curves&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epochs&quot;</span>)</span><br><span class="line">plt.legend();</span><br></pre></td></tr></table></figure>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-10.png" class="" title="PyTorch-26H-2-10">
<p>损失曲线显示损失随时间下降。请记住，损失是衡量模型错误程度的指标，因此损失越低越好。</p>
<p>由于损失函数和优化器，模型的内部参数（<code>weights</code> 和 <code>bias</code>）得到了更新，以更好地反映数据中的底层模式。</p>
<p>让我们检查模型的 <code>.state_dict()</code> 来查看模型与我们为权重和偏差设置的原始值有多接近。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find our model&#x27;s learned parameters</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The model learned the following values for weights and bias:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_0.state_dict())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nAnd the original values for weights and bias are:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;weights: <span class="subst">&#123;weight&#125;</span>, bias: <span class="subst">&#123;bias&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The model learned the following values for weights and bias:</span><br><span class="line">OrderedDict([(&#x27;weights&#x27;, tensor([0.5784])), (&#x27;bias&#x27;, tensor([0.3513]))])</span><br><span class="line"></span><br><span class="line">And the original values for weights and bias are:</span><br><span class="line">weights: 0.7, bias: 0.3</span><br></pre></td></tr></table></figure>
<p>我们的模型非常接近计算<code>weight</code>和的精确原始值<code>bias</code>（如果我们训练更长时间，它可能会更加接近）。</p>
<p>当epochs=200：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | MAE Train Loss: 0.024458957836031914 | MAE Test Loss: 0.05646304413676262 </span><br><span class="line">Epoch: 10 | MAE Train Loss: 0.021020207554101944 | MAE Test Loss: 0.04819049686193466 </span><br><span class="line">Epoch: 20 | MAE Train Loss: 0.01758546568453312 | MAE Test Loss: 0.04060482233762741 </span><br><span class="line">Epoch: 30 | MAE Train Loss: 0.014155393466353416 | MAE Test Loss: 0.03233227878808975 </span><br><span class="line">Epoch: 40 | MAE Train Loss: 0.010716589167714119 | MAE Test Loss: 0.024059748277068138 </span><br><span class="line">Epoch: 50 | MAE Train Loss: 0.0072835334576666355 | MAE Test Loss: 0.016474086791276932 </span><br><span class="line">Epoch: 60 | MAE Train Loss: 0.0038517764769494534 | MAE Test Loss: 0.008201557211577892 </span><br><span class="line">Epoch: 70 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 80 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 90 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 100 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 110 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 120 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 130 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 140 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 150 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 160 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 170 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 180 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882 </span><br><span class="line">Epoch: 190 | MAE Train Loss: 0.008932482451200485 | MAE Test Loss: 0.005023092031478882</span><br></pre></td></tr></table></figure>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-11.png" class="" title="PyTorch-26H-2-11">
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The model learned the following values for weights and bias:</span><br><span class="line">OrderedDict([(&#x27;weights&#x27;, tensor([0.6990])), (&#x27;bias&#x27;, tensor([0.3093]))])</span><br><span class="line"></span><br><span class="line">And the original values for weights and bias are:</span><br><span class="line">weights: 0.7, bias: 0.3</span><br></pre></td></tr></table></figure>
<h2 id="4-Making-predictions-with-a-trained-PyTorch-model-inference-使用训练好的-PyTorch-模型进行预测（推理）"><a href="#4-Making-predictions-with-a-trained-PyTorch-model-inference-使用训练好的-PyTorch-模型进行预测（推理）" class="headerlink" title="4. Making predictions with a trained PyTorch model (inference) 使用训练好的 PyTorch 模型进行预测（推理）"></a>4. Making predictions with a trained PyTorch model (inference) 使用训练好的 PyTorch 模型进行预测（推理）</h2><p>使用 <code>PyTorch</code> 模型进行预测（也称为执行推理）时，需要记住三件事：</p>
<ul>
<li>将模型设置为评估模式 (<code>model.eval()</code>)。</li>
<li>使用推理模式上下文管理器进行预测（使用 <code>torch.inference_mode(): ...</code>）。</li>
<li>所有预测都应使用同一设备上的对象进行（例如，仅在 GPU 上的数据和模型或仅在 CPU 上的数据和模型）。</li>
</ul>
<p>前两项确保 PyTorch 在训练期间在后台使用但对推理不必要的所有有用计算和设置均已关闭（这可加快计算速度）。第三项确保您不会遇到跨设备错误。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Set the model in evaluation mode</span></span><br><span class="line">model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Setup the inference mode context manager</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">  <span class="comment"># 3. Make sure the calculations are done with the model and data on the same device</span></span><br><span class="line">  <span class="comment"># in our case, we haven&#x27;t setup device-agnostic code yet so our data and model are</span></span><br><span class="line">  <span class="comment"># on the CPU by default.</span></span><br><span class="line">  <span class="comment"># model_0.to(device)</span></span><br><span class="line">  <span class="comment"># X_test = X_test.to(device)</span></span><br><span class="line">  y_preds = model_0(X_test)</span><br><span class="line">y_preds</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.8141],</span><br><span class="line">        [0.8256],</span><br><span class="line">        [0.8372],</span><br><span class="line">        [0.8488],</span><br><span class="line">        [0.8603],</span><br><span class="line">        [0.8719],</span><br><span class="line">        [0.8835],</span><br><span class="line">        [0.8950],</span><br><span class="line">        [0.9066],</span><br><span class="line">        [0.9182]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions(predictions=y_preds)</span><br></pre></td></tr></table></figure>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-12.png" class="" title="PyTorch-26H-2-12">
<h2 id="5-Saving-and-loading-a-PyTorch-model-保存和加载-PyTorch-模型"><a href="#5-Saving-and-loading-a-PyTorch-model-保存和加载-PyTorch-模型" class="headerlink" title="5. Saving and loading a PyTorch model 保存和加载 PyTorch 模型"></a>5. Saving and loading a PyTorch model 保存和加载 PyTorch 模型</h2><p>如果您已经训练了 PyTorch 模型，那么您可能会想要保存它并将其导出到某个地方。</p>
<p>例如，您可能在 Google Colab 或使用 GPU 的本地机器上训练它，但现在您想将其导出到其他人可以使用的某种应用程序中。</p>
<p>或者您可能想保存模型的进度，稍后再回来加载它。</p>
<p>对于在 PyTorch 中保存和加载模型，您应该了解三种主要方法（以下所有内容均取自 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference">PyTorch 保存和加载模型指南</a>）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>torch.save</code></td>
<td style="text-align:center">使用 Python 的 <code>pickle</code> 实用程序将序列化对象保存到磁盘。可以使用 <code>torch.save</code> 保存模型、张量和各种其他 <code>Python</code> 对象（如字典）。</td>
</tr>
<tr>
<td style="text-align:center"><code>torch.load</code></td>
<td style="text-align:center">使用 pickle 的 <code>unpickling</code> 功能对 <code>pickle</code> 的 Python 对象文件（如模型、张量或字典）进行反序列化并将其加载到内存中。您还可以设置将对象加载到哪个设备（CPU、GPU 等）。</td>
</tr>
<tr>
<td style="text-align:center"><code>torch.nn.Module.load_state_dict</code></td>
<td style="text-align:center">使用已保存的 <code>state_dict()</code> 对象加载模型的参数字典 (<code>model.state_dict()</code>)。</td>
</tr>
</tbody>
</table>
</div>
<p>正如 <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/pickle.html">Python 的 pickle 文档</a>所述，pickle模块实现了用于序列化和反序列化 Python 对象结构的二进制协议，pickle 模块并不安全。这意味着您只应解开（加载）您信任的数据。这也适用于加载 PyTorch 模型。只使用您信任的来源保存的 PyTorch 模型。</p>
<h3 id="Saving-a-PyTorch-model’s-state-dict-保存-PyTorch-模型的state-dict"><a href="#Saving-a-PyTorch-model’s-state-dict-保存-PyTorch-模型的state-dict" class="headerlink" title="Saving a PyTorch model’s state_dict() 保存 PyTorch 模型的state_dict()"></a>Saving a PyTorch model’s <code>state_dict()</code> 保存 PyTorch 模型的<code>state_dict()</code></h3><p>什么是<code>state_dict()</code>？</p>
<p>在 PyTorch 中，模型的可学习参数（即权重和偏差） <code>torch.nn.Module</code>包含在模型的参数中（通过 访问<code>model.parameters()</code>）。 <code>Astate_dict</code>只是一个 Python 字典对象，它将每个层映射到其参数张量。<br><code>state_dict</code> 对象是 Python 字典，因此可以轻松保存、更新、更改和恢复它们，从而为 PyTorch 模型和优化器增加大量模块化。请注意，只有具有可学习参数（卷积层、线性层等）和已注册缓冲区（<code>batchnorm</code> 的 <code>running_mean</code>）的层才会在模型的 <code>state_dict</code> 中拥有条目。优化器对象 (<code>torch.optim</code>) 也有一个 <code>state_dict</code>，其中包含有关优化器状态以及所用超参数的信息。</p>
<p>保存和加载模型进行推理（进行预测）的<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference">推荐方法</a>是保存和加载模型的 <code>state_dict()</code>。</p>
<p>让我们看看如何通过几个步骤做到这一点：</p>
<ul>
<li>我们将使用 Python 的 <code>pathlib</code> 模块创建一个目录，用于将模型保存到调用的模型中。</li>
<li>我们将创建一个文件路径来保存模型。</li>
<li>我们将调用 <code>torch.save(obj, f)</code>，其中 <code>obj</code> 是目标模型的 <code>state_dict()</code>，<code>f</code> 是保存模型的文件名。</li>
</ul>
<p>注意：PyTorch 保存的模型或对象通常以 <code>.pt</code> 或 <code>.pth</code> 结尾，例如 <code>saved_model_01.pth</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create models directory </span></span><br><span class="line">MODEL_PATH = Path(<span class="string">&quot;models&quot;</span>)</span><br><span class="line">MODEL_PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create model save path </span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;01_pytorch_workflow_model_0.pth&quot;</span></span><br><span class="line">MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Save the model state dict </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Saving model to: <span class="subst">&#123;MODEL_SAVE_PATH&#125;</span>&quot;</span>)</span><br><span class="line">torch.save(obj=model_0.state_dict(), <span class="comment"># only saving the state_dict() only saves the models learned parameters</span></span><br><span class="line">           f=MODEL_SAVE_PATH) </span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Saving model to: models\01_pytorch_workflow_model_0.pth</span><br></pre></td></tr></table></figure>
<h3 id="Loading-a-saved-PyTorch-model’s-state-dict-加载已保存的-PyTorch-模型的-state-dict"><a href="#Loading-a-saved-PyTorch-model’s-state-dict-加载已保存的-PyTorch-模型的-state-dict" class="headerlink" title="Loading a saved PyTorch model’s state_dict() 加载已保存的 PyTorch 模型的 state_dict()"></a>Loading a saved PyTorch model’s <code>state_dict()</code> 加载已保存的 PyTorch 模型的 <code>state_dict()</code></h3><p>由于我们现在在 <code>models/01_pytorch_workflow_model_0.pth</code> 处有一个保存的模型 <code>state_dict()</code>，我们现在可以使用 <code>torch.nn.Module.load_state_dict(torch.load(f))</code> 加载它，其中 <code>f</code> 是我们保存的模型 <code>state_dict()</code> 的文件路径。</p>
<p>为什么在 <code>torch.nn.Module.load_state_dict()</code> 里面调用 <code>torch.load()</code>？</p>
<p>因为我们只保存了模型的 <code>state_dict()</code>（它是学习参数的字典）而不是整个模型，所以我们首先必须使用 <code>torch.load()</code> 加载 <code>state_dict()</code>，然后将该 <code>state_dict()</code> 传递给我们模型的新实例（它是 <code>nn.Module</code> 的一个子类）。</p>
<p>为什么不保存整个模型？</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model">保存整个模型</a>而不是仅仅保存 <code>state_dict()</code> 更为直观，但是，引用 PyTorch 文档：</p>
<p>保存整个模型的缺点是序列化数据与保存模型时使用的特定类和确切的目录结构绑定在一起……<br>因此，在其他项目中使用或重构后，您的代码可能会以各种方式中断。</p>
<p>因此，我们使用灵活的方法来保存和加载 <code>state_dict()</code>，它基本上是一个模型参数的字典。</p>
<p>让我们通过创建另一个 <code>LinearRegressionModel()</code> 实例来测试它，它是 <code>torch.nn.Module</code> 的一个子类，因此具有内置方法 <code>load_state_dict()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate a new instance of our model (this will be instantiated with random weights)</span></span><br><span class="line"><span class="comment"># 实例化我们模型的新实例（这将使用随机权重实例化）</span></span><br><span class="line">loaded_model_0 = LinearRegressionModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the state_dict of our saved model (this will update the new instance of our model with trained weights)</span></span><br><span class="line"><span class="comment"># 加载我们保存的模型的 state_dict （这将使用训练后的权重更新我们模型的新实例）</span></span><br><span class="line">loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure>
<p>PyTorch 推理规则：</p>
<ul>
<li>将模型设置为评估模式 (<code>model.eval()</code>)。</li>
<li>使用推理模式上下文管理器进行预测（使用 <code>torch.inference_mode(): ...</code>）。</li>
<li>所有预测都应使用同一设备上的对象进行（例如，仅在 GPU 上的数据和模型或仅在 CPU 上的数据和模型）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Put the loaded model into evaluation mode</span></span><br><span class="line"><span class="comment"># 1. 将加载的模型置于评估模式</span></span><br><span class="line">loaded_model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Use the inference mode context manager to make predictions</span></span><br><span class="line"><span class="comment"># 2. 使用推理模式上下文管理器进行预测</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    loaded_model_preds = loaded_model_0(X_test) <span class="comment"># perform a forward pass on the test data with the loaded model# 使用加载的模型对测试数据执行前向传递</span></span><br></pre></td></tr></table></figure>
<p>现在我们已经使用加载的模型做出了一些预测，让我们看看它们是否与之前的预测相同。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Compare previous model predictions with loaded model predictions (these should be the same)</span><br><span class="line">y_preds == loaded_model_preds</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True]])</span><br></pre></td></tr></table></figure>
<p>看起来加载的模型预测与之前的模型预测（保存前所做的预测）相同。这表明我们的模型正在按预期保存和加载。</p>
<p>还有更多方法可以保存和加载 PyTorch 模型，但我将把这些方法留到课外和进一步阅读中。有关更多信息，请参阅 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-and-loading-models">PyTorch 保存和加载模型指南</a>。</p>
<h2 id="6-Putting-it-all-together"><a href="#6-Putting-it-all-together" class="headerlink" title="6. Putting it all together"></a>6. Putting it all together</h2><p>首先导入所需的标准库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import PyTorch and matplotlib</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn <span class="comment"># nn contains all of PyTorch&#x27;s building blocks for neural networks</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check PyTorch version</span></span><br><span class="line">torch.__version__</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;2.4.1&#x27;</span><br></pre></td></tr></table></figure>
<p>通过设置来使我们的代码与设备无关，device=”cuda”如果它可用，否则它将默认为device=”cpu”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup device agnostic code</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using device: <span class="subst">&#123;device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using device: cuda</span><br></pre></td></tr></table></figure>
<h3 id="6-1-Data"><a href="#6-1-Data" class="headerlink" title="6.1 Data"></a>6.1 Data</h3><p>首先，我们将对一些权重和偏差值进行硬编码。</p>
<p>然后，我们将在 0 到 1 之间设置一个数字范围，这些数字将是我们的 X 值。</p>
<p>最后，我们将使用 X 值以及权重和偏差值，通过线性回归公式 (y = 权重 * X + 偏差) 创建 y。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create weight and bias</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create range values</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.02</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create X and y (features and labels)</span></span><br><span class="line">X = torch.arange(start, end, step).unsqueeze(dim=<span class="number">1</span>) <span class="comment"># without unsqueeze, errors will happen later on (shapes within linear layers)</span></span><br><span class="line">y = weight * X + bias</span><br><span class="line">X[:<span class="number">10</span>], y[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0.0000],</span><br><span class="line">         [0.0200],</span><br><span class="line">         [0.0400],</span><br><span class="line">         [0.0600],</span><br><span class="line">         [0.0800],</span><br><span class="line">         [0.1000],</span><br><span class="line">         [0.1200],</span><br><span class="line">         [0.1400],</span><br><span class="line">         [0.1600],</span><br><span class="line">         [0.1800]]),</span><br><span class="line"> tensor([[0.3000],</span><br><span class="line">         [0.3140],</span><br><span class="line">         [0.3280],</span><br><span class="line">         [0.3420],</span><br><span class="line">         [0.3560],</span><br><span class="line">         [0.3700],</span><br><span class="line">         [0.3840],</span><br><span class="line">         [0.3980],</span><br><span class="line">         [0.4120],</span><br><span class="line">         [0.4260]]))</span><br></pre></td></tr></table></figure>
<p>现在我们有了一些数据，让我们将其分成训练集和测试集。</p>
<p>我们将使用 80/20 的分割方式，即 80% 的训练数据和 20% 的测试数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split data</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X))</span><br><span class="line">X_train, y_train = X[:train_split], y[:train_split]</span><br><span class="line">X_test, y_test = X[train_split:], y[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(40, 40, 10, 10)</span><br></pre></td></tr></table></figure>
<p>太好了，让我们将它们可视化以确保它们看起来不错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: If you&#x27;ve reset your runtime, this function won&#x27;t work, </span></span><br><span class="line"><span class="comment"># you&#x27;ll have to rerun the cell above where it&#x27;s instantiated.</span></span><br><span class="line">plot_predictions(X_train, y_train, X_test, y_test)</span><br></pre></td></tr></table></figure>
<h3 id="6-2-Building-a-PyTorch-linear-model"><a href="#6-2-Building-a-PyTorch-linear-model" class="headerlink" title="6.2 Building a PyTorch linear model"></a>6.2 Building a PyTorch linear model</h3><p>太棒了，让我们来看一下。我们已经有了一些数据，现在是时候创建一个模型了。</p>
<p>我们将创建与以前相同风格的模型，只是这次，我们不再使用 <code>nn.Parameter()</code> 手动定义模型的权重和偏差参数，而是使用 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">nn.Linear(in_features, out_features)</a> 来为我们完成这项工作。</p>
<p>其中 <code>in_features</code> 是输入数据的维度数，<code>out_features</code> 是您希望输出到的维度数。</p>
<p>在我们的例子中，这两个都是 <code>1</code>，因为我们的数据每个标签 (<code>y</code>) 有 1 个输入特征 (<code>X</code>)。对它们进行大小调整以确保它们看起来不错。</p>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-13.png" class="" title="PyTorch-26H-2-13">
<p>使用 <code>nn.Parameter</code> 创建线性回归模型，而不是使用 <code>nn.Linear</code>。<code>torch.nn</code> 模块具有预构建计算的示例还有很多，包括许多流行且有用的神经网络层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Subclass nn.Module to make our model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModelV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># Use nn.Linear() for creating the model parameters</span></span><br><span class="line">        self.linear_layer = nn.Linear(in_features=<span class="number">1</span>, </span><br><span class="line">                                      out_features=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the forward computation (input data x flows through nn.Linear())</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">return</span> self.linear_layer(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the manual seed when creating the model (this isn&#x27;t always needed but is used for demonstrative purposes, try commenting it out and seeing what happens)</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">model_1 = LinearRegressionModelV2()</span><br><span class="line">model_1, model_1.state_dict()</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(LinearRegressionModelV2(</span><br><span class="line">   (linear_layer): Linear(in_features=1, out_features=1, bias=True)</span><br><span class="line"> ),</span><br><span class="line"> OrderedDict([(&#x27;linear_layer.weight&#x27;, tensor([[0.7645]])),</span><br><span class="line">              (&#x27;linear_layer.bias&#x27;, tensor([0.8300]))]))</span><br></pre></td></tr></table></figure>
<p>注意model_1.state_dict()的输出，nn.Linear()层为我们创建了一个随机权重和偏差参数。</p>
<p>现在让我们将模型放到 GPU 上（如果可用）。</p>
<p>我们可以使用 .to(device) 更改 PyTorch 对象所在的设备。</p>
<p>首先让我们检查模型的当前设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check model device</span></span><br><span class="line"><span class="built_in">next</span>(model_1.parameters()).device</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(type=&#x27;cpu&#x27;)</span><br></pre></td></tr></table></figure>
<p>太棒了，看起来模型默认在 CPU 上运行。</p>
<p>让我们将其改为在 GPU 上运行（如果可用的话）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set model to GPU if it&#x27;s available, otherwise it&#x27;ll default to CPU</span></span><br><span class="line">model_1.to(device) <span class="comment"># the device variable was set above to be &quot;cuda&quot; if available or &quot;cpu&quot; if not</span></span><br><span class="line"><span class="built_in">next</span>(model_1.parameters()).device</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(type=&#x27;cuda&#x27;, index=0)</span><br></pre></td></tr></table></figure>
<p>太棒了！由于我们的代码与设备无关，因此无论 GPU 是否可用，上述单元都可以工作。</p>
<h3 id="6-3-Training"><a href="#6-3-Training" class="headerlink" title="6.3 Training"></a>6.3 Training</h3><p>是时候构建训练和测试循环了。</p>
<p>首先，我们需要一个损失函数loss function和一个优化器optimizer。</p>
<p>让我们使用之前使用的相同函数，<code>nn.L1Loss()</code> 和 <code>torch.optim.SGD()</code>。</p>
<p>我们必须将新模型的参数 (<code>model.parameters()</code>) 传递给优化器，以便它在训练期间进行调整。</p>
<p><code>0.01</code> 的学习率之前也很好用，所以让我们再次使用它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create loss function</span></span><br><span class="line">loss_fn = nn.L1Loss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_1.parameters(), <span class="comment"># optimize newly created model&#x27;s parameters</span></span><br><span class="line">                            lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p>损失函数和优化器已准备就绪，现在让我们使用训练和测试循环来训练和评估我们的模型。</p>
<p>与之前的训练循环相比，我们在此步骤中要做的唯一不同的事情是将数据放在目标设备上。</p>
<p>我们已经使用 <code>model_1.to(device)</code> 将我们的模型放在目标设备上。</p>
<p>我们可以对数据执行相同的操作。</p>
<p>这样，如果模型在 GPU 上，数据就在 GPU 上（反之亦然）。</p>
<p>这次让我们更进一步，设置 <code>epochs=1000</code>。</p>
<p>如果您需要 PyTorch 训练循环步骤的提醒，请参见下文。</p>
<p>PyTorch 训练循环步骤</p>
<ul>
<li>前向传递 - 模型对所有训练数据进行一次遍历，执行其 forward() 函数计算 (model(x_train))。</li>
<li>计算损失 - 将模型的输出 (预测) 与基本事实进行比较，并进行评估以查看它们的错误程度 (loss = loss_fn(y_pred, y_train)。</li>
<li>零梯度 - 优化器梯度设置为零 (默认情况下是累积的)，因此可以为特定的训练步骤重新计算它们 (optimizer.zero_grad())。</li>
<li>对损失执行反向传播 - 针对要更新的每个模型参数 (每个参数的 require_grad=True) 计算损失的梯度。这称为反向传播，因此是“向后”(loss.backward())。</li>
<li>步进优化器 (梯度下降) - 使用 require_grad=True 更新参数，以根据损失梯度来改进它们 (optimizer.step())。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs </span></span><br><span class="line">epochs = <span class="number">1000</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data on the available device</span></span><br><span class="line"><span class="comment"># Without this, error will happen (not all model/data on device)</span></span><br><span class="line">X_train = X_train.to(device)</span><br><span class="line">X_test = X_test.to(device)</span><br><span class="line">y_train = y_train.to(device)</span><br><span class="line">y_test = y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_1.train() <span class="comment"># train mode is on by default after construction</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_pred = model_1(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Zero grad optimizer</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Step the optimizer</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_1.<span class="built_in">eval</span>() <span class="comment"># put the model in evaluation mode for testing (inference)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        test_pred = model_1(X_test)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 2. Calculate the loss</span></span><br><span class="line">        test_loss = loss_fn(test_pred, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Train loss: <span class="subst">&#123;loss&#125;</span> | Test loss: <span class="subst">&#123;test_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Train loss: 0.5551779866218567 | Test loss: 0.5739762187004089</span><br><span class="line">Epoch: 100 | Train loss: 0.006215683650225401 | Test loss: 0.014086711220443249</span><br><span class="line">Epoch: 200 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 300 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 400 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 500 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 600 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 700 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 800 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br><span class="line">Epoch: 900 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：由于机器学习的随机性，您可能会得到略有不同的结果（不同的损失和预测值），具体取决于您的模型是在 CPU 还是 GPU 上训练的。即使您在任一设备上使用相同的随机种子，情况也是如此。如果差异很大，您可能需要查找错误，但是，如果差异很小（理想情况下很小），您可以忽略它。</p>
</blockquote>
<p>检查一下模型学习到的参数，并将它们与我们硬编码的原始参数进行比较。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find our model&#x27;s learned parameters</span></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint <span class="comment"># pprint = pretty print, see: https://docs.python.org/3/library/pprint.html </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The model learned the following values for weights and bias:&quot;</span>)</span><br><span class="line">pprint(model_1.state_dict())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nAnd the original values for weights and bias are:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;weights: <span class="subst">&#123;weight&#125;</span>, bias: <span class="subst">&#123;bias&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The model learned the following values for weights and bias:</span><br><span class="line">OrderedDict([(&#x27;linear_layer.weight&#x27;, tensor([[0.6968]], device=&#x27;cuda:0&#x27;)),</span><br><span class="line">             (&#x27;linear_layer.bias&#x27;, tensor([0.3025], device=&#x27;cuda:0&#x27;))])</span><br><span class="line"></span><br><span class="line">And the original values for weights and bias are:</span><br><span class="line">weights: 0.7, bias: 0.3</span><br></pre></td></tr></table></figure>
<p>请记住，在实践中，你很少会提前知道完美的参数。</p>
<p>如果你提前知道了模型必须学习的参数，机器学习还有什么乐趣呢？</p>
<p>此外，在许多现实世界的机器学习问题中，参数的数量可能超过数千万。</p>
<h3 id="6-4-Making-predictions"><a href="#6-4-Making-predictions" class="headerlink" title="6.4 Making predictions"></a>6.4 Making predictions</h3><p>现在我们已经有一个训练好的模型，让我们打开它的评估模式并做出一些预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn model into evaluation mode</span></span><br><span class="line">model_1.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test data</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_preds = model_1(X_test)</span><br><span class="line">y_preds</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.8600],</span><br><span class="line">        [0.8739],</span><br><span class="line">        [0.8878],</span><br><span class="line">        [0.9018],</span><br><span class="line">        [0.9157],</span><br><span class="line">        [0.9296],</span><br><span class="line">        [0.9436],</span><br><span class="line">        [0.9575],</span><br><span class="line">        [0.9714],</span><br><span class="line">        [0.9854]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<p>如果您使用 GPU 上的数据进行预测，您可能会注意到上面的输出在末尾有 device=’cuda:0’。这意味着数据位于 CUDA 设备 0 上（由于零索引，您的系统可以访问的第一个 GPU），如果您将来最终使用多个 GPU，这个数字可能会更高。</p>
<p>现在让我们绘制模型的预测。</p>
<blockquote>
<p>注意：许多数据科学库（例如 pandas、matplotlib 和 NumPy）无法使用存储在 GPU 上的数据。因此，当您尝试使用其中一个库中的函数处理未存储在 CPU 上的张量数据时，可能会遇到一些问题。要解决此问题，您可以在目标张量上调用 .cpu() 以返回 CPU 上的目标张量的副本。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot_predictions(predictions=y_preds) # -&gt; won&#x27;t work... data not on CPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data on the CPU and plot it</span></span><br><span class="line">plot_predictions(predictions=y_preds.cpu())</span><br></pre></td></tr></table></figure>
<img src="/2024/08/15/PyTorch-26H-2/PyTorch-26H-2-14.png" class="" title="PyTorch-26H-2-14">
<h3 id="6-5-Saving-and-loading-a-model-保存和加载模型"><a href="#6-5-Saving-and-loading-a-model-保存和加载模型" class="headerlink" title="6.5 Saving and loading a model 保存和加载模型"></a>6.5 Saving and loading a model 保存和加载模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create models directory </span></span><br><span class="line">MODEL_PATH = Path(<span class="string">&quot;models&quot;</span>)</span><br><span class="line">MODEL_PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create model save path </span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;01_pytorch_workflow_model_1.pth&quot;</span></span><br><span class="line">MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Save the model state dict </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Saving model to: <span class="subst">&#123;MODEL_SAVE_PATH&#125;</span>&quot;</span>)</span><br><span class="line">torch.save(obj=model_1.state_dict(), <span class="comment"># only saving the state_dict() only saves the models learned parameters</span></span><br><span class="line">           f=MODEL_SAVE_PATH) </span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Saving model to: models\01_pytorch_workflow_model_1.pth</span><br></pre></td></tr></table></figure>
<p>为了确保一切正常，我们将其重新加载。</p>
<ul>
<li>创建 <code>LinearRegressionModelV2()</code> 类的新实例</li>
<li>使用 <code>torch.nn.Module.load_state_dict()</code> 加载模型状态字典</li>
<li>将模型的新实例发送到目标设备（以确保我们的代码与设备无关）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate a fresh instance of LinearRegressionModelV2</span></span><br><span class="line">loaded_model_1 = LinearRegressionModelV2()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load model state dict </span></span><br><span class="line">loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)</span></span><br><span class="line">loaded_model_1.to(device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loaded model:\n<span class="subst">&#123;loaded_model_1&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model on device:\n<span class="subst">&#123;<span class="built_in">next</span>(loaded_model_1.parameters()).device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Loaded model:</span><br><span class="line">LinearRegressionModelV2(</span><br><span class="line">  (linear_layer): Linear(in_features=1, out_features=1, bias=True)</span><br><span class="line">)</span><br><span class="line">Model on device:</span><br><span class="line">cuda:0</span><br></pre></td></tr></table></figure>
<p>现在我们可以评估加载的模型，看看它的预测是否与保存之前的预测一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate loaded model</span></span><br><span class="line">loaded_model_1.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    loaded_model_1_preds = loaded_model_1(X_test)</span><br><span class="line">y_preds == loaded_model_1_preds</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True],</span><br><span class="line">        [True]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2><p>1、使用线性回归公式 () 创建直线数据集<code>weight * X + bias</code>。</p>
<ul>
<li>设置<code>weight=0.3</code>并且<code>bias=0.9</code>总共应该至少有 100 个数据点。</li>
<li>将数据分成 80％ 用于训练，20％ 用于测试。</li>
<li>绘制训练和测试数据，使其变得可视化。</li>
</ul>
<p>2、通过子类化构建 PyTorch 模型<code>nn.Module</code>。</p>
<ul>
<li>里面应该有一个随机初始化<code>nn.Parameter()</code>的<code>requires_grad=True</code>，一个为<code>weights</code>，一个为<code>bias</code>。</li>
<li>实现<code>forward()</code>在1中创建数据集时使用的计算线性回归函数的方法。</li>
<li>一旦构建了模型，就创建它的一个实例并检查它的<code>state_dict()</code>。</li>
<li>注意：如果您愿意，<code>nn.Linear()</code>也<code>nn.Parameter()</code>可以使用。</li>
</ul>
<p>3、<code>nn.L1Loss()</code>分别使用和创建损失函数和优化器<code>torch.optim.SGD(params, lr)</code>。</p>
<ul>
<li>将优化器的学习率设置为 0.01，要优化的参数应该是您在 2 中创建的模型的模型参数。</li>
<li>编写一个训练循环来执行 300 个时期的适当训练步骤。</li>
<li>训练循环应该每 20 个时期在测试数据集上测试模型。</li>
</ul>
<p>4、使用训练好的模型对测试数据进行预测。</p>
<ul>
<li>根据原始训练和测试数据对这些预测进行可视化（注意：如果您想使用不支持 CUDA 的库（例如 matplotlib 来绘图，则可能需要确保预测不在GPU 上）。</li>
</ul>
<p>5、将您训练的模型保存<code>state_dict()</code>到文件中。</p>
<ul>
<li>创建您在 2 中创建的模型类的新实例，并加载<code>state_dict()</code>您刚刚保存的内容。</li>
<li>使用加载的模型对测试数据执行预测，并确认它们与 4 中的原始模型预测相匹配。</li>
</ul>
<h2 id="Extra-curriculum"><a href="#Extra-curriculum" class="headerlink" title="Extra-curriculum"></a>Extra-curriculum</h2><ul>
<li>阅读Jeremy Howard 的《到底<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">是什么<code>torch.nn</code>？》，</a>以更深入地了解 PyTorch 中最重要的模块之一的工作原理。</li>
<li>花 10 分钟浏览并查看<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/ptcheat.html">PyTorch 文档备忘单，</a>了解您可能会遇到的所有不同 PyTorch 模块。</li>
<li>花 10 分钟阅读<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch 网站上的加载和保存文档，</a>以熟悉 PyTorch 中的不同保存和加载选项。</li>
<li>花费 1-2 小时阅读/观看以下内容，了解梯度下降和反向传播的内部原理，这两种主要算法一直在后台运行，帮助我们的模型学习。</li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">梯度下降的维基百科页面</a></li>
<li>梯度下降算法——Robert Kwiatkowski 的<a target="_blank" rel="noopener" href="https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21">深入探讨</a></li>
<li><a target="_blank" rel="noopener" href="https://youtu.be/IHZwWFHWa-w">梯度下降，神经网络如何学习视频</a>（3Blue1Brown 拍摄）</li>
<li><a target="_blank" rel="noopener" href="https://youtu.be/Ilg3gGewQ5U">反向传播到底在做什么？</a>视频由 3Blue1Brown 提供</li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Backpropagation">反向传播维基百科页面</a></li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2024/08/15/PyTorch-26H-2/">http://hibiscidai.com/2024/08/15/PyTorch-26H-2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2024/08/16/PyTorch-26H-3/"><i class="fa fa-chevron-left">  </i><span>PyTorch-26H-3</span></a></div><div class="next-post pull-right"><a href="/2024/08/14/PyTorch-26H-1/"><span>PyTorch-26H-1</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://www.paofu.cloud/auth/register?code=j4I7">好用、实惠、稳定的梯子,点击这里<img src="https://pic.imgdb.cn/item/65572abac458853aefef30cd.png" width="1000" height="124" object-fit="cover" ></a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2025 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>