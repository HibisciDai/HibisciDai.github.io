<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络"><meta name="keywords" content="学习笔记,Tensorflow,TensorflowMOOC"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E6%A0%B8"><span class="toc-number">2.</span> <span class="toc-text">循环核</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E6%A0%B8%E6%97%B6%E9%97%B4%E6%AD%A5%E5%B1%95%E5%BC%80"><span class="toc-number">3.</span> <span class="toc-text">循环核时间步展开</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E8%AE%A1%E7%AE%97%E5%B1%82"><span class="toc-number">4.</span> <span class="toc-text">循环计算层</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TF%E6%8F%8F%E8%BF%B0%E5%BE%AA%E7%8E%AF%E8%AE%A1%E7%AE%97%E5%B1%82"><span class="toc-number">5.</span> <span class="toc-text">TF描述循环计算层</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8BI"><span class="toc-number">6.</span> <span class="toc-text">循环计算过程I</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8RNN%E5%AE%9E%E7%8E%B0%E8%BE%93%E5%85%A5%E4%B8%80%E4%B8%AA%E5%AD%97%E6%AF%8D%EF%BC%8C%E9%A2%84%E6%B5%8B%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%AD%97%E6%AF%8D%EF%BC%88One-hot-%E7%BC%96%E7%A0%81%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">用RNN实现输入一个字母，预测下一个字母（One hot 编码）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8RNN%E5%AE%9E%E7%8E%B0%E8%BE%93%E5%85%A5%E5%9B%9B%E4%B8%AA%E5%AD%97%E6%AF%8D%EF%BC%8C%E9%A2%84%E6%B5%8B%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%AD%97%E6%AF%8D%EF%BC%88One-hot-%E7%BC%96%E7%A0%81%EF%BC%89"><span class="toc-number">8.</span> <span class="toc-text">用RNN实现输入四个字母，预测下一个字母（One hot 编码）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">8.0.1.</span> <span class="toc-text">代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Embedding-%E4%B8%80%E7%A7%8D%E7%BC%96%E7%A0%81%E6%96%B9%E6%B3%95"><span class="toc-number">9.</span> <span class="toc-text">Embedding-一种编码方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8RNN%E5%AE%9E%E7%8E%B0%E8%BE%93%E5%85%A5%E4%B8%80%E4%B8%AA%E5%AD%97%E6%AF%8D%EF%BC%8C%E9%A2%84%E6%B5%8B%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%AD%97%E6%AF%8D%EF%BC%88Embedding-%E7%BC%96%E7%A0%81%EF%BC%89"><span class="toc-number">10.</span> <span class="toc-text">用RNN实现输入一个字母，预测下一个字母（Embedding 编码）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8RNN%E5%AE%9E%E7%8E%B0%E8%BE%93%E5%85%A5%E5%9B%9B%E4%B8%AA%E5%AD%97%E6%AF%8D%EF%BC%8C%E9%A2%84%E6%B5%8B%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%AD%97%E6%AF%8D%EF%BC%88Embedding-%E7%BC%96%E7%A0%81%EF%BC%89"><span class="toc-number">11.</span> <span class="toc-text">用RNN实现输入四个字母，预测下一个字母（Embedding 编码）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8RNN%E5%AE%9E%E7%8E%B0%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8B"><span class="toc-number">12.</span> <span class="toc-text">用RNN实现股票预测</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8LSTM%E5%AE%9E%E7%8E%B0%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8B"><span class="toc-number">13.</span> <span class="toc-text">用LSTM实现股票预测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">13.1.</span> <span class="toc-text">LSTM计算过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TF%E6%8F%8F%E8%BF%B0LSTM%E5%B1%82"><span class="toc-number">13.2.</span> <span class="toc-text">TF描述LSTM层</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8GRU%E5%AE%9E%E7%8E%B0%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8B"><span class="toc-number">14.</span> <span class="toc-text">用GRU实现股票预测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#TF%E6%8F%8F%E8%BF%B0GRU%E5%B1%82"><span class="toc-number">14.1.</span> <span class="toc-text">TF描述GRU层</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">222</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">76</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">29</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-02-16</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">9.3k</span><span class="post-meta__separator">|</span><span>阅读时长: 39 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png" class="" title="人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络">
<p>人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络</p>
<span id="more"></span>
<p>[TOC]</p>
<h1 id="人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络"><a href="#人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络" class="headerlink" title="人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络"></a>人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络</h1><p>通过脑记忆体提取历史数据的特征，预测出接下来最可能发生的情况。</p>
<h1 id="循环核"><a href="#循环核" class="headerlink" title="循环核"></a>循环核</h1><p>参数时间共享，循环层提取时间信息。</p>
<p>循环核具有记忆力，通过不同时刻的参数共享，实现了对时间序列的信息提取。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN%E5%BE%AA%E7%8E%AF%E6%A0%B8.png" class="" title="RNN循环核">
<p>前向传播时：记忆体内存储的状态信息ht ，在每个时刻都被刷新，三个参数矩阵wxh whh why自始至终都是固定不变的。<br>反向传播时：三个参数矩阵wxh whh why被梯度下降法更新。</p>
<script type="math/tex; mode=display">y_{t} = softmax( h_{t} w_{hy} + by)</script><script type="math/tex; mode=display">h_{t} = tanh( x_{t} w_{xh} + h_{t-1} w_{hh} + bh)</script><p>可以设定记忆体个数，改变记忆体容量，当记忆体个数被指定，输入xt、输出yt维度被指定，周围这些待训练参数的维度也就被限定了。<br>记忆体内存储着每个时刻的状态信息ht，记忆体当前时刻存储的状态信息ht等于当前时刻的输入特征xt乘以矩阵wxh加上记忆体上一时刻存储的状态信息ht-1乘以矩阵whh再加上偏置项bh，他们的和过tanh激活函数。<br>当前时刻循环核的输出特征yt等于记忆体内存储的状态信息ht乘以矩阵why，再加上偏置项by，过softmax激活函数，实际上是一层全连接。<br>在前向传播时，记忆体内存储的状态信息ht，在每个时刻都被刷新，三个参数矩阵wxh、whh和why自始至终都是固定不变的。<br>只有反向传播时候，三个参数矩阵wxh、whh、why被梯度下降法更新，</p>
<h1 id="循环核时间步展开"><a href="#循环核时间步展开" class="headerlink" title="循环核时间步展开"></a>循环核时间步展开</h1><p>按照时间步骤展开，就是把循环核按照时间轴方向展开，如下图所示。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN%E5%BE%AA%E7%8E%AF%E6%A0%B8%E6%8C%89%E6%97%B6%E9%97%B4%E6%AD%A5%E5%B1%95%E5%BC%80.png" class="" title="RNN循环核按时间步展开">
<p>每个时刻记忆体状态信息ht被刷新，记忆体周围的参数矩阵wxh、whh和why是固定不变的，我们训练优化的就是这些参数矩阵。</p>
<p>训练完成后，使用效果最好的参数矩阵，执行前向传播，输出预测结果。</p>
<p>人类脑中的记忆体每时每刻都根据当前的输入而更新，当前的预测推理，是根据你以往的知识积累，用固化下来的参数矩阵进行的推理判断，循环神经网络就是借助循环核实现的时间特征提取，再把提取到的信息送入全连接网络，实现连续数据的预测，yt是整个循环网络的末层，从公式来看，就是一个全连接网络，借助全连接网络，实现连续数据预测。</p>
<h1 id="循环计算层"><a href="#循环计算层" class="headerlink" title="循环计算层"></a>循环计算层</h1><p>每个循环核构成一层循环计算层，循环计算层的层数是向输出方向增长的。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E8%AE%A1%E7%AE%97%E5%B1%82.png" class="" title="循环计算层">
<p>左图的网络有一个循环核，构成了一层循环计算层；<br>中图的网络有两个循环核，构成了两层循环计算层；<br>右图的网络有三个循环核，构成了三层循环计算层。<br>其中，三个网络中每个循环核中记忆体的个数可以根据我们的需求任意指定。</p>
<h1 id="TF描述循环计算层"><a href="#TF描述循环计算层" class="headerlink" title="TF描述循环计算层"></a>TF描述循环计算层</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.SimpleRNN(记忆体个数, activation=‘激活函数’, return_sequences=是否每个时刻输出ht到下一层)</span><br></pre></td></tr></table></figure>
<p>activation=‘激活函数’ （不写，默认使用tanh）<br>return_sequences=True 各时间步输出ht<br>return_sequences=False 仅最后时间步输出ht（默认）</p>
<p>例： SimpleRNN(3, return_sequences=True)，定义了一个具有三个记忆体的循环核，这个循环核会在每个时间步输出ht。</p>
<p>循环核在每个实践步输出ht，可以用这张图表示：</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/return_sequences=True.png" class="" title="return_sequences&#x3D;True">
<p>循环核仅在最后一个时间步输出ht，可以用这张图表示：</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/return_sequences=False.png" class="" title="return_sequences&#x3D;False">
<p>API对送入循环层的数据维度是有要求的，要求送入循环层的数据是三维的，第一维是送入样本的总数量，第二维是循环核按时间展开的步数，第三维是每个时间步输入特征的个数。</p>
<p><strong> 入RNN时， x_train维度：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数] </strong></p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN%E5%B1%82%E8%BE%93%E5%85%A5%E7%BB%B4%E5%BA%A6.png" class="" title="RNN层输入维度">
<p>左图一共要送入RNN层两组数据，每组数据经过一个时间步就会得到输出结果，每个时间步送入三个数值，输入循环层的数据维度就是 [2, 1, 3] 。</p>
<p>右图只有一组数据，分四个时间步送入循环层，每个时间步送入输入两个数值，输入循环层的数据维度就是 [1, 4, 2] 。</p>
<h1 id="循环计算过程I"><a href="#循环计算过程I" class="headerlink" title="循环计算过程I"></a>循环计算过程I</h1><p>字母预测：输入a预测出b，输入b预测出c，输入c预测出d，输入d预测出e，输入e预测出a。</p>
<p>神经网络的输入都是数字，所以我们先要把用到的 a b c d e 这五个字母，用数字表示出来，最简单直接的方法就是用独热码对这五个字母编码。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">词向量空间</th>
<th style="text-align:center">词向量空间</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10000</td>
<td style="text-align:center">a</td>
</tr>
<tr>
<td style="text-align:center">01000</td>
<td style="text-align:center">b</td>
</tr>
<tr>
<td style="text-align:center">00100</td>
<td style="text-align:center">c</td>
</tr>
<tr>
<td style="text-align:center">00010</td>
<td style="text-align:center">d</td>
</tr>
<tr>
<td style="text-align:center">00001</td>
<td style="text-align:center">e</td>
</tr>
</tbody>
</table>
</div>
<p>随机生成了Wxh、Whh和Why三个参数矩阵，记忆体的个数选取3，记忆体状态信息ht等于xt乘以wxh加上ht-1乘以whh加上bh，再过tanh激活函数。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B-1.png" class="" title="循环计算过程-1">
<p>当前输入xt[0, 1, 0, 0, 0]乘以黄色的参数矩阵wxh，得到[-2.3 0.8 1.1]，上一时刻，也就是最开始时，记忆体状态信息等于0，所以这里加上0，再加上偏置矩阵bh，[0.5 0.3 -0.2]，过tanh激活函数后，得到当前时刻的状态信息ht，记忆体存储状态信息被刷新为[-0.9 0.8 0.7]</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B-2.png" class="" title="循环计算过程-2">
<p>这个过程可以认为脑中的记忆因为当前输入的事物而更新了。</p>
<p>输出yt是把提取到的时间信息，通过全连接进行识别预测的过程，是整个网络的输出层。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B-3.png" class="" title="循环计算过程-3">
<h1 id="用RNN实现输入一个字母，预测下一个字母（One-hot-编码）"><a href="#用RNN实现输入一个字母，预测下一个字母（One-hot-编码）" class="headerlink" title="用RNN实现输入一个字母，预测下一个字母（One hot 编码）"></a>用RNN实现输入一个字母，预测下一个字母（One hot 编码）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, SimpleRNN</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">input_word = <span class="string">&quot;abcde&quot;</span></span><br><span class="line">w_to_id = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;e&#x27;</span>: <span class="number">4</span>&#125;  <span class="comment"># 单词映射到数值id的词典</span></span><br><span class="line">id_to_onehot = &#123;<span class="number">0</span>: [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], <span class="number">1</span>: [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], <span class="number">2</span>: [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>], <span class="number">3</span>: [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">                <span class="number">4</span>: [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]&#125;  <span class="comment"># id编码为one-hot</span></span><br><span class="line"></span><br><span class="line">x_train = [id_to_onehot[w_to_id[<span class="string">&#x27;a&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;b&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;c&#x27;</span>]],</span><br><span class="line">           id_to_onehot[w_to_id[<span class="string">&#x27;d&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;e&#x27;</span>]]]</span><br><span class="line">y_train = [w_to_id[<span class="string">&#x27;b&#x27;</span>], w_to_id[<span class="string">&#x27;c&#x27;</span>], w_to_id[<span class="string">&#x27;d&#x27;</span>], w_to_id[<span class="string">&#x27;e&#x27;</span>], w_to_id[<span class="string">&#x27;a&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使x_train符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。</span></span><br><span class="line"><span class="comment"># 此处整个数据集送入，送入样本数为len(x_train)；输入1个字母出结果，循环核时间展开步数为1; 表示为独热码有5个输入特征，每个时间步输入特征个数为5</span></span><br><span class="line">x_train = np.reshape(x_train, (<span class="built_in">len</span>(x_train), <span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">y_train = np.array(y_train)</span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    SimpleRNN(<span class="number">3</span>),</span><br><span class="line">    Dense(<span class="number">5</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">False</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/rnn_onehot_1pre1.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 save_best_only=<span class="literal">True</span>,</span><br><span class="line">                                                 monitor=<span class="string">&#x27;loss&#x27;</span>)  <span class="comment"># 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型</span></span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">100</span>, callbacks=[cp_callback])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(model.trainable_variables)</span></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)  <span class="comment"># 参数提取</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> model.trainable_variables:</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################    show   ###############################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示训练集和验证集的acc和loss曲线</span></span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">############### predict #############</span></span><br><span class="line"></span><br><span class="line">preNum = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;input the number of test alphabet:&quot;</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(preNum):</span><br><span class="line">    alphabet1 = <span class="built_in">input</span>(<span class="string">&quot;input test alphabet:&quot;</span>)</span><br><span class="line">    alphabet = [id_to_onehot[w_to_id[alphabet1]]]</span><br><span class="line">    <span class="comment"># 使alphabet符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。此处验证效果送入了1个样本，送入样本数为1；输入1个字母出结果，所以循环核时间展开步数为1; 表示为独热码有5个输入特征，每个时间步输入特征个数为5</span></span><br><span class="line">    alphabet = np.reshape(alphabet, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">    result = model.predict([alphabet])</span><br><span class="line">    pred = tf.argmax(result, axis=<span class="number">1</span>)</span><br><span class="line">    pred = <span class="built_in">int</span>(pred)</span><br><span class="line">    tf.<span class="built_in">print</span>(alphabet1 + <span class="string">&#x27;-&gt;&#x27;</span> + input_word[pred])</span><br></pre></td></tr></table></figure>
<h1 id="用RNN实现输入四个字母，预测下一个字母（One-hot-编码）"><a href="#用RNN实现输入四个字母，预测下一个字母（One-hot-编码）" class="headerlink" title="用RNN实现输入四个字母，预测下一个字母（One hot 编码）"></a>用RNN实现输入四个字母，预测下一个字母（One hot 编码）</h1><p>每个时刻内，参数矩阵是固定的，记忆体会在每个时刻被更新。</p>
<p>第一个时刻：b输入[0, 1, 0, 0, 0]，记忆体更新为[-0.9, 0.2, 0.2]</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%97%B6%E5%88%BB.png" class="" title="第一个时刻">
<p>第二个时刻：c输入[0, 0, 1, 0, 0]，记忆体更新[0.8, 1.0, 0.8]</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%8C%E4%B8%AA%E6%97%B6%E5%88%BB.png" class="" title="第二个时刻">
<p>第三个时刻：d输入[0, 0, 0, 1, 0]，记忆体更新为[0.6, 0.5,- 1.0]</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%B8%89%E4%B8%AA%E6%97%B6%E5%88%BB.png" class="" title="第三个时刻">
<p>第四个时刻：e输入[0, 0, 0, 0, 1]，记忆体更新为[-1.0, -1.0, 0.8]</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E5%9B%9B%E4%B8%AA%E6%97%B6%E5%88%BB.png" class="" title="第四个时刻">
<p>四个时间步骤中，所用到的参数矩阵wxh核偏置项bh数值是相同的，输出预测通过全连接完成，带入yt计算公式，得到[0.71 0.14 0.10 0.05 0.00]，输出预测结果为a。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E4%B8%AA%E6%97%B6%E5%88%BB%E8%BE%93%E5%87%BA.png" class="" title="第五个时刻输出">
<p>因为输入的是e，下一个是e可能性最小。</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>输入abcd输出e<br>输入bcde输出a<br>输入cdea输出b<br>输入deab输出c<br>输入eabc输出d</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, SimpleRNN</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">input_word = <span class="string">&quot;abcde&quot;</span></span><br><span class="line">w_to_id = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;e&#x27;</span>: <span class="number">4</span>&#125;  <span class="comment"># 单词映射到数值id的词典</span></span><br><span class="line">id_to_onehot = &#123;<span class="number">0</span>: [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], <span class="number">1</span>: [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], <span class="number">2</span>: [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>], <span class="number">3</span>: [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">                <span class="number">4</span>: [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]&#125;  <span class="comment"># id编码为one-hot</span></span><br><span class="line"></span><br><span class="line">x_train = [</span><br><span class="line">    [id_to_onehot[w_to_id[<span class="string">&#x27;a&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;b&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;c&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;d&#x27;</span>]]],</span><br><span class="line">    [id_to_onehot[w_to_id[<span class="string">&#x27;b&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;c&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;d&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;e&#x27;</span>]]],</span><br><span class="line">    [id_to_onehot[w_to_id[<span class="string">&#x27;c&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;d&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;e&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;a&#x27;</span>]]],</span><br><span class="line">    [id_to_onehot[w_to_id[<span class="string">&#x27;d&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;e&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;a&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;b&#x27;</span>]]],</span><br><span class="line">    [id_to_onehot[w_to_id[<span class="string">&#x27;e&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;a&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;b&#x27;</span>]], id_to_onehot[w_to_id[<span class="string">&#x27;c&#x27;</span>]]],</span><br><span class="line">]</span><br><span class="line">y_train = [w_to_id[<span class="string">&#x27;e&#x27;</span>], w_to_id[<span class="string">&#x27;a&#x27;</span>], w_to_id[<span class="string">&#x27;b&#x27;</span>], w_to_id[<span class="string">&#x27;c&#x27;</span>], w_to_id[<span class="string">&#x27;d&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使x_train符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。</span></span><br><span class="line"><span class="comment"># 此处整个数据集送入，送入样本数为len(x_train)；输入4个字母出结果，循环核时间展开步数为4; 表示为独热码有5个输入特征，每个时间步输入特征个数为5</span></span><br><span class="line">x_train = np.reshape(x_train, (<span class="built_in">len</span>(x_train), <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">y_train = np.array(y_train)</span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    SimpleRNN(<span class="number">3</span>),</span><br><span class="line">    Dense(<span class="number">5</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">False</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/rnn_onehot_4pre1.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 save_best_only=<span class="literal">True</span>,</span><br><span class="line">                                                 monitor=<span class="string">&#x27;loss&#x27;</span>)  <span class="comment"># 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型</span></span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">100</span>, callbacks=[cp_callback])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(model.trainable_variables)</span></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)  <span class="comment"># 参数提取</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> model.trainable_variables:</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################    show   ###############################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示训练集和验证集的acc和loss曲线</span></span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">############### predict #############</span></span><br><span class="line"></span><br><span class="line">preNum = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;input the number of test alphabet:&quot;</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(preNum):</span><br><span class="line">    alphabet1 = <span class="built_in">input</span>(<span class="string">&quot;input test alphabet:&quot;</span>)</span><br><span class="line">    alphabet = [id_to_onehot[w_to_id[a]] <span class="keyword">for</span> a <span class="keyword">in</span> alphabet1]</span><br><span class="line">    <span class="comment"># 使alphabet符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。此处验证效果送入了1个样本，送入样本数为1；输入4个字母出结果，所以循环核时间展开步数为4; 表示为独热码有5个输入特征，每个时间步输入特征个数为5</span></span><br><span class="line">    alphabet = np.reshape(alphabet, (<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">    result = model.predict([alphabet])</span><br><span class="line">    pred = tf.argmax(result, axis=<span class="number">1</span>)</span><br><span class="line">    pred = <span class="built_in">int</span>(pred)</span><br><span class="line">    tf.<span class="built_in">print</span>(alphabet1 + <span class="string">&#x27;-&gt;&#x27;</span> + input_word[pred])</span><br></pre></td></tr></table></figure>
<h1 id="Embedding-一种编码方法"><a href="#Embedding-一种编码方法" class="headerlink" title="Embedding-一种编码方法"></a>Embedding-一种编码方法</h1><p>独热码：数据量大 过于稀疏，映射之间是独立的，没有表现出关联性</p>
<p>Embedding：是一种单词编码方法，用低维向量实现了编码，这种编码通过神经网络训练优化，能表达出单词间的相关性。</p>
<p><code>tf.keras.layers.Embedding(词汇表大小, 编码维度)</code></p>
<p>编码维度就是用几个数字表达一个单词</p>
<p>对1-100进行编码， [4] 编码为 [0.25, 0.1, 0.11]<br>例 ： <code>tf.keras.layers.Embedding(100, 3)</code></p>
<p>入Embedding时， x_train维度：<br><code>[送入样本数, 循环核时间展开步数]</code></p>
<h1 id="用RNN实现输入一个字母，预测下一个字母（Embedding-编码）"><a href="#用RNN实现输入一个字母，预测下一个字母（Embedding-编码）" class="headerlink" title="用RNN实现输入一个字母，预测下一个字母（Embedding 编码）"></a>用RNN实现输入一个字母，预测下一个字母（Embedding 编码）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, SimpleRNN, Embedding</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">input_word = <span class="string">&quot;abcde&quot;</span></span><br><span class="line">w_to_id = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;e&#x27;</span>: <span class="number">4</span>&#125;  <span class="comment"># 单词映射到数值id的词典</span></span><br><span class="line"></span><br><span class="line">x_train = [w_to_id[<span class="string">&#x27;a&#x27;</span>], w_to_id[<span class="string">&#x27;b&#x27;</span>], w_to_id[<span class="string">&#x27;c&#x27;</span>], w_to_id[<span class="string">&#x27;d&#x27;</span>], w_to_id[<span class="string">&#x27;e&#x27;</span>]]</span><br><span class="line">y_train = [w_to_id[<span class="string">&#x27;b&#x27;</span>], w_to_id[<span class="string">&#x27;c&#x27;</span>], w_to_id[<span class="string">&#x27;d&#x27;</span>], w_to_id[<span class="string">&#x27;e&#x27;</span>], w_to_id[<span class="string">&#x27;a&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使x_train符合Embedding输入要求：[送入样本数， 循环核时间展开步数] ，</span></span><br><span class="line"><span class="comment"># 此处整个数据集送入所以送入，送入样本数为len(x_train)；输入1个字母出结果，循环核时间展开步数为1。</span></span><br><span class="line">x_train = np.reshape(x_train, (<span class="built_in">len</span>(x_train), <span class="number">1</span>))</span><br><span class="line">y_train = np.array(y_train)</span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    Embedding(<span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">    SimpleRNN(<span class="number">3</span>),</span><br><span class="line">    Dense(<span class="number">5</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">False</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/run_embedding_1pre1.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 save_best_only=<span class="literal">True</span>,</span><br><span class="line">                                                 monitor=<span class="string">&#x27;loss&#x27;</span>)  <span class="comment"># 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型</span></span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">100</span>, callbacks=[cp_callback])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(model.trainable_variables)</span></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)  <span class="comment"># 参数提取</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> model.trainable_variables:</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################    show   ###############################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示训练集和验证集的acc和loss曲线</span></span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">############### predict #############</span></span><br><span class="line"></span><br><span class="line">preNum = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;input the number of test alphabet:&quot;</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(preNum):</span><br><span class="line">    alphabet1 = <span class="built_in">input</span>(<span class="string">&quot;input test alphabet:&quot;</span>)</span><br><span class="line">    alphabet = [w_to_id[alphabet1]]</span><br><span class="line">    <span class="comment"># 使alphabet符合Embedding输入要求：[送入样本数， 循环核时间展开步数]。</span></span><br><span class="line">    <span class="comment"># 此处验证效果送入了1个样本，送入样本数为1；输入1个字母出结果，循环核时间展开步数为1。</span></span><br><span class="line">    alphabet = np.reshape(alphabet, (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    result = model.predict(alphabet)</span><br><span class="line">    pred = tf.argmax(result, axis=<span class="number">1</span>)</span><br><span class="line">    pred = <span class="built_in">int</span>(pred)</span><br><span class="line">    tf.<span class="built_in">print</span>(alphabet1 + <span class="string">&#x27;-&gt;&#x27;</span> + input_word[pred])</span><br></pre></td></tr></table></figure>
<h1 id="用RNN实现输入四个字母，预测下一个字母（Embedding-编码）"><a href="#用RNN实现输入四个字母，预测下一个字母（Embedding-编码）" class="headerlink" title="用RNN实现输入四个字母，预测下一个字母（Embedding 编码）"></a>用RNN实现输入四个字母，预测下一个字母（Embedding 编码）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, SimpleRNN, Embedding</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">input_word = <span class="string">&quot;abcdefghijklmnopqrstuvwxyz&quot;</span></span><br><span class="line">w_to_id = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;e&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">           <span class="string">&#x27;f&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;g&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;h&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;j&#x27;</span>: <span class="number">9</span>,</span><br><span class="line">           <span class="string">&#x27;k&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;l&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;n&#x27;</span>: <span class="number">13</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">14</span>,</span><br><span class="line">           <span class="string">&#x27;p&#x27;</span>: <span class="number">15</span>, <span class="string">&#x27;q&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;r&#x27;</span>: <span class="number">17</span>, <span class="string">&#x27;s&#x27;</span>: <span class="number">18</span>, <span class="string">&#x27;t&#x27;</span>: <span class="number">19</span>,</span><br><span class="line">           <span class="string">&#x27;u&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;v&#x27;</span>: <span class="number">21</span>, <span class="string">&#x27;w&#x27;</span>: <span class="number">22</span>, <span class="string">&#x27;x&#x27;</span>: <span class="number">23</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">24</span>, <span class="string">&#x27;z&#x27;</span>: <span class="number">25</span>&#125;  <span class="comment"># 单词映射到数值id的词典</span></span><br><span class="line"></span><br><span class="line">training_set_scaled = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>,</span><br><span class="line">                       <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>,</span><br><span class="line">                       <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>]</span><br><span class="line"></span><br><span class="line">x_train = []</span><br><span class="line">y_train = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>, <span class="number">26</span>):</span><br><span class="line">    x_train.append(training_set_scaled[i - <span class="number">4</span>:i])</span><br><span class="line">    y_train.append(training_set_scaled[i])</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使x_train符合Embedding输入要求：[送入样本数， 循环核时间展开步数] ，</span></span><br><span class="line"><span class="comment"># 此处整个数据集送入所以送入，送入样本数为len(x_train)；输入4个字母出结果，循环核时间展开步数为4。</span></span><br><span class="line">x_train = np.reshape(x_train, (<span class="built_in">len</span>(x_train), <span class="number">4</span>))</span><br><span class="line">y_train = np.array(y_train)</span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    Embedding(<span class="number">26</span>, <span class="number">2</span>),</span><br><span class="line">    SimpleRNN(<span class="number">10</span>),</span><br><span class="line">    Dense(<span class="number">26</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">False</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/rnn_embedding_4pre1.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 save_best_only=<span class="literal">True</span>,</span><br><span class="line">                                                 monitor=<span class="string">&#x27;loss&#x27;</span>)  <span class="comment"># 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型</span></span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">100</span>, callbacks=[cp_callback])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)  <span class="comment"># 参数提取</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> model.trainable_variables:</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################    show   ###############################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示训练集和验证集的acc和loss曲线</span></span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">################# predict ##################</span></span><br><span class="line"></span><br><span class="line">preNum = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;input the number of test alphabet:&quot;</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(preNum):</span><br><span class="line">    alphabet1 = <span class="built_in">input</span>(<span class="string">&quot;input test alphabet:&quot;</span>)</span><br><span class="line">    alphabet = [w_to_id[a] <span class="keyword">for</span> a <span class="keyword">in</span> alphabet1]</span><br><span class="line">    <span class="comment"># 使alphabet符合Embedding输入要求：[送入样本数， 时间展开步数]。</span></span><br><span class="line">    <span class="comment"># 此处验证效果送入了1个样本，送入样本数为1；输入4个字母出结果，循环核时间展开步数为4。</span></span><br><span class="line">    alphabet = np.reshape(alphabet, (<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">    result = model.predict([alphabet])</span><br><span class="line">    pred = tf.argmax(result, axis=<span class="number">1</span>)</span><br><span class="line">    pred = <span class="built_in">int</span>(pred)</span><br><span class="line">    tf.<span class="built_in">print</span>(alphabet1 + <span class="string">&#x27;-&gt;&#x27;</span> + input_word[pred])</span><br></pre></td></tr></table></figure>
<h1 id="用RNN实现股票预测"><a href="#用RNN实现股票预测" class="headerlink" title="用RNN实现股票预测"></a>用RNN实现股票预测</h1><p>贵州茅台股票数据，只使用C列数据</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%94%A8RNN%E5%AE%9E%E7%8E%B0%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8B-1.png" class="" title="用RNN实现股票预测-1">
<p>用连续60天开盘价预测第61天开盘价，使用以下代码下载真实数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">df1 = ts.get_k_data(<span class="string">&#x27;600519&#x27;</span>, ktype=<span class="string">&#x27;D&#x27;</span>, start=<span class="string">&#x27;2010-04-26&#x27;</span>, end=<span class="string">&#x27;2020-04-26&#x27;</span>)</span><br><span class="line"></span><br><span class="line">datapath1 = <span class="string">&quot;./SH600519.csv&quot;</span></span><br><span class="line">df1.to_csv(datapath1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dropout, Dense, SimpleRNN</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">maotai = pd.read_csv(<span class="string">&#x27;./SH600519.csv&#x27;</span>)  <span class="comment"># 读取股票文件</span></span><br><span class="line"></span><br><span class="line">training_set = maotai.iloc[<span class="number">0</span>:<span class="number">2426</span> - <span class="number">300</span>, <span class="number">2</span>:<span class="number">3</span>].values  <span class="comment"># 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价</span></span><br><span class="line">test_set = maotai.iloc[<span class="number">2426</span> - <span class="number">300</span>:, <span class="number">2</span>:<span class="number">3</span>].values  <span class="comment"># 后300天的开盘价作为测试集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">sc = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># 定义归一化：归一化到(0，1)之间</span></span><br><span class="line">training_set_scaled = sc.fit_transform(training_set)  <span class="comment"># 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化</span></span><br><span class="line">test_set = sc.transform(test_set)  <span class="comment"># 利用训练集的属性对测试集进行归一化</span></span><br><span class="line"></span><br><span class="line">x_train = []</span><br><span class="line">y_train = []</span><br><span class="line"></span><br><span class="line">x_test = []</span><br><span class="line">y_test = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集：csv表格中前2426-300=2126天数据</span></span><br><span class="line"><span class="comment"># 利用for循环，遍历整个训练集，提取训练集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建2426-300-60=2066组数据。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>, <span class="built_in">len</span>(training_set_scaled)):</span><br><span class="line">    x_train.append(training_set_scaled[i - <span class="number">60</span>:i, <span class="number">0</span>])</span><br><span class="line">    y_train.append(training_set_scaled[i, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 对训练集进行打乱</span></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"><span class="comment"># 将训练集由list格式变为array格式</span></span><br><span class="line">x_train, y_train = np.array(x_train), np.array(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。</span></span><br><span class="line"><span class="comment"># 此处整个数据集送入，送入样本数为x_train.shape[0]即2066组数据；输入60个开盘价，预测出第61天的开盘价，循环核时间展开步数为60; 每个时间步送入的特征是某一天的开盘价，只有1个数据，故每个时间步输入特征个数为1</span></span><br><span class="line">x_train = np.reshape(x_train, (x_train.shape[<span class="number">0</span>], <span class="number">60</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 测试集：csv表格中后300天数据</span></span><br><span class="line"><span class="comment"># 利用for循环，遍历整个测试集，提取测试集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建300-60=240组数据。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>, <span class="built_in">len</span>(test_set)):</span><br><span class="line">    x_test.append(test_set[i - <span class="number">60</span>:i, <span class="number">0</span>])</span><br><span class="line">    y_test.append(test_set[i, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]</span></span><br><span class="line">x_test, y_test = np.array(x_test), np.array(y_test)</span><br><span class="line">x_test = np.reshape(x_test, (x_test.shape[<span class="number">0</span>], <span class="number">60</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    SimpleRNN(<span class="number">80</span>, return_sequences=<span class="literal">True</span>),</span><br><span class="line">    Dropout(<span class="number">0.2</span>),</span><br><span class="line">    SimpleRNN(<span class="number">100</span>),</span><br><span class="line">    Dropout(<span class="number">0.2</span>),</span><br><span class="line">    Dense(<span class="number">1</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(<span class="number">0.001</span>),</span><br><span class="line">              loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)  <span class="comment"># 损失函数用均方误差</span></span><br><span class="line"><span class="comment"># 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值</span></span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/rnn_stock.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 save_best_only=<span class="literal">True</span>,</span><br><span class="line">                                                 monitor=<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">64</span>, epochs=<span class="number">50</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)  <span class="comment"># 参数提取</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> model.trainable_variables:</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">################## predict ######################</span></span><br><span class="line"><span class="comment"># 测试集输入模型进行预测</span></span><br><span class="line">predicted_stock_price = model.predict(x_test)</span><br><span class="line"><span class="comment"># 对预测数据还原---从（0，1）反归一化到原始范围</span></span><br><span class="line">predicted_stock_price = sc.inverse_transform(predicted_stock_price)</span><br><span class="line"><span class="comment"># 对真实数据还原---从（0，1）反归一化到原始范围</span></span><br><span class="line">real_stock_price = sc.inverse_transform(test_set[<span class="number">60</span>:])</span><br><span class="line"><span class="comment"># 画出真实数据和预测数据的对比曲线</span></span><br><span class="line">plt.plot(real_stock_price, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.plot(predicted_stock_price, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Predicted MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;MaoTai Stock Price Prediction&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Time&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">##########evaluate##############</span></span><br><span class="line"><span class="comment"># calculate MSE 均方误差 ---&gt; E[(预测值-真实值)^2] (预测值减真实值求平方后求均值)</span></span><br><span class="line">mse = mean_squared_error(predicted_stock_price, real_stock_price)</span><br><span class="line"><span class="comment"># calculate RMSE 均方根误差---&gt;sqrt[MSE]    (对均方误差开方)</span></span><br><span class="line">rmse = math.sqrt(mean_squared_error(predicted_stock_price, real_stock_price))</span><br><span class="line"><span class="comment"># calculate MAE 平均绝对误差-----&gt;E[|预测值-真实值|](预测值减真实值求绝对值后求均值）</span></span><br><span class="line">mae = mean_absolute_error(predicted_stock_price, real_stock_price)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均方误差: %.6f&#x27;</span> % mse)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均方根误差: %.6f&#x27;</span> % rmse)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;平均绝对误差: %.6f&#x27;</span> % mae)</span><br></pre></td></tr></table></figure>
<h1 id="用LSTM实现股票预测"><a href="#用LSTM实现股票预测" class="headerlink" title="用LSTM实现股票预测"></a>用LSTM实现股票预测</h1><p>传统RNN可以通过记忆体实现短期记忆进行连续数据的预测，当连续数据的序列变长时，会使展开时间步过长，在反向传播更新参数时，梯度要按照时间步连续相乘，会导致梯度消失。</p>
<p>LSTM 由Hochreiter &amp; Schmidhuber 于1997年提出，通过门控单元改善了RNN长期依赖问题。<br>Sepp Hochreiter,Jurgen Schmidhuber.LONG SHORT-TERM MEMORY.Neural Computation,December 1997.</p>
<h2 id="LSTM计算过程"><a href="#LSTM计算过程" class="headerlink" title="LSTM计算过程"></a>LSTM计算过程</h2><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LSTM%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B-1.png" class="" title="LSTM计算过程-1">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LSTM%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B-2.png" class="" title="LSTM计算过程-2">
<p>LSTM 引入了三个门限：<br>输入门 <script type="math/tex">i_{t}</script> </p>
<p>遗忘门 <script type="math/tex">f_{t}</script></p>
<p>输出门 <script type="math/tex">o_{t}</script></p>
<p>引入了表征长期记忆的细胞态 $ C_{t} $</p>
<p>引入了等待存入长期记忆的候选态 $ \widetilde{C}_{t} $</p>
<p>三个门限都是当前时刻的输入特征 <script type="math/tex">x_{t}</script> 和上个时刻的短期记忆 <script type="math/tex">h_{t-1}</script> 的函数，分别表示为：</p>
<ul>
<li><p>输入门（门限）： <script type="math/tex">i_{t} = \sigma(W_{i} · [h_{t-1}, x_{t}] + b_{i} )</script>， 决定了多少比例的信息会被存入当前细胞态；</p>
</li>
<li><p>遗忘门（门限）： <script type="math/tex">f_{t} = \sigma (W_{f} · [ h_{t-1} , x_{t} ] + b_{f} )</script>， 将细胞态中的信息选择性的遗忘；</p>
</li>
<li><p>输出门（门限）： <script type="math/tex">o_{t} = \sigma( W_{o} · [h_{t-1}, x_{t}] + b_{o})</script>，将细胞态中的信息选择性的进行输出；</p>
</li>
</ul>
<p>三个公式中<script type="math/tex">W_{i}</script>、 <script type="math/tex">W_{f}</script> 和 <script type="math/tex">W_{o}</script> 是待训练参数矩阵， <script type="math/tex">b_{i}</script> 、 <script type="math/tex">b_{f}</script> 和 <script type="math/tex">b_{o}</script> 是待训练偏置项。 <script type="math/tex">\sigma</script>为 sigmoid 激活函数，它可以使门限的范围在 0 到 1 之间。</p>
<p>定义<script type="math/tex">h_{t}</script>为记忆体，它表征短期记忆，是当前细胞态经过输出门得到的：</p>
<ul>
<li>记忆体（短期记忆）： <script type="math/tex">h_{t} = o_{t} × tanh(C_{t})</script></li>
</ul>
<p>候选态表示归纳出的待存入细胞态的新知识， 是当前时刻的输入特征 <script type="math/tex">x_{t}</script> 和上个时刻的短期记忆 <script type="math/tex">h_{t-1}</script> 的函数：</p>
<ul>
<li>候选态（归纳出的新知识）： <script type="math/tex">\widetilde{C}_{t} = tanh( W_{c} · [h_{t-1}, x_{t}] + b_{c} )</script></li>
</ul>
<p>细胞态 <script type="math/tex">C_{t}</script> 表示长期记忆， 它等于上个时刻的长期记忆 <script type="math/tex">C_{t-1}</script> 通过遗忘门的值和当前时刻归纳出的新知识 <script type="math/tex">\widetilde{C}_{t}</script> 通过输入门的值之和：</p>
<ul>
<li>细胞态（长期记忆） ：<script type="math/tex">C_{t} = f_{t} × C_{t-1} + i_{t} × \widetilde{C}_{t}</script></li>
</ul>
<p>当明确了这些概念， 这里举一个简单的例子理解一下 LSTM:</p>
<p>假设 LSTM 就是我们听老师讲课的过程， 目前老师讲到了第 45 页 PPT。我们的脑袋里记住的内容，是 PPT 第 1 页到第 45 页的长期记忆 <script type="math/tex">C_{t}</script> 。 </p>
<p>它由两部分组成：一部分是 PPT 第 1 页到第 44 页的内容，也就是上一时刻的长期记忆 <script type="math/tex">C_{t-1}</script> 。我们不可能一字不差的记住全部内容，会不自觉地忘记了一些，所以上个时刻的长期记忆 <script type="math/tex">C_{t-1}</script> 要乘以遗忘门，这个乘积项就表示留存在我们脑中的对过去的记忆；另一部分是当前我们归纳出的新知识 <script type="math/tex">\widetilde{C}_{t}</script> ，它由老师正在讲的第 45 页 PPT(当前时刻的输入<script type="math/tex">x_{t}</script> ) 和第 44 页 PPT 的短期记忆留存(上一时刻的短期记忆 <script type="math/tex">h_{t-1}</script> )组成。</p>
<p>将现在的记忆 <script type="math/tex">\widetilde{C}_{t}</script> 乘以输入门后与过去的记忆一同存储为当前的长期记忆 <script type="math/tex">C_{t}</script>。接下来，如果我们想把我们学到的知识(当前的长期记忆 <script type="math/tex">C_{t}</script> )复述给朋友， 我们不可能一字不落的讲出来， 所以 <script type="math/tex">C_{t}</script> 需要经过输出门筛选后才成为了输出 <script type="math/tex">h_{t}</script> 。</p>
<p>当有多层循环网络时，第二层循环网络的输入 <script type="math/tex">x_{t}</script> 就是第一层循环网络的输出 <script type="math/tex">h_{t}</script> ，即输入第二层网络的是第一层网络提取出的精华。 可以这么想， 老师现在扮演的就是第一层循环网络，每一页 PPT 都是老师从一篇一篇论文中提取出的精华，输出给我们。 作为第二层循环网络的我们，接收到的数据就是老师的长期记忆 <script type="math/tex">C_{t}</script> 过 tanh 激活函数后乘以输出门提取出的短期记忆 <script type="math/tex">h_{t}</script> 。</p>
<h2 id="TF描述LSTM层"><a href="#TF描述LSTM层" class="headerlink" title="TF描述LSTM层"></a>TF描述LSTM层</h2><p><code>tf.keras.layers.LSTM(记忆体个数， return_sequences=是否返回输出)</code></p>
<p><code>return_sequences=True</code> 各时间步输出ht</p>
<p><code>return_sequences=False</code> 仅最后时间步输出ht（默认）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">	LSTM(<span class="number">80</span>, return_sequences=<span class="literal">True</span>),</span><br><span class="line">	Dropout(<span class="number">0.2</span>),</span><br><span class="line">	LSTM(<span class="number">100</span>),</span><br><span class="line">	Dropout(<span class="number">0.2</span>),</span><br><span class="line">	Dense(<span class="number">1</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dropout, Dense, LSTM</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">maotai = pd.read_csv(<span class="string">&#x27;./SH600519.csv&#x27;</span>)  <span class="comment"># 读取股票文件</span></span><br><span class="line"></span><br><span class="line">training_set = maotai.iloc[<span class="number">0</span>:<span class="number">2426</span> - <span class="number">300</span>, <span class="number">2</span>:<span class="number">3</span>].values  <span class="comment"># 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价</span></span><br><span class="line">test_set = maotai.iloc[<span class="number">2426</span> - <span class="number">300</span>:, <span class="number">2</span>:<span class="number">3</span>].values  <span class="comment"># 后300天的开盘价作为测试集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">sc = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># 定义归一化：归一化到(0，1)之间</span></span><br><span class="line">training_set_scaled = sc.fit_transform(training_set)  <span class="comment"># 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化</span></span><br><span class="line">test_set = sc.transform(test_set)  <span class="comment"># 利用训练集的属性对测试集进行归一化</span></span><br><span class="line"></span><br><span class="line">x_train = []</span><br><span class="line">y_train = []</span><br><span class="line"></span><br><span class="line">x_test = []</span><br><span class="line">y_test = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集：csv表格中前2426-300=2126天数据</span></span><br><span class="line"><span class="comment"># 利用for循环，遍历整个训练集，提取训练集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建2426-300-60=2066组数据。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>, <span class="built_in">len</span>(training_set_scaled)):</span><br><span class="line">    x_train.append(training_set_scaled[i - <span class="number">60</span>:i, <span class="number">0</span>])</span><br><span class="line">    y_train.append(training_set_scaled[i, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 对训练集进行打乱</span></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"><span class="comment"># 将训练集由list格式变为array格式</span></span><br><span class="line">x_train, y_train = np.array(x_train), np.array(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。</span></span><br><span class="line"><span class="comment"># 此处整个数据集送入，送入样本数为x_train.shape[0]即2066组数据；输入60个开盘价，预测出第61天的开盘价，循环核时间展开步数为60; 每个时间步送入的特征是某一天的开盘价，只有1个数据，故每个时间步输入特征个数为1</span></span><br><span class="line">x_train = np.reshape(x_train, (x_train.shape[<span class="number">0</span>], <span class="number">60</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 测试集：csv表格中后300天数据</span></span><br><span class="line"><span class="comment"># 利用for循环，遍历整个测试集，提取测试集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建300-60=240组数据。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>, <span class="built_in">len</span>(test_set)):</span><br><span class="line">    x_test.append(test_set[i - <span class="number">60</span>:i, <span class="number">0</span>])</span><br><span class="line">    y_test.append(test_set[i, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]</span></span><br><span class="line">x_test, y_test = np.array(x_test), np.array(y_test)</span><br><span class="line">x_test = np.reshape(x_test, (x_test.shape[<span class="number">0</span>], <span class="number">60</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">########</span></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    LSTM(<span class="number">80</span>, return_sequences=<span class="literal">True</span>),</span><br><span class="line">    Dropout(<span class="number">0.2</span>),</span><br><span class="line">    LSTM(<span class="number">100</span>),</span><br><span class="line">    Dropout(<span class="number">0.2</span>),</span><br><span class="line">    Dense(<span class="number">1</span>)</span><br><span class="line">])</span><br><span class="line"><span class="comment">########</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(<span class="number">0.001</span>),</span><br><span class="line">              loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)  <span class="comment"># 损失函数用均方误差</span></span><br><span class="line"><span class="comment"># 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值</span></span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/LSTM_stock.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 save_best_only=<span class="literal">True</span>,</span><br><span class="line">                                                 monitor=<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">64</span>, epochs=<span class="number">50</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)  <span class="comment"># 参数提取</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> model.trainable_variables:</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">################## predict ######################</span></span><br><span class="line"><span class="comment"># 测试集输入模型进行预测</span></span><br><span class="line">predicted_stock_price = model.predict(x_test)</span><br><span class="line"><span class="comment"># 对预测数据还原---从（0，1）反归一化到原始范围</span></span><br><span class="line">predicted_stock_price = sc.inverse_transform(predicted_stock_price)</span><br><span class="line"><span class="comment"># 对真实数据还原---从（0，1）反归一化到原始范围</span></span><br><span class="line">real_stock_price = sc.inverse_transform(test_set[<span class="number">60</span>:])</span><br><span class="line"><span class="comment"># 画出真实数据和预测数据的对比曲线</span></span><br><span class="line">plt.plot(real_stock_price, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.plot(predicted_stock_price, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Predicted MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;MaoTai Stock Price Prediction&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Time&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">##########evaluate##############</span></span><br><span class="line"><span class="comment"># calculate MSE 均方误差 ---&gt; E[(预测值-真实值)^2] (预测值减真实值求平方后求均值)</span></span><br><span class="line">mse = mean_squared_error(predicted_stock_price, real_stock_price)</span><br><span class="line"><span class="comment"># calculate RMSE 均方根误差---&gt;sqrt[MSE]    (对均方误差开方)</span></span><br><span class="line">rmse = math.sqrt(mean_squared_error(predicted_stock_price, real_stock_price))</span><br><span class="line"><span class="comment"># calculate MAE 平均绝对误差-----&gt;E[|预测值-真实值|](预测值减真实值求绝对值后求均值）</span></span><br><span class="line">mae = mean_absolute_error(predicted_stock_price, real_stock_price)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均方误差: %.6f&#x27;</span> % mse)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均方根误差: %.6f&#x27;</span> % rmse)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;平均绝对误差: %.6f&#x27;</span> % mae)</span><br></pre></td></tr></table></figure>
<h1 id="用GRU实现股票预测"><a href="#用GRU实现股票预测" class="headerlink" title="用GRU实现股票预测"></a>用GRU实现股票预测</h1><p>GRU由Cho等人于2014年提出，优化LSTM结构。<br>Kyunghyun Cho,Bart van Merrienboer,Caglar Gulcehre,Dzmitry Bahdanau,Fethi Bougares,Holger<br>Schwenk,Yoshua Bengio.Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation.Computer ence, 2014</p>
<p>门控循环单元(Gated Recurrent Unit， GRU)是 LSTM 的一种变体，将 LSTM 中遗忘门与输入门合二为一为更新门，模型比 LSTM 模型更简单。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/GRU%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B-1.png" class="" title="GRU计算过程-1">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/GRU%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B-2.png" class="" title="GRU计算过程-2">
<p>GRU 使记忆体 <script type="math/tex">h_{t}</script> 融合了长期记忆和短期记忆。   <script type="math/tex">h_{t}</script> 包含了过去信息  <script type="math/tex">h_{t-1}</script> 和现在信息(候选隐藏层)  <script type="math/tex">\widetilde{h}_{t}</script> ， 由更新门 <script type="math/tex">z_{t}</script> 分配重要性：</p>
<ul>
<li>记忆体：<script type="math/tex">h_{t} = (1 - z_{t}) × h_{t-1} + z_{t} * \widetilde{h}_{t}</script></li>
</ul>
<p>现在信息是过去信息 <script type="math/tex">h_{t-1}</script>  过重置门 <script type="math/tex">r_{t}</script>  与当前输入 <script type="math/tex">x_{t}</script>  共同决定的：</p>
<ul>
<li>候选隐藏层： <script type="math/tex">\widetilde{h}_{t} = tanh(W · [r_{t} × h_{t-1}， x_{t}])</script></li>
</ul>
<p>更新门和重置门的取值范围也是 0 到 1 之间：</p>
<ul>
<li>更新门： <script type="math/tex">z_{t}  = \sigma(W_{z} · [h_{t-1}, x_{t})]</script></li>
<li>重置门： <script type="math/tex">r_{t}  = \sigma(W_{z} · [h_{t-1}, x_{t})]</script></li>
</ul>
<h2 id="TF描述GRU层"><a href="#TF描述GRU层" class="headerlink" title="TF描述GRU层"></a>TF描述GRU层</h2><p><code>tf.keras.layers.GRU(记忆体个数, return_sequences=是否返回输出)</code></p>
<p><code>return_sequences=True</code> 各时间步输出ht</p>
<p><code>return_sequences=False</code> 仅最后时间步输出ht（默认）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">	GRU(<span class="number">80</span>, return_sequences=<span class="literal">True</span>),</span><br><span class="line">	Dropout(<span class="number">0.2</span>),</span><br><span class="line">	LSTM(<span class="number">100</span>),</span><br><span class="line">	Dropout(<span class="number">0.2</span>),</span><br><span class="line">	Dense(<span class="number">1</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dropout, Dense, GRU</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">maotai = pd.read_csv(<span class="string">&#x27;./SH600519.csv&#x27;</span>)  <span class="comment"># 读取股票文件</span></span><br><span class="line"></span><br><span class="line">training_set = maotai.iloc[<span class="number">0</span>:<span class="number">2426</span> - <span class="number">300</span>, <span class="number">2</span>:<span class="number">3</span>].values  <span class="comment"># 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价</span></span><br><span class="line">test_set = maotai.iloc[<span class="number">2426</span> - <span class="number">300</span>:, <span class="number">2</span>:<span class="number">3</span>].values  <span class="comment"># 后300天的开盘价作为测试集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">sc = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># 定义归一化：归一化到(0，1)之间</span></span><br><span class="line">training_set_scaled = sc.fit_transform(training_set)  <span class="comment"># 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化</span></span><br><span class="line">test_set = sc.transform(test_set)  <span class="comment"># 利用训练集的属性对测试集进行归一化</span></span><br><span class="line"></span><br><span class="line">x_train = []</span><br><span class="line">y_train = []</span><br><span class="line"></span><br><span class="line">x_test = []</span><br><span class="line">y_test = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集：csv表格中前2426-300=2126天数据</span></span><br><span class="line"><span class="comment"># 利用for循环，遍历整个训练集，提取训练集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建2426-300-60=2066组数据。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>, <span class="built_in">len</span>(training_set_scaled)):</span><br><span class="line">    x_train.append(training_set_scaled[i - <span class="number">60</span>:i, <span class="number">0</span>])</span><br><span class="line">    y_train.append(training_set_scaled[i, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 对训练集进行打乱</span></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"><span class="comment"># 将训练集由list格式变为array格式</span></span><br><span class="line">x_train, y_train = np.array(x_train), np.array(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。</span></span><br><span class="line"><span class="comment"># 此处整个数据集送入，送入样本数为x_train.shape[0]即2066组数据；输入60个开盘价，预测出第61天的开盘价，循环核时间展开步数为60; 每个时间步送入的特征是某一天的开盘价，只有1个数据，故每个时间步输入特征个数为1</span></span><br><span class="line">x_train = np.reshape(x_train, (x_train.shape[<span class="number">0</span>], <span class="number">60</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 测试集：csv表格中后300天数据</span></span><br><span class="line"><span class="comment"># 利用for循环，遍历整个测试集，提取测试集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建300-60=240组数据。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>, <span class="built_in">len</span>(test_set)):</span><br><span class="line">    x_test.append(test_set[i - <span class="number">60</span>:i, <span class="number">0</span>])</span><br><span class="line">    y_test.append(test_set[i, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]</span></span><br><span class="line">x_test, y_test = np.array(x_test), np.array(y_test)</span><br><span class="line">x_test = np.reshape(x_test, (x_test.shape[<span class="number">0</span>], <span class="number">60</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">##################</span></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    GRU(<span class="number">80</span>, return_sequences=<span class="literal">True</span>),</span><br><span class="line">    Dropout(<span class="number">0.2</span>),</span><br><span class="line">    GRU(<span class="number">100</span>),</span><br><span class="line">    Dropout(<span class="number">0.2</span>),</span><br><span class="line">    Dense(<span class="number">1</span>)</span><br><span class="line">])</span><br><span class="line"><span class="comment">##################</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(<span class="number">0.001</span>),</span><br><span class="line">              loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)  <span class="comment"># 损失函数用均方误差</span></span><br><span class="line"><span class="comment"># 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值</span></span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/stock.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 save_best_only=<span class="literal">True</span>,</span><br><span class="line">                                                 monitor=<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">64</span>, epochs=<span class="number">50</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)  <span class="comment"># 参数提取</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> model.trainable_variables:</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">################## predict ######################</span></span><br><span class="line"><span class="comment"># 测试集输入模型进行预测</span></span><br><span class="line">predicted_stock_price = model.predict(x_test)</span><br><span class="line"><span class="comment"># 对预测数据还原---从（0，1）反归一化到原始范围</span></span><br><span class="line">predicted_stock_price = sc.inverse_transform(predicted_stock_price)</span><br><span class="line"><span class="comment"># 对真实数据还原---从（0，1）反归一化到原始范围</span></span><br><span class="line">real_stock_price = sc.inverse_transform(test_set[<span class="number">60</span>:])</span><br><span class="line"><span class="comment"># 画出真实数据和预测数据的对比曲线</span></span><br><span class="line">plt.plot(real_stock_price, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.plot(predicted_stock_price, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Predicted MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;MaoTai Stock Price Prediction&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Time&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MaoTai Stock Price&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">##########evaluate##############</span></span><br><span class="line"><span class="comment"># calculate MSE 均方误差 ---&gt; E[(预测值-真实值)^2] (预测值减真实值求平方后求均值)</span></span><br><span class="line">mse = mean_squared_error(predicted_stock_price, real_stock_price)</span><br><span class="line"><span class="comment"># calculate RMSE 均方根误差---&gt;sqrt[MSE]    (对均方误差开方)</span></span><br><span class="line">rmse = math.sqrt(mean_squared_error(predicted_stock_price, real_stock_price))</span><br><span class="line"><span class="comment"># calculate MAE 平均绝对误差-----&gt;E[|预测值-真实值|](预测值减真实值求绝对值后求均值）</span></span><br><span class="line">mae = mean_absolute_error(predicted_stock_price, real_stock_price)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均方误差: %.6f&#x27;</span> % mse)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均方根误差: %.6f&#x27;</span> % rmse)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;平均绝对误差: %.6f&#x27;</span> % mae)</span><br></pre></td></tr></table></figure></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2023/02/16/人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络/">http://hibiscidai.com/2023/02/16/人工智能实践-Tensorflow笔记-MOOC-第六讲循环神经网络/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/TensorflowMOOC/">TensorflowMOOC</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/zhifubaodashang.jpg"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/weixin.png"><div class="post-qr-code__desc">微信打赏</div></div></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><i class="fa fa-chevron-left">  </i><span>人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络</span></a></div><div class="next-post pull-right"><a href="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/"><span>人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2023 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>