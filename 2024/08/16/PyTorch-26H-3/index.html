<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="PyTorch-26H-3"><meta name="keywords" content="学习笔记,PyTorch"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>PyTorch-26H-3 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-26H-3"><span class="toc-number">1.</span> <span class="toc-text">PyTorch-26H-3</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#What-is-a-classification-problem-%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">What is a classification problem? 什么是分类问题？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#What-we%E2%80%99re-going-to-cover-%E6%88%91%E4%BB%AC%E5%B0%86%E8%A6%81%E8%AE%A8%E8%AE%BA%E7%9A%84%E5%86%85%E5%AE%B9"><span class="toc-number">3.</span> <span class="toc-text">What we’re going to cover 我们将要讨论的内容</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0-Architecture-of-a-classification-neural-network-%E5%88%86%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="toc-number">4.</span> <span class="toc-text">0. Architecture of a classification neural network 分类神经网络的架构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Make-classification-data-and-get-it-ready-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%88%B6%E4%BD%9C%E5%8F%8A%E5%87%86%E5%A4%87"><span class="toc-number">5.</span> <span class="toc-text">1. Make classification data and get it ready 分类数据制作及准备</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-Input-and-output-shapes-%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E5%BD%A2%E7%8A%B6"><span class="toc-number">5.1.</span> <span class="toc-text">1.1 Input and output shapes 输入和输出形状</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-Turn-data-into-tensors-and-create-train-and-test-splits-%E5%B0%86%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%BC%A0%E9%87%8F%E5%B9%B6%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E5%88%86%E5%89%B2"><span class="toc-number">5.2.</span> <span class="toc-text">1.2 Turn data into tensors and create train and test splits 将数据转换为张量并创建训练和测试分割</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Building-a-model-%E5%BB%BA%E7%AB%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.</span> <span class="toc-text">2. Building a model 建立模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Setup-loss-function-and-optimizer"><span class="toc-number">6.1.</span> <span class="toc-text">2.1 Setup loss function and optimizer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Train-model"><span class="toc-number">7.</span> <span class="toc-text">3. Train model</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Going-from-raw-model-outputs-to-predicted-labels-logits-gt-prediction-probabilities-gt-prediction-labels-%E4%BB%8E%E5%8E%9F%E5%A7%8B%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E5%88%B0%E9%A2%84%E6%B5%8B%E6%A0%87%E7%AD%BE%EF%BC%88logits-gt-%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87-gt-%E9%A2%84%E6%B5%8B%E6%A0%87%E7%AD%BE%EF%BC%89"><span class="toc-number">7.1.</span> <span class="toc-text">3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels) 从原始模型输出到预测标签（logits -&gt; 预测概率 -&gt; 预测标签）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Building-a-training-and-testing-loop-%E5%BB%BA%E7%AB%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E5%BE%AA%E7%8E%AF"><span class="toc-number">7.2.</span> <span class="toc-text">3.2 Building a training and testing loop 建立训练和测试循环</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Make-predictions-and-evaluate-the-model-%E5%81%9A%E5%87%BA%E9%A2%84%E6%B5%8B%E5%B9%B6%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.</span> <span class="toc-text">4. Make predictions and evaluate the model 做出预测并评估模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Improving-a-model-from-a-model-perspective-%E6%94%B9%E8%BF%9B%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BB%8E%E6%A8%A1%E5%9E%8B%E8%A7%92%E5%BA%A6%EF%BC%89"><span class="toc-number">9.</span> <span class="toc-text">5. Improving a model (from a model perspective) 改进模型（从模型角度）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-Preparing-data-to-see-if-our-model-can-model-a-straight-line-%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%EF%BC%8C%E7%9C%8B%E7%9C%8B%E6%88%91%E4%BB%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E8%83%BD%E5%BB%BA%E6%A8%A1%E7%9B%B4%E7%BA%BF"><span class="toc-number">9.1.</span> <span class="toc-text">5.1 Preparing data to see if our model can model a straight line 准备数据，看看我们的模型是否能建模直线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Adjusting-model-1-to-fit-a-straight-line-%E8%B0%83%E6%95%B4-model-1-%E4%BB%A5%E9%80%82%E5%90%88%E7%9B%B4%E7%BA%BF"><span class="toc-number">9.2.</span> <span class="toc-text">5.2 Adjusting model_1 to fit a straight line 调整 model_1 以适合直线</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-The-missing-piece-non-linearity-%E7%BC%BA%E5%A4%B1%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%9A%E9%9D%9E%E7%BA%BF%E6%80%A7"><span class="toc-number">10.</span> <span class="toc-text">6. The missing piece: non-linearity 缺失的部分：非线性</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-Recreating-non-linear-data-red-and-blue-circles-%E9%87%8D%E6%96%B0%E5%88%9B%E5%BB%BA%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BA%A2%E8%89%B2%E5%92%8C%E8%93%9D%E8%89%B2%E5%9C%86%E5%9C%88%EF%BC%89"><span class="toc-number">10.1.</span> <span class="toc-text">6.1 Recreating non-linear data (red and blue circles) 重新创建非线性数据（红色和蓝色圆圈）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-Building-a-model-with-non-linearity-%E5%BB%BA%E7%AB%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.2.</span> <span class="toc-text">6.2 Building a model with non-linearity 建立非线性模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Training-a-model-with-non-linearity-%E8%AE%AD%E7%BB%83%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.3.</span> <span class="toc-text">6.3 Training a model with non-linearity 训练非线性模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-Evaluating-a-model-trained-with-non-linear-activation-functions-%E8%AF%84%E4%BC%B0%E7%94%A8%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%AE%AD%E7%BB%83%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.4.</span> <span class="toc-text">6.4 Evaluating a model trained with non-linear activation functions 评估用非线性激活函数训练的模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Replicating-non-linear-activation-functions-%E5%A4%8D%E5%88%B6%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">11.</span> <span class="toc-text">7. Replicating non-linear activation functions 复制非线性激活函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Putting-things-together-by-building-a-multi-class-PyTorch-model-%E9%80%9A%E8%BF%87%E6%9E%84%E5%BB%BA%E5%A4%9A%E7%B1%BB-PyTorch-%E6%A8%A1%E5%9E%8B%E5%B0%86%E6%89%80%E6%9C%89%E5%86%85%E5%AE%B9%E6%95%B4%E5%90%88%E5%9C%A8%E4%B8%80%E8%B5%B7"><span class="toc-number">12.</span> <span class="toc-text">8. Putting things together by building a multi-class PyTorch model 通过构建多类 PyTorch 模型将所有内容整合在一起</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-Creating-multi-class-classification-data-%E5%88%9B%E5%BB%BA%E5%A4%9A%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE"><span class="toc-number">12.1.</span> <span class="toc-text">8.1 Creating multi-class classification data 创建多类别分类数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-Building-a-multi-class-classification-model-in-PyTorch-%E5%9C%A8-PyTorch-%E4%B8%AD%E6%9E%84%E5%BB%BA%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">12.2.</span> <span class="toc-text">8.2 Building a multi-class classification model in PyTorch 在 PyTorch 中构建多类分类模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-Creating-a-loss-function-and-optimizer-for-a-multi-class-PyTorch-model-%E4%B8%BA%E5%A4%9A%E7%B1%BB-PyTorch-%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">12.3.</span> <span class="toc-text">8.3 Creating a loss function and optimizer for a multi-class PyTorch model 为多类 PyTorch 模型创建损失函数和优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-Getting-prediction-probabilities-for-a-multi-class-PyTorch-model-%E8%8E%B7%E5%8F%96%E5%A4%9A%E7%B1%BB-PyTorch-%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87"><span class="toc-number">12.4.</span> <span class="toc-text">8.4 Getting prediction probabilities for a multi-class PyTorch model 获取多类 PyTorch 模型的预测概率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-Creating-a-training-and-testing-loop-for-a-multi-class-PyTorch-model-%E4%B8%BA%E5%A4%9A%E7%B1%BB-PyTorch-%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E5%BE%AA%E7%8E%AF"><span class="toc-number">12.5.</span> <span class="toc-text">8.5 Creating a training and testing loop for a multi-class PyTorch model 为多类 PyTorch 模型创建训练和测试循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-6-Making-and-evaluating-predictions-with-a-PyTorch-multi-class-model-%E4%BD%BF%E7%94%A8-PyTorch-%E5%A4%9A%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%E5%B9%B6%E8%AF%84%E4%BC%B0%E9%A2%84%E6%B5%8B"><span class="toc-number">12.6.</span> <span class="toc-text">8.6 Making and evaluating predictions with a PyTorch multi-class model 使用 PyTorch 多类模型进行预测并评估预测</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-More-classification-evaluation-metrics-%E6%9B%B4%E5%A4%9A%E5%88%86%E7%B1%BB%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">13.</span> <span class="toc-text">9. More classification evaluation metrics 更多分类评估指标</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Exercises"><span class="toc-number">14.</span> <span class="toc-text">Exercises</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Extra-curriculum-%E8%AF%BE%E5%A4%96%E6%B4%BB%E5%8A%A8"><span class="toc-number">15.</span> <span class="toc-text">Extra-curriculum 课外活动</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">243</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">88</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">33</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">PyTorch-26H-3</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2024-08-16</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/PyTorch/">PyTorch</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">14.9k</span><span class="post-meta__separator">|</span><span>阅读时长: 62 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3.png" class="" title="PyTorch-26H-3">
<p>PyTorch-26H-3</p>
<span id="more"></span>
<h1 id="PyTorch-26H-3"><a href="#PyTorch-26H-3" class="headerlink" title="PyTorch-26H-3"></a>PyTorch-26H-3</h1><p>主页：<a target="_blank" rel="noopener" href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p>
<p>youtub：<a target="_blank" rel="noopener" href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p>
<p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a target="_blank" rel="noopener" href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p>
<p>PyTorch documentation：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>
<h1 id="What-is-a-classification-problem-什么是分类问题？"><a href="#What-is-a-classification-problem-什么是分类问题？" class="headerlink" title="What is a classification problem? 什么是分类问题？"></a>What is a classification problem? 什么是分类问题？</h1><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Statistical_classification">classification problem</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">问题类型</th>
<th style="text-align:center">解释</th>
<th style="text-align:center">例子</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">二元分类(Binary classification)</td>
<td style="text-align:center">目标可以是两个选项之一，例如是或否</td>
<td style="text-align:center">根据某人的健康参数预测他是否患有心脏病。</td>
</tr>
<tr>
<td style="text-align:center">多类别分类(Multi-class classification)</td>
<td style="text-align:center">目标可以是两个以上选项之一</td>
<td style="text-align:center">确定照片中是食物、人还是狗。</td>
</tr>
<tr>
<td style="text-align:center">多标签分类(Multi-label classification)</td>
<td style="text-align:center">目标可以分配多个选项</td>
<td style="text-align:center">预测应为维基百科文章分配哪些类别（例如数学、科学和哲学）。</td>
</tr>
</tbody>
</table>
</div>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-1.png" class="" title="PyTorch-26H-3-1">
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-2.png" class="" title="PyTorch-26H-3-2">
<p>分类和回归是最常见的机器学习问题类型之一。</p>
<p>换句话说，获取一组输入并预测该组输入属于哪个类别。</p>
<h1 id="What-we’re-going-to-cover-我们将要讨论的内容"><a href="#What-we’re-going-to-cover-我们将要讨论的内容" class="headerlink" title="What we’re going to cover 我们将要讨论的内容"></a>What we’re going to cover 我们将要讨论的内容</h1><ul>
<li>Architecture of a neural network classification model</li>
<li><p>神经网络分类模型的架构</p>
</li>
<li><p>Input shapes and output shapes of a classification model (features and labels)</p>
</li>
<li><p>分类模型的输入形状和输出形状（特征和标签）</p>
</li>
<li><p>Creating custom data to view, fit on and predict on</p>
</li>
<li><p>创建自定义数据以查看、拟合和预测</p>
</li>
<li><p>Steps in modelling</p>
</li>
<li><p>建模步骤</p>
</li>
<li><p>Creating a model, setting a loss function and optimiser, creating a training loop, evaluating a<br>model</p>
</li>
<li><p>创建模型、设置损失函数和优化器、创建训练循环、评估</p>
</li>
<li><p>Saving and loading models</p>
</li>
<li><p>保存和加载模型</p>
</li>
<li><p>Harnessing the power of non-linearity</p>
</li>
<li><p>利用非线性的力量</p>
</li>
<li><p>Different classification evaluation methods</p>
</li>
<li>不同的分类评估方法</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">话题</th>
<th style="text-align:center">内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0. 分类神经网络的架构</td>
<td style="text-align:center">神经网络几乎可以具有任何形状和大小，但它们通常遵循类似的平面图。</td>
</tr>
<tr>
<td style="text-align:center">1. 准备二元分类数据</td>
<td style="text-align:center">数据几乎可以是任何东西，但首先我们将创建一个简单的二元分类数据集。</td>
</tr>
<tr>
<td style="text-align:center">2.构建 PyTorch 分类模型</td>
<td style="text-align:center">在这里我们将创建一个模型来学习数据中的模式，我们还将选择一个损失函数、优化器并构建一个特定于分类的训练循环。</td>
</tr>
<tr>
<td style="text-align:center">3. 将模型拟合到数据（训练）</td>
<td style="text-align:center">我们有数据和模型，现在让我们让模型（尝试）在（训练）数据中寻找模式。</td>
</tr>
<tr>
<td style="text-align:center">4. 做出预测并评估模型（推理）</td>
<td style="text-align:center">我们的模型在数据中发现了模式，让我们将它的发现与实际（测试）数据进行比较。</td>
</tr>
<tr>
<td style="text-align:center">5. 改进模型（从模型角度）</td>
<td style="text-align:center">我们已经训练并评估了一个模型，但它不起作用，让我们尝试一些方法来改进它。</td>
</tr>
<tr>
<td style="text-align:center">6.非线性</td>
<td style="text-align:center">到目前为止，我们的模型只具有对直线进行建模的能力，那么非线性（非直线）线又如何呢？</td>
</tr>
<tr>
<td style="text-align:center">7. 复制非线性函数</td>
<td style="text-align:center">我们使用非线性函数来帮助建模非线性数据，但是这些函数是什么样子的？</td>
</tr>
<tr>
<td style="text-align:center">8. 将所有内容与多类别分类结合起来</td>
<td style="text-align:center">让我们将迄今为止为二元分类所做的一切与多类分类问题放在一起。</td>
</tr>
</tbody>
</table>
</div>
<h1 id="0-Architecture-of-a-classification-neural-network-分类神经网络的架构"><a href="#0-Architecture-of-a-classification-neural-network-分类神经网络的架构" class="headerlink" title="0. Architecture of a classification neural network 分类神经网络的架构"></a>0. Architecture of a classification neural network 分类神经网络的架构</h1><p>分类神经网络的一般架构：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">超参数</th>
<th style="text-align:center">二元分类</th>
<th style="text-align:center">多类分类</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输入层形状 Input layer shape (in_features)</td>
<td style="text-align:center">与特征数量相同（例如，心脏病预测中的年龄、性别、身高、体重、吸烟状况为 5）</td>
<td style="text-align:center">与二元分类相同</td>
</tr>
<tr>
<td style="text-align:center">隐藏层 Hidden layer(s)</td>
<td style="text-align:center">针对具体问题，最小值 = 1，最大值 = 无限制</td>
<td style="text-align:center">与二元分类相同</td>
</tr>
<tr>
<td style="text-align:center">每个隐藏层的神经元 Neurons per hidden layer</td>
<td style="text-align:center">具体问题具体分析，一般为 10 到 512</td>
<td style="text-align:center">与二元分类相同</td>
</tr>
<tr>
<td style="text-align:center">输出层形状 Output layer shape (out_features)</td>
<td style="text-align:center">1（一个类或另一个类）</td>
<td style="text-align:center">每类 1 张（例如，食物、人物或狗的照片各 3 张）</td>
</tr>
<tr>
<td style="text-align:center">隐藏层激活 Hidden layer activation</td>
<td style="text-align:center">通常是<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU</a>（整流线性单元），<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions">其他激活</a></td>
<td style="text-align:center">与二元分类相同</td>
</tr>
<tr>
<td style="text-align:center">输出激活 Output activation</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a> <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html">torch.sigmoid</a></td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a> <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">torch.softmax</a></td>
</tr>
<tr>
<td style="text-align:center">损失函数 Loss function</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cross-entropy#Cross-entropy_loss_function_and_logistic_regression">二元交叉熵Binary crossentropy</a> <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">torch.nn.BCELoss</a></td>
<td style="text-align:center">交叉熵 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">torch.nn.CrossEntropyLoss</a></td>
</tr>
<tr>
<td style="text-align:center">优化器 Optimizer</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD stochastic gradient descent</a> ，<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam</a>，<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch.optim</a></td>
<td style="text-align:center">与二元分类相同</td>
</tr>
</tbody>
</table>
</div>
<p>这个分类神经网络组件的成分列表会根据您正在处理的问题而有所不同。</p>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-2_1.png" class="" title="PyTorch-26H-3-2_1">
<h1 id="1-Make-classification-data-and-get-it-ready-分类数据制作及准备"><a href="#1-Make-classification-data-and-get-it-ready-分类数据制作及准备" class="headerlink" title="1. Make classification data and get it ready 分类数据制作及准备"></a>1. Make classification data and get it ready 分类数据制作及准备</h1><p>使用 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html">make_circles()</a> 中的 <code>Scikit-Learn</code> 方法生成两个具有不同颜色的圆圈。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conda install scikit-learn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make 1000 samples </span></span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create circles</span></span><br><span class="line">X, y = make_circles(n_samples,</span><br><span class="line">                    noise = <span class="number">0.03</span>, <span class="comment"># a little bit of noise to the dots</span></span><br><span class="line">                    random_state = <span class="number">42</span>) <span class="comment"># keep random state so we get the same values</span></span><br></pre></td></tr></table></figure>
<p>查看前5个X值y。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First 5 X features:\n<span class="subst">&#123;X[:<span class="number">5</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nFirst 5 y labels:\n<span class="subst">&#123;y[:<span class="number">5</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">First 5 X features:</span><br><span class="line">[[ 0.75424625  0.23148074]</span><br><span class="line"> [-0.75615888  0.15325888]</span><br><span class="line"> [-0.81539193  0.17328203]</span><br><span class="line"> [-0.39373073  0.69288277]</span><br><span class="line"> [ 0.44220765 -0.89672343]]</span><br><span class="line"></span><br><span class="line">First 5 y labels:</span><br><span class="line">[1 1 1 1 0]</span><br></pre></td></tr></table></figure>
<p>可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make DataFrame of circle data</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">circles = pd.DataFrame(&#123;<span class="string">&quot;X1&quot;</span>: X[:, <span class="number">0</span>],</span><br><span class="line">    				   <span class="string">&quot;X2&quot;</span>: X[:, <span class="number">1</span>],</span><br><span class="line">    				   <span class="string">&quot;label&quot;</span>: y&#125;)</span><br><span class="line">circles.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">	X1	X2	label</span><br><span class="line">0	0.754246	0.231481	1</span><br><span class="line">1	-0.756159	0.153259	1</span><br><span class="line">2	-0.815392	0.173282	1</span><br><span class="line">3	-0.393731	0.692883	1</span><br><span class="line">4	0.442208	-0.896723	0</span><br><span class="line">5	-0.479646	0.676435	1</span><br><span class="line">6	-0.013648	0.803349	1</span><br><span class="line">7	0.771513	0.147760	1</span><br><span class="line">8	-0.169322	-0.793456	1</span><br><span class="line">9	-0.121486	1.021509	0</span><br></pre></td></tr></table></figure>
<p>看起来每对X特征（X1和X2）都有一个标签（y）值，即 0 或 1。</p>
<p>这告诉我们我们的问题是二元分类，因为只有两个选项（0 或 1）。</p>
<p>每个类别有多少个值？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check different labels</span></span><br><span class="line">circles.label.value_counts()</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1    500</span><br><span class="line">0    500</span><br><span class="line">Name: label, dtype: int64</span><br></pre></td></tr></table></figure>
<p>0和1各五百个</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize with a plot</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(x=X[:, <span class="number">0</span>], </span><br><span class="line">            y=X[:, <span class="number">1</span>], </span><br><span class="line">            c=y, </span><br><span class="line">            cmap=plt.cm.RdYlBu);</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-3.png" class="" title="PyTorch-26H-3-3">
<p>如何构建 PyTorch 神经网络来将点分类为红色（0）或蓝色（1）。</p>
<blockquote>
<p>在机器学习中，这个数据集通常被视为玩具问题（用于尝试和测试事物的问题）。但它代表了分类的主要关键，您有一些以数值表示的数据，并且您想要构建一个能够对其进行分类的模型，在我们的例子中，将其分成红点或蓝点。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://scikit-learn.org/1.5/datasets/toy_dataset.html">scikit-learn-toy datasets</a></p>
<h2 id="1-1-Input-and-output-shapes-输入和输出形状"><a href="#1-1-Input-and-output-shapes-输入和输出形状" class="headerlink" title="1.1 Input and output shapes 输入和输出形状"></a>1.1 Input and output shapes 输入和输出形状</h2><img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-3_1.png" class="" title="PyTorch-26H-3-3_1">
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-3_2.png" class="" title="PyTorch-26H-3-3_2">
<p>可以设置32的batch size。使用大型 minibatch 进行训练对测试错误不利。</p>
<p>深度学习中最常见的错误之一是形状错误。</p>
<p>张量形状和张量运算不匹配将导致模型出现错误。</p>
<p>我们将会在整个课程中看到很多这样的情况。</p>
<p>输入和输出形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the shapes of our features and labels</span></span><br><span class="line">X.shape, y.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((1000, 2), (1000,))</span><br></pre></td></tr></table></figure>
<p>看起来我们在每个维度的第一维度上都找到了匹配项。</p>
<p>有 1000 个 X 和 1000 个 y。</p>
<p>但是 X 的第二维度是什么？</p>
<p>查看单个样本（特征和标签）的值和形状通常很有帮助。</p>
<p>这样做将帮助您了解您希望从模型中获得什么样的输入和输出形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View the first example of features and labels</span></span><br><span class="line">X_sample = X[<span class="number">0</span>]</span><br><span class="line">y_sample = y[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Values for one sample of X: <span class="subst">&#123;X_sample&#125;</span> and the same for y: <span class="subst">&#123;y_sample&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shapes for one sample of X: <span class="subst">&#123;X_sample.shape&#125;</span> and the same for y: <span class="subst">&#123;y_sample.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1</span><br><span class="line">Shapes for one sample of X: (2,) and the same for y: ()</span><br></pre></td></tr></table></figure>
<p>这告诉我们 X 的第二个维度意味着它有两个特征（向量vector），而 y 只有一个特征（标量scalar）。</p>
<p>我们有两个输入和一个输出。</p>
<h2 id="1-2-Turn-data-into-tensors-and-create-train-and-test-splits-将数据转换为张量并创建训练和测试分割"><a href="#1-2-Turn-data-into-tensors-and-create-train-and-test-splits-将数据转换为张量并创建训练和测试分割" class="headerlink" title="1.2 Turn data into tensors and create train and test splits 将数据转换为张量并创建训练和测试分割"></a>1.2 Turn data into tensors and create train and test splits 将数据转换为张量并创建训练和测试分割</h2><p>1、将我们的数据转换成张量（现在我们的数据在 NumPy 数组中，PyTorch 更喜欢使用 PyTorch 张量）。<br>2、<code>X</code>将我们的数据分成训练集和测试集（我们将在训练集上训练一个模型来学习和之间的模式，<code>y</code>然后在测试数据集上评估这些学习到的模式）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn data into tensors</span></span><br><span class="line"><span class="comment"># Otherwise this causes issues with computations later on</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X = torch.from_numpy(X).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line">y = torch.from_numpy(y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the first five samples</span></span><br><span class="line">X[:<span class="number">5</span>], y[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.7542,  0.2315],</span><br><span class="line">         [-0.7562,  0.1533],</span><br><span class="line">         [-0.8154,  0.1733],</span><br><span class="line">         [-0.3937,  0.6929],</span><br><span class="line">         [ 0.4422, -0.8967]]),</span><br><span class="line"> tensor([1., 1., 1., 1., 0.]))</span><br></pre></td></tr></table></figure>
<p>现在我们的数据是张量格式，让我们将其分成训练集和测试集。</p>
<p>使用 Scikit-Learn 中 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split()</a> 函数。</p>
<p>我们将使用<code>test_size=0.2</code>（80％训练，20％测试），并且由于分割在数据中随机发生，<code>random_state=42</code>因此我们使用可重现的分割。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split data into train and test sets</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, </span><br><span class="line">                                                    y, </span><br><span class="line">                                                    test_size=<span class="number">0.2</span>, <span class="comment"># 20% test, 80% train</span></span><br><span class="line">                                                    random_state=<span class="number">42</span>) <span class="comment"># make the random split reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(800, 200, 800, 200)</span><br></pre></td></tr></table></figure>
<p>现在有 800 个训练样本和 200 个测试样本。</p>
<h1 id="2-Building-a-model-建立模型"><a href="#2-Building-a-model-建立模型" class="headerlink" title="2. Building a model 建立模型"></a>2. Building a model 建立模型</h1><p>模型需要分为几个部分。</p>
<p>1、设置与设备无关的代码（这样我们的模型可以在 CPU 或 GPU 上运行）。<br>2、通过子类化构建模型 <code>nn.Module</code>。<br>3、定义损失函数和优化器。<br>4、创建训练循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Standard PyTorch imports</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make device agnostic code</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">device</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;cuda&#x27;</span><br></pre></td></tr></table></figure>
<p>我们需要一个模型，能够处理我们的<code>X</code>数据作为输入，并生成与我们的数据形状相同<code>y</code>的输出。</p>
<p>换句话说，给定<code>X</code>（特征feature），我们希望我们的模型预测<code>y</code>（标签label）。</p>
<p>这种具有特征和标签的设置称为<code>监督学习</code>。因为你的数据会告诉你的模型，给定某个输入，应该得到什么样的输出。</p>
<p>要创建这样的模型，需要处理<code>X</code>和的输入和输出形状<code>y</code>。</p>
<p>创建一个模型类：</p>
<p>1、子类 <code>nn.Module</code>（几乎所有 PyTorch 模型都是 <code>nn.Module</code> 的子类）。<br>2、在构造函数中创建 2 个 <code>nn.Linear</code> 层，能够处理 <code>X</code> 和 <code>y</code> 的输入和输出形状。<br>3、定义一个包含模型前向传递计算的 <code>forward()</code> 方法。<br>4、实例化模型类并将其发送到目标设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Construct a model class that subclasses nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CircleModelV0</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes</span></span><br><span class="line">        self.layer_1 = nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">5</span>) <span class="comment"># takes in 2 features (X), produces 5 features</span></span><br><span class="line">        self.layer_2 = nn.Linear(in_features=<span class="number">5</span>, out_features=<span class="number">1</span>) <span class="comment"># takes in 5 features, produces 1 feature (y)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. Define a forward method containing the forward pass computation</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Return the output of layer_2, a single feature, the same shape as y</span></span><br><span class="line">        <span class="keyword">return</span> self.layer_2(self.layer_1(x)) <span class="comment"># computation goes through layer_1 first then the output of layer_1 goes through layer_2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create an instance of the model and send it to target device</span></span><br><span class="line">model_0 = CircleModelV0().to(device)</span><br><span class="line">model_0</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CircleModelV0(</span><br><span class="line">  (layer_1): Linear(in_features=2, out_features=5, bias=True)</span><br><span class="line">  (layer_2): Linear(in_features=5, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>唯一的重大变化是 <code>self.layer_1</code> 和 <code>self.layer_2</code> 之间发生的事情。</p>
<p><code>self.layer_1</code> 接受 2 个输入特征 <code>in_features=2</code> 并产生 5 个输出特征 <code>out_features=5</code>。</p>
<p>这被称为具有 5 个隐藏单元或神经元。该层将输入数据从 2 个特征变为 5 个特征。</p>
<p>这使得模型可以从 5 个数字而不是仅仅 2 个数字中学习模式，从而可能产生更好的输出。</p>
<p>在神经网络层中使用的隐藏单元的数量是一个<code>超参数</code>（可以自己设置的值），并且没有必须使用的固定值。<br>通常情况下，数量越多越好，但也可能太多。您选择的数量取决于您的模型类型和您正在使用的数据集。</p>
<p>由于我们的数据集很小而且简单，因此我们会将其保持较小。</p>
<p>隐藏单元的唯一规则是下一层（在我们的例子中为 <code>self.layer_2</code>）必须采用与前一层 <code>out_features</code> 相同的 <code>in_features</code>。</p>
<p>这就是为什么 <code>self.layer_2</code> 有 <code>in_features=5</code>，它从 <code>self.layer_1</code> 中获取 <code>out_features=5</code> 并对它们执行线性计算，将它们转换为 <code>out_features=1</code>（与 y 相同的形状）。</p>
<p>与我们刚刚构建的分类神经网络类似的视觉示例。尝试在 <a target="_blank" rel="noopener" href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.57514&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">TensorFlow Playground</a> 网站上创建一个您自己的神经网络。</p>
<p>您也可以使用 <code>nn.Sequential</code> 执行与上述相同的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replicate CircleModelV0 with nn.Sequential</span></span><br><span class="line">model_0 = nn.Sequential(</span><br><span class="line">    nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">5</span>),</span><br><span class="line">    nn.Linear(in_features=<span class="number">5</span>, out_features=<span class="number">1</span>)</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line">model_0</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Linear(in_features=2, out_features=5, bias=True)</span><br><span class="line">  (1): Linear(in_features=5, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这看起来比子类化简单多了 <code>nn.Module</code>，为什么不总是使用呢<code>nn.Sequential</code>？</p>
<p><code>nn.Sequential</code>对于直接计算来说非常棒，但是，正如命名空间所说，它总是按顺序运行。</p>
<p>因此，如果您希望发生其他事情（而不仅仅是直接的顺序计算），您将需要定义自己的自定义<code>nn.Module</code>子类。</p>
<p>现们有一个模型，让我们看看当我们通过它传递一些数据时会发生什么。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions with the model</span></span><br><span class="line">untrained_preds = model_0(X_test.to(device))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of predictions: <span class="subst">&#123;<span class="built_in">len</span>(untrained_preds)&#125;</span>, Shape: <span class="subst">&#123;untrained_preds.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of test samples: <span class="subst">&#123;<span class="built_in">len</span>(y_test)&#125;</span>, Shape: <span class="subst">&#123;y_test.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nFirst 10 predictions:\n<span class="subst">&#123;untrained_preds[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nFirst 10 test labels:\n<span class="subst">&#123;y_test[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Length of predictions: 200, Shape: torch.Size([200, 1])</span><br><span class="line">Length of test samples: 200, Shape: torch.Size([200])</span><br><span class="line"></span><br><span class="line">First 10 predictions:</span><br><span class="line">tensor([[-0.1631],</span><br><span class="line">        [-0.4051],</span><br><span class="line">        [ 0.3693],</span><br><span class="line">        [-0.3135],</span><br><span class="line">        [ 0.2072],</span><br><span class="line">        [ 0.0607],</span><br><span class="line">        [-0.4923],</span><br><span class="line">        [-0.3836],</span><br><span class="line">        [ 0.3753],</span><br><span class="line">        [-0.4231]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">First 10 test labels:</span><br><span class="line">tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])</span><br></pre></td></tr></table></figure>
<p>预测的数量与测试标签的数量相同，但是预测的形式或形状看起来与测试标签不一样。</p>
<h2 id="2-1-Setup-loss-function-and-optimizer"><a href="#2-1-Setup-loss-function-and-optimizer" class="headerlink" title="2.1 Setup loss function and optimizer"></a>2.1 Setup loss function and optimizer</h2><p>不同类型的问题需要不同的损失函数。</p>
<p>回归问题（预测数字）：使用平均绝对误差（MAE）损失。<br>二元分类问题（目前的问题），使用<a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">二元交叉熵</a>作为损失函数。</p>
<p>相同的优化器函数通常可用于不同的问题空间。</p>
<p>随机梯度下降优化器（<code>SGD，torch.optim.SGD()</code>）可用于解决一系列问题，Adam 优化器（<code>torch.optim.Adam()</code>）同样适用。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">损失函数/优化器</th>
<th style="text-align:center">问题类型</th>
<th style="text-align:center">PyTorch代码</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">随机梯度下降（SGD）优化器</td>
<td style="text-align:center">分类、回归等</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">torch.optim.SGD()</a></td>
</tr>
<tr>
<td style="text-align:center">Adam 优化器</td>
<td style="text-align:center">分类、回归等</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">torch.optim.Adam()</a></td>
</tr>
<tr>
<td style="text-align:center">二元交叉熵损失</td>
<td style="text-align:center">二元分类</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">torch.nn.BCELossWithLogits</a> 或者 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">torch.nn.BCELoss</a></td>
</tr>
<tr>
<td style="text-align:center">交叉熵损失</td>
<td style="text-align:center">多类别分类</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">torch.nn.CrossEntropyLoss</a></td>
</tr>
<tr>
<td style="text-align:center">平均绝对误差 (MAE) 或 L1 损失</td>
<td style="text-align:center">回归</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html">torch.nn.L1Loss</a></td>
</tr>
<tr>
<td style="text-align:center">均方误差 (MSE) 或 L2 损失</td>
<td style="text-align:center">回归</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss">torch.nn.MSELoss</a></td>
</tr>
</tbody>
</table>
</div>
<p>由于我们正在处理二元分类问题，因此我们使用二元交叉熵损失函数。</p>
<p>PyTorch 有两种二元交叉熵实现：</p>
<p>1、<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">torch.nn.BCELoss()</a>- 创建一个损失函数，测量目标（标签）和输入（特征）之间的二元交叉熵。<br>2、<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">torch.nn.BCEWithLogitsLoss()</a> - 这与上面的相同，只是它有一个内置的 sigmoid 层 (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html">nn.Sigmoid</a>)</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">torch.nn.BCEWithLogitsLoss()</a> 的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">文档</a>指出，它比在 <code>nn.Sigmoid</code> 层之后使用 <code>torch.nn.BCELoss()</code> 更具数值稳定性。</p>
<p>通常，实现 2 是更好的选择。但是对于高级用法，可能希望分离 <code>nn.Sigmoid</code> 和 <code>torch.nn.BCELoss()</code> 的组合，但这超出了本笔记本的范围。</p>
<p>了解了这一点，让我们创建一个损失函数和一个优化器。</p>
<p>对于优化器，我们将使用 <code>torch.optim.SGD()</code> 以学习率为 0.1 来优化模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a loss function</span></span><br><span class="line"><span class="comment"># loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in</span></span><br><span class="line">loss_fn = nn.BCEWithLogitsLoss() <span class="comment"># BCEWithLogitsLoss = sigmoid built-in</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_0.parameters(), </span><br><span class="line">                            lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>现在让我们创建一个<code>评估指标</code>。<br>评估指标可用于提供模型运行情况的另一个视角。<br>如果损失函数衡量模型的错误程度，我喜欢将评估指标视为衡量模型的正确程度。<br>当然，您可以说这两者都在做同样的事情，但评估指标提供了不同的视角。<br>毕竟，在评估模型时，最好从多个角度看待事物。<br>有几种评估指标可用于分类问题，但让我们从准确度开始。<br>准确度可以通过将正确预测的总数除以预测总数来衡量。<br>例如，如果一个模型在 100 个预测中做出 99 个正确的预测，则准确度为 99%。<br>让我们编写一个函数来实现这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate accuracy (a classification metric)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy_fn</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    correct = torch.eq(y_true, y_pred).<span class="built_in">sum</span>().item() <span class="comment"># torch.eq() calculates where two tensors are equal</span></span><br><span class="line">    acc = (correct / <span class="built_in">len</span>(y_pred)) * <span class="number">100</span> </span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>
<p>太棒了！我们现在可以在训练模型时使用此功能来测量其性能和损失。</p>
<h1 id="3-Train-model"><a href="#3-Train-model" class="headerlink" title="3. Train model"></a>3. Train model</h1><p>PyTorch 训练循环步骤：</p>
<p>1、前向传递 Forward pass - 模型对所有训练数据进行一次遍历，执行其 forward() 函数计算 (<code>model(x_train)</code>)。<br>2、计算损失 Calculate the loss  - 将模型的输出 (预测) 与基本事实进行比较，并进行评估以查看其错误程度 (<code>loss = loss_fn(y_pred, y_train)</code>)。<br>3、零梯度 Zero gradients  - 优化器梯度设置为零 (默认情况下是累积的)，因此可以为特定的训练步骤重新计算 (<code>optimizer.zero_grad()</code>)。<br>4、对损失执行反向传播 Perform backpropagation on the loss - 针对要更新的每个模型参数 (每个参数的 require_grad=True) 计算损失的梯度。这称为反向传播，因此为“向后”(<code>loss.backward()</code>)。<br>5、步进优化器 (梯度下降) Step the optimizer (gradient descent)  - 使用 <code>require_grad=True</code> 更新参数，以根据损失梯度改进它们 (<code>optimizer.step()</code>)。</p>
<h2 id="3-1-Going-from-raw-model-outputs-to-predicted-labels-logits-gt-prediction-probabilities-gt-prediction-labels-从原始模型输出到预测标签（logits-gt-预测概率-gt-预测标签）"><a href="#3-1-Going-from-raw-model-outputs-to-predicted-labels-logits-gt-prediction-probabilities-gt-prediction-labels-从原始模型输出到预测标签（logits-gt-预测概率-gt-预测标签）" class="headerlink" title="3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels) 从原始模型输出到预测标签（logits -&gt; 预测概率 -&gt; 预测标签）"></a>3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels) 从原始模型输出到预测标签（logits -&gt; 预测概率 -&gt; 预测标签）</h2><p>在训练循环步骤之前，让我们看看在前向传递过程中我们的模型会产生什么结果（前向传递由方法定义<code>forward()</code>）。</p>
<p>为此，让我们向模型传递一些数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View the frist 5 outputs of the forward pass on the test data</span></span><br><span class="line">y_logits = model_0(X_test.to(device))[:<span class="number">5</span>]</span><br><span class="line">y_logits</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1631],</span><br><span class="line">        [-0.4051],</span><br><span class="line">        [ 0.3693],</span><br><span class="line">        [-0.3135],</span><br><span class="line">        [ 0.2072]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>由于我们的模型尚未经过训练，这些输出基本上是随机的。</p>
<p>但它们是什么？它们是我们的 <code>forward()</code> 方法的输出。</p>
<p>它实现了两层 <code>nn.Linear()</code>，它在内部调用以下方程：</p>
<script type="math/tex; mode=display">\mathbf{y} = x \cdot \mathbf{Weights}^T + \mathbf{bias}</script><p>该方程（$\mathbf{y}$）的原始输出（未修改），反过来，我们模型的原始输出通常被称为<code>logits</code>。</p>
<p>这就是我们的模型在接受输入数据（等式中的 x 或代码中的 <code>X_test</code>）时输出的内容，<code>logits</code>。</p>
<p>然而，这些数字很难解释。</p>
<p>我们希望有一些数字可以与我们的真实标签相媲美。</p>
<p>为了将我们模型的原始输出（logits）变成这种形式，我们可以使用 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html">sigmoid 激活函数</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use sigmoid on model logits</span></span><br><span class="line">y_pred_probs = torch.sigmoid(y_logits)</span><br><span class="line">y_pred_probs</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4593],</span><br><span class="line">        [0.4001],</span><br><span class="line">        [0.5913],</span><br><span class="line">        [0.4223],</span><br><span class="line">        [0.5516]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SigmoidBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>看起来输出现在具有某种一致性（即使它们仍然是随机的）。</p>
<p>它们现在采用<code>预测概率</code>的形式（我通常将其称为 <code>y_pred_probs</code>），换句话说，这些值现在是模型认为数据点属于一个类或另一个类的程度。</p>
<p>在我们的例子中，由于我们正在处理二元分类，所以我们的理想输出是 0 或 1。</p>
<p>因此这些值可以被视为决策边界。</p>
<p>越接近0，模型越认为该样本属于0类，越接近1，模型越认为该样本属于1类。</p>
<p>更具体地说：</p>
<p>如果 <code>y_pred_probs&gt;= 0.5</code>，<code>y=1</code>（第 1 类）<br>如果 <code>y_pred_probs&lt; 0.5</code>，<code>y=0</code>（0 类）</p>
<p>为了将我们的预测概率转化为预测标签，我们可以对 sigmoid  激活函数的输出进行四舍五入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find the predicted labels (round the prediction probabilities)</span></span><br><span class="line">y_preds = torch.<span class="built_in">round</span>(y_pred_probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In full</span></span><br><span class="line">y_pred_labels = torch.<span class="built_in">round</span>(torch.sigmoid(model_0(X_test.to(device))[:<span class="number">5</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check for equality</span></span><br><span class="line"><span class="built_in">print</span>(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get rid of extra dimension</span></span><br><span class="line">y_preds.squeeze()</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True, True], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([0., 0., 1., 0., 1.], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SqueezeBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>现在看起来我们的模型的预测与我们的真实标签 ( <code>y_test</code>) 的形式相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_test[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 0., 1., 0., 1.])</span><br></pre></td></tr></table></figure>
<p>这意味着我们将能够将模型的预测与测试标签进行比较，以了解其表现如何。</p>
<p>回顾一下，我们使用 sigmoid 激活函数将模型的原始输出 (logits) 转换为预测概率。</p>
<p>然后通过四舍五入将预测概率转换为预测标签。</p>
<blockquote>
<p>注意：sigmoid 激活函数通常仅用于二分类 logits。对于多类分类，我们将考虑使用 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">softmax 激活函数</a>。<br>将模型的原始输出传递给 nn.BCEWithLogitsLoss 时不需要使用 sigmoid激活函数（logits loss 中的“logits”是因为它适用于模型的原始 logits 输出），这是因为它内置了 sigmoid函数。</p>
</blockquote>
<h2 id="3-2-Building-a-training-and-testing-loop-建立训练和测试循环"><a href="#3-2-Building-a-training-and-testing-loop-建立训练和测试循环" class="headerlink" title="3.2 Building a training and testing loop 建立训练和测试循环"></a>3.2 Building a training and testing loop 建立训练和测试循环</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data to target device</span></span><br><span class="line">X_train, y_train = X_train.to(device), y_train.to(device)</span><br><span class="line">X_test, y_test = X_test.to(device), y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build training and evaluation loop</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_0.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass (model outputs raw logits)</span></span><br><span class="line">    y_logits = model_0(X_train).squeeze() <span class="comment"># squeeze to remove extra `1` dimensions, this won&#x27;t work unless model and data are on same device </span></span><br><span class="line">    y_pred = torch.<span class="built_in">round</span>(torch.sigmoid(y_logits)) <span class="comment"># turn logits -&gt; pred probs -&gt; pred labls</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 2. Calculate loss/accuracy</span></span><br><span class="line">    <span class="comment"># loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()</span></span><br><span class="line">    <span class="comment">#                y_train) </span></span><br><span class="line">    loss = loss_fn(y_logits, <span class="comment"># Using nn.BCEWithLogitsLoss works with raw logits</span></span><br><span class="line">                   y_train) </span><br><span class="line">    acc = accuracy_fn(y_true=y_train, </span><br><span class="line">                      y_pred=y_pred) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_0.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        <span class="comment"># 1. Forward pass</span></span><br><span class="line">        test_logits = model_0(X_test).squeeze() </span><br><span class="line">        test_pred = torch.<span class="built_in">round</span>(torch.sigmoid(test_logits))</span><br><span class="line">        <span class="comment"># 2. Caculate loss/accuracy</span></span><br><span class="line">        test_loss = loss_fn(test_logits,</span><br><span class="line">                            y_test)</span><br><span class="line">        test_acc = accuracy_fn(y_true=y_test,</span><br><span class="line">                               y_pred=test_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening every 10 epochs</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Accuracy: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% | Test loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>, Test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Loss: 0.71041, Accuracy: 49.50% | Test loss: 0.69582, Test acc: 53.00%</span><br><span class="line">Epoch: 10 | Loss: 0.70593, Accuracy: 49.12% | Test loss: 0.69352, Test acc: 54.00%</span><br><span class="line">Epoch: 20 | Loss: 0.70281, Accuracy: 49.25% | Test loss: 0.69213, Test acc: 54.00%</span><br><span class="line">Epoch: 30 | Loss: 0.70055, Accuracy: 49.12% | Test loss: 0.69132, Test acc: 54.50%</span><br><span class="line">Epoch: 40 | Loss: 0.69888, Accuracy: 49.12% | Test loss: 0.69087, Test acc: 54.00%</span><br><span class="line">Epoch: 50 | Loss: 0.69762, Accuracy: 48.75% | Test loss: 0.69066, Test acc: 53.00%</span><br><span class="line">Epoch: 60 | Loss: 0.69666, Accuracy: 48.75% | Test loss: 0.69061, Test acc: 53.50%</span><br><span class="line">Epoch: 70 | Loss: 0.69592, Accuracy: 48.88% | Test loss: 0.69067, Test acc: 54.50%</span><br><span class="line">Epoch: 80 | Loss: 0.69535, Accuracy: 49.00% | Test loss: 0.69079, Test acc: 54.00%</span><br><span class="line">Epoch: 90 | Loss: 0.69489, Accuracy: 49.00% | Test loss: 0.69096, Test acc: 54.00%</span><br></pre></td></tr></table></figure>
<p>每次数据分割的准确率几乎不超过 50%。</p>
<p>因为我们正在处理平衡的二元分类问题，所以这意味着我们的模型表现与随机猜测一样好（有 500 个 0 类和 1 类样本，每次预测 1 类的模型准确率都会达到 50%）。</p>
<h1 id="4-Make-predictions-and-evaluate-the-model-做出预测并评估模型"><a href="#4-Make-predictions-and-evaluate-the-model-做出预测并评估模型" class="headerlink" title="4. Make predictions and evaluate the model 做出预测并评估模型"></a>4. Make predictions and evaluate the model 做出预测并评估模型</h1><p>对于50%准确率的模型，几乎等于瞎猜。</p>
<p>我们将编写一些代码，从Learn PyTorch for Deep Learning仓库下载并导入helper_functions.py脚本。</p>
<p>它包含一个名为plot_decision_boundary（）的有用函数，该函数创建了一个NumPy网格，以直观地绘制我们的模型预测某些类的不同点。</p>
<p>将结果可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path </span><br><span class="line"></span><br><span class="line"><span class="comment"># Download helper functions from Learn PyTorch repo (if not already downloaded)</span></span><br><span class="line"><span class="keyword">if</span> Path(<span class="string">&quot;helper_functions.py&quot;</span>).is_file():</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;helper_functions.py already exists, skipping download&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;Downloading helper_functions.py&quot;</span>)</span><br><span class="line">  request = requests.get(<span class="string">&quot;https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py&quot;</span>)</span><br><span class="line">  <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;helper_functions.py&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(request.content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> helper_functions <span class="keyword">import</span> plot_predictions, plot_decision_boundary</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot decision boundaries for training and test sets</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_0, X_train, y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_0, X_test, y_test)</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-4.png" class="" title="PyTorch-26H-3-4">
<p>由于数据是圆形的，因此画一条直线最多只能将其从中间切开。</p>
<p>用机器学习术语来说，模型拟合不足<code>underfitting</code>，意味着它没有从数据中学习预测模式。</p>
<h1 id="5-Improving-a-model-from-a-model-perspective-改进模型（从模型角度）"><a href="#5-Improving-a-model-from-a-model-perspective-改进模型（从模型角度）" class="headerlink" title="5. Improving a model (from a model perspective) 改进模型（从模型角度）"></a>5. Improving a model (from a model perspective) 改进模型（从模型角度）</h1><p>修复模型的欠拟合问题。</p>
<p>特别关注模型（而不是数据），我们可以通过几种方式来做到这一点。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型改进技术</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">添加更多层 Add more layers</td>
<td style="text-align:center">每一层都可能增加模型的学习能力，因为每一层都能够学习数据中的某种新模式。更多层通常被称为使神经网络更深。</td>
</tr>
<tr>
<td style="text-align:center">添加更多隐藏单元 Add more hidden units</td>
<td style="text-align:center">与上述类似，每层隐藏单元越多，模型的学习能力就越强。更多隐藏单元通常被称为使神经网络更宽。</td>
</tr>
<tr>
<td style="text-align:center">更长训练时间（更多循环） Fitting for longer (more epochs)</td>
<td style="text-align:center">如果您的模型有更多机会查看数据，它可能会学到更多东西。</td>
</tr>
<tr>
<td style="text-align:center">改变激活函数 Changing the activation functions</td>
<td style="text-align:center">有些数据无法仅用直线来拟合（就像我们所看到的），使用非线性激活函数可以帮助解决这个问题（提示，提示）。</td>
</tr>
<tr>
<td style="text-align:center">改变学习率 Change the learning rate</td>
<td style="text-align:center">虽然与模型不太相关，但仍然相关，优化器的学习率决定了模型每一步应该改变多少参数，太多则模型过度修正，太少则学习不够。</td>
</tr>
<tr>
<td style="text-align:center">改变损失函数 Change the loss function</td>
<td style="text-align:center">同样，虽然模型特定性不强但仍然很重要，不同的问题需要不同的损失函数。例如，二元交叉熵损失函数不适用于多类分类问题。</td>
</tr>
<tr>
<td style="text-align:center">使用迁移学习 Use transfer learning</td>
<td style="text-align:center">从与您的问题领域类似的问题中获取预训练模型，并根据您自己的问题进行调整。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>可以手动调整→超参数<br>机器学习为一半科学一半艺术，需要通过不断实验进行。</p>
</blockquote>
<p>让我们看看如果我们在模型中添加一个额外的层，适应更长的时间（<code>epochs=1000</code> 而不是 <code>epochs=100</code>），并将隐藏单元的数量从 <code>5</code> 增加到 <code>10</code>，会发生什么。</p>
<p>我们将遵循上述相同的步骤，但会更改一些超参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CircleModelV1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer_1 = nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        self.layer_2 = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>) <span class="comment"># extra layer</span></span><br><span class="line">        self.layer_3 = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># note: always make sure forward is spelt correctly!</span></span><br><span class="line">        <span class="comment"># Creating a model like this is the same as below, though below</span></span><br><span class="line">        <span class="comment"># generally benefits from speedups where possible.</span></span><br><span class="line">        <span class="comment"># z = self.layer_1(x)</span></span><br><span class="line">        <span class="comment"># z = self.layer_2(z)</span></span><br><span class="line">        <span class="comment"># z = self.layer_3(z)</span></span><br><span class="line">        <span class="comment"># return z</span></span><br><span class="line">        <span class="keyword">return</span> self.layer_3(self.layer_2(self.layer_1(x)))</span><br><span class="line"></span><br><span class="line">model_1 = CircleModelV1().to(device)</span><br><span class="line">model_1</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CircleModelV1(</span><br><span class="line">  (layer_1): Linear(in_features=2, out_features=10, bias=True)</span><br><span class="line">  (layer_2): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">  (layer_3): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>现在我们有了一个模型，我们将使用与之前相同的设置重新创建一个损失函数和优化器实例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># loss_fn = nn.BCELoss() # Requires sigmoid on input</span></span><br><span class="line">loss_fn = nn.BCEWithLogitsLoss() <span class="comment"># Does not require sigmoid on input</span></span><br><span class="line">optimizer = torch.optim.SGD(model_1.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>这次我们将进行更长时间的训练（epochs=1000 vs epochs=100），看看它是否能改进我们的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">1000</span> <span class="comment"># Train for longer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data to target device</span></span><br><span class="line">X_train, y_train = X_train.to(device), y_train.to(device)</span><br><span class="line">X_test, y_test = X_test.to(device), y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_logits = model_1(X_train).squeeze()</span><br><span class="line">    y_pred = torch.<span class="built_in">round</span>(torch.sigmoid(y_logits)) <span class="comment"># logits -&gt; prediction probabilities -&gt; prediction labels</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate loss/accuracy</span></span><br><span class="line">    loss = loss_fn(y_logits, y_train)</span><br><span class="line">    acc = accuracy_fn(y_true=y_train, </span><br><span class="line">                      y_pred=y_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_1.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        <span class="comment"># 1. Forward pass</span></span><br><span class="line">        test_logits = model_1(X_test).squeeze() </span><br><span class="line">        test_pred = torch.<span class="built_in">round</span>(torch.sigmoid(test_logits))</span><br><span class="line">        <span class="comment"># 2. Caculate loss/accuracy</span></span><br><span class="line">        test_loss = loss_fn(test_logits,</span><br><span class="line">                            y_test)</span><br><span class="line">        test_acc = accuracy_fn(y_true=y_test,</span><br><span class="line">                               y_pred=test_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening every 10 epochs</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Accuracy: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% | Test loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>, Test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%</span><br><span class="line">Epoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%</span><br><span class="line">Epoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%</span><br><span class="line">Epoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%</span><br><span class="line">Epoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%</span><br><span class="line">Epoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%</span><br><span class="line">Epoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</span><br><span class="line">Epoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</span><br><span class="line">Epoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</span><br><span class="line">Epoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</span><br></pre></td></tr></table></figure>
<p>我们的模型训练的时间更长，并且增加了一层，但它看起来仍然没有学到比随机猜测更好的模式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot decision boundaries for training and test sets</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_1, X_train, y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_1, X_test, y_test)</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-5.png" class="" title="PyTorch-26H-3-5">
<p>我们的模型仍然在红点和蓝点之间画一条直线。</p>
<p>如果我们的模型画的是直线，那么它能模拟线性数据吗？</p>
<h2 id="5-1-Preparing-data-to-see-if-our-model-can-model-a-straight-line-准备数据，看看我们的模型是否能建模直线"><a href="#5-1-Preparing-data-to-see-if-our-model-can-model-a-straight-line-准备数据，看看我们的模型是否能建模直线" class="headerlink" title="5.1 Preparing data to see if our model can model a straight line 准备数据，看看我们的模型是否能建模直线"></a>5.1 Preparing data to see if our model can model a straight line 准备数据，看看我们的模型是否能建模直线</h2><p>创建一些线性数据来看看我们的模型是否能够对其进行建模，而不仅仅是使用一个无法学习任何东西的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some data (same as notebook 01)</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data</span></span><br><span class="line">X_regression = torch.arange(start, end, step).unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">y_regression = weight * X_regression + bias <span class="comment"># linear regression formula</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(X_regression))</span><br><span class="line">X_regression[:<span class="number">5</span>], y_regression[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">100</span><br><span class="line">(tensor([[0.0000],</span><br><span class="line">         [0.0100],</span><br><span class="line">         [0.0200],</span><br><span class="line">         [0.0300],</span><br><span class="line">         [0.0400]]),</span><br><span class="line"> tensor([[0.3000],</span><br><span class="line">         [0.3070],</span><br><span class="line">         [0.3140],</span><br><span class="line">         [0.3210],</span><br><span class="line">         [0.3280]]))</span><br></pre></td></tr></table></figure>
<p>将数据分成训练集和测试集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create train and test splits</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X_regression)) <span class="comment"># 80% of data used for training set</span></span><br><span class="line">X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]</span><br><span class="line">X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the lengths of each split</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(X_train_regression), </span><br><span class="line">    <span class="built_in">len</span>(y_train_regression), </span><br><span class="line">    <span class="built_in">len</span>(X_test_regression), </span><br><span class="line">    <span class="built_in">len</span>(y_test_regression))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">80 80 20 20</span><br></pre></td></tr></table></figure>
<p>漂亮，让我们看看数据是什么样子的。</p>
<p>为此，我们将使用我们在笔记本 01 中创建的 plot_predictions() 函数。</p>
<p>它包含在我们上面下载的 Learn PyTorch for Deep Learning 存储库中的 helper_functions.py 脚本中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions(train_data=X_train_regression,</span><br><span class="line">    train_labels=y_train_regression,</span><br><span class="line">    test_data=X_test_regression,</span><br><span class="line">    test_labels=y_test_regression</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-6.png" class="" title="PyTorch-26H-3-6">
<h2 id="5-2-Adjusting-model-1-to-fit-a-straight-line-调整-model-1-以适合直线"><a href="#5-2-Adjusting-model-1-to-fit-a-straight-line-调整-model-1-以适合直线" class="headerlink" title="5.2 Adjusting model_1 to fit a straight line 调整 model_1 以适合直线"></a>5.2 Adjusting <code>model_1</code> to fit a straight line 调整 <code>model_1</code> 以适合直线</h2><p>重新创建<code>model_1</code>，但使用适合我们的回归数据的损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Same architecture as model_1 (but using nn.Sequential)</span></span><br><span class="line">model_2 = nn.Sequential(</span><br><span class="line">    nn.Linear(in_features=<span class="number">1</span>, out_features=<span class="number">10</span>),</span><br><span class="line">    nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>),</span><br><span class="line">    nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">1</span>)</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line">model_2</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Linear(in_features=1, out_features=10, bias=True)</span><br><span class="line">  (1): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">  (2): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将损失函数设置为<code>nn.L1Loss()</code>（与平均绝对误差相同），并将优化器设置为<code>torch.optim.SGD()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss and optimizer</span></span><br><span class="line">loss_fn = nn.L1Loss()</span><br><span class="line">optimizer = torch.optim.SGD(model_2.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>现在让我们使用常规训练循环步骤来训练模型，<code>epochs=1000</code>（就像<code>model_1</code>一样）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data to target device</span></span><br><span class="line">X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)</span><br><span class="line">X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training </span></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_pred = model_2(X_train_regression)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. Calculate loss (no accuracy since it&#x27;s a regression problem, not classification)</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train_regression)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_2.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass</span></span><br><span class="line">      test_pred = model_2(X_test_regression)</span><br><span class="line">      <span class="comment"># 2. Calculate the loss </span></span><br><span class="line">      test_loss = loss_fn(test_pred, y_test_regression)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>: </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Train loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Test loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Train loss: 0.75986, Test loss: 0.54143</span><br><span class="line">Epoch: 100 | Train loss: 0.09309, Test loss: 0.02901</span><br><span class="line">Epoch: 200 | Train loss: 0.07376, Test loss: 0.02850</span><br><span class="line">Epoch: 300 | Train loss: 0.06745, Test loss: 0.00615</span><br><span class="line">Epoch: 400 | Train loss: 0.06107, Test loss: 0.02004</span><br><span class="line">Epoch: 500 | Train loss: 0.05698, Test loss: 0.01061</span><br><span class="line">Epoch: 600 | Train loss: 0.04857, Test loss: 0.01326</span><br><span class="line">Epoch: 700 | Train loss: 0.06109, Test loss: 0.02127</span><br><span class="line">Epoch: 800 | Train loss: 0.05599, Test loss: 0.01426</span><br><span class="line">Epoch: 900 | Train loss: 0.05571, Test loss: 0.00603</span><br></pre></td></tr></table></figure>
<p>好的，与分类数据上的 <code>model_1</code> 不同，<code>model_2</code> 的损失似乎实际上在下降。</p>
<p>让我们绘制它的预测图，看看是否如此。</p>
<p>请记住，由于我们的模型和数据正在使用目标设备，并且该设备可能是 GPU，因此我们的绘图函数使用 <code>matplotlib</code>，而 <code>matplotlib</code> 无法处理 GPU 上的数据。</p>
<p>为了处理这个问题，当我们将所有数据传递给 <code>plot_predictions()</code> 时，我们将使用 <code>.cpu()</code> 将所有数据发送到 CPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn on evaluation mode</span></span><br><span class="line">model_2.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions (inference)</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_preds = model_2(X_test_regression)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot data and predictions with data on the CPU (matplotlib can&#x27;t handle data on the GPU)</span></span><br><span class="line"><span class="comment"># (try removing .cpu() from one of the below and see what happens)</span></span><br><span class="line">plot_predictions(train_data=X_train_regression.cpu(),</span><br><span class="line">                 train_labels=y_train_regression.cpu(),</span><br><span class="line">                 test_data=X_test_regression.cpu(),</span><br><span class="line">                 test_labels=y_test_regression.cpu(),</span><br><span class="line">                 predictions=y_preds.cpu());</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-7.png" class="" title="PyTorch-26H-3-7">
<p>模型比在直线上随机猜测要好得多。这意味着我们的模型至少具有一定的学习能力。</p>
<blockquote>
<p>构建深度学习模型时，一个有用的故障排除步骤是先从尽可能小的模型开始，看看模型是否有效，然后再将其扩大。<br>这可能意味着从一个简单的神经网络（层数不多，隐藏神经元也不多）和一个小的数据集（就像我们制作的数据集）开始，然后在这个小例子上进行过度拟合overfitting（使模型表现得太好了），然后再增加数据量或模型 大小 / 设计 以减少过度拟合。</p>
</blockquote>
<h1 id="6-The-missing-piece-non-linearity-缺失的部分：非线性"><a href="#6-The-missing-piece-non-linearity-缺失的部分：非线性" class="headerlink" title="6. The missing piece: non-linearity 缺失的部分：非线性"></a>6. The missing piece: non-linearity 缺失的部分：非线性</h1><p>由于模型具有线性层，因此它可以绘制直线（线性）。</p>
<p>但是我们如何赋予它绘制非直线（非线性）线条的能力呢？</p>
<h2 id="6-1-Recreating-non-linear-data-red-and-blue-circles-重新创建非线性数据（红色和蓝色圆圈）"><a href="#6-1-Recreating-non-linear-data-red-and-blue-circles-重新创建非线性数据（红色和蓝色圆圈）" class="headerlink" title="6.1 Recreating non-linear data (red and blue circles) 重新创建非线性数据（红色和蓝色圆圈）"></a>6.1 Recreating non-linear data (red and blue circles) 重新创建非线性数据（红色和蓝色圆圈）</h2><p>重新创建数据以从头开始。我们将使用与之前相同的设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make and plot data</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line"></span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">X, y = make_circles(n_samples=<span class="number">1000</span>,</span><br><span class="line">    noise=<span class="number">0.03</span>,</span><br><span class="line">    random_state=<span class="number">42</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.RdBu);</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-8.png" class="" title="PyTorch-26H-3-8">
<p>太棒了！现在让我们将其分成训练集和测试集，其中 80% 的数据用于训练，20% 的数据用于测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert to tensors and split into train and test sets</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn data into tensors</span></span><br><span class="line">X = torch.from_numpy(X).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line">y = torch.from_numpy(y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split into train and test sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, </span><br><span class="line">                                                    y, </span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">X_train[:<span class="number">5</span>], y_train[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.6579, -0.4651],</span><br><span class="line">         [ 0.6319, -0.7347],</span><br><span class="line">         [-1.0086, -0.1240],</span><br><span class="line">         [-0.9666, -0.2256],</span><br><span class="line">         [-0.1666,  0.7994]]),</span><br><span class="line"> tensor([1., 0., 0., 0., 1.]))</span><br></pre></td></tr></table></figure>
<h2 id="6-2-Building-a-model-with-non-linearity-建立非线性模型"><a href="#6-2-Building-a-model-with-non-linearity-建立非线性模型" class="headerlink" title="6.2 Building a model with non-linearity 建立非线性模型"></a>6.2 Building a model with non-linearity 建立非线性模型</h2><p>可以用无限的直线（线性）和非直线（非线性）绘制什么样的图案？</p>
<p>到目前为止，我们的神经网络仅使用线性（直线）函数。</p>
<p>但我们处理的数据是非线性的（圆圈）。</p>
<p>当我们为模型引入使用非线性激活函数的能力时</p>
<p>PyTorch 有一堆<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">现成的非线性激活函数</a>，它们可以执行类似但不同的事情。</p>
<p>最常见且性能最好的一种是<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks">ReLU</a>)（整流线性单元，<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html">torch.nn.ReLU()</a>）。</p>
<p>将它放在神经网络中前向传递的隐藏层之间，看看会发生什么。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build model with non-linear activation function</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CircleModelV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer_1 = nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        self.layer_2 = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        self.layer_3 = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">1</span>)</span><br><span class="line">        self.relu = nn.ReLU() <span class="comment"># &lt;- add in ReLU activation function</span></span><br><span class="line">        <span class="comment"># Can also put sigmoid in the model </span></span><br><span class="line">        <span class="comment"># This would mean you don&#x27;t need to use it on the predictions</span></span><br><span class="line">        <span class="comment"># self.sigmoid = nn.Sigmoid()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">      <span class="comment"># Intersperse the ReLU activation function between layers</span></span><br><span class="line">       <span class="keyword">return</span> self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))</span><br><span class="line"></span><br><span class="line">model_3 = CircleModelV2().to(device)</span><br><span class="line"><span class="built_in">print</span>(model_3)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CircleModelV2(</span><br><span class="line">  (layer_1): Linear(in_features=2, out_features=10, bias=True)</span><br><span class="line">  (layer_2): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">  (layer_3): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">  (relu): ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>与我们刚刚构建的分类神经网络（使用 ReLU 激活）类似的分类神经网络的视觉示例。尝试在 TensorFlow Playground 网站上创建一个您自己的神经网络。</p>
<blockquote>
<p>问题：构建神经网络时，我应该把非线性激活函数放在哪里？<br>经验法则是将它们放在隐藏层之间，紧接着输出层，但是，没有一成不变的选择。随着您对神经网络和深度学习的了解越来越多，您会发现很多不同的组合方法。与此同时，最好不断实验、实验、再实验。</p>
</blockquote>
<p>现在我们已经准备好了模型，让我们创建一个二元分类损失函数以及一个优化器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup loss and optimizer </span></span><br><span class="line">loss_fn = nn.BCEWithLogitsLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model_3.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="6-3-Training-a-model-with-non-linearity-训练非线性模型"><a href="#6-3-Training-a-model-with-non-linearity-训练非线性模型" class="headerlink" title="6.3 Training a model with non-linearity 训练非线性模型"></a>6.3 Training a model with non-linearity 训练非线性模型</h2><p>训练、模型、损失函数、优化器已准备就绪，让我们创建一个训练和测试循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put all data on target device</span></span><br><span class="line">X_train, y_train = X_train.to(device), y_train.to(device)</span><br><span class="line">X_test, y_test = X_test.to(device), y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_logits = model_3(X_train).squeeze()</span><br><span class="line">    y_pred = torch.<span class="built_in">round</span>(torch.sigmoid(y_logits)) <span class="comment"># logits -&gt; prediction probabilities -&gt; prediction labels</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. Calculate loss and accuracy</span></span><br><span class="line">    loss = loss_fn(y_logits, y_train) <span class="comment"># BCEWithLogitsLoss calculates loss using logits</span></span><br><span class="line">    acc = accuracy_fn(y_true=y_train, </span><br><span class="line">                      y_pred=y_pred)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_3.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass</span></span><br><span class="line">      test_logits = model_3(X_test).squeeze()</span><br><span class="line">      test_pred = torch.<span class="built_in">round</span>(torch.sigmoid(test_logits)) <span class="comment"># logits -&gt; prediction probabilities -&gt; prediction labels</span></span><br><span class="line">      <span class="comment"># 2. Calculate loss and accuracy</span></span><br><span class="line">      test_loss = loss_fn(test_logits, y_test)</span><br><span class="line">      test_acc = accuracy_fn(y_true=y_test,</span><br><span class="line">                             y_pred=test_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Accuracy: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% | Test Loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>, Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%</span><br><span class="line">Epoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%</span><br><span class="line">Epoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%</span><br><span class="line">Epoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%</span><br><span class="line">Epoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%</span><br><span class="line">Epoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%</span><br><span class="line">Epoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%</span><br><span class="line">Epoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%</span><br><span class="line">Epoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%</span><br><span class="line">Epoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%</span><br></pre></td></tr></table></figure>
<h2 id="6-4-Evaluating-a-model-trained-with-non-linear-activation-functions-评估用非线性激活函数训练的模型"><a href="#6-4-Evaluating-a-model-trained-with-non-linear-activation-functions-评估用非线性激活函数训练的模型" class="headerlink" title="6.4 Evaluating a model trained with non-linear activation functions 评估用非线性激活函数训练的模型"></a>6.4 Evaluating a model trained with non-linear activation functions 评估用非线性激活函数训练的模型</h2><p>还记得我们的圆形数据是非线性的吗？好吧，让我们看看现在模型的预测结果如何，该模型已经用非线性激活函数进行了训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions</span></span><br><span class="line">model_3.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_preds = torch.<span class="built_in">round</span>(torch.sigmoid(model_3(X_test))).squeeze()</span><br><span class="line">y_preds[:<span class="number">10</span>], y[:<span class="number">10</span>] <span class="comment"># want preds in same format as truth labels</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device=&#x27;cuda:0&#x27;),</span><br><span class="line"> tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot decision boundaries for training and test sets</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_1, X_train, y_train) <span class="comment"># model_1 = no non-linearity</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_3, X_test, y_test) <span class="comment"># model_3 = has non-linearity</span></span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-9.png" class="" title="PyTorch-26H-3-9">
<h1 id="7-Replicating-non-linear-activation-functions-复制非线性激活函数"><a href="#7-Replicating-non-linear-activation-functions-复制非线性激活函数" class="headerlink" title="7. Replicating non-linear activation functions 复制非线性激活函数"></a>7. Replicating non-linear activation functions 复制非线性激活函数</h1><blockquote>
<p>您在自然中遇到的大部分数据都是非线性的（或线性和非线性的组合）。现在我们一直在处理二维图上的点。但想象一下，如果您有想要分类的植物图像，会有很多不同的植物形状。或者您想要总结的维基百科文本，有很多不同的单词组合方式（线性和非线性模式）。</p>
</blockquote>
<p>但是非线性激活是什么样的？我们如何复制一些并看看它们的作用如何？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a toy tensor (similar to the data going into our model(s))</span></span><br><span class="line">A = torch.arange(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">A</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,</span><br><span class="line">          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the toy tensor</span></span><br><span class="line">plt.plot(A);</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-10.png" class="" title="PyTorch-26H-3-10">
<p>一条直线。</p>
<p>现在让我们看看 ReLU 激活函数如何影响它。</p>
<p>我们不会使用 PyTorch 的 ReLU (<code>torch.nn.ReLU</code>)，而是自己重新创建它。</p>
<p>ReLU 函数将所有负值变为 0，并保持正值不变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create ReLU function by hand </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> torch.maximum(torch.tensor(<span class="number">0</span>), x) <span class="comment"># inputs must be tensors</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pass toy tensor through ReLU function</span></span><br><span class="line">relu(A)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,</span><br><span class="line">        8., 9.])</span><br></pre></td></tr></table></figure>
<p>看起来我们的 ReLU 函数起作用了，所有负值都是零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot ReLU activated toy tensor</span></span><br><span class="line">plt.plot(relu(A));</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-11.png" class="" title="PyTorch-26H-3-11">
<p>太棒了！这看起来和 ReLU <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks">维基百科页面上的 ReLU 函数</a>) 形状一模一样。</p>
<p>我们试试我们一直在使用的 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid函数</a> 怎么样？</p>
<p>sigmoid 函数公式如下：</p>
<script type="math/tex; mode=display">out_i = \frac{1}{1+e^{-input_i}}</script><p>Or using $x$ as input:</p>
<script type="math/tex; mode=display">S(x) = \frac{1}{1+e^{-x_i}}</script><p>其中 $S$ 代表 sigmoid 函数，$e$ 代表<a href="">指数</a>（<a href="">torch.exp()</a>），$i$ 代表张量中的特定元素。</p>
<p>让我们用 PyTorch 构建一个函数来复制 sigmoid 函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a custom sigmoid function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + torch.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test custom sigmoid on toy tensor</span></span><br><span class="line">sigmoid(A)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,</span><br><span class="line">        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,</span><br><span class="line">        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,</span><br><span class="line">        9.9966e-01, 9.9988e-01])</span><br></pre></td></tr></table></figure>
<p>这些值看起来很像我们之前看到的预测概率，让我们看看它们的可视化效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot sigmoid activated toy tensor</span></span><br><span class="line">plt.plot(sigmoid(A));</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-12.png" class="" title="PyTorch-26H-3-12">
<p>看起来不错！我们已经从直线变成了曲线。</p>
<p>现在 PyTorch 中存在许多我们尚未尝试过的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">非线性激活函数</a>。</p>
<p>但这两个是最常见的两个。</p>
<p>问题仍然存在，您可以使用无限数量的线性（直线）和非线性（非直线）线来绘制什么图案？</p>
<p>几乎任何东西都可以，对吗？</p>
<p>当我们结合线性和非线性函数时，这正是我们的模型所做的事情。</p>
<p>我们不是告诉模型要做什么，而是给它工具来找出如何最好地发现数据中的模式。</p>
<p>这些工具是线性和非线性函数。</p>
<h1 id="8-Putting-things-together-by-building-a-multi-class-PyTorch-model-通过构建多类-PyTorch-模型将所有内容整合在一起"><a href="#8-Putting-things-together-by-building-a-multi-class-PyTorch-model-通过构建多类-PyTorch-模型将所有内容整合在一起" class="headerlink" title="8. Putting things together by building a multi-class PyTorch model 通过构建多类 PyTorch 模型将所有内容整合在一起"></a>8. Putting things together by building a multi-class PyTorch model 通过构建多类 PyTorch 模型将所有内容整合在一起</h1><p>使用多类分类问题将它们放在一起。</p>
<p><code>二元分类</code>问题是将某物归类为两个选项之一（例如，将一张照片归类为猫的照片或狗的照片）。而<code>多类分类</code>问题是从两个以上的选项列表中对某物进行分类（例如，将一张照片归类为猫、狗或鸡）。</p>
<h2 id="8-1-Creating-multi-class-classification-data-创建多类别分类数据"><a href="#8-1-Creating-multi-class-classification-data-创建多类别分类数据" class="headerlink" title="8.1 Creating multi-class classification data 创建多类别分类数据"></a>8.1 Creating multi-class classification data 创建多类别分类数据</h2><p>为了开始多类分类问题，让我们创建一些多类数据。</p>
<p>为此，我们可以利用 <code>Scikit-Learn</code> 的 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">make_blobs()</a> 方法。</p>
<p>此方法将创建我们想要的任意数量的类（使用 <code>centers</code> 参数）。</p>
<p>具体来说，我们可以这样做：</p>
<p>1、使用 <code>make_blobs()</code> 创建一些多类数据。<br>2、将数据转换为张量（默认 <code>make_blobs()</code> 使用NumPy数组）。<br>3、使用 <code>train_test_split()</code> 将数据分为训练集和测试集 。<br>4、使数据可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import dependencies</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the hyperparameters for data creation</span></span><br><span class="line">NUM_CLASSES = <span class="number">4</span></span><br><span class="line">NUM_FEATURES = <span class="number">2</span></span><br><span class="line">RANDOM_SEED = <span class="number">42</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create multi-class data</span></span><br><span class="line">X_blob, y_blob = make_blobs(n_samples=<span class="number">1000</span>,</span><br><span class="line">    n_features=NUM_FEATURES, <span class="comment"># X features</span></span><br><span class="line">    centers=NUM_CLASSES, <span class="comment"># y labels </span></span><br><span class="line">    cluster_std=<span class="number">1.5</span>, <span class="comment"># give the clusters a little shake up (try changing this to 1.0, the default)</span></span><br><span class="line">    random_state=RANDOM_SEED</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Turn data into tensors</span></span><br><span class="line">X_blob = torch.from_numpy(X_blob).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line">y_blob = torch.from_numpy(y_blob).<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line"><span class="built_in">print</span>(X_blob[:<span class="number">5</span>], y_blob[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Split into train and test sets</span></span><br><span class="line">X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,</span><br><span class="line">    y_blob,</span><br><span class="line">    test_size=<span class="number">0.2</span>,</span><br><span class="line">    random_state=RANDOM_SEED</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Plot data</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plt.scatter(X_blob[:, <span class="number">0</span>], X_blob[:, <span class="number">1</span>], c=y_blob, cmap=plt.cm.RdYlBu);</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-8.4134,  6.9352],</span><br><span class="line">        [-5.7665, -6.4312],</span><br><span class="line">        [-6.0421, -6.7661],</span><br><span class="line">        [ 3.9508,  0.6984],</span><br><span class="line">        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-13.png" class="" title="PyTorch-26H-3-13">
<p>准备好了一些多类数据。建立一个模型来分离彩色斑点。</p>
<p>问题：这个数据集需要非线性吗？或者你可以画出一系列直线来分离它吗？</p>
<h2 id="8-2-Building-a-multi-class-classification-model-in-PyTorch-在-PyTorch-中构建多类分类模型"><a href="#8-2-Building-a-multi-class-classification-model-in-PyTorch-在-PyTorch-中构建多类分类模型" class="headerlink" title="8.2 Building a multi-class classification model in PyTorch 在 PyTorch 中构建多类分类模型"></a>8.2 Building a multi-class classification model in PyTorch 在 PyTorch 中构建多类分类模型</h2><p>到目前为止，我们已经在 PyTorch 中创建了一些模型。</p>
<p>您或许还开始了解神经网络的灵活性。</p>
<p>如何构建一个类似<code>model_3</code>但仍然能够处理多类数据的系统呢？</p>
<p>创建一个<code>nn.Module</code>包含三个超参数的子类：</p>
<ul>
<li><code>input_features</code> X 进入模型的特征数量。</li>
<li><code>output_features</code> 我们想要的输出特征的理想数量（这将等同于NUM_CLASSES或等于多类分类问题中的类数）。</li>
<li><code>hidden_units</code>  我们希望每个隐藏层使用的隐藏神经元的数量。</li>
</ul>
<p>然后我们将使用上面的超参数创建模型类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create device agnostic code</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">device</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;cuda&#x27;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BlobModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, output_features, hidden_units=<span class="number">8</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Initializes all required hyperparameters for a multi-class classification model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            input_features (int): Number of input features to the model.</span></span><br><span class="line"><span class="string">            out_features (int): Number of output features of the model</span></span><br><span class="line"><span class="string">              (how many classes there are).</span></span><br><span class="line"><span class="string">            hidden_units (int): Number of hidden units between layers, default 8.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear_layer_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_features, out_features=hidden_units),</span><br><span class="line">            <span class="comment"># nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)</span></span><br><span class="line">            nn.Linear(in_features=hidden_units, out_features=hidden_units),</span><br><span class="line">            <span class="comment"># nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)</span></span><br><span class="line">            nn.Linear(in_features=hidden_units, out_features=output_features), <span class="comment"># how many classes are there?</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear_layer_stack(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an instance of BlobModel and send it to the target device</span></span><br><span class="line">model_4 = BlobModel(input_features=NUM_FEATURES, </span><br><span class="line">                    output_features=NUM_CLASSES, </span><br><span class="line">                    hidden_units=<span class="number">8</span>).to(device)</span><br><span class="line">model_4</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BlobModel(</span><br><span class="line">  (linear_layer_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=2, out_features=8, bias=True)</span><br><span class="line">    (1): Linear(in_features=8, out_features=8, bias=True)</span><br><span class="line">    (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="8-3-Creating-a-loss-function-and-optimizer-for-a-multi-class-PyTorch-model-为多类-PyTorch-模型创建损失函数和优化器"><a href="#8-3-Creating-a-loss-function-and-optimizer-for-a-multi-class-PyTorch-model-为多类-PyTorch-模型创建损失函数和优化器" class="headerlink" title="8.3 Creating a loss function and optimizer for a multi-class PyTorch model 为多类 PyTorch 模型创建损失函数和优化器"></a>8.3 Creating a loss function and optimizer for a multi-class PyTorch model 为多类 PyTorch 模型创建损失函数和优化器</h2><p>由于我们正在研究多类分类问题，我们将使用该<code>nn.CrossEntropyLoss()</code>方法作为我们的损失函数。</p>
<p>我们将坚持使用学习率为 0.1 的 SGD 来优化我们的<code>model_4</code>参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create loss and optimizer</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model_4.parameters(), </span><br><span class="line">                            lr=<span class="number">0.1</span>) <span class="comment"># exercise: try changing the learning rate here and seeing what happens to the model&#x27;s performance</span></span><br></pre></td></tr></table></figure>
<h2 id="8-4-Getting-prediction-probabilities-for-a-multi-class-PyTorch-model-获取多类-PyTorch-模型的预测概率"><a href="#8-4-Getting-prediction-probabilities-for-a-multi-class-PyTorch-model-获取多类-PyTorch-模型的预测概率" class="headerlink" title="8.4 Getting prediction probabilities for a multi-class PyTorch model 获取多类 PyTorch 模型的预测概率"></a>8.4 Getting prediction probabilities for a multi-class PyTorch model 获取多类 PyTorch 模型的预测概率</h2><p>准备好了损失函数和优化器，并且准备好训练我们的模型，但在此之前，让我们对我们的模型进行一次前向传递，看看它是否有效。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Perform a single forward pass on the data (we&#x27;ll need to put it to the target device for it to work)</span></span><br><span class="line">model_4(X_blob_train.to(device))[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-1.2711, -0.6494, -1.4740, -0.7044],</span><br><span class="line">        [ 0.2210, -1.5439,  0.0420,  1.1531],</span><br><span class="line">        [ 2.8698,  0.9143,  3.3169,  1.4027],</span><br><span class="line">        [ 1.9576,  0.3125,  2.2244,  1.1324],</span><br><span class="line">        [ 0.5458, -1.2381,  0.4441,  1.1804]], device=&#x27;cuda:0&#x27;,</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>为每个样本的每个特征都获得了一个值。检查一下形状以确认。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># How many elements in a single prediction sample?</span></span><br><span class="line">model_4(X_blob_train.to(device))[<span class="number">0</span>].shape, NUM_CLASSES </span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([4]), 4)</span><br></pre></td></tr></table></figure>
<p>模型正在为每个类别预测一个值。</p>
<p>你还记得我们模型的原始输出叫什么吗？</p>
<p>提示：它与“frog splits”押韵（在制作这些材料时没有伤害任何动物）。</p>
<p>如果你猜是 logits，那你就猜对了。</p>
<p>所以现在我们的模型正在输出 logits，但如果我们想弄清楚样本到底是哪个标签，该怎么办？</p>
<p>如何从 <code>logits</code> -&gt; <code>prediction probabilities</code> -&gt; <code>prediction labels</code>，就像我们处理二元分类问题一样？</p>
<p>这就是 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Softmax_function">softmax 激活函数</a> 发挥作用的地方。</p>
<p>softmax 函数计算每个预测类相对于所有其他可能类成为实际预测类的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make prediction logits with model</span></span><br><span class="line"><span class="comment"># 使用模型进行预测逻辑</span></span><br><span class="line">y_logits = model_4(X_blob_test.to(device))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform softmax calculation on logits across dimension 1 to get prediction probabilities</span></span><br><span class="line"><span class="comment"># 对 1 维上的 logits 执行 softmax 计算，得到预测概率</span></span><br><span class="line">y_pred_probs = torch.softmax(y_logits, dim=<span class="number">1</span>) </span><br><span class="line"><span class="built_in">print</span>(y_logits[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(y_pred_probs[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-1.2549, -0.8112, -1.4795, -0.5696],</span><br><span class="line">        [ 1.7168, -1.2270,  1.7367,  2.1010],</span><br><span class="line">        [ 2.2400,  0.7714,  2.6020,  1.0107],</span><br><span class="line">        [-0.7993, -0.3723, -0.9138, -0.5388],</span><br><span class="line">        [-0.4332, -1.6117, -0.6891,  0.6852]], device=&#x27;cuda:0&#x27;,</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line">tensor([[0.1872, 0.2918, 0.1495, 0.3715],</span><br><span class="line">        [0.2824, 0.0149, 0.2881, 0.4147],</span><br><span class="line">        [0.3380, 0.0778, 0.4854, 0.0989],</span><br><span class="line">        [0.2118, 0.3246, 0.1889, 0.2748],</span><br><span class="line">        [0.1945, 0.0598, 0.1506, 0.5951]], device=&#x27;cuda:0&#x27;,</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>softmax 函数的输出可能看起来仍然是乱码（确实如此，因为我们的模型尚未经过训练，并且使用随机模式进行预测），但每个样本都有非常具体的区别。</p>
<p>将 logits 传递到 softmax 函数后，每个样本现在都加到 1（或非常接近）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sum the first sample output of the softmax activation function</span></span><br><span class="line"><span class="comment"># 对softmax激活函数的第一个样本输出求和</span></span><br><span class="line">torch.<span class="built_in">sum</span>(y_pred_probs[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(1., device=&#x27;cuda:0&#x27;, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>这些预测概率本质上说明了模型认为目标 X 样本（输入）映射到每个类的程度。</p>
<p>由于 <code>y_pred_probs</code> 中每个类都有一个值，因此最高值的索引就是模型认为特定数据样本最属于的类。</p>
<p>我们可以使用 <code>torch.argmax()</code> 检查哪个索引具有最高值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Which class does the model think is *most* likely at the index 0 sample?</span></span><br><span class="line"><span class="comment"># 模型认为在索引 0 样本中哪个类最有可能？</span></span><br><span class="line"><span class="built_in">print</span>(y_pred_probs[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.argmax(y_pred_probs[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.1872, 0.2918, 0.1495, 0.3715], device=&#x27;cuda:0&#x27;,</span><br><span class="line">       grad_fn=&lt;SelectBackward0&gt;)</span><br><span class="line">tensor(3, device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<p>您可以看到 <code>torch.argmax()</code> 的输出返回 3，因此对于索引 0 处的样本的特征 (<code>X</code>)，模型预测最可能的类值 (<code>y</code>) 是 3。</p>
<p>当然，现在这只是随机猜测，所以它有 25% 的正确率（因为有四个类）。但我们可以通过训练模型来提高这些机会。</p>
<blockquote>
<p>模型的原始输出称为 logits。<br>对于多类分类问题，要将 logits 转换为预测概率，请使用 softmax 激活函数 (torch.softmax)。<br>具有最高预测概率的值的索引是模型认为在给定该样本的输入特征的情况下最有可能的类号（虽然这是一个预测，但并不意味着它是正确的）。</p>
</blockquote>
<h2 id="8-5-Creating-a-training-and-testing-loop-for-a-multi-class-PyTorch-model-为多类-PyTorch-模型创建训练和测试循环"><a href="#8-5-Creating-a-training-and-testing-loop-for-a-multi-class-PyTorch-model-为多类-PyTorch-模型创建训练和测试循环" class="headerlink" title="8.5 Creating a training and testing loop for a multi-class PyTorch model 为多类 PyTorch 模型创建训练和测试循环"></a>8.5 Creating a training and testing loop for a multi-class PyTorch model 为多类 PyTorch 模型创建训练和测试循环</h2><p>好了，现在我们已经完成了所有准备步骤，让我们编写一个训练和测试循环来改进和评估我们的模型。</p>
<p>我们之前已经完成了很多这些步骤，所以其中很多都是练习。</p>
<p>唯一的区别是，我们将调整步骤，将模型输出（<code>logits</code>）转换为预测概率（使用softmax激活函数），然后转换为预测标签（通过取softmax激活函数输出的argmax）。</p>
<p>让我们训练模型<code>epochs=100</code>，并每10个epochs评估一次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set number of epochs</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data to target device</span></span><br><span class="line">X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)</span><br><span class="line">X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_4.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_logits = model_4(X_blob_train) <span class="comment"># model outputs raw logits </span></span><br><span class="line">    y_pred = torch.softmax(y_logits, dim=<span class="number">1</span>).argmax(dim=<span class="number">1</span>) <span class="comment"># go from logits -&gt; prediction probabilities -&gt; prediction labels</span></span><br><span class="line">    <span class="comment"># print(y_logits)</span></span><br><span class="line">    <span class="comment"># 2. Calculate loss and accuracy</span></span><br><span class="line">    loss = loss_fn(y_logits, y_blob_train) </span><br><span class="line">    acc = accuracy_fn(y_true=y_blob_train,</span><br><span class="line">                      y_pred=y_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Optimizer zero grad</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Optimizer step</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_4.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass</span></span><br><span class="line">      test_logits = model_4(X_blob_test)</span><br><span class="line">      test_pred = torch.softmax(test_logits, dim=<span class="number">1</span>).argmax(dim=<span class="number">1</span>)</span><br><span class="line">      <span class="comment"># 2. Calculate test loss and accuracy</span></span><br><span class="line">      test_loss = loss_fn(test_logits, y_blob_test)</span><br><span class="line">      test_acc = accuracy_fn(y_true=y_blob_test,</span><br><span class="line">                             y_pred=test_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Loss: <span class="subst">&#123;loss:<span class="number">.5</span>f&#125;</span>, Acc: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% | Test Loss: <span class="subst">&#123;test_loss:<span class="number">.5</span>f&#125;</span>, Test Acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>) </span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%</span><br><span class="line">Epoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%</span><br><span class="line">Epoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%</span><br><span class="line">Epoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%</span><br><span class="line">Epoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%</span><br><span class="line">Epoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%</span><br><span class="line">Epoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%</span><br><span class="line">Epoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%</span><br><span class="line">Epoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%</span><br><span class="line">Epoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%</span><br><span class="line">Epoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%</span><br><span class="line">Epoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%</span><br><span class="line">Epoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%</span><br><span class="line">Epoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%</span><br><span class="line">Epoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%</span><br><span class="line">Epoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%</span><br><span class="line">Epoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%</span><br></pre></td></tr></table></figure>
<h2 id="8-6-Making-and-evaluating-predictions-with-a-PyTorch-multi-class-model-使用-PyTorch-多类模型进行预测并评估预测"><a href="#8-6-Making-and-evaluating-predictions-with-a-PyTorch-multi-class-model-使用-PyTorch-多类模型进行预测并评估预测" class="headerlink" title="8.6 Making and evaluating predictions with a PyTorch multi-class model 使用 PyTorch 多类模型进行预测并评估预测"></a>8.6 Making and evaluating predictions with a PyTorch multi-class model 使用 PyTorch 多类模型进行预测并评估预测</h2><p>看起来我们训练过的模型表现得相当不错。</p>
<p>但为了确保这一点，让我们做一些预测并将它们可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions</span></span><br><span class="line">model_4.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_logits = model_4(X_blob_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the first 10 predictions</span></span><br><span class="line">y_logits[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  4.3377,  10.3539, -14.8948,  -9.7642],</span><br><span class="line">        [  5.0142, -12.0371,   3.3860,  10.6699],</span><br><span class="line">        [ -5.5885, -13.3448,  20.9894,  12.7711],</span><br><span class="line">        [  1.8400,   7.5599,  -8.6016,  -6.9942],</span><br><span class="line">        [  8.0726,   3.2906, -14.5998,  -3.6186],</span><br><span class="line">        [  5.5844, -14.9521,   5.0168,  13.2890],</span><br><span class="line">        [ -5.9739, -10.1913,  18.8655,   9.9179],</span><br><span class="line">        [  7.0755,  -0.7601,  -9.5531,   0.1736],</span><br><span class="line">        [ -5.5918, -18.5990,  25.5309,  17.5799],</span><br><span class="line">        [  7.3142,   0.7197, -11.2017,  -1.2011]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<p>看起来我们模型的预测仍然是 <code>logit</code> 形式。</p>
<p>但为了评估它们，它们必须与我们的标签 (<code>y_blob_test</code>) 具有相同的形式，后者是整数形式。</p>
<p>让我们将模型的预测 <code>logit</code> 转换为预测概率（使用 <code>torch.softmax()</code>），然后转换为预测标签（通过获取每个样本的 <code>argmax()</code>）。</p>
<blockquote>
<p>可以跳过 <code>torch.softmax()</code> 函数，直接在 <code>logits</code> 上调用 <code>torch.argmax()</code>，从预测 <code>logits</code> -&gt; <code>predicted labels</code> 直接进入。<br>例如，<code>y_preds = torch.argmax(y_logits, dim=1)</code>，这节省了一个计算步骤（没有 <code>torch.softmax()</code>），但导致没有可用的预测概率。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn predicted logits in prediction probabilities</span></span><br><span class="line"><span class="comment"># 将预测的逻辑转换为预测概率</span></span><br><span class="line">y_pred_probs = torch.softmax(y_logits, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn prediction probabilities into prediction labels</span></span><br><span class="line"><span class="comment"># 将预测概率转化为预测标签</span></span><br><span class="line">y_preds = y_pred_probs.argmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare first 10 model preds and test labels</span></span><br><span class="line"><span class="comment"># 比较前 10 个模型预测和测试标签</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predictions: <span class="subst">&#123;y_preds[:<span class="number">10</span>]&#125;</span>\nLabels: <span class="subst">&#123;y_blob_test[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test accuracy: <span class="subst">&#123;accuracy_fn(y_true=y_blob_test, y_pred=y_preds)&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Predictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device=&#x27;cuda:0&#x27;)</span><br><span class="line">Labels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device=&#x27;cuda:0&#x27;)</span><br><span class="line">Test accuracy: 99.5%</span><br></pre></td></tr></table></figure>
<p>模型预测现在与测试标签的形式相同。</p>
<p>使用 <code>plot_decision_boundary()</code> 将它们可视化，请记住，因为我们的数据在 GPU 上，所以我们必须将其移动到 CPU 以便与 matplotlib 一起使用（<code>plot_decision_boundary()</code> 会自动为我们执行此操作）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_4, X_blob_train, y_blob_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_decision_boundary(model_4, X_blob_test, y_blob_test)</span><br></pre></td></tr></table></figure>
<img src="/2024/08/16/PyTorch-26H-3/PyTorch-26H-3-14.png" class="" title="PyTorch-26H-3-14">
<h1 id="9-More-classification-evaluation-metrics-更多分类评估指标"><a href="#9-More-classification-evaluation-metrics-更多分类评估指标" class="headerlink" title="9. More classification evaluation metrics 更多分类评估指标"></a>9. More classification evaluation metrics 更多分类评估指标</h1><p>到目前为止，我们仅介绍了评估分类模型的几种方法（准确性、损失和可视化预测）。</p>
<p>这些是您会遇到的一些最常见的方法，并且是一个很好的起点。</p>
<p>可能希望使用更多指标来评估分类模型，例如：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">指标名称/评估方法</th>
<th style="text-align:center">定义</th>
<th style="text-align:center">代码</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">预测精度Accuracy</td>
<td style="text-align:center">在 100 个预测中，您的模型有多少个预测正确？例如，95% 的准确率意味着 100 个预测中有 95 个正确。</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#id3">torchmetrics.Accuracy()</a> or <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">sklearn.metrics.accuracy_score()</a></td>
</tr>
<tr>
<td style="text-align:center">准确率Precision</td>
<td style="text-align:center">真阳性与样本总数的比例。精度越高，假阳性越少（模型预测为 1，但实际应该是 0）。</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://torchmetrics.readthedocs.io/en/stable/classification/precision.html#id4">torchmetrics.Precision()</a> or <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">sklearn.metrics.precision_score()</a></td>
</tr>
<tr>
<td style="text-align:center">召回 Recall</td>
<td style="text-align:center">真阳性占真阳性和假阴性总数的比例（模型预测为 0，但实际应为 1）。召回率越高，假阴性越少。</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://torchmetrics.readthedocs.io/en/stable/classification/recall.html#id5">torchmetrics.Recall()</a> or <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html">sklearn.metrics.recall_score()</a></td>
</tr>
<tr>
<td style="text-align:center">F1分数 F1-score</td>
<td style="text-align:center">将精度和召回率结合为一个指标。1 表示最好，0 表示最差。</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://torchmetrics.readthedocs.io/en/stable/classification/f1_score.html#f1score">torchmetrics.F1Score()</a> or <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">sklearn.metrics.f1_score()</a></td>
</tr>
<tr>
<td style="text-align:center">混淆矩阵 <a target="_blank" rel="noopener" href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">Confusion matrix</a></td>
<td style="text-align:center">以表格方式将预测值与真实值进行比较，如果 100% 正确，矩阵中的所有值将从左上角到右下角（对角线）。</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html#confusionmatrix">torchmetrics.ConfusionMatrix</a> or <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions">sklearn.metrics.plot_confusion_matrix()</a></td>
</tr>
<tr>
<td style="text-align:center">分类报告 Classification report</td>
<td style="text-align:center">收集一些主要的分类指标，例如精确度、召回率和 f1 分数。</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html">sklearn.metrics.classification_report()</a></td>
</tr>
</tbody>
</table>
</div>
<p>Scikit-Learn（一个流行的、世界一流的机器学习库）对上述指标有许多实现，如果你正在寻找一个类似 PyTorch 的版本，请查看 <a target="_blank" rel="noopener" href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics</a>，尤其是 <a target="_blank" rel="noopener" href="https://torchmetrics.readthedocs.io/en/stable/pages/classification.html">TorchMetrics 分类部分</a>。</p>
<p>尝试一下 <code>torchmetrics.Accuracy</code> 指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> torchmetrics <span class="keyword">import</span> Accuracy</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    !pip install torchmetrics==<span class="number">0.9</span><span class="number">.3</span> <span class="comment"># this is the version we&#x27;re using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)</span></span><br><span class="line">    <span class="keyword">from</span> torchmetrics <span class="keyword">import</span> Accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup metric and make sure it&#x27;s on the target device</span></span><br><span class="line">torchmetrics_accuracy = Accuracy(task=<span class="string">&#x27;multiclass&#x27;</span>, num_classes=<span class="number">4</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate accuracy</span></span><br><span class="line">torchmetrics_accuracy(y_preds, y_blob_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.9950, device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><p>所有练习都集中于练习以上部分中的代码。</p>
<p>您应该能够通过参考每个部分或按照链接的资源来完成它们。</p>
<p>所有练习都应使用<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">设备激动代码</a>来完成。</p>
<p>资源：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/02_pytorch_classification_exercises.ipynb">练习模板笔记本02</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/02_pytorch_classification_exercise_solutions.ipynb">02 的示例解决方案笔记本</a>（在查看<em>之前先</em>尝试练习）</li>
</ul>
<ol>
<li><p>使用 Scikit-Learn 的函数创建二元分类数据集  <code>make_moons()</code>。</p>
<ul>
<li>为了一致性，数据集应该有 1000 个样本和一个<code>random_state=42</code>。</li>
<li>将数据转换为 PyTorch 张量。将数据分为训练集和测试集，<code>train_test_split</code>其中 80% 用于训练，20% 用于测试。</li>
</ul>
</li>
<li><p>通过子类化构建一个模型<code>nn.Module</code>，该模型包含非线性激活函数，并且能够拟合您在 1 中创建的数据。</p>
<ul>
<li>请随意使用您想要的 PyTorch 层（线性和非线性）的任意组合。</li>
</ul>
</li>
<li><p>设置二元分类兼容的损失函数和优化器，以便在训练模型时使用。</p>
</li>
<li><p>创建一个训练和测试循环，以使您在 2 中创建的模型适合您在 1 中创建的数据。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://torchmetrics.readthedocs.io/en/latest/">为了测量模型准确性，您可以创建自己的准确性函数或使用TorchMetrics</a>中的准确性函数。</li>
<li>对模型进行足够长时间的训练，以达到 96% 以上的准确率。</li>
<li>训练循环应该每 10 个时期输出一次模型训练和测试集损失和准确率的进度。</li>
</ul>
</li>
<li><p>使用训练好的模型进行预测，并使用<code>plot_decision_boundary()</code>此笔记本中创建的函数绘制它们。</p>
</li>
<li><p>在纯 PyTorch 中复制 Tanh（双曲正切）激活函数。</p>
<ul>
<li>请随意参考<a target="_blank" rel="noopener" href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh">ML 备忘单网站</a>来获取该公式。</li>
</ul>
</li>
<li><p>使用<a target="_blank" rel="noopener" href="https://cs231n.github.io/neural-networks-case-study/">CS231n 中的螺旋数据创建功能</a> 创建多类数据集（代码见下文）。</p>
<ul>
<li>构建一个能够拟合数据的模型（您可能需要线性和非线性层的组合）。</li>
<li>构建一个能够处理多类数据的损失函数和优化器（可选扩展：使用 Adam 优化器而不是 SGD，您可能必须尝试不同的学习率值才能使其发挥作用）。</li>
<li>对多类数据进行训练和测试循环，并在其上训练模型以达到 95% 以上的测试准确率（您可以在此处使用任何您喜欢的准确率测量函数）。</li>
<li>根据模型预测在螺旋数据集上绘制决策边界，该<code>plot_decision_boundary()</code>函数也适用于该数据集。</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Code for creating a spiral dataset from CS231n</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">N = <span class="number">100</span> <span class="comment"># number of points per class</span></span><br><span class="line">D = <span class="number">2</span> <span class="comment"># dimensionality</span></span><br><span class="line">K = <span class="number">3</span> <span class="comment"># number of classes</span></span><br><span class="line">X = np.zeros((N*K,D)) <span class="comment"># data matrix (each row = single example)</span></span><br><span class="line">y = np.zeros(N*K, dtype=<span class="string">&#x27;uint8&#x27;</span>) <span class="comment"># class labels</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">  ix = <span class="built_in">range</span>(N*j,N*(j+<span class="number">1</span>))</span><br><span class="line">  r = np.linspace(<span class="number">0.0</span>,<span class="number">1</span>,N) <span class="comment"># radius</span></span><br><span class="line">  t = np.linspace(j*<span class="number">4</span>,(j+<span class="number">1</span>)*<span class="number">4</span>,N) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># theta</span></span><br><span class="line">  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]</span><br><span class="line">  y[ix] = j</span><br><span class="line"><span class="comment"># lets visualize the data</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="Extra-curriculum-课外活动"><a href="#Extra-curriculum-课外活动" class="headerlink" title="Extra-curriculum 课外活动"></a>Extra-curriculum 课外活动</h1><ul>
<li>写下 3 个您认为机器分类可能有用的问题（可以是任何问题，您可以发挥创造力，例如，根据购买金额和购买地点特征将信用卡交易分类为欺诈或非欺诈）。</li>
<li>研究基于梯度的优化器（如 SGD 或 Adam）中的“动量”概念，它是什么意思？</li>
<li>花 10 分钟阅读<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions">Wikipedia 上关于不同激活函数的页面</a>，其中有多少个你能与<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">PyTorch 的激活函数</a>相媲美？</li>
<li>研究何时准确度可能不是一个好的衡量标准（提示：阅读<a target="_blank" rel="noopener" href="https://willkoehrsen.github.io/statistics/learning/beyond-accuracy-precision-and-recall/">Will Koehrsen 的《超越准确度》</a>来获取想法）。</li>
<li><strong>观看：</strong>要了解我们的神经网络内部发生的情况以及它们如何学习，请观看<a target="_blank" rel="noopener" href="https://youtu.be/7sB052Pz0sQ">麻省理工学院的深度学习简介视频</a>。</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2024/08/16/PyTorch-26H-3/">http://hibiscidai.com/2024/08/16/PyTorch-26H-3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2024/08/17/PyTorch-26H-4/"><i class="fa fa-chevron-left">  </i><span>PyTorch-26H-4</span></a></div><div class="next-post pull-right"><a href="/2024/08/15/PyTorch-26H-2/"><span>PyTorch-26H-2</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://www.paofu.cloud/auth/register?code=j4I7">好用、实惠、稳定的梯子,点击这里<img src="https://pic.imgdb.cn/item/65572abac458853aefef30cd.png" width="1000" height="124" object-fit="cover" ></a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2024 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>