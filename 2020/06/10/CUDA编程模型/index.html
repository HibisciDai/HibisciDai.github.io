<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="CUDA编程模型"><meta name="keywords" content="CUDA,高性能计算,并行计算"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>CUDA编程模型 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">CUDA编程模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CUDA-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E2%80%93-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">CUDA 编程模型– 基本概念和数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-%E2%80%93-C-%E7%A8%8B%E5%BA%8F"><span class="toc-number">2.1.</span> <span class="toc-text">CUDA – C 程序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-%E8%AE%BE%E5%A4%87%E5%92%8C%E7%BA%BF%E7%A8%8B-Threads"><span class="toc-number">2.2.</span> <span class="toc-text">CUDA 设备和线程(Threads)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA%E6%89%A9%E5%B1%95%E7%9A%84C"><span class="toc-number">2.3.</span> <span class="toc-text">CUDA扩展的C</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A3%B0%E6%98%8E%E8%A7%84%E8%8C%83"><span class="toc-number">2.3.1.</span> <span class="toc-text">声明规范</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E5%AD%97%EF%BC%88%E5%86%85%E5%BB%BA%E5%8F%98%E9%87%8F%EF%BC%89"><span class="toc-number">2.3.2.</span> <span class="toc-text">关键字（内建变量）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BA%E6%9C%89%E7%9A%84"><span class="toc-number">2.3.3.</span> <span class="toc-text">固有的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%97%B6API"><span class="toc-number">2.3.4.</span> <span class="toc-text">运行时API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8"><span class="toc-number">2.3.5.</span> <span class="toc-text">函数调用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%95%B0%E7%BB%84"><span class="toc-number">2.4.</span> <span class="toc-text">并行的线程数组</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E5%9D%97-Thread-Block-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%90%88%E4%BD%9C"><span class="toc-number">2.5.</span> <span class="toc-text">线程块(Thread Block): 可扩展的合作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Block-IDs-%E5%92%8C-Thread-IDs"><span class="toc-number">2.6.</span> <span class="toc-text">Block IDs 和 Thread IDs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0%E2%80%BB"><span class="toc-number">2.7.</span> <span class="toc-text">CUDA 内存模型概述※</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CUDA-%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%BC%96%E7%A8%8B%E6%8E%A5%E5%8F%A3API-%E5%9F%BA%E6%9C%AC"><span class="toc-number">3.</span> <span class="toc-text">CUDA 应用程序编程接口API-基本</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-%E8%AE%BE%E5%A4%87%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D"><span class="toc-number">3.1.</span> <span class="toc-text">CUDA 设备内存分配</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cudaMalloc"><span class="toc-number">3.1.1.</span> <span class="toc-text">cudaMalloc()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cudaFree"><span class="toc-number">3.1.2.</span> <span class="toc-text">cudaFree()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-number">3.1.3.</span> <span class="toc-text">代码示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-Host-Device%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93"><span class="toc-number">3.2.</span> <span class="toc-text">CUDA Host-Device数据传输</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cudaMemcpy"><span class="toc-number">3.2.1.</span> <span class="toc-text">cudaMemcpy()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E4%BC%A0%E8%BE%93"><span class="toc-number">3.2.2.</span> <span class="toc-text">异步传输</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">3.2.3.</span> <span class="toc-text">代码示例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CUDA-%E5%87%BD%E6%95%B0%E5%A3%B0%E6%98%8E"><span class="toc-number">4.</span> <span class="toc-text">CUDA 函数声明</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E7%94%A8Kernel%E5%87%BD%E6%95%B0-%E2%80%93-%E7%BA%BF%E7%A8%8B%E5%88%9B%E5%BB%BA"><span class="toc-number">4.1.</span> <span class="toc-text">调用Kernel函数 – 线程创建</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8F%AF%E8%BF%90%E8%A1%8C%E7%9A%84%E4%BE%8B%E5%AD%90-%E7%9F%A2%E9%87%8F%E7%9B%B8%E5%8A%A0"><span class="toc-number">5.</span> <span class="toc-text">一个简单的可运行的例子-矢量相加</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-Syntax-%E2%80%93-Unified-Memory-%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98-%E4%B8%8D%E5%BB%BA%E8%AE%AE%E4%BD%BF%E7%94%A8"><span class="toc-number">5.1.</span> <span class="toc-text">CUDA Syntax – Unified Memory 统一内存(不建议使用)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A2%E9%87%8F%E7%9B%B8%E5%8A%A0Unified-Memory%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.1.1.</span> <span class="toc-text">矢量相加Unified Memory实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%85%E4%BB%85%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AAThread-Block%E5%A4%84%E7%90%86%E7%9F%A2%E9%87%8F"><span class="toc-number">5.2.</span> <span class="toc-text">仅仅使用一个Thread Block处理矢量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E4%BB%BB%E6%84%8F%E5%A4%A7%E5%B0%8F%E7%9A%84%E7%9F%A2%E9%87%8F"><span class="toc-number">5.3.</span> <span class="toc-text">处理任意大小的矢量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%99%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9B%B8%E5%8A%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%A8%8B%E5%BA%8F%EF%BC%8C%E7%9F%A9%E9%98%B5%E5%A4%A7%E5%B0%8F%E4%B8%BA35-35%EF%BC%8C%E8%AF%B7%E7%94%A8%E4%BB%BB%E6%84%8F%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5A-B"><span class="toc-number">5.4.</span> <span class="toc-text">写一个矩阵相加的并行计算程序，矩阵大小为35 * 35，请用任意数初始化两个矩阵A,B.</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%BB%E6%84%8F%E5%8F%AF%E6%89%A7%E8%A1%8C%E7%9A%84CUDA%E4%BB%A3%E7%A0%81%E9%9C%80%E8%A6%81%E4%B8%A4%E4%B8%AA%E5%8A%A8%E6%80%81%E5%BA%93"><span class="toc-number">6.</span> <span class="toc-text">任意可执行的CUDA代码需要两个动态库</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%9C%BA%E5%99%A8%E6%80%A7%E8%83%BD"><span class="toc-number">7.</span> <span class="toc-text">查看机器性能</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E7%BA%BF%E7%A8%8B%E5%9D%97%EF%BC%88Thread-Blocks%EF%BC%89"><span class="toc-number">8.</span> <span class="toc-text">执行线程块（Thread Blocks）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8block%E7%B2%92%E5%BA%A6%E4%B8%8A%EF%BC%8C%E7%BA%BF%E7%A8%8B%E6%98%AF%E8%A2%AB%E5%88%86%E9%85%8D%E5%88%B0%E6%B5%81%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8-Streaming-Multiprocessors-%E7%89%B9%E5%AE%9A%E6%9E%B6%E6%9E%84"><span class="toc-number">8.1.</span> <span class="toc-text">在block粒度上，线程是被分配到流多处理器(Streaming Multiprocessors)[特定架构]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%98%AF%E5%B9%B6%E5%8F%91%E5%90%8C%E6%97%B6%E6%89%A7%E8%A1%8C"><span class="toc-number">8.2.</span> <span class="toc-text">线程的执行是并发同时执行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6"><span class="toc-number">9.</span> <span class="toc-text">线程调度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AF%8F%E4%B8%AABlock%E7%9A%84%E6%89%A7%E8%A1%8C%E5%A6%8232-thread-Warps"><span class="toc-number">9.1.</span> <span class="toc-text">每个Block的执行如32-thread Warps</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C3-Blocks%E8%A2%AB%E5%88%86%E9%85%8D%E5%88%B0%E4%B8%80%E4%B8%AASM%E4%B8%AD%EF%BC%8C%E6%AF%8F%E4%B8%AABlock%E6%9C%89-256-threads-%E9%82%A3%E4%B9%88%E5%9C%A81%E4%B8%AASM%E6%9C%89%E5%A4%9A%E5%B0%91%E4%B8%AAWarps"><span class="toc-number">9.2.</span> <span class="toc-text">如果3 Blocks被分配到一个SM中，每个Block有 256 threads, 那么在1个SM有多少个Warps?</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E5%9D%97%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86"><span class="toc-number">10.</span> <span class="toc-text">线程块如何切分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Thread-blocks-%E8%A2%AB%E5%88%87%E8%BF%9Bwarps"><span class="toc-number">10.1.</span> <span class="toc-text">Thread blocks 被切进warps</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%87%E5%88%86%E6%80%BB%E6%98%AF%E7%9B%B8%E5%90%8C%E7%9A%84"><span class="toc-number">10.2.</span> <span class="toc-text">切分总是相同的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%84%B6%E8%80%8C%EF%BC%8Cwarps%E4%B9%8B%E9%97%B4%E4%B8%8D%E5%AD%98%E5%9C%A8%E4%BB%BB%E4%BD%95%E9%A1%BA%E5%BA%8F%E7%9A%84%E4%BE%9D%E8%B5%96"><span class="toc-number">10.3.</span> <span class="toc-text">然而，warps之间不存在任何顺序的依赖</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A7%E5%88%B6%E6%B5%81%E6%8C%87%E4%BB%A4"><span class="toc-number">11.</span> <span class="toc-text">控制流指令</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%B9%B6%E8%A1%8C%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%80%A7%E8%83%BD%E4%B8%BB%E8%A6%81%E8%80%83%E9%87%8F%E7%9A%84%E5%88%86%E6%94%AF%E6%98%AF%E5%8F%91%E6%95%A3%E7%9A%84"><span class="toc-number">11.1.</span> <span class="toc-text">影响并行程序的性能主要考量的分支是发散的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E5%B8%B8%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95-%E5%BD%93%E8%BF%99%E4%B8%AA%E5%88%86%E6%94%AF%E6%9D%A1%E4%BB%B6%E6%98%AFthread-ID%E7%9A%84%E5%87%BD%E6%95%B0%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E8%A6%81%E9%81%BF%E5%85%8D%E8%BF%99%E7%A7%8D%E5%8F%91%E6%95%A3%E6%83%85%E5%86%B5%E7%9A%84%E5%8F%91%E7%94%9F"><span class="toc-number">11.2.</span> <span class="toc-text">通常处理方法: 当这个分支条件是thread ID的函数的时候，要避免这种发散情况的发生</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9D%97%E5%B0%BA%E5%AF%B8%E8%80%83%E8%99%91"><span class="toc-number">12.</span> <span class="toc-text">块尺寸考虑</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98%EF%BC%8C%E9%9C%80%E8%A6%81%E8%A6%81%E5%A4%9A%E4%B8%AABlocks%EF%BC%8C%E9%82%A3%E4%B9%88%E5%88%B0%E5%BA%95%E7%94%A88X8-16X16-%E8%BF%98%E6%98%AF-32X32-%E7%9A%84%E5%9D%97%E5%B0%BA%E5%AF%B8"><span class="toc-number">12.1.</span> <span class="toc-text">对于矩阵相乘，需要要多个Blocks，那么到底用8X8, 16X16 还是 32X32 的块尺寸?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E5%A0%82%E7%BB%83%E4%B9%A0"><span class="toc-number">12.2.</span> <span class="toc-text">课堂练习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4%E7%BA%BF%E7%A8%8B%E5%BA%94%E7%94%A8%EF%BC%9A%E6%96%B9%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98"><span class="toc-number">13.</span> <span class="toc-text">多维线程应用：方矩阵相乘</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98CPU%E7%AB%AF%E4%BB%A3%E7%A0%81"><span class="toc-number">13.1.</span> <span class="toc-text">矩阵相乘CPU端代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E7%9F%A9%E9%98%B5%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93-Host%E7%AB%AF%E4%BB%A3%E7%A0%81"><span class="toc-number">13.2.</span> <span class="toc-text">输入矩阵数据传输(Host端代码)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kernel%E5%87%BD%E6%95%B0"><span class="toc-number">13.3.</span> <span class="toc-text">Kernel函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kernel%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8-host-side-Code"><span class="toc-number">13.4.</span> <span class="toc-text">Kernel函数调用(host-side Code)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%85%E9%80%82%E7%94%A8%E4%B8%80%E4%B8%AAThread-Block"><span class="toc-number">13.4.1.</span> <span class="toc-text">仅适用一个Thread Block</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E4%BB%BB%E6%84%8F%E5%A4%A7%E5%B0%8F%E7%9A%84%E6%96%B9%E9%98%B5"><span class="toc-number">13.5.</span> <span class="toc-text">处理任意大小的方阵</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%A8%8D%E5%A4%A7%E4%BA%9B%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">13.5.1.</span> <span class="toc-text">一个稍大些的例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E5%86%8D%E5%A4%A7%E4%B8%80%E4%BA%9B%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">13.5.2.</span> <span class="toc-text">一个再大一些的例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-C-%E7%9A%84%E2%80%9C%E8%A1%8C%E4%B8%BB%E2%80%9D%E5%88%86%E5%B8%83"><span class="toc-number">13.5.3.</span> <span class="toc-text">C&#x2F;C++的“行主”分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Blcok-0-0"><span class="toc-number">13.5.4.</span> <span class="toc-text">Blcok(0,0)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Blcok-0-1"><span class="toc-number">13.5.5.</span> <span class="toc-text">Blcok(0,1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98Kernel"><span class="toc-number">13.5.6.</span> <span class="toc-text">一个简单的矩阵相乘Kernel</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">223</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">76</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">29</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">CUDA编程模型</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-10</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/CUDA/">CUDA</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3.6k</span><span class="post-meta__separator">|</span><span>阅读时长: 14 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.png" class="" title="CUDA编程模型">
<p>CUDA编程模型</p>
<span id="more"></span>
<p>[TOC]</p>
<h1 id="CUDA编程模型"><a href="#CUDA编程模型" class="headerlink" title="CUDA编程模型"></a>CUDA编程模型</h1><p>数据传输有cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);<br>常量有<code>cudaMemcpyHostToDevice</code>，<code>cudaMemcpyDeviceToHost</code>，<code>cudaMemcpyDeviceToDevice</code>。</p>
<h1 id="CUDA-编程模型–-基本概念和数据类型"><a href="#CUDA-编程模型–-基本概念和数据类型" class="headerlink" title="CUDA 编程模型– 基本概念和数据类型"></a>CUDA 编程模型– 基本概念和数据类型</h1><h2 id="CUDA-–-C-程序"><a href="#CUDA-–-C-程序" class="headerlink" title="CUDA – C 程序"></a>CUDA – C 程序</h2><p>整合 host+device 两端执行的 C 程序，异构程序。</p>
<ul>
<li>串行或者并行度不高的部分放在host 端</li>
<li>并行度高的部分放在device端<br>通过 SPMD kernel C 代码执行</li>
</ul>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-1.png" class="" title="CUDA编程模型-1">
<h2 id="CUDA-设备和线程-Threads"><a href="#CUDA-设备和线程-Threads" class="headerlink" title="CUDA 设备和线程(Threads)"></a>CUDA 设备和线程(Threads)</h2><ul>
<li><p>一个计算设备<br>（1）对于host或者CPU来说是一个协作处理器<br>（2）具有自己 DRAM (设备内存)<br>（3）并行运行许多个线程<br>（4）通常典型来说是针对于GPU，但是也可以是其它类型的并行处理设备</p>
</li>
<li><p>一个应用的数据并行部分可以在CUDA设备通过kernels调用许多个线程来执行</p>
</li>
<li><p>GPU 和CPU 线程之间的差异<br>GPU 线程是相当轻量级的<br>非常小的创建开销（由硬件负责）<br>GPU 通过需要至少创建1000个线程以上来达到高效率<br>多核 CPU 仅仅需要创建几个线程</p>
</li>
</ul>
<h2 id="CUDA扩展的C"><a href="#CUDA扩展的C" class="headerlink" title="CUDA扩展的C"></a>CUDA扩展的C</h2><h3 id="声明规范"><a href="#声明规范" class="headerlink" title="声明规范"></a>声明规范</h3><p><code>global</code>, <code>device</code>, <code>shared</code>, <code>local</code>, <code>constant</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">float</span> filter[N]; </span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">convolve</span> <span class="params">(<span class="type">float</span> *image)</span>  &#123;</span><br><span class="line"></span><br><span class="line">__shared__ <span class="type">float</span> region[M];<span class="comment">//共享内存变量，只针对一个block内的共享</span></span><br></pre></td></tr></table></figure>
<h3 id="关键字（内建变量）"><a href="#关键字（内建变量）" class="headerlink" title="关键字（内建变量）"></a>关键字（内建变量）</h3><p><code>threadIdx</code>, <code>blockIdx</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">region[threadIdx] = image[i]; </span><br></pre></td></tr></table></figure>
<h3 id="固有的"><a href="#固有的" class="headerlink" title="固有的"></a>固有的</h3><p><code>__syncthreads</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncthreads()  </span><br></pre></td></tr></table></figure>
<h3 id="运行时API"><a href="#运行时API" class="headerlink" title="运行时API"></a>运行时API</h3><p><code>Memory</code>, <code>symbol</code>, <code>execution management</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Allocate GPU memory</span></span><br><span class="line"><span class="type">void</span> *myimage = cudaMalloc(bytes)</span><br></pre></td></tr></table></figure>
<h3 id="函数调用"><a href="#函数调用" class="headerlink" title="函数调用"></a>函数调用</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 100 blocks, 10 threads per block</span></span><br><span class="line">convolve&lt;&lt;&lt;<span class="number">100</span>, <span class="number">10</span>&gt;&gt;&gt; (myimage);</span><br></pre></td></tr></table></figure>
<h2 id="并行的线程数组"><a href="#并行的线程数组" class="headerlink" title="并行的线程数组"></a>并行的线程数组</h2><ul>
<li>一个 CUDA kernel的执行是通过一个线程数组实现的。<br>（1）所有的线程执行相同的代码(SPMD)<br>（2）每一个线程有自己的ID，通过线程ID来计算访问内存地址和做出控制决定</li>
</ul>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-2.png" class="" title="CUDA编程模型-2">
<h2 id="线程块-Thread-Block-可扩展的合作"><a href="#线程块-Thread-Block-可扩展的合作" class="headerlink" title="线程块(Thread Block): 可扩展的合作"></a>线程块(Thread Block): 可扩展的合作</h2><ul>
<li>把整体的线程数组划分到多个块中<br>（1）一个Block中的Threads通过<code>shared memory</code>, <code>atomic operations</code> 和 <code>barrier synchronization</code>来进行合作<br>（2）不同Block中的Threads 不能合作</li>
</ul>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-3.png" class="" title="CUDA编程模型-3">
<h2 id="Block-IDs-和-Thread-IDs"><a href="#Block-IDs-和-Thread-IDs" class="headerlink" title="Block IDs 和 Thread IDs"></a>Block IDs 和 Thread IDs</h2><ul>
<li>每一个thread用IDs 来决定对那块数据进行操作<br>（1）Block ID: 1D 或者 2D<br>（2）Thread ID: 1D, 2D, 或者3D </li>
</ul>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-4.png" class="" title="CUDA编程模型-4">
<ul>
<li>这样对于多维数据，可以简化其内存访问模式<br>（1）图像处理<br>（2）在体元范围内解PDE方程…</li>
</ul>
<h2 id="CUDA-内存模型概述※"><a href="#CUDA-内存模型概述※" class="headerlink" title="CUDA 内存模型概述※"></a>CUDA 内存模型概述※</h2><ul>
<li>全局内存<br>（1）<code>host</code> 和 <code>device</code>端主要的数据R/W交流手段<br>（2）其内容对于所有线程都是可见的<br>（3）较长的访问延迟 </li>
<li>当前，首先关注全局内存的特点</li>
</ul>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png" class="" title="CUDA编程模型-5">
<h1 id="CUDA-应用程序编程接口API-基本"><a href="#CUDA-应用程序编程接口API-基本" class="headerlink" title="CUDA 应用程序编程接口API-基本"></a>CUDA 应用程序编程接口API-基本</h1><ul>
<li>这个 API 是对于 ANSI C 程序语言的一个扩展<br>低学习曲线</li>
<li>其专门设计的硬件可以启用轻量级的运行时API以及驱动<br>高性能</li>
</ul>
<h2 id="CUDA-设备内存分配"><a href="#CUDA-设备内存分配" class="headerlink" title="CUDA 设备内存分配"></a>CUDA 设备内存分配</h2><h3 id="cudaMalloc"><a href="#cudaMalloc" class="headerlink" title="cudaMalloc()"></a>cudaMalloc()</h3><ul>
<li>在设备端的 Global Memory 上分配内存</li>
<li>需要两个参数<br>（1）指向待分配对象的<code>指针的地址</code><br>（2）待分配对象的规模<code>大小</code></li>
</ul>
<h3 id="cudaFree"><a href="#cudaFree" class="headerlink" title="cudaFree()"></a>cudaFree()</h3><ul>
<li>从设备端的Global Memory释放内存<br>（1）指向待释放对象的指针</li>
</ul>
<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><ul>
<li>分配一个64单精度浮点数组</li>
<li>将分配空间绑定到aD</li>
<li>“D” 通常用来指示一个设备端的数据或数据结构</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">64</span>;</span><br><span class="line"><span class="type">float</span> *aD = <span class="literal">NULL</span>;</span><br><span class="line"><span class="type">int</span> size = N * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">cudaMalloc((<span class="type">void</span>**)&amp;aD, size);</span><br><span class="line">cudaFree(Md);</span><br></pre></td></tr></table></figure>
<h2 id="CUDA-Host-Device数据传输"><a href="#CUDA-Host-Device数据传输" class="headerlink" title="CUDA Host-Device数据传输"></a>CUDA Host-Device数据传输</h2><h3 id="cudaMemcpy"><a href="#cudaMemcpy" class="headerlink" title="cudaMemcpy()"></a>cudaMemcpy()</h3><ul>
<li>内存数据传输</li>
<li>需要四个参数<br>  （1）指向目标数据的指针<br>  （2）指向源数据的指针<br>  （3）需要拷贝的数据量(bytes)<br>  （4）传输类型（方向） <ul>
<li>Host to Host</li>
<li>Host to Device</li>
<li>Device to Host</li>
<li>Device to Device</li>
</ul>
</li>
</ul>
<h3 id="异步传输"><a href="#异步传输" class="headerlink" title="异步传输"></a>异步传输</h3><h3 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h3><ul>
<li>传输一个64 单精度浮点数组</li>
<li>a位于host 内存 ， aD 位于device内存</li>
<li><code>cudaMemcpyHostToDevice</code> 和 <code>cudaMemcpyDeviceToHost</code> 是符号常量</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy(aD, a, size, cudaMemcpyHostToDevice);</span><br><span class="line">cudaMemcpy(a, aD, size, cudaMemcpyDeviceToHost);</span><br></pre></td></tr></table></figure>
<h1 id="CUDA-函数声明"><a href="#CUDA-函数声明" class="headerlink" title="CUDA 函数声明"></a>CUDA 函数声明</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Executed  on the:</th>
<th style="text-align:center">Only  callable from the:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>__device__ float DeviceFunc()</code></td>
<td style="text-align:center">device</td>
<td style="text-align:center">devic</td>
</tr>
<tr>
<td style="text-align:center"><code>__global__ void KernelFunc()</code></td>
<td style="text-align:center">device</td>
<td style="text-align:center">host</td>
</tr>
<tr>
<td style="text-align:center"><code>__host__ float HostFunc()</code></td>
<td style="text-align:center">host</td>
<td style="text-align:center">host</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><code>__global__</code> 定义一个 <code>kernel</code> 函数必须返回void</li>
<li><code>__device__</code> 和 <code>__host__</code> 能一起使用，编译器会把这个函数编译为host和device通用的函数</li>
<li><code>__device__</code> 函数没有函数地址，也没有指向它的函数指针</li>
<li>在device端执行的函数有下面的限制<br>（1）没有递归<br>（2）函数内部没有静态变量<br>（3）参数的数量是固定的</li>
</ul>
<h2 id="调用Kernel函数-–-线程创建"><a href="#调用Kernel函数-–-线程创建" class="headerlink" title="调用Kernel函数 – 线程创建"></a>调用Kernel函数 – 线程创建</h2><ul>
<li>一个kernel函数在调用之前，必须在执行之前进行线程配置:</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">KernelFunc</span><span class="params">(...)</span>;</span><br><span class="line">dim3   <span class="title function_">DimGrid</span><span class="params">(<span class="number">100</span>, <span class="number">50</span>)</span>;    <span class="comment">// 5000 thread blocks </span></span><br><span class="line">dim3   <span class="title function_">DimBlock</span><span class="params">(<span class="number">4</span>, <span class="number">8</span>, <span class="number">8</span>)</span>;   <span class="comment">// 256 threads per block </span></span><br><span class="line"><span class="type">size_t</span> SharedMemBytes = <span class="number">64</span>; <span class="comment">// 64 bytes of shared memory</span></span><br><span class="line">KernelFunc&lt;&lt;&lt; DimGrid, DimBlock, SharedMemBytes &gt;&gt;&gt;(...);</span><br></pre></td></tr></table></figure>
<p>一个SM有64K，<br>1个block，SharedMemBytes=64K<br>2个block，SharedMemBytes=32K<br>4个block，SharedMemBytes=16K</p>
<p><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/deep-learning-ai/products/titan-rtx/#specs">NVIDA最新卡的信息</a></p>
<ul>
<li>Kernel函数是支持异步执行的</li>
</ul>
<h1 id="一个简单的可运行的例子-矢量相加"><a href="#一个简单的可运行的例子-矢量相加" class="headerlink" title="一个简单的可运行的例子-矢量相加"></a>一个简单的可运行的例子-矢量相加</h1><ul>
<li>一个简单的矢量相加的例子演示了在CUDA程序中的内存基本特性和线程管理方法<br>  （1）shared memory 的使用将放在后面的章节<br>  （2）局部及寄存器的使用<br>  （3）Thread ID 的使用<br>  （4）host 和device之间内存数据传输API的使用</li>
</ul>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-6.png" class="" title="CUDA编程模型-6">
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-7.png" class="" title="CUDA编程模型-7">
<p>通用代码↑</p>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-8.png" class="" title="CUDA编程模型-8">
<p>简单版本代码↑</p>
<h2 id="CUDA-Syntax-–-Unified-Memory-统一内存-不建议使用"><a href="#CUDA-Syntax-–-Unified-Memory-统一内存-不建议使用" class="headerlink" title="CUDA Syntax – Unified Memory 统一内存(不建议使用)"></a>CUDA Syntax – Unified Memory 统一内存(不建议使用)</h2><ul>
<li>CUDA 6 introduced Unified Memory<br>（1）Use <code>managed memory</code> instead of explicitly declaring memory for the host and device<br>  代替host和device内存概念<br>（2）<code>cudaMallocManaged(void **devPtr, size_t size)</code></li>
</ul>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-9.png" class="" title="CUDA编程模型-9">
<h3 id="矢量相加Unified-Memory实现"><a href="#矢量相加Unified-Memory实现" class="headerlink" title="矢量相加Unified Memory实现"></a>矢量相加Unified Memory实现</h3><p>在CPU和GPU都创建内存函数</p>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-10.png" class="" title="CUDA编程模型-10">
<h2 id="仅仅使用一个Thread-Block处理矢量"><a href="#仅仅使用一个Thread-Block处理矢量" class="headerlink" title="仅仅使用一个Thread Block处理矢量"></a>仅仅使用一个Thread Block处理矢量</h2><ul>
<li>一个thread block来计算矢量相加<br>（1）每一个线程计算矢量中的一个元素</li>
<li>每一个线程<br>（1）装载矢量a的一个数据<br>（2）装载矢量b的一个数据<br>（3）对于每一对a和b中的元素执行一次加法操作</li>
<li>同时矢量的尺寸受限于一个thread block中规定的线程数</li>
</ul>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-11.png" class="" title="CUDA编程模型-11">
<p>对于c=a+b，a有四个线程，b有四个线程，运行受限制，这个就要考虑多个block联用。</p>
<h2 id="处理任意大小的矢量"><a href="#处理任意大小的矢量" class="headerlink" title="处理任意大小的矢量"></a>处理任意大小的矢量</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">SumArrays</span><span class="params">(<span class="type">float</span>* <span class="type">const</span> a, <span class="type">float</span>* <span class="type">const</span> b, <span class="type">float</span>* <span class="type">const</span> c, <span class="type">int</span> <span class="type">const</span> N)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; N)</span><br><span class="line">        c[i] = a[i] + b[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> blkNum = ((N + BLOCK_SIZE - <span class="number">1</span>) / BLOCK_SIZE);	<span class="comment">//设置block数量</span></span><br><span class="line">SumArrays&lt;&lt;&lt;blkNum, BLOCK_SIZE&gt;&gt;&gt;(aD, bD, cD, N);	<span class="comment">//</span></span><br></pre></td></tr></table></figure>
<h2 id="写一个矩阵相加的并行计算程序，矩阵大小为35-35，请用任意数初始化两个矩阵A-B"><a href="#写一个矩阵相加的并行计算程序，矩阵大小为35-35，请用任意数初始化两个矩阵A-B" class="headerlink" title="写一个矩阵相加的并行计算程序，矩阵大小为35 * 35，请用任意数初始化两个矩阵A,B."></a>写一个矩阵相加的并行计算程序，矩阵大小为35 * 35，请用任意数初始化两个矩阵A,B.</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">MatAddonDevice</span><span class="params">(<span class="type">float</span> * aH,<span class="type">float</span>* bH, <span class="type">float</span> *cH, <span class="type">int</span> N)</span>  <span class="comment">// &#123;</span></span><br><span class="line">   <span class="type">float</span>* aD, * bD, * cD;</span><br><span class="line">   <span class="type">int</span> N_BYTES = N * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">   </span><br><span class="line">   <span class="comment">// 分配内存</span></span><br><span class="line">   cudaMalloc((<span class="type">void</span>**)&amp;aD, N_BYTES);</span><br><span class="line">   cudaMalloc((<span class="type">void</span>**)&amp;bD, N_BYTES);</span><br><span class="line">   cudaMalloc((<span class="type">void</span>**)&amp;cD, N_BYTES);</span><br><span class="line">   </span><br><span class="line">   <span class="comment">// host -&gt; Device</span></span><br><span class="line">   cudaMemcpy(aD, aH, N_BYTES, cudaMemcpyHostToDevice);</span><br><span class="line">   cudaMemcpy(bD, bH, N_BYTES, cudaMemcpyHostToDevice);</span><br><span class="line">   </span><br><span class="line">   dim3 <span class="title function_">threadsPerblock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span>;</span><br><span class="line">   dim3 <span class="title function_">blocksPerGrid</span><span class="params">((N - <span class="number">1</span>) / <span class="number">16</span> + <span class="number">1</span>,(N - <span class="number">1</span>) / <span class="number">16</span> + <span class="number">1</span>)</span>;</span><br><span class="line">   </span><br><span class="line">   vectorAdd&lt;&lt;&lt;blocksPerGrid, threadsPerblock&gt;&gt;&gt;(aD, bD, cD, N);</span><br><span class="line">   </span><br><span class="line">   cudaMemcpy(cH, cD, size, cudaMemcpyDeviceToHost);</span><br><span class="line">   </span><br><span class="line">   cudaFree(aD);</span><br><span class="line">   cudaFree(bD);</span><br><span class="line">   cudaFree(cD);</span><br><span class="line">   </span><br><span class="line">   <span class="built_in">free</span>(aH);</span><br><span class="line">   <span class="built_in">free</span>(bH);</span><br><span class="line">   <span class="built_in">free</span>(cH);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">___global___ <span class="type">void</span> <span class="title function_">MatAddKernel</span><span class="params">(<span class="type">float</span> *a, <span class="type">float</span>*b, <span class="type">float</span> *c, <span class="type">int</span> N)</span></span><br><span class="line">&#123;</span><br><span class="line">   <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">   <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">   <span class="keyword">if</span> (Col &lt; WIDTH &amp;&amp; Row &lt; HEIGHT)</span><br><span class="line">	&#123;</span><br><span class="line">		c[Row * N + Col] = a[Row * N + Col] + b[Row * N + Col];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="任意可执行的CUDA代码需要两个动态库"><a href="#任意可执行的CUDA代码需要两个动态库" class="headerlink" title="任意可执行的CUDA代码需要两个动态库"></a>任意可执行的CUDA代码需要两个动态库</h1><p>（1）CUDA 运行时库 (<code>cudart</code>)<br>（2）CUDA 核心库 (<code>cuda</code>)</p>
<h1 id="查看机器性能"><a href="#查看机器性能" class="headerlink" title="查看机器性能"></a>查看机器性能</h1><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-12.png" class="" title="CUDA编程模型-12">
<h1 id="执行线程块（Thread-Blocks）"><a href="#执行线程块（Thread-Blocks）" class="headerlink" title="执行线程块（Thread Blocks）"></a>执行线程块（Thread Blocks）</h1><ul>
<li>在<code>Block</code>层面上的资源分配</li>
</ul>
<p>grid-&gt;blocks-&gt;threads(256)</p>
<ul>
<li>在<code>Warp</code>层面上的调度形式</li>
</ul>
<p>数据存储读取问题</p>
<ul>
<li><code>SIMD</code>执行的 SIMT实现</li>
</ul>
<p>单指令多数据-SIMD在CUDA中是单指令多线程-SIMT</p>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E6%89%A7%E8%A1%8C%E7%BA%BF%E7%A8%8B%E5%9D%97-1.png" class="" title="执行线程块-1">
<h2 id="在block粒度上，线程是被分配到流多处理器-Streaming-Multiprocessors-特定架构"><a href="#在block粒度上，线程是被分配到流多处理器-Streaming-Multiprocessors-特定架构" class="headerlink" title="在block粒度上，线程是被分配到流多处理器(Streaming Multiprocessors)[特定架构]"></a>在<code>block</code>粒度上，线程是被分配到流多处理器(<code>Streaming Multiprocessors</code>)[特定架构]</h2><ul>
<li>每个<code>SM</code>最多运行放<code>8 blocks</code></li>
<li>每个<code>Fermi SM</code>能接收<code>1536 threads</code><br>  可以装 256 (threads/block) <em> 6 blocks<br>  或者 512 (threads/block) </em> 3 blocks，等。</li>
</ul>
<h2 id="线程的执行是并发同时执行"><a href="#线程的执行是并发同时执行" class="headerlink" title="线程的执行是并发同时执行"></a>线程的执行是并发同时执行</h2><ul>
<li>SM 维护thread/block id </li>
<li>SM 管理/调度 线程执行</li>
</ul>
<h1 id="线程调度"><a href="#线程调度" class="headerlink" title="线程调度"></a>线程调度</h1><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6-1.png" class="" title="线程调度-1">
<h2 id="每个Block的执行如32-thread-Warps"><a href="#每个Block的执行如32-thread-Warps" class="headerlink" title="每个Block的执行如32-thread Warps"></a>每个Block的执行如32-thread Warps</h2><ul>
<li>Warps非CUDA编程模型的一部分，是一个<code>硬件层</code>的设施。</li>
<li>grid/block/thread是编程模型。</li>
<li>Warps是SM中的调度单元。</li>
</ul>
<h2 id="如果3-Blocks被分配到一个SM中，每个Block有-256-threads-那么在1个SM有多少个Warps"><a href="#如果3-Blocks被分配到一个SM中，每个Block有-256-threads-那么在1个SM有多少个Warps" class="headerlink" title="如果3 Blocks被分配到一个SM中，每个Block有 256 threads, 那么在1个SM有多少个Warps?"></a>如果3 Blocks被分配到一个SM中，每个Block有 256 threads, 那么在1个SM有多少个Warps?</h2><p>每个Block被划分成 256 / 32 = 8 Warps。<br>这里将会有 8 * 3 = 24 Warps 。</p>
<h1 id="线程块如何切分"><a href="#线程块如何切分" class="headerlink" title="线程块如何切分"></a>线程块如何切分</h1><h2 id="Thread-blocks-被切进warps"><a href="#Thread-blocks-被切进warps" class="headerlink" title="Thread blocks 被切进warps"></a>Thread blocks 被切进warps</h2><p>在一个Warp中的Thread IDs 是连续递增的。<br><code>Warp 0</code>开始于<code>Thread ID 0</code>。</p>
<h2 id="切分总是相同的"><a href="#切分总是相同的" class="headerlink" title="切分总是相同的"></a>切分总是相同的</h2><p>在控制流中，需要用到这部分内容。<br>然而，下一代的硬件设备可能改变Warps的规模大小。<br>一些内容后面将涉及。</p>
<h2 id="然而，warps之间不存在任何顺序的依赖"><a href="#然而，warps之间不存在任何顺序的依赖" class="headerlink" title="然而，warps之间不存在任何顺序的依赖"></a>然而，warps之间不存在任何顺序的依赖</h2><p>如果线程之间出现任何依赖关系，必须用<code>__syncthreads()</code>来得到正确的结果。</p>
<h1 id="控制流指令"><a href="#控制流指令" class="headerlink" title="控制流指令"></a>控制流指令</h1><h2 id="影响并行程序的性能主要考量的分支是发散的"><a href="#影响并行程序的性能主要考量的分支是发散的" class="headerlink" title="影响并行程序的性能主要考量的分支是发散的"></a>影响并行程序的性能主要考量的分支是发散的</h2><ul>
<li>在单一的<code>warp</code>中<code>Threads</code>出现了不同的分支路径。</li>
<li>不同的执行路径在当前的<code>GPUs</code>中的执行时串行的。<br>在一个<code>warp</code>中<code>Threads</code>所采用的控制路径将会遍历执行，直到再没有更多的路径。</li>
</ul>
<h2 id="通常处理方法-当这个分支条件是thread-ID的函数的时候，要避免这种发散情况的发生"><a href="#通常处理方法-当这个分支条件是thread-ID的函数的时候，要避免这种发散情况的发生" class="headerlink" title="通常处理方法: 当这个分支条件是thread ID的函数的时候，要避免这种发散情况的发生"></a>通常处理方法: 当这个分支条件是thread ID的函数的时候，要避免这种发散情况的发生</h2><ul>
<li>出现发散情况的例子</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">If (threadIdx.x &gt; <span class="number">2</span>) &#123; &#125;</span><br></pre></td></tr></table></figure>
<p>这里将会在一个block 中，为所有线程创建两条不同的控制路径。<br>分支间隔尺寸&lt; warp size；在第一个warp中，相对于其它线程，threads 0, 1 和 2 将会遵循不同的路径。</p>
<ul>
<li>没有发散情况的例子:</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">If (threadIdx.x / WARP_SIZE &gt; <span class="number">2</span>) &#123; &#125;</span><br></pre></td></tr></table></figure>
<p>同样为所有线程创建两条不同的控制路径。<br>分支间隔尺寸是整个warp的大小；在第一个warp中，所有的线程将会遵循相同的路径。</p>
<h1 id="块尺寸考虑"><a href="#块尺寸考虑" class="headerlink" title="块尺寸考虑"></a>块尺寸考虑</h1><h2 id="对于矩阵相乘，需要要多个Blocks，那么到底用8X8-16X16-还是-32X32-的块尺寸"><a href="#对于矩阵相乘，需要要多个Blocks，那么到底用8X8-16X16-还是-32X32-的块尺寸" class="headerlink" title="对于矩阵相乘，需要要多个Blocks，那么到底用8X8, 16X16 还是 32X32 的块尺寸?"></a>对于矩阵相乘，需要要多个Blocks，那么到底用8X8, 16X16 还是 32X32 的块尺寸?</h2><ul>
<li><p>对于 8 × 8，每一个Block有 64 threads。 因为每个SM能接收1536 threads，这里就会有24 Blocks。 然而，每一个SM只能接收 8 Blocks，那么仅仅 512 threads 将会进入到每个SM，仅仅能利用到 1 / 3 的SM的线程能力。</p>
</li>
<li><p>对于 16 × 16，每一个Block有 256 threads。 因为每个SM能接收 1536 threads，这里就会有 6 Blocks，刚好能获得最大的SM利用率。</p>
</li>
<li><p>对于 32 × 32，每一个Block有 1024 threads。因为每个SM能接收 1536 threads，这里仅仅就只能用到 2 / 3 的SM的线程能力。</p>
</li>
</ul>
<h2 id="课堂练习"><a href="#课堂练习" class="headerlink" title="课堂练习"></a>课堂练习</h2><p>已知费米架构（英伟达显卡架构之一）下每个SM能接收 1536 threads，每一个SM能接收 8 Blocks。求下列线程块配置时SM的占用率（利用率）。</p>
<ul>
<li>(1) int threadPerBlock = 128;</li>
</ul>
<p>128 * 8 = 1024<br>1024 / 1536 = 66.67%</p>
<ul>
<li>(2) int threadPerBlock = 256;</li>
</ul>
<p>256 * 6 = 1536<br>1536 / 1536 = 100%</p>
<ul>
<li>(3) int threadPerBlock = 512;</li>
</ul>
<p>512 * 3 = 1536<br>1536 / 1536 = 100%</p>
<ul>
<li>(4) int threadPerBlock = 1024；</li>
</ul>
<p>1024 * 1 = 1024<br>1024 / 1536 = 66.67%</p>
<blockquote>
<p>费米架构：英伟达显卡架构之一，不同的架构下，每个SM的性能是不一样的。</p>
</blockquote>
<h1 id="多维线程应用：方矩阵相乘"><a href="#多维线程应用：方矩阵相乘" class="headerlink" title="多维线程应用：方矩阵相乘"></a>多维线程应用：方矩阵相乘</h1><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-1.png" class="" title="矩阵相乘-1">
<ul>
<li>P = M <em> N，尺寸为 WIDTH </em> HEIGHT；</li>
<li>并行计算可以采用一个线程计算 P 中的一个元素。</li>
<li>M 与 N 将会从 global memory 被装载 WIDTH 次</li>
</ul>
<blockquote>
<p>并行计算不能有数据依赖，采用不同线程计算P中间的不同的元素。</p>
</blockquote>
<h2 id="矩阵相乘CPU端代码"><a href="#矩阵相乘CPU端代码" class="headerlink" title="矩阵相乘CPU端代码"></a>矩阵相乘CPU端代码</h2><p>矩阵相乘时间复杂度为O^3，对应位置相乘最后相加。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Matrix multiplication on the (CPU) host in double precision，N*N</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">MatrixMulOnHost</span><span class="params">(<span class="type">float</span>* M, <span class="type">float</span>* N, <span class="type">float</span>* P, <span class="type">int</span> Width)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; Width; ++i)</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; Width; ++j) &#123;</span><br><span class="line">            <span class="type">double</span> sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; Width; ++k) &#123;</span><br><span class="line">                <span class="type">double</span> a = M[i * width + k];</span><br><span class="line">                <span class="type">double</span> b = N[k * width + j];</span><br><span class="line">                sum += a * b;</span><br><span class="line">            &#125;</span><br><span class="line">            P[i * Width + j] = sum;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="输入矩阵数据传输-Host端代码"><a href="#输入矩阵数据传输-Host端代码" class="headerlink" title="输入矩阵数据传输(Host端代码)"></a>输入矩阵数据传输(Host端代码)</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">MatrixMulOnDevice</span><span class="params">(<span class="type">float</span>* M, <span class="type">float</span>* N, <span class="type">float</span>* P, <span class="type">int</span> Width)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> size = Width * Width * <span class="keyword">sizeof</span>(<span class="type">float</span>); </span><br><span class="line">    <span class="type">float</span>* Md, Nd, Pd;</span><br><span class="line">    …</span><br><span class="line"><span class="comment">//1.//  Allocate and Load M, N to device memory </span></span><br><span class="line">    cudaMalloc(&amp;Md, size);</span><br><span class="line">    cudaMemcpy(Md, M, size, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    cudaMalloc(&amp;Nd, size);</span><br><span class="line">    cudaMemcpy(Nd, N, size, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate P on the device</span></span><br><span class="line">    cudaMalloc(&amp;Pd, size);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.// Kernel invocation code – 后面显示调用过程</span></span><br><span class="line">     …</span><br><span class="line">     </span><br><span class="line"><span class="comment">//3.// Read P from the device</span></span><br><span class="line">    cudaMemcpy(P, Pd, size, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free device matrices</span></span><br><span class="line">    cudaFree(Md);</span><br><span class="line">    cudaFree(Nd);</span><br><span class="line">    cudaFree(Pd);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Kernel函数"><a href="#Kernel函数" class="headerlink" title="Kernel函数"></a>Kernel函数</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Matrix multiplication kernel – per thread code</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">MatrixMulKernel</span><span class="params">(<span class="type">float</span>* Md, <span class="type">float</span>* Nd, <span class="type">float</span>* Pd, <span class="type">int</span> Width)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 2D Thread ID</span></span><br><span class="line">    <span class="type">int</span> tx = threadIdx.x;	<span class="comment">//对应列 col</span></span><br><span class="line">    <span class="type">int</span> ty = threadIdx.y;	<span class="comment">//对应行 row</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Pvalue 用来存储结果矩阵的元素</span></span><br><span class="line">    <span class="comment">// 通过每一个线程来计算结果矩阵的每一个元素</span></span><br><span class="line">    <span class="type">float</span> Pvalue = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//用线程去计算</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; Width; ++k)</span><br><span class="line">    &#123; </span><br><span class="line">         <span class="type">float</span> Melement = Md[ty * Width + k];</span><br><span class="line">         <span class="type">float</span> Nelement = Nd[k * Width + tx];</span><br><span class="line">         Pvalue += Melement * Nelement;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Write the matrix to device memory;</span></span><br><span class="line">    <span class="comment">// each thread writes one element;</span></span><br><span class="line">    Pd[ty * Width + tx] = Pvalue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-2.png" class="" title="矩阵相乘-2">
<img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-3.png" class="" title="矩阵相乘-3">
<h2 id="Kernel函数调用-host-side-Code"><a href="#Kernel函数调用-host-side-Code" class="headerlink" title="Kernel函数调用(host-side Code)"></a>Kernel函数调用(host-side Code)</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 线程配置</span></span><br><span class="line">dim3 <span class="title function_">dimBlock</span><span class="params">(Width, Width)</span>;</span><br><span class="line">dim3 <span class="title function_">dimGrid</span><span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 开始在device端执行该Kernel函数!</span></span><br><span class="line">MatrixMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(Md, Nd, Pd);</span><br></pre></td></tr></table></figure>
<h3 id="仅适用一个Thread-Block"><a href="#仅适用一个Thread-Block" class="headerlink" title="仅适用一个Thread Block"></a>仅适用一个Thread Block</h3><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-4.png" class="" title="矩阵相乘-4">
<ul>
<li>一个thread block来计算矩阵 Pd</li>
</ul>
<p>每一个线程计算Pd中的一个元素</p>
<ul>
<li>每一个线程</li>
</ul>
<p>装载矩阵<code>Md</code>的一行数据<br>装载矩阵<code>Nd</code>的一列数据<br>对于每一对<code>Md</code>和<code>Nd</code>中的元素执行一次乘法和加法操作<br>计算与内存访问比例接近1:1 (不是非常高)</p>
<ul>
<li>同时矩阵的尺寸也受限于一个<code>thread block</code>中规定的线程数</li>
</ul>
<h2 id="处理任意大小的方阵"><a href="#处理任意大小的方阵" class="headerlink" title="处理任意大小的方阵"></a>处理任意大小的方阵</h2><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-5.png" class="" title="矩阵相乘-5">
<p>每一个 <code>2D thread block</code> 计算结果矩阵中一个大小为 <code>(TILE_WIDTH)^2</code> 的子矩阵 (瓷砖化)，每一个都有 <code>(TILE_WIDTH)^2</code> threads。<br>产生一个2D GRID，其具有 <code>(WIDTH/TILE_WIDTH)^2</code> blocks。</p>
<p>如果 <code>WIDTH/TILE_WIDTH</code> 大于最大的grid大小 (64K)!，需要用多个循环来处理。</p>
<h3 id="一个稍大些的例子"><a href="#一个稍大些的例子" class="headerlink" title="一个稍大些的例子"></a>一个稍大些的例子</h3><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-6.png" class="" title="矩阵相乘-6">
<p>WIDTH = 8；TILE_WIDTH = 2；<br>每一个block有 2 * 2 = 4 threads</p>
<p>WIDTH / TILE_WIDTH = 4<br>使用 4* 4 = 16 blocks</p>
<h3 id="一个再大一些的例子"><a href="#一个再大一些的例子" class="headerlink" title="一个再大一些的例子"></a>一个再大一些的例子</h3><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-7.png" class="" title="矩阵相乘-7">
<p>WIDTH = 8；TILE_WIDTH = 4；<br>每一个block有 4 * 4 = 16 threads</p>
<p>WIDTH / TILE_WIDTH = 2<br>使用 2* 2 = 4 blocks</p>
<h3 id="C-C-的“行主”分布"><a href="#C-C-的“行主”分布" class="headerlink" title="C/C++的“行主”分布"></a>C/C++的“行主”分布</h3><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-8.png" class="" title="矩阵相乘-8">
<h3 id="Blcok-0-0"><a href="#Blcok-0-0" class="headerlink" title="Blcok(0,0)"></a>Blcok(0,0)</h3><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-9.png" class="" title="矩阵相乘-9">
<h3 id="Blcok-0-1"><a href="#Blcok-0-1" class="headerlink" title="Blcok(0,1)"></a>Blcok(0,1)</h3><img src="/2020/06/10/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-10.png" class="" title="矩阵相乘-10">
<h3 id="一个简单的矩阵相乘Kernel"><a href="#一个简单的矩阵相乘Kernel" class="headerlink" title="一个简单的矩阵相乘Kernel"></a>一个简单的矩阵相乘Kernel</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">MatrixMulKernel</span><span class="params">(<span class="type">float</span>* d_M, <span class="type">float</span>* d_N, <span class="type">float</span>* d_P, <span class="type">int</span> Width)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 计算输入数据d_M和输出数据d_P的行号以确定处理的元素</span></span><br><span class="line">    <span class="type">int</span> Row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="comment">// 计算输入数据d_N和输出数据d_P的行号以确定处理的元素</span></span><br><span class="line">    <span class="type">int</span> Col = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((Row &lt; Width) &amp;&amp; (Col &lt; Width)) &#123;</span><br><span class="line">        <span class="type">float</span> Pvalue = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 每一个线程处理输出数据d_P的一个元素</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; Width; ++k)</span><br><span class="line">            Pvalue += d_M[Row * Width + k] * d_N[k * Width + Col];</span><br><span class="line">        d_P[Row * Width + Col] = Pvalue;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>高性能计算第4次课-PartA.mp4<br>Lecture3B_CUDA编程模型.ppt 15/29</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2020/06/10/CUDA编程模型/">http://hibiscidai.com/2020/06/10/CUDA编程模型/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CUDA/">CUDA</a><a class="post-meta__tags" href="/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/">高性能计算</a><a class="post-meta__tags" href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/">并行计算</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/06/15/Linux%E5%AE%9E%E6%93%8D-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"><i class="fa fa-chevron-left">  </i><span>Linux实操-网络配置</span></a></div><div class="next-post pull-right"><a href="/2020/06/10/CUDA%E5%9F%BA%E7%A1%80-%E5%AE%9E%E4%BE%8B%E6%BC%94%E7%A4%BA/"><span>CUDA基础-实例演示</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://xn--mesr8b36x.agency/#/register?code=R5RS1JHy">好用、实惠、稳定的梯子,点击这里<img src="https://pic.imgdb.cn/item/65572abac458853aefef30cd.png" width="1000" height="124" object-fit="cover" ></a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2023 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>