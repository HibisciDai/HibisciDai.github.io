<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络"><meta name="keywords" content="学习笔记,Tensorflow,TensorflowMOOC"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%A0%B8%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">卷积核计算过程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">3.</span> <span class="toc-text">感受野</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%A8%E9%9B%B6%E5%A1%AB%E5%85%85"><span class="toc-number">4.</span> <span class="toc-text">全零填充</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TF%E6%8F%8F%E8%BF%B0%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E5%B1%82"><span class="toc-number">5.</span> <span class="toc-text">TF描述卷积计算层</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96-Batch-Normalization-BN"><span class="toc-number">6.</span> <span class="toc-text">批标准化(Batch Normalization, BN)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96-Pooling"><span class="toc-number">7.</span> <span class="toc-text">池化(Pooling)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#TF%E6%8F%8F%E8%BF%B0%E6%B1%A0%E5%8C%96"><span class="toc-number">7.1.</span> <span class="toc-text">TF描述池化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%88%8D%E5%BC%83-Dropout"><span class="toc-number">8.</span> <span class="toc-text">舍弃(Dropout)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">9.</span> <span class="toc-text">卷积神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cifar10%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">10.</span> <span class="toc-text">cifar10数据集</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E7%A4%BA%E4%BE%8B"><span class="toc-number">11.</span> <span class="toc-text">卷积神经网络搭建示例</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LeNet"><span class="toc-number">12.</span> <span class="toc-text">LeNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AlexNet"><span class="toc-number">13.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#VGGNet"><span class="toc-number">14.</span> <span class="toc-text">VGGNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#InceptionNet"><span class="toc-number">15.</span> <span class="toc-text">InceptionNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ResNet"><span class="toc-number">16.</span> <span class="toc-text">ResNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E5%8F%8A%E7%BD%91%E7%BB%9C%E5%B0%8F%E7%BB%93"><span class="toc-number">17.</span> <span class="toc-text">经典卷积及网络小结</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">244</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">88</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">33</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-02-16</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">8.2k</span><span class="post-meta__separator">|</span><span>阅读时长: 32 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png" class="" title="人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络">
<p>人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络</p>
<span id="more"></span>
<p>[TOC]</p>
<h1 id="人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络"><a href="#人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络" class="headerlink" title="人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络"></a>人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络</h1><h1 id="卷积核计算过程"><a href="#卷积核计算过程" class="headerlink" title="卷积核计算过程"></a>卷积核计算过程</h1><p>全连接NN：每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出为预测的结果。</p>
<p>全连接网络对识别和预测都有非常好的效果，可以实现分类和预测。</p>
<p>全连接网络的参数个数为：<script type="math/tex">\sum_{各层}(前层×后层+后层)</script></p>
<p>如下图所示，针对一张分辨率仅为 28 <em> 28 的黑白图像（像素值个数为 28 </em> 28 * 1 = 784），全连接网络的参数总量就有将近 10 万个。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F.png" class="" title="全连接网络的参数量">
<p>在实际应用中，图像的分辨率远高于此，且大多数是彩色图像，如下图所示。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%81%B0%E5%BA%A6%E5%9B%BE%E4%B8%8E%E5%BD%A9%E8%89%B2%E5%9B%BE.png" class="" title="灰度图与彩色图">
<p>虽然全连接网络一般被认为是分类预测的最佳网络，但待优化的参数过多，容易导致模型过拟合。</p>
<p>为了解决参数量过大而导致模型过拟合的问题，一般不会将原始图像直接输入，而是先对图像进行特征提取，再将提取到的特征输入全连接网络，如下图所示，就是将汽车图片经过多次特征提取后再喂入全连接网络。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%E7%9A%84%E6%94%B9%E8%BF%9B.png" class="" title="全连接网络的改进">
<ul>
<li>卷积计算可认为是一种有效提取图像特征的方法。</li>
<li>一般会用一个正方形的卷积核，按指定步长， 在输入特征图上滑动， 遍历输入特征图中的每个像素点。每一个步长，卷积核会与输入特征图出现重合区域，重合区域对应元素相乘、求和再加上偏置项得到输出特征的一个像素点。</li>
</ul>
<p>如果输入是灰度图，使用单通道卷积核。</p>
<p>如果输入特征是三通道彩色图，可以使用 3 <em> 3 </em> 3 的卷积核，或者 5 <em> 5 </em> 3 的卷积核。</p>
<p>要使卷积核的通道数与输入特征图的通道数一致，要想卷积核与输入特征图对应点匹配上，必须让卷积核的深度与输入特征图的深度一致。</p>
<p>输入特征图的深度（channel数），决定了当前层卷积核的深度；</p>
<p>由于每个卷积核在卷积计算后会得到一张输出特征图，当前层使用了几个卷积核，就有几张输出特征图；当前层卷积核的个数，决定了当前层输出特征图的深度。</p>
<p>如果某层模型的特征提取能力不足，可以在这一层多用几个卷积核提高这一层的特征提取能力。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E6%A0%B8%E4%BB%8B%E7%BB%8D.png" class="" title="卷积核介绍">
<p>每一个小颗粒都存储一个带训练的参数。在执行卷积计算时，卷积核里的参数都是固定的。在每次反向传播时，这些小颗粒中存储的带训练参数，会被梯度下降法更新，卷积就是利用立体卷积核，实现了参数的空间共享。</p>
<p>对于输入图是单通道的，选择单通道卷积核，这个例子的输入特征图是5行5列单通道，选用3 * 3单通道卷积核，滑动步长是1，在输入图上滑动，每滑动一步，输入特征图与卷积核里的9个元素重合，他们对应元素相乘求和再加上偏置项b</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97.png" class="" title="神经网络中的卷积计算">
<p>对于输入特征图是三通道的，选择3通道卷积核，该例子输入特征图是5行5列红绿蓝三通道数据，选用3 * 3 三通道卷积核，滑动步长是1，在这个输入特征上滑动，每滑动一步，输入特征图与卷积核里的27个元素重合，对应元素相乘再加上偏置项b，得到输出特则图中的一个像素值。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%89%E9%80%9A%E9%81%93%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97.png" class="" title="三通道彩色图像的卷积计算">
<p>卷积核在输入特征图上按指定步长滑动，每个步长，卷积核会与输入特征图上部分像素点重合，重合区域，输入特征图与卷积核对应元素相乘求和，得到输出特征图中的一个像素点，当输入特征图被遍历完成，得到一张输出特征图，完成了一个卷积核的卷积计算过程。当有n个卷积核时，会有n张输出特征图，叠加在这张输出特征图的后边。<br>来源：<a target="_blank" rel="noopener" href="https://mlnotebook.github.io/post/CNN1/">https://mlnotebook.github.io/post/CNN1/</a></p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/The%20kernel%20is%20moved%20over%20the%20image%20performing%20the%20convolution%20as%20it%20goes.gif" class="" alt="The kernel is moved over the image performing the convolution as it goes">
<p>输出特征尺寸计算： 在了解神经网络中卷积计算的整个过程后， 就可以对输出特征图的尺寸进行计算，如上图所示， 5 × 5 的图像经过 3 × 3 大小的卷积核做卷积计算后输出特征尺寸为 3 × 3。</p>
<h1 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h1><p>感受野（Receptive Field）：卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%84%9F%E5%8F%97%E9%87%8E%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="感受野示意图">
<p>如果对 5 <em> 5 的原始图片用黄色的 3 </em> 3 卷积核作用，会输出绿色这样一个 3 <em> 3 的输出特征图，输出特征图上的每个像素点，映射到原始图片是 3 </em> 3 的区域，所以它的感受野是3。</p>
<p>对 3 <em> 3 的特征图用绿色的 3 </em> 3卷积核作用，会输出一个 1 <em> 1 的输出特征图，该输出特征的像素点，映射到原始图片是 5 </em> 5 的区域，其感受野是5。</p>
<p>如果对 5 <em> 5 的原始图片直接用蓝色的 5 </em> 5 卷积核作用，会输出一个 1 <em> 1 的输出特征图，这个像素点映射到原始输入图片是 5 </em> 5 的区域，其感受野是5。</p>
<p>同样一个 5 <em> 5 的原始图片，经过两层 3 </em> 3 的卷积核作用，和经过一层 5 <em> 5 的卷积核作用，都得到一个感受野是 5 的输出特征图，这两层 3 </em> 3 卷积核和一层 5 * 5 卷积核的特征提取能力是一样的。 </p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%A4%E5%B1%823x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E4%B8%8E%E4%B8%80%E5%B1%825x5%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E5%AF%B9%E6%AF%94.png" class="" title="两层3x3卷积核与一层5x5卷积核的对比">
<p>对于两层 3 <em> 3 的卷积核与一层 5 </em> 5 的卷积核的选取，需要考虑带训练参数量和计算量。</p>
<p>假设输入特征图宽、高为x，卷积计算步长为1。显然，两个 3 <em> 3 卷积核的参数量为 9 + 9 = 18，小于 5 </em> 5 卷积核的 25，前者的参数量更少。  </p>
<p>对于两层 3 <em> 3 卷积核计算，待训练参数共有18个，每个 3 </em> 3 <em> 1 的卷积核计算得到一个输出像素点，需要9次乘加计算，两层 3 </em> 3 卷积核共需要 <script type="math/tex">18 x^{2} - 108 x + 180</script> 次的乘加计算。第一个 3 <em> 3 卷积核输出特征图共有 <script type="math/tex">(x – 3 + 1)^2</script> 个像素点，每个像素点需要进行 3 </em> 3 = 9 次乘加运算， 第二个 3 <em> 3 卷积核输出特征图共有 <script type="math/tex">(x – 3 + 1 – 3 + 1)^2</script>个像素点， 每个像素点同样需要进行 9 次乘加运算，总计算量为 $$ 9 </em> (x – 3 + 1)^2 + 9 * (x – 3 + 1 – 3 + 1)^2 = 18 x^2 – 108x + 180 $$。  </p>
<p>对于一层 5 <em> 5 卷积核计算，待训练参数共有25个，每个 5 </em> 5 <em> 1 的卷积核计算得到一个输出像素点，需要25次乘加计算，一层 5 </em> 5 卷积核，共需要 <script type="math/tex">25 x^{2} - 200 x + 400</script> 次乘加计算。输出特征图共有 <script type="math/tex">(x – 5 + 1)^2</script> 个像素点， 每个像素点需要进行 5 <em> 5 = 25 次乘加运算，总计算量为 $$ 25 </em> (x – 5 + 1)^2 = 25x^2 – 200x + 400 $$ 。</p>
<p>当输入特征图边长大于10个像素点时，两层 3 <em> 3 卷积核比一层 5 </em> 5 卷积性能要好，这也就是为什么现在的神经网络在卷积计算中常使用两层 3 <em> 3 卷积核替换一层 5 </em> 5 卷积核的原因。</p>
<p>对二者的总计算量（乘加运算的次数） 进行对比，<script type="math/tex">18 x^2 – 200x + 400 < 25x^2 – 200x + 400</script> ，经过简单数学运算可得 <script type="math/tex">x < 22/7 or x > 10</script>， x 作为特征图的边长，在大多数情况下显然会是一个大于 10 的值（非常简单的 MNIST 数据集的尺寸也达到了 28 <em> 28），所以两层 3 </em> 3 卷积核的参数量和计算量，在通常情况下都优于一层 5 <em> 5 卷积核，尤其是当特征图尺寸比较大的情况下，两层 3 </em> 3 卷积核在计算量上的优势会更加明显。  </p>
<h1 id="全零填充"><a href="#全零填充" class="headerlink" title="全零填充"></a>全零填充</h1><p>卷积计算保持输入特征图的尺寸不变，可以使用全零填充，在输入特征图周围填充0。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%85%A8%E9%9B%B6%E5%A1%AB%E5%85%85.png" class="" title="全零填充">
<p>如图所示，在 5×5×1 的输入图像周围填 0，在通过 3×3×1的卷积核，进行步长为1的卷积计算，输出特征图仍是 5×5×1。</p>
<p>在 Tensorflow 框架中， 用参数 <code>padding = &#39;SAME&#39;</code> 或 <code>padding = &#39;VALID&#39;</code> 表示是否进行全零填充，其对输出特征尺寸大小的影响如下：</p>
<script type="math/tex; mode=display">padding \left\{ \begin{array} & SAME(全0填充) & \frac{入长}{步长} (向上取整) \\ VALID(不全0填充) & \frac{入长 - 核长 + 1}{步长} (向上取整) \end{array} \right.</script><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%85%A8%E9%9B%B6%E5%A1%AB%E5%85%85%E8%BE%93%E5%87%BA%E7%89%B9%E5%BE%81%E5%9B%BE%E5%A4%A7%E5%B0%8F.png" class="" title="全零填充输出特征图大小">
<p>上下两行分别代表对输入图像进行全零填充或不进行填充， 对于 5×5×1 的图像来说，当 <code>padding = &#39;SAME&#39;</code> 时， 步长是1，输出图像边长为 5 / 1 = 5； 当 <code>padding = &#39;VALID&#39;</code> 时， 核长是3，步长是1，输出图像边长为 (5 - 3 + 1) / 1 = 3。</p>
<h1 id="TF描述卷积计算层"><a href="#TF描述卷积计算层" class="headerlink" title="TF描述卷积计算层"></a>TF描述卷积计算层</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Conv2D(	</span><br><span class="line">	filters = 卷积核个数,</span><br><span class="line">	kernel_size = 卷积核尺寸,	<span class="comment">#正方形写核长整数，或（核高h，核宽w）</span></span><br><span class="line">	strides = 卷积步长,	<span class="comment">#横纵向相同写步长整数，或(纵向步长h，横向步长w)，默认1</span></span><br><span class="line">	padding = ‘SAME’ <span class="keyword">or</span> ‘VALID’,	<span class="comment">#使用全零填充是“same”，不使用“valid”（默认）</span></span><br><span class="line">	activation = ‘relu’ <span class="keyword">or</span> ‘sigmoid’ <span class="keyword">or</span> ‘tanh’ <span class="keyword">or</span> ‘softmax’等 <span class="comment">#使用什么激活函数，如果这一层卷积后还有批标准化操作，不在这里进行激活，如有 BN 则此处不用写</span></span><br><span class="line">	input_shape = (高, 宽, 通道数), <span class="comment">#输入特征图纬度，可以省略仅在第一层有</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 描述三层卷积计算，每一层用自己的表述形式</span></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    Conv2D(<span class="number">6</span>, <span class="number">5</span>, padding=<span class="string">&#x27;valid&#x27;</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),<span class="comment">##</span></span><br><span class="line">    MaxPool2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">6</span>, (<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&#x27;valid&#x27;</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),<span class="comment">##</span></span><br><span class="line">    MaxPool2D(<span class="number">2</span>, (<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">    </span><br><span class="line">    Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&#x27;valid&#x27;</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),	<span class="comment">##关键字传递参数，可读性更强</span></span><br><span class="line">    </span><br><span class="line">    MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>),</span><br><span class="line">    Flatten(),</span><br><span class="line">    Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>使用此函数构建卷积层时，需要给出的信息有：<br>A） 输入图像的信息，即宽高和通道数；<br>B） 卷积核的个数以及尺寸，如 filters = 16, kernel_size = (3, 3)代表采用 16 个大小为 3×3 的<br>卷积核；<br>C） 卷积步长，即卷积核在输入图像上滑动的步长，纵向步长与横向步长通常是相同的，默<br>认值为 1；<br>D） 是否进行全零填充，全零填充的具体作用上文有描述；<br>E） 采用哪种激活函数，例如 relu、 softmax 等，各种函数的具体效果在前面章节中有详细描述；<br>这里需要注意的是，在利用 Tensorflow 框架构建卷积网络时，一般会利用 BatchNormalization<br>函数来构建 BN 层，进行批归一化操作，所以在 Conv2D 函数中经常不写 BN。 BN 操作的<br>具体含义和作用见下文。</p>
<h1 id="批标准化-Batch-Normalization-BN"><a href="#批标准化-Batch-Normalization-BN" class="headerlink" title="批标准化(Batch Normalization, BN)"></a>批标准化(Batch Normalization, BN)</h1><p>神经网络对0附近的数据更敏感，但是随着网络层数的增加，特征数据会出现偏离0均值的情况，标准化可以使数据符合以0为均值，1为标准差的标准正态分布。把偏移的特征数据重新拉回到0附近，常用在卷积操作和激活操作之间。</p>
<p>标准化：使数据符合0均值，1为标准差的分布。</p>
<p>批标准化：对一小批数据（batch），做标准化处理。</p>
<p>批标准化后，第k个卷积核的输出特征图（feature map）中第i个像素点。</p>
<p>可以通过以下公式计算批标准化后的输出特征图：</p>
<script type="math/tex; mode=display">H_{i}^{’k} = \frac {H_{i}^{k} - \mu_{batch}^{k} } { \sigma_{batch}^{k} }</script><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96%E5%90%8E%E8%BE%93%E5%87%BA%E5%9B%BE.png" class="" title="批标准化后输出图">
<p>$ H<em>{i}^{k} $ ：批标准化前，第k个卷积核，输出特征图中第 i 个像素点<br>$ \mu</em>{batch}^{k} $ ： 批标准化前，第k个卷积核， batch张输出特征图中所有像素点平均值<br>$ \sigma_{batch}^{k} $ ： 批标准化前，第k个卷积核， batch张输出特征图中所有像素点标准差</p>
<p>批标准化操作，会让每个像素点进行减均值除以标准差的自更新计算，对于 第k个卷积核batch张输出特征图中所有像素点平均值和标准差，是对 第k个卷积核产生的batch张输出特征图 集体求均值和标准差。</p>
<p>其中 $ \mu<em>{batch}^{k} $ 和 $ \sigma</em>{batch}^{k} $ 就是对第k个卷积核计算出来的batch张输出特征图中的所有像素点求均值和标准差。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Batch%20Normalization%20%E7%9A%84%E4%BD%9C%E7%94%A8.png" class="" alt="Batch Normalization 的作用">
<p>BN操作将原本偏移的特征数据重新拉回到0均值，使进入激活函数的数据分布在激活函数线性区，使得输入数据的微小变化更明显体现到激活函数的输出，提升了激活函数对输入数据的区分力。</p>

<p>但是这种简单的特征数据标准化，使特征数据完全满足标准正态分布，集中在激活函数中心的线性区域，使激活函数丧失了非线性特性，因此在BN操作中为每个卷积核引入了两个可训练参数。缩放因子γ和偏移因子β。</p>
<p>反向传播时，缩放因子γ和偏移因子β会与其他待训练参数一同被训练优化，使标准正态分布后的特征数据通过缩放因子γ和偏移因子β优化了特征数据分布的宽窄和偏移量，保证了网络的非线性表达力。</p>
<p><em>BN层位于卷积层之后，激活层之前。</em></p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BN%E5%B1%82%E4%BD%8D%E4%BA%8E%E5%8D%B7%E7%A7%AF%E5%B1%82%E4%B9%8B%E5%90%8E%E6%BF%80%E6%B4%BB%E5%B1%82%E4%B9%8B%E5%89%8D.png" class="" title="BN层位于卷积层之后激活层之前">
<p>BN操作函数</p>
<p><code>tf.keras.layers.BatchNormalization()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&#x27;same&#x27;</span>), <span class="comment"># 卷积层</span></span><br><span class="line">    BatchNormalization(), <span class="comment"># BN层</span></span><br><span class="line">    Activation(<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 激活层</span></span><br><span class="line">    MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>), <span class="comment"># 池化层</span></span><br><span class="line">    Dropout(<span class="number">0.2</span>), <span class="comment"># dropout层	</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h1 id="池化-Pooling"><a href="#池化-Pooling" class="headerlink" title="池化(Pooling)"></a>池化(Pooling)</h1><p>池化操作作用于减少卷积神经网络中特征数据量（降维）。</p>
<p>池化的方法有最大池化和均值池化。<br>最大值池化可提取图片纹理，均值池化可保留背景特征。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%9C%80%E5%A4%A7%E5%80%BC%E6%B1%A0%E5%8C%96%E4%B8%8E%E5%9D%87%E5%80%BC%E6%B1%A0%E5%8C%96.png" class="" title="最大值池化与均值池化">
<p>如果用2×2的池化核对输入图片以2为步长进行池化，输出图片将变为输入图片的四分之一大小。</p>
<p>最大池化是用2×2的池化核框住左上4个像素点，选择最大的6输出。步长为2滑动到右上，选择最大的8输出，依次遍历图片。<br>均值池化和最大池化类似，提取的是4个像素点的均值，得到输出图片。</p>
<h2 id="TF描述池化"><a href="#TF描述池化" class="headerlink" title="TF描述池化"></a>TF描述池化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.MaxPool2D(</span><br><span class="line">    pool_size = 池化核尺寸,	<span class="comment">#正方形写核长整数，或（核高h，核宽w）</span></span><br><span class="line">    strides = 池化步长,	<span class="comment">#步长整数， 或(纵向步长h，横向步长w)，默认为pool_size</span></span><br><span class="line">    padding = <span class="string">&#x27;valid&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;same&#x27;</span>	<span class="comment">#使用全零填充是“same”， 不使用是“valid”（默认）</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tf.keras.layers.AveragePooling2D(</span><br><span class="line">    pool_size = 池化核尺寸,	<span class="comment">#正方形写核长整数，或（核高h，核宽w）</span></span><br><span class="line">    strides = 池化步长,	<span class="comment">#步长整数， 或(纵向步长h，横向步长w)，默认为pool_size</span></span><br><span class="line">    padding = <span class="string">&#x27;valid&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;same&#x27;</span> <span class="comment">#使用全零填充是“same”， 不使用是“valid”（默认）</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    Conv2D(filters = <span class="number">6</span>, kernel_size = (<span class="number">5</span>, <span class="number">5</span>), padding = <span class="string">&#x27;same&#x27;</span>), <span class="comment"># 卷积层</span></span><br><span class="line">    BatchNormalization(), <span class="comment"># BN层</span></span><br><span class="line">    Activation(<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 激活层</span></span><br><span class="line">    MaxPool2D(pool_size = (<span class="number">2</span>, <span class="number">2</span>), strides = <span class="number">2</span>, padding = <span class="string">&#x27;same&#x27;</span>), <span class="comment"># 池化层</span></span><br><span class="line">    Dropout(<span class="number">0.2</span>), <span class="comment"># dropout层</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h1 id="舍弃-Dropout"><a href="#舍弃-Dropout" class="headerlink" title="舍弃(Dropout)"></a>舍弃(Dropout)</h1><p>为了缓解神经网络过拟合，在神经网络训练过程中，常将隐藏层的部分神经元按照一定比例从神经网络中临时舍弃。在使用神经网络时，再把所有神经元恢复到神经网络中。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%88%8D%E5%BC%83-Dropout%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="舍弃-Dropout示意图">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Dropout(舍弃的概率)</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&#x27;same&#x27;</span>), <span class="comment"># 卷积层</span></span><br><span class="line">    BatchNormalization(), <span class="comment"># BN层</span></span><br><span class="line">    Activation(<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 激活层</span></span><br><span class="line">    MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>), <span class="comment"># 池化层</span></span><br><span class="line">    Dropout(<span class="number">0.2</span>), <span class="comment"># dropout层,随机舍弃掉20%神经元</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经网络：借助卷积核提取特征后，送入全连接网络。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97.png" class="" title="卷积神经网络网络的主要模块">
<p><strong>卷积是什么？ 卷积就是特征提取器，就是CBAPD</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">C	Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&#x27;same&#x27;</span>), <span class="comment"># 卷积层</span></span><br><span class="line">B	BatchNormalization(), <span class="comment"># BN层</span></span><br><span class="line">A	Activation(<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 激活层</span></span><br><span class="line">P	MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>), <span class="comment"># 池化层</span></span><br><span class="line">D	Dropout(<span class="number">0.2</span>), <span class="comment"># dropout层</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h1 id="cifar10数据集"><a href="#cifar10数据集" class="headerlink" title="cifar10数据集"></a>cifar10数据集</h1><p>提供 5万张 32 <em> 32 像素点的十分类彩色图片和标签，用于训练。<br>提供 1万张 32 </em> 32 像素点的十分类彩色图片和标签，用于测试。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/cifar10%E6%95%B0%E6%8D%AE%E9%9B%86.png" class="" title="cifar10数据集">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold=np.inf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入cifar10数据集</span></span><br><span class="line">cifar10 = tf.keras.datasets.cifar10</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化训练集输入特征的第一个元素</span></span><br><span class="line">plt.imshow(x_train[<span class="number">0</span>])  <span class="comment"># 绘制图片</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印出训练集输入特征的第一个元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_train[0]:\n&quot;</span>, x_train[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 打印出训练集标签的第一个元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_train[0]:\n&quot;</span>, y_train[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印出整个训练集输入特征形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_train.shape:\n&quot;</span>, x_train.shape)</span><br><span class="line"><span class="comment"># 打印出整个训练集标签的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_train.shape:\n&quot;</span>, y_train.shape)</span><br><span class="line"><span class="comment"># 打印出整个测试集输入特征的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_test.shape:\n&quot;</span>, x_test.shape)</span><br><span class="line"><span class="comment"># 打印出整个测试集标签的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_test.shape:\n&quot;</span>, y_test.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"> [[177 144 116]</span><br><span class="line">  [168 129  94]</span><br><span class="line">  [179 142  87]</span><br><span class="line">  [188 149  67]</span><br><span class="line">  [202 168  68]</span><br><span class="line">  [218 189  76]</span><br><span class="line">  [218 191  72]</span><br><span class="line">  [207 181  70]</span><br><span class="line">  [191 163  79]</span><br><span class="line">  [175 143  82]</span><br><span class="line">  [166 132  86]</span><br><span class="line">  [163 128  92]</span><br><span class="line">  [163 127  94]</span><br><span class="line">  [161 123  92]</span><br><span class="line">  [153 114  84]</span><br><span class="line">  [159 120  90]</span><br><span class="line">  [162 124  93]</span><br><span class="line">  [149 116  91]</span><br><span class="line">  [140 104  83]</span><br><span class="line">  [148 103  77]</span><br><span class="line">  [161 105  69]</span><br><span class="line">  [144  95  55]</span><br><span class="line">  [112  90  59]</span><br><span class="line">  [119  91  58]</span><br><span class="line">  [130  96  65]</span><br><span class="line">  [120  87  59]</span><br><span class="line">  [ 92  67  46]</span><br><span class="line">  [103  78  57]</span><br><span class="line">  [170 140 104]</span><br><span class="line">  [216 184 140]</span><br><span class="line">  [151 118  84]</span><br><span class="line">  [123  92  72]]]</span><br><span class="line">y_train[0]:</span><br><span class="line"> [6]</span><br><span class="line">x_train.shape:</span><br><span class="line"> (50000, 32, 32, 3)</span><br><span class="line">y_train.shape:</span><br><span class="line"> (50000, 1)</span><br><span class="line">x_test.shape:</span><br><span class="line"> (10000, 32, 32, 3)</span><br><span class="line">y_test.shape:</span><br><span class="line"> (10000, 1)</span><br></pre></td></tr></table></figure>
<h1 id="卷积神经网络搭建示例"><a href="#卷积神经网络搭建示例" class="headerlink" title="卷积神经网络搭建示例"></a>卷积神经网络搭建示例</h1><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E7%A4%BA%E4%BE%8B.png" class="" title="卷积神经网络搭建示例">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold=np.inf)</span><br><span class="line"></span><br><span class="line">cifar10 = tf.keras.datasets.cifar10</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#网络结构--start</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Baseline</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Baseline, self).__init__()</span><br><span class="line">        self.c1 = Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&#x27;same&#x27;</span>)  <span class="comment"># 卷积层</span></span><br><span class="line">        self.b1 = BatchNormalization()  <span class="comment"># BN层</span></span><br><span class="line">        self.a1 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层</span></span><br><span class="line">        self.p1 = MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)  <span class="comment"># 池化层</span></span><br><span class="line">        self.d1 = Dropout(<span class="number">0.2</span>)  <span class="comment"># dropout层</span></span><br><span class="line"></span><br><span class="line">        self.flatten = Flatten()</span><br><span class="line">        self.f1 = Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d2 = Dropout(<span class="number">0.2</span>)</span><br><span class="line">        self.f2 = Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.c1(x)</span><br><span class="line">        x = self.b1(x)</span><br><span class="line">        x = self.a1(x)</span><br><span class="line">        x = self.p1(x)</span><br><span class="line">        x = self.d1(x)</span><br><span class="line"></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.f1(x)</span><br><span class="line">        x = self.d2(x)</span><br><span class="line">        y = self.f2(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = Baseline()</span><br><span class="line"></span><br><span class="line"><span class="comment">#网络结构--end</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">False</span>), metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/Baseline.ckpt&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=<span class="literal">True</span>, save_best_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,  callbacks=[cp_callback])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(model.trainable_variables)</span></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> model.trainable_variables:</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(<span class="built_in">str</span>(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################    show   ###############################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示训练集和验证集的acc和loss曲线</span></span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(val_acc, label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LeNet-1.png" class="" title="LeNet-1">
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/726791">LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition.Proceedings of the IEEE, 1998, 86(11): 2278-2324.</a></p>
<p>通过共享卷积核减少了网络参数。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LeNet-2.png" class="" title="LeNet-2">
<p>输入： 32<em>32</em>3<br>C1卷积<br>C（核： 6<em>5</em>5，步长： 1，填充： valid ）<br>B（ None）<br>A（ sigmoid）<br>P（ max，核： 2*2，步长： 2，填充： valid ）<br>D（ None）</p>
<p>C3卷积<br>C（核： 16<em>5</em>5，步长： 1，填充： valid ）<br>B（ None）<br>A（ sigmoid）<br>P（ max，核： 2*2，步长： 2，填充： valid ）<br>D（ None）</p>
<p>Flatten<br>Dense（神经元： 120，激活： sigmoid）<br>Dense（神经元： 84，激活： sigmoid）<br>Dense（神经元： 10，激活： softmax）</p>
<p>统计卷积神经网络层数时，一般只统计卷积计算层和全连接计算层，其余操作可以认为是卷积计算层的附属。</p>
<p>LeNet一共有五层网络</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LeNet-3.png" class="" title="LeNet-3">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet5</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet5, self).__init__()</span><br><span class="line">        self.c1 = Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">        self.p1 = MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.c2 = Conv2D(filters=<span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">        self.p2 = MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.flatten = Flatten()</span><br><span class="line">        self.f1 = Dense(<span class="number">120</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">        self.f2 = Dense(<span class="number">84</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">        self.f3 = Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AlexNet-1.png" class="" title="AlexNet-1">
<p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional NeuralNetworks. In NIPS, 2012.</a></p>
<p>使用relu激活函数提升激活速度，使用了dropout缓解了过拟合，</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AlexNet-2.png" class="" title="AlexNet-2">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AlexNet-3.png" class="" title="AlexNet-3">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet8</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet8, self).__init__()</span><br><span class="line">        self.c1 = Conv2D(filters=<span class="number">96</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.b1 = BatchNormalization()</span><br><span class="line">        self.a1 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.p1 = MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.c2 = Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.b2 = BatchNormalization()</span><br><span class="line">        self.a2 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.p2 = MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.c3 = Conv2D(filters=<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                         </span><br><span class="line">        self.c4 = Conv2D(filters=<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                         </span><br><span class="line">        self.c5 = Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>)        </span><br><span class="line">        self.p3 = MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.flatten = Flatten()</span><br><span class="line">        self.f1 = Dense(<span class="number">2048</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d1 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.f2 = Dense(<span class="number">2048</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d2 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.f3 = Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/VGGNet-1.png" class="" title="VGGNet-1">
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1556">K. Simonyan, A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition.In ICLR,2015</a></p>
<p>适合硬件加速</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/VGGNet-2.png" class="" title="VGGNet-2">
<p>16层VGG网络<br><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/VGGNet-3.png" class="" title="VGGNet-3"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG16</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VGG16, self).__init__()</span><br><span class="line">        <span class="comment"># A） 第一部分： 两次卷积（64 个 3 * 3 卷积核、 BN、 Relu 激活） →最大池化→Dropout</span></span><br><span class="line">        self.c1 = Conv2D(filters=<span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)  <span class="comment"># 卷积层1</span></span><br><span class="line">        self.b1 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a1 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        </span><br><span class="line">        self.c2 = Conv2D(filters=<span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, )</span><br><span class="line">        self.b2 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a2 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        self.p1 = MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.d1 = Dropout(<span class="number">0.2</span>)  <span class="comment"># dropout层</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># B） 第二部分： 两次卷积（128 个 3 * 3 卷积核、 BN、 Relu 激活） →最大池化→Dropout</span></span><br><span class="line">        self.c3 = Conv2D(filters=<span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b3 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a3 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        </span><br><span class="line">        self.c4 = Conv2D(filters=<span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b4 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a4 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        self.p2 = MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.d2 = Dropout(<span class="number">0.2</span>)  <span class="comment"># dropout层</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># C） 第三部分： 三次卷积（256 个 3 * 3 卷积核、 BN、 Relu 激活） →最大池化→Dropout</span></span><br><span class="line">        self.c5 = Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b5 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a5 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        </span><br><span class="line">        self.c6 = Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b6 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a6 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        </span><br><span class="line">        self.c7 = Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b7 = BatchNormalization()</span><br><span class="line">        self.a7 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.p3 = MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.d3 = Dropout(<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># D） 第四部分： 三次卷积（512 个 3 * 3 卷积核、 BN、 Relu 激活） →最大池化→Dropout</span></span><br><span class="line">        self.c8 = Conv2D(filters=<span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b8 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a8 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        </span><br><span class="line">        self.c9 = Conv2D(filters=<span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b9 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a9 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        </span><br><span class="line">        self.c10 = Conv2D(filters=<span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b10 = BatchNormalization()</span><br><span class="line">        self.a10 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.p4 = MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.d4 = Dropout(<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># E） 第五部分： 三次卷积（ 512 个 3 * 3 卷积核、 BN、 Relu 激活） →最大池化→Dropout</span></span><br><span class="line">        self.c11 = Conv2D(filters=<span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b11 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a11 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        </span><br><span class="line">        self.c12 = Conv2D(filters=<span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b12 = BatchNormalization()  <span class="comment"># BN层1</span></span><br><span class="line">        self.a12 = Activation(<span class="string">&#x27;relu&#x27;</span>)  <span class="comment"># 激活层1</span></span><br><span class="line">        </span><br><span class="line">        self.c13 = Conv2D(filters=<span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.b13 = BatchNormalization()</span><br><span class="line">        self.a13 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.p5 = MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.d5 = Dropout(<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># F） 第六部分： 全连接（ 512 个神经元） →Dropout→全连接（ 512 个神经元） →Dropout→全连接（ 10 个神经元）</span></span><br><span class="line">        self.flatten = Flatten()</span><br><span class="line">        self.f1 = Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d6 = Dropout(<span class="number">0.2</span>)</span><br><span class="line">        self.f2 = Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d7 = Dropout(<span class="number">0.2</span>)</span><br><span class="line">        self.f3 = Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>卷积核的个数从64到128到256到512，逐渐增加，因为越靠后，特征图尺寸越小，通过增加卷积核的个数，增加了特征图深度，保持了信息的承载能力。</p>
<h1 id="InceptionNet"><a href="#InceptionNet" class="headerlink" title="InceptionNet"></a>InceptionNet</h1><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/InceptionNet-1.png" class="" title="InceptionNet-1">
<p><a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html">Szegedy C, Liu W, Jia Y, et al. Going Deeper with Convolutions. In CVPR, 2015.</a></p>
<p>InceptionNet引入了Inception结构块，在同一层网络内使用不同尺寸的卷积核，提升了模型感知力，使用了批标准化，缓解了梯度消失。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Inception%E7%BB%93%E6%9E%84%E5%9D%97.png" class="" title="Inception结构块">
<p>Inception结构块在同一层网络中使用了多个尺寸的卷积核，可以提取不同尺寸的特征，通过1 × 1卷积核，作用到输入特征图的每个像素点。通过设定少于输入特征图深度的1 × 1卷积核个数，减少了输出特征图深度，起到了降维的作用，减少了参数量和计算量。</p>
<p>一个Inception结构块包含四个分支，<br>分别经过 1 × 1 卷积核输出到卷积连接器，<br>经过 1 × 1 卷积核配合 3 × 3 卷积核输出到卷积连接器，<br>经过 1 × 1 卷积核配合 5 × 5 卷积核输出到卷积连接器，<br>经过 3 × 3 最大池化核配合 1 × 1 卷积核输出到卷积连接器。</p>
<p>送到卷积连接器的特征数据尺寸相同，卷积连接器会把收到的这四路特征数据按深度方向拼接，形成Inception结构块的输出。</p>
<p>Inception结构块中的卷积操作均采用了CBA结构，先卷积，再BN，再采用relu激活函数，所以将其定义成一个新的类<code>ConvBNRelu</code>，可以减少代码长度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNRelu</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ch, kernelsz=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvBNRelu, self).__init__()</span><br><span class="line">        self.model = tf.keras.models.Sequential([</span><br><span class="line">            Conv2D(ch, kernelsz, strides=strides, padding=padding),</span><br><span class="line">            BatchNormalization(),</span><br><span class="line">            Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model(x, training=<span class="literal">False</span>) <span class="comment">#在training=False时，BN通过整个训练集计算均值、方差去做批归一化，training=True时，通过当前batch的均值、方差去做批归一化。推理时 training=False效果好</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Inception%E7%BB%93%E6%9E%84%E5%9D%97%E5%AE%9E%E7%8E%B0.png" class="" title="Inception结构块实现">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionBlk</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ch, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionBlk, self).__init__()</span><br><span class="line">        self.ch = ch</span><br><span class="line">        self.strides = strides</span><br><span class="line">        self.c1 = ConvBNRelu(ch, kernelsz=<span class="number">1</span>, strides=strides)</span><br><span class="line">        self.c2_1 = ConvBNRelu(ch, kernelsz=<span class="number">1</span>, strides=strides)</span><br><span class="line">        self.c2_2 = ConvBNRelu(ch, kernelsz=<span class="number">3</span>, strides=<span class="number">1</span>)</span><br><span class="line">        self.c3_1 = ConvBNRelu(ch, kernelsz=<span class="number">1</span>, strides=strides)</span><br><span class="line">        self.c3_2 = ConvBNRelu(ch, kernelsz=<span class="number">5</span>, strides=<span class="number">1</span>)</span><br><span class="line">        self.p4_1 = MaxPool2D(<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.c4_2 = ConvBNRelu(ch, kernelsz=<span class="number">1</span>, strides=strides)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = self.c1(x)</span><br><span class="line">        x2_1 = self.c2_1(x)</span><br><span class="line">        x2_2 = self.c2_2(x2_1)</span><br><span class="line">        x3_1 = self.c3_1(x)</span><br><span class="line">        x3_2 = self.c3_2(x3_1)</span><br><span class="line">        x4_1 = self.p4_1(x)</span><br><span class="line">        x4_2 = self.c4_2(x4_1)</span><br><span class="line">        <span class="comment"># concat along axis=channel</span></span><br><span class="line">        x = tf.concat([x1, x2_2, x3_2, x4_2], axis=<span class="number">3</span>)<span class="comment"># 深度方向堆叠</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%B2%BE%E7%AE%80%E7%89%88%E6%9C%ACInceptionNet.png" class="" title="精简版本InceptionNet">
<p>每两个inception结构块组合成一个block，每个block中第一个Inception结构块卷积步长是2，第二个Inception结构块卷积步长是1，这使得第一个Inception结构块输出特征图尺寸减半。因此将输出图特征图深度加深，尽可能保证特征抽取中信息的承载量一致，block_0 设置的通道数是16，经过了四个分支，输出的深度为 4 × 16 = 64。在这里给通道数加倍了，所以 block_1 通道数是 block_0 通道数的两倍，是32，同样，经过了四个分支，输出深度是 4 × 32 = 128。128个数据会被送进平均池化，送进十个分类的全连接，这里实例化了Inception10的类，指定了InceptionNet的block数是2，也就是 block_0 和 block_1 ，而后网络指定分几类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Inception10</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_blocks, num_classes, init_ch=<span class="number">16</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception10, self).__init__(**kwargs)</span><br><span class="line">        self.in_channels = init_ch</span><br><span class="line">        self.out_channels = init_ch</span><br><span class="line">        self.num_blocks = num_blocks</span><br><span class="line">        self.init_ch = init_ch</span><br><span class="line">        self.c1 = ConvBNRelu(init_ch)</span><br><span class="line">        self.blocks = tf.keras.models.Sequential()</span><br><span class="line">        <span class="keyword">for</span> block_id <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks):</span><br><span class="line">            <span class="keyword">for</span> layer_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">                <span class="keyword">if</span> layer_id == <span class="number">0</span>:</span><br><span class="line">                    block = InceptionBlk(self.out_channels, strides=<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    block = InceptionBlk(self.out_channels, strides=<span class="number">1</span>)</span><br><span class="line">                self.blocks.add(block)</span><br><span class="line">            <span class="comment"># enlarger out_channels per block</span></span><br><span class="line">            self.out_channels *= <span class="number">2</span></span><br><span class="line">        self.p1 = GlobalAveragePooling2D()</span><br><span class="line">        self.f1 = Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.c1(x)</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.p1(x)</span><br><span class="line">        y = self.f1(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Inception10(num_blocks=<span class="number">2</span>, num_classes=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>训练时候可以调节batchsize大小，使显卡达到70%利用率即可。</p>
<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ResNet-1.png" class="" title="ResNet-1">
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Kaiming He, Xiangyu Zhang, Shaoqing Ren. Deep Residual Learning for Image Recognition. In CPVR,2016</a></p>
<p>ResNet提出了层间残差跳连，引入前方信息，缓解梯度消失，使神经网络层数增加成为可能。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ResNet-2.png" class="" title="ResNet-2">
<p>通过加深网络层数，取得了越来越好的效果。</p>
<p>然56层卷积网络错误率要高于20层卷积网络。</p>
<p>单纯的堆叠网络层数会使得网络模型退化，以至于后边特征丢失了前边特征的原本模样。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ResNet%E4%B8%AD%E7%9A%84%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84-1.png" class="" title="ResNet中的残差结构-1">
<p>于是使用跳连线，将前边特征直接接到了后边，使得 H(x) 包含了堆叠卷积的非线性输出 F(x) ，和跳过这两层堆叠卷积，直接连接过来的恒等映射x，让它们的对应元素相加，这一操作有效缓解了神经网络模型堆叠导致的退化，使得神经网络可以向更深层级发展。</p>
<blockquote>
<p>Inception块中的 “+” 是沿深度方向叠加（千层蛋糕层数叠加）<br>ResNet块中的 “+” 是特征图对应元素值相加（矩阵值相加）</p>
</blockquote>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ResNet%E4%B8%AD%E7%9A%84%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84-2.png" class="" title="ResNet中的残差结构-2">
<p>ResNet块中有两种情况，一种情况用图中的实线表示，这种情况两层堆叠卷积，没有改变特征图的维度，也就是它们特征图的个数、高、宽和深度都相同，可以直接将 F(x) 与 x 相加。<br>另一种情况用图中的虚线表示，这种情况中两层堆叠卷积改变了特征图的维度，需要借助 1 × 1 的卷积来调整 x 的维度，使 W(x) 与 F(x) 的维度一致。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ResNet%E4%B8%AD%E7%9A%84%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84-3.png" class="" title="ResNet中的残差结构-3">
<p>ResNet块有两种形式，一种在堆叠卷积前后维度相同，另一种在堆叠卷积前后维度不同。<br>将ResNet块的两种结构封装到一个橙色块中，写出ResnetBlock类，每调用一次ResnetBlock类，会生成一个黄色块，如果堆叠卷积前后维度不同，residual_path 等于1，调用红色块里的代码，使用1 × 1卷积操作，调整输入特征图inputs的尺寸或深度后，将堆叠卷积输出特征 y 和 if 语句计算出的residual 相加，过激活，输出。<br>如果堆叠卷积前后维度相同，不执行红色块内的代码，直接将堆叠卷积输出特征y和输入特征图inputs相加，过激活，输出。<br>后续代码用橙色块拼接完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResnetBlock</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filters, strides=<span class="number">1</span>, residual_path=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResnetBlock, self).__init__()</span><br><span class="line">        self.filters = filters</span><br><span class="line">        self.strides = strides</span><br><span class="line">        self.residual_path = residual_path</span><br><span class="line"></span><br><span class="line">        self.c1 = Conv2D(filters, (<span class="number">3</span>, <span class="number">3</span>), strides=strides, padding=<span class="string">&#x27;same&#x27;</span>, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.b1 = BatchNormalization()</span><br><span class="line">        self.a1 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        self.c2 = Conv2D(filters, (<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span>, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.b2 = BatchNormalization()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># residual_path为True时，对输入进行下采样，即用1x1的卷积核做卷积操作，保证x能和F(x)维度相同，顺利相加</span></span><br><span class="line">        <span class="keyword">if</span> residual_path:</span><br><span class="line">            self.down_c1 = Conv2D(filters, (<span class="number">1</span>, <span class="number">1</span>), strides=strides, padding=<span class="string">&#x27;same&#x27;</span>, use_bias=<span class="literal">False</span>)</span><br><span class="line">            self.down_b1 = BatchNormalization()</span><br><span class="line">        </span><br><span class="line">        self.a2 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        residual = inputs  <span class="comment"># residual等于输入值本身，即residual=x</span></span><br><span class="line">        <span class="comment"># 将输入通过卷积、BN层、激活层，计算F(x)</span></span><br><span class="line">        x = self.c1(inputs)</span><br><span class="line">        x = self.b1(x)</span><br><span class="line">        x = self.a1(x)</span><br><span class="line"></span><br><span class="line">        x = self.c2(x)</span><br><span class="line">        y = self.b2(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.residual_path:</span><br><span class="line">            residual = self.down_c1(inputs)</span><br><span class="line">            residual = self.down_b1(residual)</span><br><span class="line"></span><br><span class="line">        out = self.a2(y + residual)  <span class="comment"># 最后输出的是两部分的和，即F(x)+x或F(x)+Wx,再过激活函数</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ResNet%E4%B8%AD%E7%9A%84%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84-4.png" class="" title="ResNet中的残差结构-4">
<p>左侧给出的框图是ResNet18用CBAPD表示的结构，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet18</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block_list, initial_filters=<span class="number">64</span></span>):  <span class="comment"># block_list表示每个block有几个卷积层</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet18, self).__init__()</span><br><span class="line">        self.num_blocks = <span class="built_in">len</span>(block_list)  <span class="comment"># 共有几个block</span></span><br><span class="line">        self.block_list = block_list</span><br><span class="line">        self.out_filters = initial_filters</span><br><span class="line">        self.c1 = Conv2D(self.out_filters, (<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span>, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.b1 = BatchNormalization()</span><br><span class="line">        self.a1 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.blocks = tf.keras.models.Sequential()</span><br><span class="line">        <span class="comment"># 构建ResNet网络结构</span></span><br><span class="line">        <span class="keyword">for</span> block_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(block_list)):  <span class="comment"># 第几个resnet block</span></span><br><span class="line">            <span class="keyword">for</span> layer_id <span class="keyword">in</span> <span class="built_in">range</span>(block_list[block_id]):  <span class="comment"># 第几个卷积层</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> block_id != <span class="number">0</span> <span class="keyword">and</span> layer_id == <span class="number">0</span>:  <span class="comment"># 对除第一个block以外的每个block的输入进行下采样</span></span><br><span class="line">                    block = ResnetBlock(self.out_filters, strides=<span class="number">2</span>, residual_path=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    block = ResnetBlock(self.out_filters, residual_path=<span class="literal">False</span>)</span><br><span class="line">                self.blocks.add(block)  <span class="comment"># 将构建好的block加入resnet</span></span><br><span class="line">            self.out_filters *= <span class="number">2</span>  <span class="comment"># 下一个block的卷积核数是上一个block的2倍</span></span><br><span class="line">        self.p1 = tf.keras.layers.GlobalAveragePooling2D()</span><br><span class="line">        self.f1 = tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>, kernel_regularizer=tf.keras.regularizers.l2())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        x = self.c1(inputs)</span><br><span class="line">        x = self.b1(x)</span><br><span class="line">        x = self.a1(x)</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.p1(x)</span><br><span class="line">        y = self.f1(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = ResNet18([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h1 id="经典卷积及网络小结"><a href="#经典卷积及网络小结" class="headerlink" title="经典卷积及网络小结"></a>经典卷积及网络小结</h1><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C-1.png" class="" title="经典卷积网络-1">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%94%E8%AE%B2%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%B0%8F%E7%BB%93.png" class="" title="经典卷积网络小结">
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2023/02/16/人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络/">http://hibiscidai.com/2023/02/16/人工智能实践-Tensorflow笔记-MOOC-第五讲卷积神经网络/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/TensorflowMOOC/">TensorflowMOOC</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%BA%8C%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/"><i class="fa fa-chevron-left">  </i><span>人工智能实践-Tensorflow笔记-MOOC-第二讲神经网络优化</span></a></div><div class="next-post pull-right"><a href="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E5%9B%9B%E8%AE%B2%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1%E6%89%A9%E5%B1%95/"><span>人工智能实践-Tensorflow笔记-MOOC-第四讲网络八股扩展</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://www.paofu.cloud/auth/register?code=j4I7">好用、实惠、稳定的梯子,点击这里<img src="https://pic.imgdb.cn/item/65572abac458853aefef30cd.png" width="1000" height="124" object-fit="cover" ></a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2025 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>