<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Hadoop-HDFS详解"><meta name="keywords" content="学习笔记,Hadoop,大数据"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>Hadoop-HDFS详解 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop-HDFS%E8%AF%A6%E8%A7%A3"><span class="toc-number">1.</span> <span class="toc-text">Hadoop-HDFS详解</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS%E6%A6%82%E8%BF%B0"><span class="toc-number">2.</span> <span class="toc-text">HDFS概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E4%BA%A7%E5%87%BA%E8%83%8C%E6%99%AF"><span class="toc-number">2.1.</span> <span class="toc-text">HDFS产出背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%AE%9A%E4%B9%89"><span class="toc-number">2.2.</span> <span class="toc-text">HDFS定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">2.3.</span> <span class="toc-text">HDFS优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">2.3.1.</span> <span class="toc-text">优点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E5%AE%B9%E9%94%99%E6%80%A7"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">高容错性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%82%E5%90%88%E5%A4%84%E7%90%86%E5%A4%A7%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">适合处理大数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">2.3.2.</span> <span class="toc-text">缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E9%80%82%E5%90%88%E4%BD%8E%E5%BB%B6%E6%97%B6%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE%EF%BC%8C%E6%AF%94%E5%A6%82%E6%AF%AB%E7%A7%92%E7%BA%A7%E7%9A%84%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%98%AF%E5%81%9A%E4%B8%8D%E5%88%B0%E7%9A%84%E3%80%82"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A0%E6%B3%95%E9%AB%98%E6%95%88%E7%9A%84%E5%AF%B9%E5%A4%A7%E9%87%8F%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E5%AD%98%E5%82%A8%E3%80%82"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">无法高效的对大量小文件进行存储。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E6%94%AF%E6%8C%81%E5%B9%B6%E5%8F%91%E5%86%99%E5%85%A5%E3%80%81%E6%96%87%E4%BB%B6%E9%9A%8F%E6%9C%BA%E4%BF%AE%E6%94%B9%E3%80%82"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">不支持并发写入、文件随机修改。</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84"><span class="toc-number">2.4.</span> <span class="toc-text">HDFS组成架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NameNode-m"><span class="toc-number">2.4.1.</span> <span class="toc-text">NameNode(m)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataNode"><span class="toc-number">2.4.2.</span> <span class="toc-text">DataNode</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client"><span class="toc-number">2.4.3.</span> <span class="toc-text">Client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Secondary-NameNode"><span class="toc-number">2.4.4.</span> <span class="toc-text">Secondary NameNode</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F%E2%80%BB"><span class="toc-number">2.5.</span> <span class="toc-text">HDFS文件块大小※</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS%E7%9A%84Shell%E6%93%8D%E4%BD%9C%E2%80%BB"><span class="toc-number">3.</span> <span class="toc-text">HDFS的Shell操作※</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-number">3.1.</span> <span class="toc-text">基本语法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8"><span class="toc-number">3.2.</span> <span class="toc-text">命令大全</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AE%9E%E6%93%8D"><span class="toc-number">3.3.</span> <span class="toc-text">常用命令实操</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C%E2%80%BB"><span class="toc-number">4.</span> <span class="toc-text">HDFS客户端操作※</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">4.1.</span> <span class="toc-text">HDFS客户端环境准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%B5%8B%E8%AF%95"><span class="toc-number">4.2.</span> <span class="toc-text">HDFS客户端测试</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS%E7%9A%84API%E6%93%8D%E4%BD%9C"><span class="toc-number">5.</span> <span class="toc-text">HDFS的API操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%EF%BC%88%E6%B5%8B%E8%AF%95%E5%8F%82%E6%95%B0%E4%BC%98%E5%85%88%E7%BA%A7%EF%BC%89"><span class="toc-number">5.1.</span> <span class="toc-text">HDFS文件上传（测试参数优先级）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD"><span class="toc-number">5.2.</span> <span class="toc-text">HDFS文件下载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E5%88%A0%E9%99%A4"><span class="toc-number">5.3.</span> <span class="toc-text">HDFS文件删除</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E6%9B%B4%E5%90%8D"><span class="toc-number">5.4.</span> <span class="toc-text">HDFS文件更名</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E8%AF%A6%E6%83%85%E6%9F%A5%E7%9C%8B"><span class="toc-number">5.5.</span> <span class="toc-text">HDFS文件详情查看</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9%E5%88%A4%E6%96%AD"><span class="toc-number">5.6.</span> <span class="toc-text">HDFS文件和文件夹判断</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS%E7%9A%84I-O%E6%B5%81%E6%93%8D%E4%BD%9C"><span class="toc-number">6.</span> <span class="toc-text">HDFS的I&#x2F;O流操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0IO"><span class="toc-number">6.1.</span> <span class="toc-text">HDFS文件上传IO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82"><span class="toc-number">6.1.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-number">6.1.2.</span> <span class="toc-text">编写代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BDIO"><span class="toc-number">6.2.</span> <span class="toc-text">HDFS文件下载IO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-1"><span class="toc-number">6.2.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81-1"><span class="toc-number">6.2.2.</span> <span class="toc-text">编写代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E5%AE%9A%E4%BD%8D%E8%AF%BB%E5%8F%96IO"><span class="toc-number">6.3.</span> <span class="toc-text">HDFS文件定位读取IO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-2"><span class="toc-number">6.3.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81-2"><span class="toc-number">6.3.2.</span> <span class="toc-text">编写代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E2%80%BB"><span class="toc-number">7.</span> <span class="toc-text">HDFS的数据流※</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">7.1.</span> <span class="toc-text">HDFS写数据流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%96%E6%9E%90%E6%96%87%E4%BB%B6%E5%86%99%E5%85%A5"><span class="toc-number">7.1.1.</span> <span class="toc-text">剖析文件写入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">7.1.2.</span> <span class="toc-text">网络拓扑-节点距离计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%EF%BC%88%E5%89%AF%E6%9C%AC%E5%AD%98%E5%82%A8%E8%8A%82%E7%82%B9%E9%80%89%E6%8B%A9%EF%BC%89"><span class="toc-number">7.1.3.</span> <span class="toc-text">机架感知（副本存储节点选择）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">7.2.</span> <span class="toc-text">HDFS读数据流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NameNode%E5%92%8CSecondaryNameNode%E2%80%BB"><span class="toc-number">8.</span> <span class="toc-text">NameNode和SecondaryNameNode※</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">8.1.</span> <span class="toc-text">NN和2NN工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE"><span class="toc-number">8.2.</span> <span class="toc-text">NN和2NN工作原理图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9ANameNode%E5%90%AF%E5%8A%A8"><span class="toc-number">8.2.1.</span> <span class="toc-text">第一阶段：NameNode启动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9ASecondary-NameNode%E5%B7%A5%E4%BD%9C"><span class="toc-number">8.2.2.</span> <span class="toc-text">第二阶段：Secondary NameNode工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3%EF%BC%9A"><span class="toc-number">8.2.3.</span> <span class="toc-text">NN和2NN工作机制详解：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fsimage%E5%92%8CEdits%E8%A7%A3%E6%9E%90"><span class="toc-number">8.3.</span> <span class="toc-text">Fsimage和Edits解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">8.3.1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#oiv%E6%9F%A5%E7%9C%8BFsimage%E6%96%87%E4%BB%B6"><span class="toc-number">8.3.2.</span> <span class="toc-text">oiv查看Fsimage文件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8Boiv%E5%92%8Coev%E5%91%BD%E4%BB%A4"><span class="toc-number">8.3.2.1.</span> <span class="toc-text">查看oiv和oev命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95-1"><span class="toc-number">8.3.2.2.</span> <span class="toc-text">基本语法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E6%93%8D%E6%A1%88%E4%BE%8B"><span class="toc-number">8.3.2.3.</span> <span class="toc-text">实操案例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#oev%E6%9F%A5%E7%9C%8BEdits%E6%96%87%E4%BB%B6"><span class="toc-number">8.3.3.</span> <span class="toc-text">oev查看Edits文件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95-2"><span class="toc-number">8.3.3.1.</span> <span class="toc-text">基本语法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">8.3.3.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CheckPoint%E6%97%B6%E9%97%B4%E8%AE%BE%E7%BD%AE"><span class="toc-number">8.4.</span> <span class="toc-text">CheckPoint时间设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NameNode%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86"><span class="toc-number">8.5.</span> <span class="toc-text">NameNode故障处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E5%B0%86SecondaryNameNode%E4%B8%AD%E6%95%B0%E6%8D%AE%E6%8B%B7%E8%B4%9D%E5%88%B0NameNode%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%9A%84%E7%9B%AE%E5%BD%95%EF%BC%9B"><span class="toc-number">8.5.1.</span> <span class="toc-text">方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E4%BD%BF%E7%94%A8-importCheckpoint%E9%80%89%E9%A1%B9%E5%90%AF%E5%8A%A8NameNode%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%B0%86SecondaryNameNode%E4%B8%AD%E6%95%B0%E6%8D%AE%E6%8B%B7%E8%B4%9D%E5%88%B0NameNode%E7%9B%AE%E5%BD%95%E4%B8%AD%E3%80%82"><span class="toc-number">8.5.2.</span> <span class="toc-text">方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F"><span class="toc-number">8.6.</span> <span class="toc-text">集群安全模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">8.6.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95-3"><span class="toc-number">8.6.2.</span> <span class="toc-text">基本语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B"><span class="toc-number">8.6.3.</span> <span class="toc-text">案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NameNode%E5%A4%9A%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE"><span class="toc-number">8.7.</span> <span class="toc-text">NameNode多目录配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E9%85%8D%E7%BD%AE"><span class="toc-number">8.7.1.</span> <span class="toc-text">具体配置</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DataNode%E2%80%BB"><span class="toc-number">9.</span> <span class="toc-text">DataNode※</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">9.1.</span> <span class="toc-text">DataNode工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7"><span class="toc-number">9.2.</span> <span class="toc-text">数据完整性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">9.3.</span> <span class="toc-text">掉线时限参数设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%8D%E5%BD%B9%E6%96%B0%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9"><span class="toc-number">9.4.</span> <span class="toc-text">服役新数据节点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-3"><span class="toc-number">9.4.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">9.4.2.</span> <span class="toc-text">环境准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%8D%E5%BD%B9%E6%96%B0%E8%8A%82%E7%82%B9%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4"><span class="toc-number">9.4.3.</span> <span class="toc-text">服役新节点具体步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%80%E5%BD%B9%E6%97%A7%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9"><span class="toc-number">9.5.</span> <span class="toc-text">退役旧数据节点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E7%99%BD%E5%90%8D%E5%8D%95-%E8%BE%83%E5%AE%89%E5%85%A8"><span class="toc-number">9.5.1.</span> <span class="toc-text">添加白名单(较安全)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%BB%91%E5%90%8D%E5%8D%95%E9%80%80%E5%BD%B9-%E6%A0%87%E5%87%86%E7%A8%8B%E5%BA%8F"><span class="toc-number">9.5.2.</span> <span class="toc-text">黑名单退役(标准程序)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Datanode%E5%A4%9A%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE"><span class="toc-number">9.6.</span> <span class="toc-text">Datanode多目录配置</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7"><span class="toc-number">10.</span> <span class="toc-text">HDFS 2.X新特性</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E9%97%B4%E6%95%B0%E6%8D%AE%E6%8B%B7%E8%B4%9D"><span class="toc-number">10.1.</span> <span class="toc-text">集群间数据拷贝</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E5%AD%98%E6%A1%A3"><span class="toc-number">10.2.</span> <span class="toc-text">小文件存档</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%AD%98%E5%82%A8%E5%B0%8F%E6%96%87%E4%BB%B6%E5%BC%8A%E7%AB%AF"><span class="toc-number">10.2.1.</span> <span class="toc-text">HDFS存储小文件弊端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E5%AD%98%E5%82%A8%E5%B0%8F%E6%96%87%E4%BB%B6%E5%8A%9E%E6%B3%95%E4%B9%8B%E4%B8%80"><span class="toc-number">10.2.2.</span> <span class="toc-text">解决存储小文件办法之一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D-1"><span class="toc-number">10.2.3.</span> <span class="toc-text">案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E6%94%B6%E7%AB%99"><span class="toc-number">10.3.</span> <span class="toc-text">回收站</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">10.3.1.</span> <span class="toc-text">回收站参数设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E6%94%B6%E7%AB%99%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">10.3.2.</span> <span class="toc-text">回收站工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E7%94%A8%E5%9B%9E%E6%94%B6%E7%AB%99"><span class="toc-number">10.3.3.</span> <span class="toc-text">启用回收站</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E5%9B%9E%E6%94%B6%E7%AB%99"><span class="toc-number">10.3.4.</span> <span class="toc-text">查看回收站</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E8%AE%BF%E9%97%AE%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AB%99%E7%94%A8%E6%88%B7%E5%90%8D%E7%A7%B0"><span class="toc-number">10.3.5.</span> <span class="toc-text">修改访问垃圾回收站用户名称</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E7%A8%8B%E5%BA%8F%E5%88%A0%E9%99%A4%E7%9A%84%E6%96%87%E4%BB%B6%E4%B8%8D%E4%BC%9A%E7%BB%8F%E8%BF%87%E5%9B%9E%E6%94%B6%E7%AB%99%EF%BC%8C%E9%9C%80%E8%A6%81%E8%B0%83%E7%94%A8moveToTrash-%E6%89%8D%E8%BF%9B%E5%85%A5%E5%9B%9E%E6%94%B6%E7%AB%99"><span class="toc-number">10.3.6.</span> <span class="toc-text">通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%81%A2%E5%A4%8D%E5%9B%9E%E6%94%B6%E7%AB%99%E6%95%B0%E6%8D%AE"><span class="toc-number">10.3.7.</span> <span class="toc-text">恢复回收站数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B8%85%E7%A9%BA%E5%9B%9E%E6%94%B6%E7%AB%99"><span class="toc-number">10.3.8.</span> <span class="toc-text">清空回收站</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86"><span class="toc-number">10.4.</span> <span class="toc-text">快照管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86API"><span class="toc-number">10.4.1.</span> <span class="toc-text">快照管理API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D-2"><span class="toc-number">10.4.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS-HA%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="toc-number">11.</span> <span class="toc-text">HDFS HA高可用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HA%E6%A6%82%E8%BF%B0"><span class="toc-number">11.1.</span> <span class="toc-text">HA概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS-HA%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">11.2.</span> <span class="toc-text">HDFS-HA工作机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-HA%E5%B7%A5%E4%BD%9C%E8%A6%81%E7%82%B9"><span class="toc-number">11.2.1.</span> <span class="toc-text">HDFS-HA工作要点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-HA%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">11.2.2.</span> <span class="toc-text">HDFS-HA自动故障转移工作机制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS-HA%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE"><span class="toc-number">11.3.</span> <span class="toc-text">HDFS-HA集群配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-1"><span class="toc-number">11.3.1.</span> <span class="toc-text">环境准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%84%E5%88%92%E9%9B%86%E7%BE%A4"><span class="toc-number">11.3.2.</span> <span class="toc-text">规划集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEZookeeper%E9%9B%86%E7%BE%A4"><span class="toc-number">11.3.3.</span> <span class="toc-text">配置Zookeeper集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEHDFS-HA%E9%9B%86%E7%BE%A4"><span class="toc-number">11.3.4.</span> <span class="toc-text">配置HDFS-HA集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8HDFS-HA%E9%9B%86%E7%BE%A4"><span class="toc-number">11.3.5.</span> <span class="toc-text">启动HDFS-HA集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEHDFS-HA%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="toc-number">11.3.6.</span> <span class="toc-text">配置HDFS-HA自动故障转移</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YARN-HA%E9%85%8D%E7%BD%AE"><span class="toc-number">11.4.</span> <span class="toc-text">YARN-HA配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#YARN-HA%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">11.4.1.</span> <span class="toc-text">YARN-HA工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEYARN-HA%E9%9B%86%E7%BE%A4"><span class="toc-number">11.4.2.</span> <span class="toc-text">配置YARN-HA集群</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS-Federation%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">11.5.</span> <span class="toc-text">HDFS Federation架构设计</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">243</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">88</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">33</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">Hadoop-HDFS详解</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-07-27</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hadoop/">Hadoop</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">18.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 79 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/Hadoop-HDFS%E8%AF%A6%E8%A7%A3.png" class="" title="Hadoop-HDFS详解">
<p>Hadoop-HDFS详解</p>
<span id="more"></span>
<p>[TOC]</p>
<h1 id="Hadoop-HDFS详解"><a href="#Hadoop-HDFS详解" class="headerlink" title="Hadoop-HDFS详解"></a>Hadoop-HDFS详解</h1><h1 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h1><h2 id="HDFS产出背景"><a href="#HDFS产出背景" class="headerlink" title="HDFS产出背景"></a>HDFS产出背景</h2><p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。<br>HDFS只是分布式文件管理系统中的一种。<br>NTFS为windows文件管理系统。</p>
<h2 id="HDFS定义"><a href="#HDFS定义" class="headerlink" title="HDFS定义"></a>HDFS定义</h2><p>HDFS (Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。</p>
<h2 id="HDFS优缺点"><a href="#HDFS优缺点" class="headerlink" title="HDFS优缺点"></a>HDFS优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><h4 id="高容错性"><a href="#高容错性" class="headerlink" title="高容错性"></a>高容错性</h4><ul>
<li>(1)数据自动保存多个副本。它通过增加副本的形式，提高容错性。</li>
<li>(2)某一个副本丢失以后，它可以自动恢复。</li>
</ul>
<h4 id="适合处理大数据"><a href="#适合处理大数据" class="headerlink" title="适合处理大数据"></a>适合处理大数据</h4><ul>
<li>(1)数据规模：能够处理数据规模达到GB、TB、 甚至PB级别的数据。</li>
<li>(2)文件规模：能够处理百万规模以上的文件数量，数量相当之大。</li>
<li>(3)可构建在廉价机器上，通过多副本机制，提高可靠性。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><h4 id="不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。"><a href="#不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。" class="headerlink" title="不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。"></a>不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。</h4><h4 id="无法高效的对大量小文件进行存储。"><a href="#无法高效的对大量小文件进行存储。" class="headerlink" title="无法高效的对大量小文件进行存储。"></a>无法高效的对大量小文件进行存储。</h4><ul>
<li>(1)存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内 存总是有限的;</li>
<li>(2)小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。</li>
</ul>
<h4 id="不支持并发写入、文件随机修改。"><a href="#不支持并发写入、文件随机修改。" class="headerlink" title="不支持并发写入、文件随机修改。"></a>不支持并发写入、文件随机修改。</h4><ul>
<li>(1)一个文件只能有一个写，不允许多个线程同时写;</li>
<li>(2)仅支持数据append (追加)，不支持文件的随机修改。</li>
</ul>
<h2 id="HDFS组成架构"><a href="#HDFS组成架构" class="headerlink" title="HDFS组成架构"></a>HDFS组成架构</h2><img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84-1.png" class="" title="HDFS组成架构-1">
<h3 id="NameNode-m"><a href="#NameNode-m" class="headerlink" title="NameNode(m)"></a>NameNode(m)</h3><p>就是Master，它是一个主管、管理者。</p>
<ul>
<li>(1)管理HDFS的名称空间；</li>
<li>(2)配置副本策略；</li>
<li>(3)管理数据块(Block)映射信息；</li>
<li>(4)处理客户端读写请求。</li>
</ul>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>就是Slave。 NameNode下达命令, DataNode执行实际的操作。</p>
<ul>
<li>(1)存储实际的数据块；</li>
<li>(2)执行数据块的读/写操作。</li>
</ul>
<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><p>就是客户端。</p>
<ul>
<li>(1)文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block(128M)，然后进行上传；</li>
<li>(2)与NameNode交互， 获取文件的位置信息；</li>
<li>(3)与DataNode交互，读取或者写入数据；</li>
<li>(4)Client提供一些命令来管理HDFS，比如NameNode格式化；</li>
<li>(5)Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作；</li>
</ul>
<h3 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h3><p>并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</p>
<ul>
<li>(1)辅助NameNode，分担其工作量，比如定期合并Fsimage(镜像文件)和Edits(编译日志)，并推送给NameNode；</li>
<li>(2)在紧急情况下，可辅助恢复NameNode。</li>
</ul>
<h2 id="HDFS文件块大小※"><a href="#HDFS文件块大小※" class="headerlink" title="HDFS文件块大小※"></a>HDFS文件块大小※</h2><p>HDFS中的文件在物理上是分块存储(Block) ，块的大小可以通过配置参数(dfs. blocksize)来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M。</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F-1.png" class="" title="HDFS文件块大小-1">
<ul>
<li>1.集群中的block</li>
<li>2.寻址时间10ms，即查找到目标block的时间为10ms。</li>
<li>3.寻址时间为传输时间的<code>1%</code>时，则为最佳状态。因此，传输时间=<code>10ms/0.01=1000ms=1s</code></li>
<li>4.而目前磁盘的传输速率普遍为100MB/s。(决定块大小，趋向256M)</li>
<li>5.block大小 = <code>1s * 100MB / s = 100MB</code></li>
</ul>
<blockquote>
<p>为什么块的大小不能设置太小，也不能设置太大?<br>(1)HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置;<br>(2)如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。<br>HDFS块的大小设置主要取决与磁盘传输速率。</p>
</blockquote>
<h1 id="HDFS的Shell操作※"><a href="#HDFS的Shell操作※" class="headerlink" title="HDFS的Shell操作※"></a>HDFS的Shell操作※</h1><h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs 具体命令</span><br><span class="line"></span><br><span class="line">$ bin/hdfs dfs 具体命令</span><br><span class="line"></span><br><span class="line">dfs是fs的实现类。</span><br></pre></td></tr></table></figure>
<h2 id="命令大全"><a href="#命令大全" class="headerlink" title="命令大全"></a>命令大全</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs</span><br><span class="line"></span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-<span class="built_in">cat</span> [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">chgrp</span> [-R] GROUP PATH...]</span><br><span class="line">	[-<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">cp</span> [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-<span class="built_in">df</span> [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">du</span> [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-<span class="built_in">nl</span>] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">	[-<span class="built_in">ls</span> [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">mkdir</span> [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">mv</span> &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">rmdir</span> [--ignore-fail-on-non-empty] &lt;<span class="built_in">dir</span>&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">tail</span> [-f] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">truncate</span> [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value <span class="keyword">for</span> given property</span><br><span class="line">-fs &lt;<span class="built_in">local</span>|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include <span class="keyword">in</span> the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general <span class="built_in">command</span> line syntax is</span><br><span class="line">bin/hadoop <span class="built_in">command</span> [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>
<h2 id="常用命令实操"><a href="#常用命令实操" class="headerlink" title="常用命令实操"></a>常用命令实操</h2><ul>
<li>（0）启动Hadoop集群（方便后续的测试）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
<ul>
<li>（1）<code>-help</code>：输出这个命令参数</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">help</span> <span class="built_in">rm</span></span><br><span class="line"></span><br><span class="line">-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] &lt;src&gt; ... :</span><br><span class="line">Delete all files that match the specified file pattern. Equivalent to the Unix</span><br><span class="line"><span class="built_in">command</span> <span class="string">&quot;rm &lt;src&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">-skipTrash  option bypasses trash, <span class="keyword">if</span> enabled, and immediately deletes &lt;src&gt;</span><br><span class="line">-f If the file does not exist, <span class="keyword">do</span> not display a diagnostic message or modify the <span class="built_in">exit</span> status to reflect an error.            </span><br><span class="line">-[rR] Recursively deletes directories</span><br></pre></td></tr></table></figure>
<ul>
<li>（2）<code>-ls</code>: 显示目录信息</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> /</span><br><span class="line"></span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x - atguigu supergroup 0 2020-07-26 04:33 /user</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> -R</span><br><span class="line"></span><br><span class="line">drwxr-xr-x - atguigu supergroup 0 2020-07-26 05:32 input</span><br><span class="line">-rw-r--r-- 3 atguigu supergroup 1366 2020-07-26 05:29 input/README.txt</span><br><span class="line">-rw-r--r-- 3 atguigu supergroup 197657687 2020-07-26 05:32 input/hadoop-2.7.2.tar.gz</span><br></pre></td></tr></table></figure>
<ul>
<li>（3）<code>-mkdir</code>：在HDFS上创建目录</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">mkdir</span> -p /sanguo/shuguo</span><br></pre></td></tr></table></figure>
<blockquote>
<p>多级目录创建需要加<code>-p</code>。</p>
</blockquote>
<ul>
<li>（4）<code>-moveFromLocal</code>：从本地剪切粘贴到HDFS</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">touch</span> kongming.txt</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ vim kongming.txt</span><br><span class="line">wo shi zhu ge liang</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt  /sanguo/shuguo</span><br></pre></td></tr></table></figure>
<ul>
<li>（5）<code>-appendToFile</code>：追加一个文件到已经存在的文件末尾</li>
</ul>
<blockquote>
<p>hdfs只支持追加不支持修改。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">touch</span> liubei.txt</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ vim liubei.txt</span><br><span class="line">san gu mao lu</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>（6）<code>-cat</code>：显示文件内容</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">cat</span> /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">wo shi zhu ge liang</span><br><span class="line">san gu mao lu</span><br></pre></td></tr></table></figure>
<ul>
<li>（7）<code>-chgrp</code> 、<code>-chmod</code>、<code>-chown</code>：Linux文件系统中的用法一样，修改文件所属权限</li>
</ul>
<p>更改组、权限、拥有者</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">chgrp</span> atguigu /sanguo/shuguo/kongming.txt</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">chmod</span> 666 /sanguo/shuguo/kongming.txt</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">chown</span> atguigu:atguigu /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>（8）<code>-copyFromLocal</code>：从本地文件系统中拷贝文件到HDFS路径去</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /</span><br></pre></td></tr></table></figure>
<ul>
<li>（9）<code>-copyToLocal</code>：从HDFS拷贝到本地</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>
<ul>
<li>（10）<code>-cp</code> ：从HDFS的一个路径拷贝到HDFS的另一个路径</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">cp</span> /sanguo/shuguo/kongming.txt /zhuge.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>（11）<code>-mv</code>：在HDFS目录中移动文件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">mv</span> /zhuge.txt /sanguo/shuguo/</span><br></pre></td></tr></table></figure>
<ul>
<li>（12）<code>-get</code>：等同于copyToLocal，就是从HDFS下载文件到本地</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>
<ul>
<li>（13）<code>-getmerge</code>：合并下载多个文件，比如HDFS的目录 /user/atguigu/test下有多个文件:<code>log.1, log.2,log.3,...</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/atguigu/test/* ./zaiyiqi.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>（14）<code>-put</code>：等同于copyFromLocal</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/atguigu/test/</span><br></pre></td></tr></table></figure>
<ul>
<li>（15）<code>-tail</code>：显示一个文件的末尾，动态监控</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">tail</span> /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>（16）<code>-rm</code>：删除文件或文件夹</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">rm</span> /user/atguigu/test/jinlian2.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>（17）<code>-rmdir</code>：删除空目录</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">mkdir</span> /test</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">rmdir</span> /test</span><br></pre></td></tr></table></figure>
<ul>
<li>（18）<code>-du</code>统计文件夹的大小信息</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有文件</span></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">du</span> -s -h /user/atguigu/test</span><br><span class="line">2.7 K  /user/atguigu/test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 详细文件</span></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">du</span>  -h /user/atguigu/test</span><br><span class="line">1.3 K  /user/atguigu/test/README.txt</span><br><span class="line">15     /user/atguigu/test/jinlian.txt</span><br><span class="line">1.4 K  /user/atguigu/test/zaiyiqi.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>（19）<code>-setrep</code>：设置HDFS中文件的副本数量</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
</blockquote>
<h1 id="HDFS客户端操作※"><a href="#HDFS客户端操作※" class="headerlink" title="HDFS客户端操作※"></a>HDFS客户端操作※</h1><h2 id="HDFS客户端环境准备"><a href="#HDFS客户端环境准备" class="headerlink" title="HDFS客户端环境准备"></a>HDFS客户端环境准备</h2><ul>
<li><p>1.据自己电脑的操作系统拷贝对应的编译后的hadoop jar包到非中文路径（例如：E:\hadoop-2.7.2）。</p>
</li>
<li><p>配置<code>HADOOP_HOME</code>环境变量</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=E:\hadoop-2.7.2</span><br></pre></td></tr></table></figure>
<ul>
<li>配置<code>Path</code>环境变量。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Path=%HADOOP_HOME%\bin</span><br></pre></td></tr></table></figure>
<ul>
<li>修改<code>hadoop-env.cmd</code>java路径</li>
</ul>
<blockquote>
<p>JAVA_HOME路径不允许有空格。</p>
</blockquote>
<p>将<code>$HADOOP_HOME/etc/hadoop/hadoop-env.cmd</code>文件中的<code>set JAVA_HOME=C:\Program Files\Java\jdk1.8.0_191</code> 修改为 <code>set JAVA_HOME=C:\PROGRA~1\Java\jdk1.8.0_191</code> 保存</p>
<ul>
<li>创建一个Maven工程<code>HdfsClientDemo</code></li>
</ul>
<p>包名称：<code>com.atguigu.hdfs</code></p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAMaven%E5%B7%A5%E7%A8%8BHdfsClientDemo-1.png" class="" title="创建一个Maven工程HdfsClientDemo-1">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAMaven%E5%B7%A5%E7%A8%8BHdfsClientDemo-2.png" class="" title="创建一个Maven工程HdfsClientDemo-2">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAMaven%E5%B7%A5%E7%A8%8BHdfsClientDemo-3.png" class="" title="创建一个Maven工程HdfsClientDemo-3">
<ul>
<li>导入配置</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>创建HdfsClient类</li>
</ul>
<p>在包<code>com.atguigu.hdfs</code>里创建</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HdfsClient</span>&#123;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">// 配置在集群上运行</span></span><br><span class="line">        <span class="comment">// configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;);</span></span><br><span class="line">        <span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line"></span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 创建目录</span></span><br><span class="line">        fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/1108/daxian/banzhang&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>需要在项目的<code>src/main/resources</code>目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout</span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n</span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender</span><br><span class="line">log4j.appender.logfile.File=target/spring.log</span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure>
<h2 id="HDFS客户端测试"><a href="#HDFS客户端测试" class="headerlink" title="HDFS客户端测试"></a>HDFS客户端测试</h2><p>如果使用eclipse打开项目，运行时需要配置用户名称。</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%B5%8B%E8%AF%95-1.png" class="" title="HDFS客户端测试-1">
<p>客户端去操作HDFS时，是有一个用户身份的。默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：<code>-DHADOOP_USER_NAME=atguigu，atguigu</code>为用户名称。</p>
<h1 id="HDFS的API操作"><a href="#HDFS的API操作" class="headerlink" title="HDFS的API操作"></a>HDFS的API操作</h1><h2 id="HDFS文件上传（测试参数优先级）"><a href="#HDFS文件上传（测试参数优先级）" class="headerlink" title="HDFS文件上传（测试参数优先级）"></a>HDFS文件上传（测试参数优先级）</h2><ul>
<li>编写源代码</li>
</ul>
<p>继续在<code>HDFSCilent.java</code>中书写方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件上传</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取fs对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);<span class="comment">//第一个路径是源数据路径，第二个数据是目标路径</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.执行上传API</span></span><br><span class="line">    fs.copyFromLocalFile(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/shangchuanceshi.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shangchuanceshi.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>在<code>resources</code>文件夹下创建<code>hdfs-site.xml</code>文件</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>再次执行：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//修改resource文件，副本数为1</span></span><br><span class="line">fs.copyFromLocalFile(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/shangchuanceshi-2.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shangchuanceshi-2.txt&quot;</span>));</span><br></pre></td></tr></table></figure>
<p>服务器副本数变为1。</p>
<p>再次执行：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//客户端修改文件，副本数为2</span></span><br><span class="line"><span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;2&quot;</span>);</span><br><span class="line"><span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">fs.copyFromLocalFile(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/shangchuanceshi-3.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shangchuanceshi-3.txt&quot;</span>));</span><br></pre></td></tr></table></figure>
<p>服务器副本数变为1。</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0-1.png" class="" title="HDFS文件上传-1">
<ul>
<li>参数优先级</li>
</ul>
<p>参数优先级排序：</p>
<p><code>（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置</code></p>
<h2 id="HDFS文件下载"><a href="#HDFS文件下载" class="headerlink" title="HDFS文件下载"></a>HDFS文件下载</h2><p>继续在<code>HDFSCilent.java</code>中书写方法</p>
<ul>
<li>普通下载</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件下载</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.执行下载操作</span></span><br><span class="line">    <span class="comment">//fs.copyToLocalFile(Path src, Path dst);简单下载，从src下载到dst</span></span><br><span class="line">    fs.copyToLocalFile(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shangchuanceshi.txt&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/shangchuanceshi-xiazai.txt&quot;</span>));<span class="comment">//第一个路径是源数据路径，第二个数据是目标路径。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务器上<code>shangchuanceshi.txt</code>保留；本地出现<code>shangchuanceshi-xiazai.txt</code>和<code>.shangchuanceshi-xiazai.txt.crc</code>两个文件。</p>
<ul>
<li>剪切下载</li>
</ul>
<p>再次执行方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//fs.copyToLocalFile(boolean delSrc, Path src, Path dst);//剪切模式，true代表删除源文件，flase代表保留源文件</span></span><br><span class="line">fs.copyToLocalFile(<span class="literal">true</span>,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shangchuanceshi.txt&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/shangchuanceshi-xiazai2.txt&quot;</span>));</span><br></pre></td></tr></table></figure>
<p>服务器上<code>shangchuanceshi.txt</code>被删除；本地出现<code>.shangchuanceshi-xiazai2.txt</code>和<code>.shangchuanceshi-xiazai2.txt.crc</code>两个文件。</p>
<ul>
<li>本地模式下载</li>
</ul>
<p>再次执行方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//fs.copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);//最后一个参数代表是否使用本地模式</span></span><br><span class="line">fs.copyToLocalFile(<span class="literal">false</span>,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shangchuanceshi-2.txt&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/shangchuanceshi-xiazai3.txt&quot;</span>),<span class="literal">true</span>);</span><br></pre></td></tr></table></figure>
<p>服务器上<code>shangchuanceshi-2.txt</code>没有被删除；本地出现<code>shangchuanceshi-xiazai3.txt</code>一个文件。</p>
<blockquote>
<p>直接下载文件会出现同名文件的<code>.crc</code>后缀，使用本地文件模式就可以解决此问题。</p>
</blockquote>
<h2 id="HDFS文件删除"><a href="#HDFS文件删除" class="headerlink" title="HDFS文件删除"></a>HDFS文件删除</h2><p>继续在<code>HDFSCilent.java</code>中书写方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件删除</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.执行删除操作</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//fs.delete(Path f)//已经过时</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//fs.delete(Path f, boolean recursive);//f为要删除的路径，当要删除为文件夹时设置recursive为true，递归删除</span></span><br><span class="line">    fs.delete(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/0529&quot;</span>), <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="HDFS文件更名"><a href="#HDFS文件更名" class="headerlink" title="HDFS文件更名"></a>HDFS文件更名</h2><p>继续在<code>HDFSCilent.java</code>中书写方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件更名</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testRename</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.执行更名操作</span></span><br><span class="line">    <span class="comment">//fs.rename(Path src, Path dst)//src为源路径，dst为重命名</span></span><br><span class="line">    fs.rename(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shangchuanceshi-2.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shangchuanceshi-2-rename.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="HDFS文件详情查看"><a href="#HDFS文件详情查看" class="headerlink" title="HDFS文件详情查看"></a>HDFS文件详情查看</h2><p>查看文件名称、权限、长度、块信息</p>
<p>继续在<code>HDFSCilent.java</code>中书写方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件详情查看</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.查看文件详情</span></span><br><span class="line">    <span class="comment">//RemoteIterator&lt;LocatedFileStatus&gt; fs.listFiles(final Path f, final boolean recursive)//文件路径，是否递归查看,返回一个迭代器。</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>), <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//遍历迭代器</span></span><br><span class="line">    <span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">        <span class="type">LocatedFileStatus</span> <span class="variable">status</span> <span class="operator">=</span> listFiles.next();<span class="comment">//获取状态信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//查看文件名称、权限、长度、块信息。</span></span><br><span class="line">        System.out.print(status.getPath().getName());<span class="comment">//文件名称</span></span><br><span class="line">        System.out.print(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        System.out.print(status.getPermission());<span class="comment">//文件权限</span></span><br><span class="line">        System.out.print(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        System.out.print(status.getLen());<span class="comment">//文件长度</span></span><br><span class="line">        System.out.print(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//BlockLocation[] bl = status.getBlockLocations();//文件块信息，返回一个数组</span></span><br><span class="line">        BlockLocation[] blockLocations = status.getBlockLocations();<span class="comment">//文件块信息</span></span><br><span class="line">        <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">            String[] hosts = blockLocation.getHosts();<span class="comment">//主机名称</span></span><br><span class="line">            <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">                System.out.print(host);</span><br><span class="line">                System.out.print(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println();</span><br><span class="line">        System.out.println(<span class="string">&quot;------------&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">README.txt	rw-r--r--	1366	hadoop103	hadoop102	hadoop104	</span><br><span class="line">------------</span><br><span class="line">kongming.txt	rw-r--r--	34	hadoop103	hadoop102	hadoop104	</span><br><span class="line">------------</span><br><span class="line">shangchuanceshi-2-rename.txt	rw-r--r--	46	hadoop102	</span><br><span class="line">------------</span><br><span class="line">shangchuanceshi-3.txt	rw-r--r--	46	hadoop102	hadoop104	</span><br><span class="line">------------</span><br><span class="line">README.txt	rw-r--r--	1366	hadoop103	hadoop102	hadoop104	</span><br><span class="line">------------</span><br><span class="line">hadoop-2.7.2.tar.gz	rw-r--r--	197657687	hadoop103	hadoop102	hadoop104	hadoop103	hadoop102	hadoop104	</span><br><span class="line">------------</span><br><span class="line">zhuge.txt	rw-r--r--	34	hadoop103	hadoop102	hadoop104	</span><br><span class="line">------------</span><br></pre></td></tr></table></figure>
<h2 id="HDFS文件和文件夹判断"><a href="#HDFS文件和文件夹判断" class="headerlink" title="HDFS文件和文件夹判断"></a>HDFS文件和文件夹判断</h2><p>继续在<code>HDFSCilent.java</code>中书写方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件和文件夹判断</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.判断是文件还是文件夹</span></span><br><span class="line">    FileStatus[] listStatues = fs.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : listStatues) &#123;</span><br><span class="line">        <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">            <span class="comment">//文件</span></span><br><span class="line">            System.out.println(<span class="string">&quot;f:&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//文件夹</span></span><br><span class="line">            System.out.println(<span class="string">&quot;d:&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//3.关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f:README.txt</span><br><span class="line">d:sanguo</span><br><span class="line">f:shangchuanceshi-2-rename.txt</span><br><span class="line">f:shangchuanceshi-3.txt</span><br><span class="line">d:user</span><br><span class="line">f:zhuge.txt</span><br></pre></td></tr></table></figure>
<h1 id="HDFS的I-O流操作"><a href="#HDFS的I-O流操作" class="headerlink" title="HDFS的I/O流操作"></a>HDFS的I/O流操作</h1><p>上面我们学的API操作HDFS系统都是框架封装好的。那么如果我们想自己实现上述API的操作该怎么实现呢？</p>
<p>我们可以采用IO流的方式实现数据的上传和下载。</p>
<h2 id="HDFS文件上传IO"><a href="#HDFS文件上传IO" class="headerlink" title="HDFS文件上传IO"></a>HDFS文件上传IO</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>把本地e盘上的<code>IO-shangchuan.txt</code>文件上传到HDFS根目录</p>
<h3 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h3><p>新建<code>HDFSIO.java</code>文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//IO文件上传</span></span><br><span class="line"><span class="comment">//把本地e盘上的`IO-shangchuan.txt`文件上传到HDFS根目录</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">putFileToHDFS</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.获取输入流</span></span><br><span class="line">    <span class="type">FileInputStream</span> <span class="variable">fileInputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;e:/IO-shangchuan.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.获取输出流</span></span><br><span class="line">    <span class="type">FSDataOutputStream</span> <span class="variable">fsDataOutputStream</span> <span class="operator">=</span> fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IO-shangchuan-shangchuan.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fsDataOutputStream);</span><br><span class="line">    IOUtils.closeStream(fileInputStream);</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="HDFS文件下载IO"><a href="#HDFS文件下载IO" class="headerlink" title="HDFS文件下载IO"></a>HDFS文件下载IO</h2><h3 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h3><p>从HDFS上下载<code>IO-shangchuan-shangchuan.txt</code>文件到本地e盘上</p>
<h3 id="编写代码-1"><a href="#编写代码-1" class="headerlink" title="编写代码"></a>编写代码</h3><p>继续在<code>HDFSIO.java</code>文件中书写</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件下载IO</span></span><br><span class="line"><span class="comment">//从HDFS上下载`IO-shangchuan-shangchuan.txt`文件到本地e盘上</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getFileFormHDFS</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.获取输入流</span></span><br><span class="line">    <span class="type">FSDataInputStream</span> <span class="variable">fsDataInputStream</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IO-shangchuan-shangchuan.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.获取输出流</span></span><br><span class="line">    <span class="type">FileOutputStream</span> <span class="variable">fileOutputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;e:/IO-shangchuan-xiazai.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>IO流传输不会产生<code>.crc</code>文件。</p>
</blockquote>
<h2 id="HDFS文件定位读取IO"><a href="#HDFS文件定位读取IO" class="headerlink" title="HDFS文件定位读取IO"></a>HDFS文件定位读取IO</h2><h3 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h3><p>分块读取HDFS上的大文件，比如根目录下的<code>/hadoop-2.7.2.tar.gz</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -put /opt/software/hadoop-2.7.2.tar.gz /</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Block 0</span><br><span class="line">Block ID: 1073741838</span><br><span class="line">Block Pool ID: BP-1347968132-192.168.228.102-1595708538420</span><br><span class="line">Generation Stamp: 1019</span><br><span class="line">Size: 134217728</span><br><span class="line">Availability:</span><br><span class="line">	hadoop103</span><br><span class="line">	hadoop104</span><br><span class="line">	hadoop102</span><br><span class="line"></span><br><span class="line">Block 1</span><br><span class="line">Block ID: 1073741839</span><br><span class="line">Block Pool ID: BP-1347968132-192.168.228.102-1595708538420</span><br><span class="line">Generation Stamp: 1020</span><br><span class="line">Size: 63439959</span><br><span class="line">Availability:</span><br><span class="line">	hadoop103</span><br><span class="line">	hadoop102</span><br><span class="line">	hadoop104</span><br></pre></td></tr></table></figure>
<blockquote>
<p>网页端点击<code>Download</code>时候会下载完整文件。</p>
</blockquote>
<p>如果对于大日志文件10G，我只想下载最新block或者查看一部分block，没有必要下载全部文件。</p>
<h3 id="编写代码-2"><a href="#编写代码-2" class="headerlink" title="编写代码"></a>编写代码</h3><p>继续在<code>HDFSIO.java</code>文件中书写</p>
<ul>
<li>（1）下载第一块</li>
</ul>
<p>下载前128M文件。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//下载第一块</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.获取输入流</span></span><br><span class="line">    <span class="type">FSDataInputStream</span> <span class="variable">fsDataInputStream</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.获取输出流</span></span><br><span class="line">    <span class="type">FileOutputStream</span> <span class="variable">fileOutputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;e:/hadoop-2.7.2-xiazai.tar.gz.part1&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.流的对拷（只拷128M）</span></span><br><span class="line">    <span class="type">byte</span>[] buf = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++) &#123;</span><br><span class="line">        fsDataInputStream.read(buf);</span><br><span class="line">        fileOutputStream.write(buf);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>（2）下载第二块</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//下载第二块</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取对象</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.获取输入流</span></span><br><span class="line">    <span class="type">FSDataInputStream</span> <span class="variable">fsDataInputStream</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.设置指定读取的起点</span></span><br><span class="line">    fsDataInputStream.seek(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.获取输出流</span></span><br><span class="line">    <span class="type">FileOutputStream</span> <span class="variable">fileOutputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;e:/hadoop-2.7.2-xiazai.tar.gz.part2&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>（3）合并文件</li>
</ul>
<p>打开e盘目录发现出现两个文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop-2.7.2-xiazai.tar.gz.part2	61954KB</span><br><span class="line">hadoop-2.7.2-xiazai.tar.gz.part1	131072KB</span><br></pre></td></tr></table></figure>
<p>文件资源管理器输入<code>cmd</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E:\&gt;<span class="built_in">type</span> hadoop-2.7.2-xiazai.tar.gz.part2 &gt;&gt; hadoop-2.7.2-xiazai.tar.gz.part1</span><br></pre></td></tr></table></figure>
<p>合并完成后修改<code>hadoop-2.7.2-xiazai.tar.gz.part1    1932026KB</code>后缀名为<code>hadoop-2.7.2-xiazai.tar.gz</code>，解压检查。</p>
<h1 id="HDFS的数据流※"><a href="#HDFS的数据流※" class="headerlink" title="HDFS的数据流※"></a>HDFS的数据流※</h1><h2 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h2><h3 id="剖析文件写入"><a href="#剖析文件写入" class="headerlink" title="剖析文件写入"></a>剖析文件写入</h3><p>HDFS写数据流程</p>
<p>现有集群如下：</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-1.png" class="" title="HDFS写数据流程-1">
<p>有客户端：</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-2.png" class="" title="HDFS写数据流程-2">
<p>要将200M的<code>ss.avi</code>文件通过客户端上传到集群。</p>
<ul>
<li>1）客户端通过<code>Distributed FileSystem</code>模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-3.png" class="" title="HDFS写数据流程-3">
<ul>
<li>2）NameNode返回是否可以上传。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-4.png" class="" title="HDFS写数据流程-4">
<ul>
<li>3）客户端请求第一个 Block上传到哪几个DataNode服务器上。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-5.png" class="" title="HDFS写数据流程-5">
<ul>
<li>4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-6.png" class="" title="HDFS写数据流程-6">
<blockquote>
<p>根据节点距离、负载决定节点。（谁离客户端越近，就优先选谁。）</p>
</blockquote>
<ul>
<li>5）客户端通过<code>FSDataOutputStream</code>模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-7.png" class="" title="HDFS写数据流程-7">
<ul>
<li>6）dn1、dn2、dn3逐级应答客户端。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-8.png" class="" title="HDFS写数据流程-8">
<ul>
<li>7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以<code>Packet</code>为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-9.png" class="" title="HDFS写数据流程-9">
<ul>
<li>8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-10.png" class="" title="HDFS写数据流程-10">
<h3 id="网络拓扑-节点距离计算"><a href="#网络拓扑-节点距离计算" class="headerlink" title="网络拓扑-节点距离计算"></a>网络拓扑-节点距离计算</h3><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？<br>例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。</p>
<p><strong>节点距离</strong>：两个节点到达最近的共同祖先的距离总和。</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97-1.png" class="" title="网络拓扑-节点距离计算-1">
<ul>
<li>Distance(/d1/r1/n0, /d1/r1/n0)=0（同一节点上的进程）</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97-2.png" class="" title="网络拓扑-节点距离计算-2">
<p>共同祖先是自己，0</p>
<ul>
<li>Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点）</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97-3.png" class="" title="网络拓扑-节点距离计算-3">
<p>共同祖先是r1，n-1向上有1到r1，n-2向上有1到r2；1+1=2</p>
<ul>
<li>Distance(/d1/r2/n0, /d1/r3/n2)=4（同一数据中心不同机架上的节点）</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97-4.png" class="" title="网络拓扑-节点距离计算-4">
<p>共同祖先是d1，n-0向上有2，n-2向上有2；2+2=4</p>
<ul>
<li>Distance(/d1/r2/n1, /d2/r4/n1)=6（不同数据中心的节点）</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97-5.png" class="" title="网络拓扑-节点距离计算-5">
<p>共同祖先是数据中心，n-1向上有3到数据中心，n-1向上有3到数据中心；3+3=6</p>
<ul>
<li>抽象图</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97-6.png" class="" title="网络拓扑-节点距离计算-6">
<p>上图显示9和5之间的距离可以用线段数来表示，9到8为1，5到8为2；1+2=3</p>
<h3 id="机架感知（副本存储节点选择）"><a href="#机架感知（副本存储节点选择）" class="headerlink" title="机架感知（副本存储节点选择）"></a>机架感知（副本存储节点选择）</h3><ul>
<li>官方ip地址</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication">hadoop-机架</a></p>
<ul>
<li>Hadoop2.7.2副本节点选择</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5-1.png" class="" title="机架感知-1">
<p>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。<br>第二个副本和第一个副本位于相同机架，随机节选。<br>第三个副本位于不同机架，随机节点。</p>
<h2 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h2><p>现有集群如下：</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-1.png" class="" title="HDFS读数据流程-1">
<p>有客户端：</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-2.png" class="" title="HDFS读数据流程-2">
<ul>
<li>1）客户端通过<code>Distributed FileSystem</code>向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-3.png" class="" title="HDFS读数据流程-3">
<ul>
<li>2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-4.png" class="" title="HDFS读数据流程-4">
<ul>
<li>3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-5.png" class="" title="HDFS读数据流程-5">
<ul>
<li>4）客户端以<code>Packet</code>为单位接收，先在本地缓存，然后写入目标文件。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-6.png" class="" title="HDFS读数据流程-6">
<h1 id="NameNode和SecondaryNameNode※"><a href="#NameNode和SecondaryNameNode※" class="headerlink" title="NameNode和SecondaryNameNode※"></a>NameNode和SecondaryNameNode※</h1><h2 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h2><ul>
<li>思考：NameNode中的元数据是存储在哪里的？</li>
</ul>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的<code>FsImage</code>。(可靠)</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入<code>Edits</code>文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。(效率)</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。(FsImage+Edits=集群)</p>
<h2 id="NN和2NN工作原理图"><a href="#NN和2NN工作原理图" class="headerlink" title="NN和2NN工作原理图"></a>NN和2NN工作原理图</h2><p>现有集群和客户端：</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-1.png" class="" title="NN和2NN工作原理图-1">
<h3 id="第一阶段：NameNode启动"><a href="#第一阶段：NameNode启动" class="headerlink" title="第一阶段：NameNode启动"></a>第一阶段：NameNode启动</h3><ul>
<li>（1）第一次启动NameNode格式化后，创建<code>Fsimage</code>和<code>Edits</code>文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-2.png" class="" title="NN和2NN工作原理图-2">
<ul>
<li>（2）客户端对元数据进行增删改的请求。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-3.png" class="" title="NN和2NN工作原理图-3">
<ul>
<li>（3）NameNode记录操作日志，更新滚动日志。</li>
</ul>
<blockquote>
<p>先记录操作日志然后更新内存数据。</p>
</blockquote>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-4.png" class="" title="NN和2NN工作原理图-4">
<ul>
<li>（4）NameNode在内存中对数据进行增删改。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-5.png" class="" title="NN和2NN工作原理图-5">
<h3 id="第二阶段：Secondary-NameNode工作"><a href="#第二阶段：Secondary-NameNode工作" class="headerlink" title="第二阶段：Secondary NameNode工作"></a>第二阶段：Secondary NameNode工作</h3><img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-6.png" class="" title="NN和2NN工作原理图-6">
<ul>
<li>（1）Secondary NameNode询问NameNode是否需要<code>CheckPoint</code>。直接带回NameNode是否检查结果。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-7.png" class="" title="NN和2NN工作原理图-7">
<blockquote>
<p>定时1小时 || Edits数据达到100万条 ==&gt; 执行合并操作</p>
</blockquote>
<ul>
<li><p>（2）Secondary NameNode请求执行CheckPoint。</p>
</li>
<li><p>（3）NameNode滚动正在写的Edits日志。</p>
</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-8.png" class="" title="NN和2NN工作原理图-8">
<blockquote>
<p>回滚原来edits保存，然后生成空的edits写入</p>
</blockquote>
<ul>
<li>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-9.png" class="" title="NN和2NN工作原理图-9">
<ul>
<li>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-10.png" class="" title="NN和2NN工作原理图-10">
<ul>
<li>（6）生成新的镜像文件<code>fsimage.chkpoint</code>。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-11.png" class="" title="NN和2NN工作原理图-11">
<ul>
<li>（7）拷贝fsimage.chkpoint到NameNode。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-12.png" class="" title="NN和2NN工作原理图-12">
<ul>
<li>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</li>
</ul>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE-13.png" class="" title="NN和2NN工作原理图-13">
<h3 id="NN和2NN工作机制详解："><a href="#NN和2NN工作机制详解：" class="headerlink" title="NN和2NN工作机制详解："></a>NN和2NN工作机制详解：</h3><p><code>Fsimage</code>：NameNode内存中元数据序列化后形成的文件。<br><code>Edits</code>：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</p>
<p>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</p>
<p>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p>
<h2 id="Fsimage和Edits解析"><a href="#Fsimage和Edits解析" class="headerlink" title="Fsimage和Edits解析"></a>Fsimage和Edits解析</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>NameNode被格式化之后，将在<code>/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current</code>目录产生如下文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fsimage_0000000000000000000.md5</span><br><span class="line">fsimage_0000000000000000000.md5</span><br><span class="line">seen_txid</span><br><span class="line">VERSION</span><br></pre></td></tr></table></figure>
<p>(1) Fsimage文件: HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。</p>
<p>(2) Edits文件:存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。</p>
<p>(3) seen<em>txid 文件保存的是一个数字，就是最后一个edits</em> 的数字。</p>
<p>(4)每次NameNode 启动的时候都会将Fsimage文件读入内存，加载Edits里 面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode 启动的时候就将Fsimage和Edits文件进行了合并。</p>
<blockquote>
<p>将三个集群服务停止然后删除data和logs文件夹。格式化namenode。然后启动hdfs，出现新edit_inprogress_000000000000001。创建文件夹然后上传文件，</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current</span><br><span class="line">[atguigu@hadoop102 current]$ ll</span><br><span class="line">总用量 1052</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      42 8月   4 15:44 edits_0000000000000000001-0000000000000000002</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1048576 8月   4 15:45 edits_inprogress_0000000000000000003</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     354 8月   4 15:42 fsimage_0000000000000000000</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 8月   4 15:42 fsimage_0000000000000000000.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     354 8月   4 15:44 fsimage_0000000000000000002</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 8月   4 15:44 fsimage_0000000000000000002.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu       2 8月   4 15:44 seen_txid</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     208 8月   4 15:42 VERSION</span><br></pre></td></tr></table></figure>
<blockquote>
<p>直接使用cat查看出现乱码，因为是序列化文件。</p>
</blockquote>
<h3 id="oiv查看Fsimage文件"><a href="#oiv查看Fsimage文件" class="headerlink" title="oiv查看Fsimage文件"></a>oiv查看Fsimage文件</h3><h4 id="查看oiv和oev命令"><a href="#查看oiv和oev命令" class="headerlink" title="查看oiv和oev命令"></a>查看oiv和oev命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ hdfs</span><br><span class="line">oiv		apply the offline fsimage viewer to an fsimage</span><br><span class="line">oev		apply the offline edits viewer to an edits file</span><br></pre></td></tr></table></figure>
<h4 id="基本语法-1"><a href="#基本语法-1" class="headerlink" title="基本语法"></a>基本语法</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径</span><br></pre></td></tr></table></figure>
<h4 id="实操案例"><a href="#实操案例" class="headerlink" title="实操案例"></a>实操案例</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000000 -o fsimage-1.xml</span><br><span class="line">[atguigu@hadoop102 current]$ <span class="built_in">cat</span> fsimage-1.xml </span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">fsimage</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">NameSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV1</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">genstampV1</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV2</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">genstampV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV1Limit</span>&gt;</span>0<span class="tag">&lt;/<span class="name">genstampV1Limit</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">lastAllocatedBlockId</span>&gt;</span>1073741824<span class="tag">&lt;/<span class="name">lastAllocatedBlockId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">txid</span>&gt;</span>0<span class="tag">&lt;/<span class="name">txid</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">NameSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">lastInodeId</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">lastInodeId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span><span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>0<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">permission</span>&gt;</span>atguigu:supergroup:rwxr-xr-x<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>9223372036854775807<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeReferenceSection</span>&gt;</span><span class="tag">&lt;/<span class="name">INodeReferenceSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">SnapshotSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshotCounter</span>&gt;</span>0<span class="tag">&lt;/<span class="name">snapshotCounter</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">SnapshotSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeDirectorySection</span>&gt;</span><span class="tag">&lt;/<span class="name">INodeDirectorySection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">FileUnderConstructionSection</span>&gt;</span><span class="tag">&lt;/<span class="name">FileUnderConstructionSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">SnapshotDiffSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">diff</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">inodeid</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">inodeid</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">diff</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">SnapshotDiffSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">SecretManagerSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">currentId</span>&gt;</span>0<span class="tag">&lt;/<span class="name">currentId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tokenSequenceNumber</span>&gt;</span>0<span class="tag">&lt;/<span class="name">tokenSequenceNumber</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">SecretManagerSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">CacheManagerSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">nextDirectiveId</span>&gt;</span>1<span class="tag">&lt;/<span class="name">nextDirectiveId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">CacheManagerSection</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">fsimage</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>此时只有一个目录的路径，没有刚才上传的文件状态。说明文件现在在edits日志文件中。</p>
<blockquote>
<p>可以看出，Fsimage中没有记录块所对应DataNode。<br>因为在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p>
</blockquote>
<h3 id="oev查看Edits文件"><a href="#oev查看Edits文件" class="headerlink" title="oev查看Edits文件"></a>oev查看Edits文件</h3><h4 id="基本语法-2"><a href="#基本语法-2" class="headerlink" title="基本语法"></a>基本语法</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oev -p 文件类型 -i 编辑日记 -o 转换后后文件输出路径</span><br></pre></td></tr></table></figure>
<h4 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h4><p>接上个案例</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ hdfs oev -p XML -i edits_inprogress_0000000000000000003 -o edits-1.xml</span><br><span class="line">[atguigu@hadoop102 current]$ <span class="built_in">cat</span> edits-1.xml </span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-63<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>3<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_MKDIR<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>4<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/user<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1596527114560<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>493<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_MKDIR<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>5<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16387<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/user/atguigu<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1596527114570<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>493<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_MKDIR<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>6<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16388<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/user/atguigu/input<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1596527114571<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>493<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>7<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16389<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/user/atguigu/input/README.txt._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>3<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1596527135651<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1596527135651<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span>DFSClient_NONMAPREDUCE_-99231412_1<span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span>192.168.228.102<span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>true<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>f3c4b912-3a97-4d18-af16-e47a4ad530eb<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>3<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ALLOCATE_BLOCK_ID<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>8<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741825<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_SET_GENSTAMP_V2<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>9<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMPV2</span>&gt;</span>1001<span class="tag">&lt;/<span class="name">GENSTAMPV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD_BLOCK<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>10<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/user/atguigu/input/README.txt._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741825<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>0<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1001<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span><span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>-2<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_CLOSE<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>11<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/user/atguigu/input/README.txt._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>3<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1596527136685<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1596527135651<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span><span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span><span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>false<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741825<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>1366<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1001<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_RENAME_OLD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>12<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">SRC</span>&gt;</span>/user/atguigu/input/README.txt._COPYING_<span class="tag">&lt;/<span class="name">SRC</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">DST</span>&gt;</span>/user/atguigu/input/README.txt<span class="tag">&lt;/<span class="name">DST</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1596527136694<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>f3c4b912-3a97-4d18-af16-e47a4ad530eb<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>9<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">EDITS</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>可以看到创建文件夹和上传文件命令。<br>可以看到多级目录是分步骤的。<br>上传一个文件：<code>OP_ADD</code>,<code>OP_ALLOCATE_BLOCK_ID</code>,<code>OP_SET_GENSTAMP_V2</code>,<code>OP_ADD_BLOCK</code>,<code>OP_CLOSE</code>,<code>OP_RENAME_OLD</code>。</p>
<blockquote>
<p>NameNode如何确定下次开机启动的时候合并哪些Edits？<br><code>seen_txid</code>记录最新的edits。</p>
</blockquote>
<h2 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h2><ul>
<li>（1）通常情况下，SecondaryNameNode每隔一小时执行一次。</li>
</ul>
<p><code>hdfs-default.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>（2）一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure>
<h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p>NameNode故障后，可以采用如下两种方法恢复数据。</p>
<h3 id="方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；"><a href="#方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；" class="headerlink" title="方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；"></a>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</h3><ul>
<li><ol>
<li>kill -9 NameNode进程</li>
</ol>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 sbin]$ jps</span><br><span class="line">3745 NameNode</span><br><span class="line">3893 DataNode</span><br><span class="line">4359 Jps</span><br><span class="line">4233 NodeManager</span><br><span class="line">[atguigu@hadoop102 sbin]$ <span class="built_in">kill</span> -9 3745</span><br><span class="line">[atguigu@hadoop102 sbin]$ jps</span><br><span class="line">4371 Jps</span><br><span class="line">3893 DataNode</span><br><span class="line">4233 NodeManager</span><br></pre></td></tr></table></figure>
<ul>
<li><ol>
<li>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</li>
</ol>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure>
<ul>
<li><ol>
<li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li>
</ol>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 name]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs/name</span><br><span class="line">[atguigu@hadoop102 name]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./</span><br></pre></td></tr></table></figure>
<ul>
<li><ol>
<li>重新启动NameNode</li>
</ol>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<h3 id="方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。"><a href="#方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。" class="headerlink" title="方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。"></a>方法二：使用<code>-importCheckpoint</code>选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</h3><ul>
<li>1.修改<code>hdfs-site.xml</code>中的</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ vim etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure>
<p>减少存盘检查时间</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ xsync etc/hadoop/</span><br></pre></td></tr></table></figure>
<ul>
<li>2.kill -9 NameNode进程</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ jps</span><br><span class="line">6962 Jps</span><br><span class="line">3893 DataNode</span><br><span class="line">4424 NameNode</span><br><span class="line">4233 NodeManager</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">kill</span> -9 3893</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ jps</span><br><span class="line">4424 NameNode</span><br><span class="line">4233 NodeManager</span><br><span class="line">6974 Jps</span><br></pre></td></tr></table></figure>
<ul>
<li>3.删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure>
<ul>
<li>4.如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除<code>in_use.lock</code>文件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 namesecondary]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary</span><br><span class="line">[atguigu@hadoop102 namesecondary]$ <span class="built_in">rm</span> -rf in_use.lock</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 dfs]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line">[atguigu@hadoop102 dfs]$ <span class="built_in">ls</span></span><br><span class="line">data  name  namesecondary</span><br></pre></td></tr></table></figure>
<ul>
<li>5.导入检查点数据（等待一会ctrl+c结束掉）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure>
<ul>
<li>6.启动NameNode</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<h2 id="集群安全模式"><a href="#集群安全模式" class="headerlink" title="集群安全模式"></a>集群安全模式</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>1、NameNode启动</p>
<p>NameNode启动时，首先将镜像文件(Fsimage) 载入内存，并执行编辑日志(Edits) 中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。<br>这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的。</p>
<p>2、DataNode启动</p>
<p>系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。<br>在系统的正常操作期间，NameNode 会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。</p>
<p>3、安全模式退出判断</p>
<p>如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。<br>所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别(默认值: dfs replication.min=1)。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。</p>
<h3 id="基本语法-3"><a href="#基本语法-3" class="headerlink" title="基本语法"></a>基本语法</h3><p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p>
<p>（1）<code>bin/hdfs dfsadmin -safemode get</code> （功能描述：查看安全模式状态）<br>（2）<code>bin/hdfs dfsadmin -safemode enter</code>  （功能描述：进入安全模式状态）<br>（3）<code>bin/hdfs dfsadmin -safemode leave</code> （功能描述：离开安全模式状态）<br>（4）<code>bin/hdfs dfsadmin -safemode wait</code> （功能描述：等待安全模式状态）</p>
<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p>模拟等待安全模式</p>
<p>退出安全模式之后立马干一些事情。</p>
<p>可在<code>http://hadoop102:50070/dfshealth.html#tab-overview</code>浏览器端查看Safemode状态。</p>
<p>（1）查看当前模式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure>
<p>（2）先进入安全模式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter</span><br><span class="line">Safe mode is ON</span><br></pre></td></tr></table></figure>
<p>安全模式下不允许上传</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -put README.txt /</span><br><span class="line">put: Cannot create file/README.txt._COPYING_. Name node is <span class="keyword">in</span> safe mode.</span><br></pre></td></tr></table></figure>
<p>（3）创建并执行下面的脚本</p>
<p>在/opt/module/hadoop-2.7.2路径上，编辑一个脚本safemode.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">touch</span> safemode.sh</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ vim safemode.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode wait</span><br><span class="line">hdfs dfs -put /opt/module/hadoop-2.7.2/README.txt /</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">chmod</span> 777 safemode.sh</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ ./safemode.sh</span><br></pre></td></tr></table></figure>
<p>（4）再打开一个窗口，执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure>
<h2 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h2><p>先停止节点服务并且删除data和logs</p>
<blockquote>
<p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。</p>
</blockquote>
<h3 id="具体配置"><a href="#具体配置" class="headerlink" title="具体配置"></a>具体配置</h3><p>（1）在<code>hdfs-site.xml</code>文件中增加如下内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ vim etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ xsync etc/hadoop/</span><br></pre></td></tr></table></figure>
<p>（2）停止集群，删除data和logs中所有数据。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data/ logs/</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data/ logs/</span><br><span class="line">[atguigu@hadoop104 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data/ logs/</span><br></pre></td></tr></table></figure>
<p>（3）格式化集群并启动。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>（4）查看结果</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line">[atguigu@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 atguigu atguigu 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure>
<p>（5）上传测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -put README.txt /</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 current]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs/name2/current</span><br><span class="line">[atguigu@hadoop102 current]$ ll</span><br><span class="line">总用量 1052</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      42 8月   8 22:07 edits_0000000000000000001-0000000000000000002</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1048576 8月   8 22:09 edits_inprogress_0000000000000000003</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     354 8月   8 22:05 fsimage_0000000000000000000</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 8月   8 22:05 fsimage_0000000000000000000.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     354 8月   8 22:07 fsimage_0000000000000000002</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 8月   8 22:07 fsimage_0000000000000000002.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu       2 8月   8 22:07 seen_txid</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     206 8月   8 22:05 VERSION</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 current]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs/name1/current</span><br><span class="line">[atguigu@hadoop102 current]$ ll</span><br><span class="line">总用量 1052</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      42 8月   8 22:07 edits_0000000000000000001-0000000000000000002</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1048576 8月   8 22:09 edits_inprogress_0000000000000000003</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     354 8月   8 22:05 fsimage_0000000000000000000</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 8月   8 22:05 fsimage_0000000000000000000.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     354 8月   8 22:07 fsimage_0000000000000000002</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 8月   8 22:07 fsimage_0000000000000000002.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu       2 8月   8 22:07 seen_txid</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     206 8月   8 22:05 VERSION</span><br></pre></td></tr></table></figure>
<h1 id="DataNode※"><a href="#DataNode※" class="headerlink" title="DataNode※"></a>DataNode※</h1><h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6-1.png" class="" title="DataNode工作机制-1">
<p>1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6-2.png" class="" title="DataNode工作机制-2">
<p>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6-3.png" class="" title="DataNode工作机制-3">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6-4.png" class="" title="DataNode工作机制-4">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6-5.png" class="" title="DataNode工作机制-5">
<p>3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。（只有重启之后才可以工作）</p>
<p>4）集群运行中可以安全加入和退出一些机器。</p>
<h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><blockquote>
<p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？</p>
</blockquote>
<p>奇偶校验验证数据完整性：<br><img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7-1.png" class="" title="DataNode数据完整性-1"><br>如果出现多点故障，一般不考虑，只考虑单点故障。</p>
<p>crc校验：<br><img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7-2.png" class="" title="DataNode数据完整性-2"></p>
<p>如下是DataNode节点保证数据完整性的方法。</p>
<p>1）当DataNode读取Block的时候，它会计算CheckSum。</p>
<p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</p>
<p>3）Client读取其他DataNode上的Block。</p>
<p>4）DataNode在其文件创建后周期验证CheckSum。</p>
<h2 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h2><img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE-1.png" class="" title="DataNode掉线时限参数设置-1">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE-2.png" class="" title="DataNode掉线时限参数设置-2">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE-3.png" class="" title="DataNode掉线时限参数设置-3">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/DataNode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE-4.png" class="" title="DataNode掉线时限参数设置-4">
<p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="服役新数据节点"><a href="#服役新数据节点" class="headerlink" title="服役新数据节点"></a>服役新数据节点</h2><h3 id="需求-3"><a href="#需求-3" class="headerlink" title="需求"></a>需求</h3><p>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。</p>
<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>（1）在hadoop104主机上再克隆一台hadoop105主机<br>（2）修改IP地址和主机名称<br>（3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）</p>
<blockquote>
<p>web浏览器端可以看到datanodes列表。在没有删除原来数据前启动hadoop105会导致，104和105会相互变化，因为104和105的配置相同。</p>
</blockquote>
<p>（4）source一下配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</span><br><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager</span><br><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果想要下次启动加入该节点，需要配置集群群起步骤并设置ssh</p>
</blockquote>
<h3 id="服役新节点具体步骤"><a href="#服役新节点具体步骤" class="headerlink" title="服役新节点具体步骤"></a>服役新节点具体步骤</h3><p>（1）直接启动DataNode，即可关联到集群</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</span><br><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%9C%8D%E5%BD%B9%E6%96%B0%E8%8A%82%E7%82%B9%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4-1.png" class="" title="服役新节点具体步骤-1">
<p>（2）在hadoop105上上传文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ hadoop fs -<span class="built_in">mkdir</span> -p /user/atguigu/input</span><br><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ hadoop fs -put README.txt /user/atguigu/input</span><br><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ hadoop fs -put /opt/module/hadoop-2.7.2/LICENSE.txt /</span><br></pre></td></tr></table></figure>
<p>（3）如果数据不均衡，可以用命令实现集群的再平衡</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 sbin]$ ./start-balancer.sh</span><br><span class="line">starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out</span><br><span class="line">Time Stamp               Iteration<span class="comment">#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果第三方设置一台与节点相同配置的服务器，挂载后即可下载数据到本节点，存在风险漏洞。</p>
</blockquote>
<h2 id="退役旧数据节点"><a href="#退役旧数据节点" class="headerlink" title="退役旧数据节点"></a>退役旧数据节点</h2><h3 id="添加白名单-较安全"><a href="#添加白名单-较安全" class="headerlink" title="添加白名单(较安全)"></a>添加白名单(较安全)</h3><p>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。</p>
<p>配置白名单的具体步骤如下：</p>
<p>（1）在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件(文件名称可以不固定)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[atguigu@hadoop102 hadoop]$ <span class="built_in">touch</span> dfs.hosts</span><br><span class="line">[atguigu@hadoop102 hadoop]$ vim dfs.hosts</span><br><span class="line">添加如下主机名称（不添加hadoop105）</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>
<p>（2）在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>（3）配置文件分发</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync hdfs-site.xml</span><br></pre></td></tr></table></figure>
<p>（4）刷新NameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure>
<p>（5）更新ResourceManager节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ yarn rmadmin -refreshNodes</span><br><span class="line">20/08/21 23:03:01 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.228.103:8033</span><br></pre></td></tr></table></figure>
<p>（6）在web浏览器上查看</p>
<p>hadoop105节点已经被干掉。</p>
<p>（7）如果数据不均衡，可以用命令实现集群的再平衡</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 sbin]$ ./start-balancer.sh</span><br><span class="line">starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out</span><br><span class="line">Time Stamp               Iteration<span class="comment">#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span></span><br></pre></td></tr></table></figure>
<h3 id="黑名单退役-标准程序"><a href="#黑名单退役-标准程序" class="headerlink" title="黑名单退役(标准程序)"></a>黑名单退役(标准程序)</h3><p>在黑名单上面的主机都会被强制退出。</p>
<blockquote>
<p>白不允许白名单和黑名单中同时出现同一个主机名称。<br>先恢复上一步操作的白名单操作，修改配置文件，同步。hadoop102刷新namenode，再启动hadoop105。</p>
</blockquote>
<ol>
<li>NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[atguigu@hadoop102 hadoop]$ <span class="built_in">touch</span> dfs.hosts.exclude</span><br><span class="line">[atguigu@hadoop102 hadoop]$ vim dfs.hosts.exclude</span><br><span class="line">添加如下主机名称（要退役的节点）</span><br><span class="line">hadoop105</span><br></pre></td></tr></table></figure>
<p>2．在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>3．刷新NameNode、刷新ResourceManager</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync hdfs-site.xml</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br><span class="line">[atguigu@hadoop102 hadoop]$ yarn rmadmin -refreshNodes</span><br><span class="line">20/08/21 23:41:12 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.228.103:8033</span><br></pre></td></tr></table></figure>
<ol>
<li><p>检查Web浏览器，退役节点的状态为<code>decommission in progress</code>（退役中），说明数据节点正在复制块到其他节点。</p>
</li>
<li><p>等待退役节点状态为<code>decommissioned</code>（所有块已经复制完成），停止该节点及节点资源管理器。</p>
</li>
</ol>
<blockquote>
<p>注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役。</p>
</blockquote>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E9%BB%91%E5%90%8D%E5%8D%95%E9%80%80%E5%BD%B9-1.png" class="" title="黑名单退役-1">
<blockquote>
<p>此时只是正常退役，但是节点列表还可以读取到。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode</span><br><span class="line">stopping datanode</span><br><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager</span><br><span class="line">stopping nodemanager</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关掉节点之后，开始进行last contact计时，重新启动节点之后不再显示该节点。</p>
</blockquote>
<ol>
<li>如果数据不均衡，可以用命令实现集群的再平衡</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh </span><br><span class="line">starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out</span><br><span class="line">Time Stamp               Iteration<span class="comment">#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span></span><br></pre></td></tr></table></figure>
<h2 id="Datanode多目录配置"><a href="#Datanode多目录配置" class="headerlink" title="Datanode多目录配置"></a>Datanode多目录配置</h2><ol>
<li>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</li>
</ol>
<p>2．具体配置如下</p>
<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>需要集群全部停止然后删除数据重新配置，格式化，启动服务。<br>上传文件测试后，分的只是路径，文件并没有实现备份，可以存在不同磁盘里边去。<br>NameNode的多目录是每个目录东西都一样，DataNode多目录是多文件多目录。</p>
</blockquote>
<h1 id="HDFS-2-X新特性"><a href="#HDFS-2-X新特性" class="headerlink" title="HDFS 2.X新特性"></a>HDFS 2.X新特性</h1><h2 id="集群间数据拷贝"><a href="#集群间数据拷贝" class="headerlink" title="集群间数据拷贝"></a>集群间数据拷贝</h2><p>1．scp实现两个远程主机之间的文件复制</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r hello.txt root@hadoop103:/user/atguigu/hello.txt    // 推 push</span><br><span class="line">scp -r root@hadoop103:/user/atguigu/hello.txt  hello.txt    // 拉 pull</span><br><span class="line">scp -r root@hadoop103:/user/atguigu/hello.txt root@hadoop104:/user/atguigu    //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。</span><br></pre></td></tr></table></figure>
<p>2．采用distcp命令实现两个Hadoop集群之间的递归数据复制</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$  bin/hadoop distcp</span><br><span class="line">hdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt</span><br></pre></td></tr></table></figure>
<h2 id="小文件存档"><a href="#小文件存档" class="headerlink" title="小文件存档"></a>小文件存档</h2><h3 id="HDFS存储小文件弊端"><a href="#HDFS存储小文件弊端" class="headerlink" title="HDFS存储小文件弊端"></a>HDFS存储小文件弊端</h3><p>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。</p>
<p>例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。</p>
<h3 id="解决存储小文件办法之一"><a href="#解决存储小文件办法之一" class="headerlink" title="解决存储小文件办法之一"></a>解决存储小文件办法之一</h3><p>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。</p>
<p>具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体， 减少了NameNode的内存。</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E5%B0%8F%E6%96%87%E4%BB%B6%E5%AD%98%E6%A1%A3-1.png" class="" title="小文件存档-1">
<h3 id="案例实操-1"><a href="#案例实操-1" class="headerlink" title="案例实操"></a>案例实操</h3><p>（1）需要启动YARN进程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>（2）归档文件</p>
<p>把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop archive -archiveName input.har -p /user/atguigu/input /user/atguigu/output</span><br></pre></td></tr></table></figure>
<p>（3）查看归档</p>
<blockquote>
<p>har协议使用har解析，否则查看是一个整体。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> -R /user/atguigu/output/input.har</span><br><span class="line">-rw-r--r--   3 atguigu supergroup          0 2020-08-22 17:26 /user/atguigu/output/input.har/_SUCCESS</span><br><span class="line">-rw-r--r--   5 atguigu supergroup        302 2020-08-22 17:26 /user/atguigu/output/input.har/_index</span><br><span class="line">-rw-r--r--   5 atguigu supergroup         23 2020-08-22 17:26 /user/atguigu/output/input.har/_masterindex</span><br><span class="line">-rw-r--r--   3 atguigu supergroup       1414 2020-08-22 17:26 /user/atguigu/output/input.har/part-0</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> -R har:///user/atguigu/output/input.har</span><br><span class="line">-rw-r--r--   3 atguigu supergroup       1366 2020-08-22 17:23 har:///user/atguigu/output/input.har/README.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         34 2020-08-22 17:23 har:///user/atguigu/output/input.har/kongming.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         14 2020-08-22 17:23 har:///user/atguigu/output/input.har/liubei.txt</span><br></pre></td></tr></table></figure>
<p>（4）解归档文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">cp</span> har:///user/atguigu/output/input.har/* /user/atguigu</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">ls</span> -R /user/atguigu</span><br><span class="line">-rw-r--r--   3 atguigu supergroup       1366 2020-08-22 17:34 /user/atguigu/README.txt</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2020-08-22 17:23 /user/atguigu/input</span><br><span class="line">-rw-r--r--   3 atguigu supergroup       1366 2020-08-22 17:23 /user/atguigu/input/README.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         34 2020-08-22 17:23 /user/atguigu/input/kongming.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         14 2020-08-22 17:23 /user/atguigu/input/liubei.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         34 2020-08-22 17:34 /user/atguigu/kongming.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         14 2020-08-22 17:34 /user/atguigu/liubei.txt</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2020-08-22 17:26 /user/atguigu/output</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2020-08-22 17:26 /user/atguigu/output/input.har</span><br><span class="line">-rw-r--r--   3 atguigu supergroup          0 2020-08-22 17:26 /user/atguigu/output/input.har/_SUCCESS</span><br><span class="line">-rw-r--r--   5 atguigu supergroup        302 2020-08-22 17:26 /user/atguigu/output/input.har/_index</span><br><span class="line">-rw-r--r--   5 atguigu supergroup         23 2020-08-22 17:26 /user/atguigu/output/input.har/_masterindex</span><br><span class="line">-rw-r--r--   3 atguigu supergroup       1414 2020-08-22 17:26 /user/atguigu/output/input.har/part-0</span><br></pre></td></tr></table></figure>
<h2 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h2><p>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。</p>
<h3 id="回收站参数设置"><a href="#回收站参数设置" class="headerlink" title="回收站参数设置"></a>回收站参数设置</h3><p>开启回收站功能参数说明</p>
<ol>
<li>默认值<code>fs.trash.interval=0</code>，0表示禁用回收站；其他值表示设置文件的存活时间。</li>
<li>默认值<code>fs.trash.checkpoint.interval=0</code>，检查回收站的时间间隔。如果该值为0，则该值设置和<code>fs.trash.interval</code>的参数值相等。</li>
<li>要求<code>fs.trash.checkpoint.interval&lt;=fs.trash.interval</code></li>
</ol>
<h3 id="回收站工作机制"><a href="#回收站工作机制" class="headerlink" title="回收站工作机制"></a>回收站工作机制</h3><img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E5%9B%9E%E6%94%B6%E7%AB%99%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" class="" title="回收站工作机制">
<h3 id="启用回收站"><a href="#启用回收站" class="headerlink" title="启用回收站"></a>启用回收站</h3><p>修改core-site.xml，配置垃圾回收时间为1分钟。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="查看回收站"><a href="#查看回收站" class="headerlink" title="查看回收站"></a>查看回收站</h3><p>回收站在集群中的路径：<code>/user/atguigu/.Trash/…</code></p>
<h3 id="修改访问垃圾回收站用户名称"><a href="#修改访问垃圾回收站用户名称" class="headerlink" title="修改访问垃圾回收站用户名称"></a>修改访问垃圾回收站用户名称</h3><p>进入垃圾回收站用户名称，默认是dr.who，修改core-site.xml为atguigu用户</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意同步分发文件。</p>
</blockquote>
<h3 id="通过程序删除的文件不会经过回收站，需要调用moveToTrash-才进入回收站"><a href="#通过程序删除的文件不会经过回收站，需要调用moveToTrash-才进入回收站" class="headerlink" title="通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站"></a>通过程序删除的文件不会经过回收站，需要调用<code>moveToTrash()</code>才进入回收站</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">rm</span> /liubei.txt</span><br><span class="line"></span><br><span class="line">20/08/22 19:30:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1 minutes, Emptier interval = 0 minutes.</span><br><span class="line">Moved: <span class="string">&#x27;hdfs://hadoop102:9000/liubei.txt&#x27;</span> to trash at: hdfs://hadoop102:9000/user/atguigu/.Trash/Current</span><br></pre></td></tr></table></figure>
<blockquote>
<p>执行删除操作后，文件移动至目标文件夹，一分钟后自动删除。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Trash</span> <span class="variable">trash</span> <span class="operator">=</span> New <span class="title function_">Trash</span><span class="params">(conf)</span>;</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure>
<h3 id="恢复回收站数据"><a href="#恢复回收站数据" class="headerlink" title="恢复回收站数据"></a>恢复回收站数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -<span class="built_in">mv</span></span><br><span class="line">/user/atguigu/.Trash/Current/user/atguigu/input    /user/atguigu/input</span><br></pre></td></tr></table></figure>
<h3 id="清空回收站"><a href="#清空回收站" class="headerlink" title="清空回收站"></a>清空回收站</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -expunge</span><br></pre></td></tr></table></figure>
<blockquote>
<p>按照时间戳生成包，时间到期自动删除。</p>
</blockquote>
<h2 id="快照管理"><a href="#快照管理" class="headerlink" title="快照管理"></a>快照管理</h2><p>快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。</p>
<h3 id="快照管理API"><a href="#快照管理API" class="headerlink" title="快照管理API"></a>快照管理API</h3><ul>
<li><code>hdfs dfsadmin -allowSnapshot 路径</code>，开启指定目录的快照功能。</li>
<li><code>hdfs dfsadmin -disallowSnapshot 路径</code>，禁用指定目录的快照功能，默认是禁用。</li>
<li><code>hdfs dfs -createSnapshot 路径</code>，对目录创建快照(默认使用时间戳命名)。</li>
<li><code>hdfs dfs -createSnapshot 路径 名称</code>，指定名称创建快照。</li>
<li><code>hdfs dfs -renameSnapshot 路径 旧名称 新名称</code>，重命名快照。</li>
<li><code>hdfs lsSnapshottableDir</code>，列出当前用户所有可快照目录。</li>
<li><code>hdfs snapshotDiff 路径1 路径2</code>，比较两个快照目录的不同之处。</li>
<li><code>hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt;</code>，删除快照。</li>
</ul>
<h3 id="案例实操-2"><a href="#案例实操-2" class="headerlink" title="案例实操"></a>案例实操</h3><p>（1）开启/禁用指定目录的快照功能</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/atguigu/input</span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/atguigu/input</span><br></pre></td></tr></table></figure>
<p>（2）对目录创建快照</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/input</span><br><span class="line"></span><br><span class="line">Created snapshot /user/atguigu/input/.snapshot/s20200822-195648.481</span><br><span class="line"> </span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/atguigu/input/.snapshot/</span><br></pre></td></tr></table></figure>
<blockquote>
<p>浏览器端访问看不到隐藏文件夹，需要手动输入目录地址才可以。</p>
</blockquote>
<p>（3）指定名称创建快照</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/input  miao170508</span><br></pre></td></tr></table></figure>
<p>（4）重命名快照</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/atguigu/input/  miao170508 atguigu170508</span><br></pre></td></tr></table></figure>
<p>（5）列出当前用户所有可快照目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir</span><br></pre></td></tr></table></figure>
<p>（6）比较两个快照目录的不同之处</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff</span><br><span class="line"> /user/atguigu/input/ . .snapshot/atguigu170508</span><br></pre></td></tr></table></figure>
<p>（7）恢复快照</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -<span class="built_in">cp</span></span><br><span class="line">/user/atguigu/input/.snapshot/s20170708-134303.027 /user</span><br></pre></td></tr></table></figure>
<h1 id="HDFS-HA高可用"><a href="#HDFS-HA高可用" class="headerlink" title="HDFS HA高可用"></a>HDFS HA高可用</h1><h2 id="HA概述"><a href="#HA概述" class="headerlink" title="HA概述"></a>HA概述</h2><p>1）所谓HA（High Available），即高可用（7*24小时不中断服务）。<br>2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。<br>3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。<br>4）NameNode主要在以下两个方面影响HDFS集群<br>    NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启<br>    NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用<br>    HDFS HA功能通过配置Active/Standby两个NameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。</p>
<h2 id="HDFS-HA工作机制"><a href="#HDFS-HA工作机制" class="headerlink" title="HDFS-HA工作机制"></a>HDFS-HA工作机制</h2><p>通过双NameNode消除单点故障</p>
<h3 id="HDFS-HA工作要点"><a href="#HDFS-HA工作要点" class="headerlink" title="HDFS-HA工作要点"></a>HDFS-HA工作要点</h3><ol>
<li>元数据管理方式需要改变</li>
</ol>
<p>内存中各自保存一份元数据；<br>Edits日志只有Active状态的NameNode节点可以做写操作；<br>两个NameNode都可以读取Edits；<br>共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；</p>
<ol>
<li>需要一个状态管理功能模块</li>
</ol>
<p>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在NameNode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。</p>
<ol>
<li>必须保证两个NameNode之间能够ssh无密码登录</li>
<li>隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务</li>
</ol>
<h3 id="HDFS-HA自动故障转移工作机制"><a href="#HDFS-HA自动故障转移工作机制" class="headerlink" title="HDFS-HA自动故障转移工作机制"></a>HDFS-HA自动故障转移工作机制</h3><p>前面学习了使用命令<code>hdfs haadmin -failover</code>手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。</p>
<p>自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。</p>
<p>ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能：</p>
<p>1）<code>故障检测</code>：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</p>
<p>2）<code>现役NameNode选择</code>：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。<br>ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：</p>
<p>1）<code>健康监测</code>：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</p>
<p>2）<code>ZooKeeper会话管理</code>：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</p>
<p>3）<code>基于ZooKeeper的选择</code>：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为Active状态。</p>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB-1.png" class="" title="故障转移-1">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB-2.png" class="" title="故障转移-2">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB-3.png" class="" title="故障转移-3">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB-4.png" class="" title="故障转移-4">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB-5.png" class="" title="故障转移-5">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB-6.png" class="" title="故障转移-6">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB-7.png" class="" title="故障转移-7">
<h2 id="HDFS-HA集群配置"><a href="#HDFS-HA集群配置" class="headerlink" title="HDFS-HA集群配置"></a>HDFS-HA集群配置</h2><h3 id="环境准备-1"><a href="#环境准备-1" class="headerlink" title="环境准备"></a>环境准备</h3><ol>
<li>修改IP</li>
<li>修改主机名及主机名和IP地址的映射</li>
<li>关闭防火墙</li>
<li>ssh免密登录</li>
<li>安装JDK，配置环境变量等</li>
</ol>
<h3 id="规划集群"><a href="#规划集群" class="headerlink" title="规划集群"></a>规划集群</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">hadoop102</th>
<th style="text-align:center">hadoop103</th>
<th style="text-align:center">hadoop104</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">NameNode</td>
<td style="text-align:center">NameNode</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">JournalNode</td>
<td style="text-align:center">JournalNode</td>
<td style="text-align:center">JournalNode</td>
</tr>
<tr>
<td style="text-align:center">DataNode</td>
<td style="text-align:center">DataNode</td>
<td style="text-align:center">DataNode</td>
</tr>
<tr>
<td style="text-align:center">ZK</td>
<td style="text-align:center">ZK</td>
<td style="text-align:center">ZK</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">ResourceManager</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">NodeManager</td>
<td style="text-align:center">NodeManager</td>
<td style="text-align:center">NodeManager</td>
</tr>
</tbody>
</table>
</div>
<h3 id="配置Zookeeper集群"><a href="#配置Zookeeper集群" class="headerlink" title="配置Zookeeper集群"></a>配置Zookeeper集群</h3><ol>
<li>集群规划</li>
</ol>
<p>在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。</p>
<ol>
<li>解压安装</li>
</ol>
<p>（1）解压Zookeeper安装包到/opt/module/目录下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>
<p>（2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p zkData</span><br></pre></td></tr></table></figure>
<p>（3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>
<ol>
<li>配置zoo.cfg文件</li>
</ol>
<p>（1）具体配置</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper-3.4.10/zkData</span><br></pre></td></tr></table></figure>
<p>增加如下配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#######################cluster##########################</span></span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br></pre></td></tr></table></figure></p>
<p>（2）配置参数解读</p>
<p>Server.A=B:C:D。<br>A是一个数字，表示这个是第几号服务器；<br>B是这个服务器的IP地址；<br>C是这个服务器与集群中的Leader服务器交换信息的端口；<br>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。<br>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p>
<ol>
<li>集群操作</li>
</ol>
<p>（1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">touch</span> myid</span><br></pre></td></tr></table></figure>
<p>添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码</p>
<p>（2）编辑myid文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim myid</span><br></pre></td></tr></table></figure>
<p>在文件中添加与server对应的编号：如2</p>
<p>（3）拷贝配置好的zookeeper到其他机器上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r zookeeper-3.4.10/ root@hadoop103.atguigu.com:/opt/app/</span><br><span class="line">scp -r zookeeper-3.4.10/ root@hadoop104.atguigu.com:/opt/app/</span><br></pre></td></tr></table></figure>
<p>并分别修改myid文件中内容为3、4</p>
<p>（4）分别启动zookeeper</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh start</span></span><br><span class="line">[root@hadoop103 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh start</span></span><br><span class="line">[root@hadoop104 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh start</span></span><br></pre></td></tr></table></figure>
<p>（5）查看状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh status</span></span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">[root@hadoop103 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh status</span></span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: leader</span><br><span class="line">[root@hadoop104 zookeeper-3.4.5]<span class="comment"># bin/zkServer.sh status</span></span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure>
<h3 id="配置HDFS-HA集群"><a href="#配置HDFS-HA集群" class="headerlink" title="配置HDFS-HA集群"></a>配置HDFS-HA集群</h3><ol>
<li>官方地址：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></li>
<li>在opt目录下创建一个ha文件夹</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> ha</span><br></pre></td></tr></table></figure>
<ol>
<li>将/opt/app/下的 hadoop-2.7.2拷贝到/opt/ha目录下</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> -r hadoop-2.7.2/ /opt/ha/</span><br></pre></td></tr></table></figure>
<ol>
<li>配置hadoop-env.sh</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>
<ol>
<li>配置core-site.xml</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/ha/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol>
<li>配置hdfs-site.xml</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 完全分布式集群名称 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 集群中NameNode节点都有哪些 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/atguigu/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 声明journalnode服务器存储目录--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/ha/hadoop-2.7.2/data/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 关闭权限检查--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol>
<li>拷贝配置好的hadoop环境到其他节点</li>
</ol>
<h3 id="启动HDFS-HA集群"><a href="#启动HDFS-HA集群" class="headerlink" title="启动HDFS-HA集群"></a>启动HDFS-HA集群</h3><ol>
<li>在各个JournalNode节点上，输入以下命令启动journalnode服务</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>
<ol>
<li>在[nn1]上，对其进行格式化，并启动</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<ol>
<li>在[nn2]上，同步nn1的元数据信息</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>
<ol>
<li>启动[nn2]</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<ol>
<li>查看web页面显示</li>
</ol>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E5%90%AF%E5%8A%A8HDFS-HA%E9%9B%86%E7%BE%A4-1.png" class="" title="启动HDFS-HA集群-1">
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E5%90%AF%E5%8A%A8HDFS-HA%E9%9B%86%E7%BE%A4-2.png" class="" title="启动HDFS-HA集群-2">
<ol>
<li>在[nn1]上，启动所有datanode</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemons.sh start datanode</span><br></pre></td></tr></table></figure>
<ol>
<li>将[nn1]切换为Active</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs haadmin -transitionToActive nn1</span><br></pre></td></tr></table></figure>
<ol>
<li>查看是否Active</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs haadmin -getServiceState nn1</span><br></pre></td></tr></table></figure>
<h3 id="配置HDFS-HA自动故障转移"><a href="#配置HDFS-HA自动故障转移" class="headerlink" title="配置HDFS-HA自动故障转移"></a>配置HDFS-HA自动故障转移</h3><ol>
<li>具体配置</li>
</ol>
<p>（1）在hdfs-site.xml中增加</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>（2）在core-site.xml文件中增加</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol>
<li>启动</li>
<li>（1）关闭所有HDFS服务：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>
<p>（2）启动Zookeeper集群：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zkServer.sh start</span><br></pre></td></tr></table></figure>
<p>（3）初始化HA在Zookeeper中状态：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure>
<p>（4）启动HDFS服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>（5）在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemin.sh start zkfc</span><br></pre></td></tr></table></figure>
<ol>
<li>验证<br>（1）将Active NameNode进程kill</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 namenode的进程<span class="built_in">id</span></span><br></pre></td></tr></table></figure>
<p>（2）将Active NameNode机器断开网络</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service network stop</span><br></pre></td></tr></table></figure>
<h2 id="YARN-HA配置"><a href="#YARN-HA配置" class="headerlink" title="YARN-HA配置"></a>YARN-HA配置</h2><h3 id="YARN-HA工作机制"><a href="#YARN-HA工作机制" class="headerlink" title="YARN-HA工作机制"></a>YARN-HA工作机制</h3><ol>
<li>官方文档：</li>
</ol>
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html</a></p>
<ol>
<li>YARN-HA工作机制</li>
</ol>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/YARN-HA%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" class="" title="YARN-HA工作机制">
<h3 id="配置YARN-HA集群"><a href="#配置YARN-HA集群" class="headerlink" title="配置YARN-HA集群"></a>配置YARN-HA集群</h3><ol>
<li>环境准备</li>
</ol>
<p>（1）修改IP<br>（2）修改主机名及主机名和IP地址的映射<br>（3）关闭防火墙<br>（4）ssh免密登录<br>（5）安装JDK，配置环境变量等<br>（6）配置Zookeeper集群</p>
<ol>
<li>规划集群</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">hadoop102</th>
<th style="text-align:center">hadoop103</th>
<th style="text-align:center">hadoop104</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">NameNode</td>
<td style="text-align:center">NameNode</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">JournalNode</td>
<td style="text-align:center">JournalNode</td>
<td style="text-align:center">JournalNode</td>
</tr>
<tr>
<td style="text-align:center">DataNode</td>
<td style="text-align:center">DataNode</td>
<td style="text-align:center">DataNode</td>
</tr>
<tr>
<td style="text-align:center">ZK</td>
<td style="text-align:center">ZK</td>
<td style="text-align:center">ZK</td>
</tr>
<tr>
<td style="text-align:center">ResourceManager</td>
<td style="text-align:center">ResourceManager</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">NodeManager</td>
<td style="text-align:center">NodeManager</td>
<td style="text-align:center">NodeManager</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>具体配置</li>
</ol>
<p>（1）yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--启用resourcemanager ha--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!--声明两台resourcemanager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-yarn1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!--指定zookeeper集群的地址--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--启用自动恢复--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span>     <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>（2）同步更新其他节点的配置信息</p>
<ol>
<li>启动hdfs </li>
</ol>
<p>（1）在各个JournalNode节点上，输入以下命令启动journalnode服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>
<p>（2）在[nn1]上，对其进行格式化，并启动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<p>（3）在[nn2]上，同步nn1的元数据信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>
<p>（4）启动[nn2]：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<p>（5）启动所有DataNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemons.sh start datanode</span><br></pre></td></tr></table></figure>
<p>（6）将[nn1]切换为Active</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs haadmin -transitionToActive nn1</span><br></pre></td></tr></table></figure>
<ol>
<li>启动YARN </li>
</ol>
<p>（1）在hadoop102中执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>（2）在hadoop103中执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>
<p>（3）查看服务状态。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn rmadmin -getServiceState rm1</span><br></pre></td></tr></table></figure>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/%E5%90%AF%E5%8A%A8yarn.png" class="" title="启动yarn">
<h2 id="HDFS-Federation架构设计"><a href="#HDFS-Federation架构设计" class="headerlink" title="HDFS Federation架构设计"></a>HDFS Federation架构设计</h2><ol>
<li>NameNode架构的局限性</li>
</ol>
<p>（1）Namespace（命名空间）的限制</p>
<p>由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB。</p>
<p>（2）隔离问题</p>
<p>由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。</p>
<p>（3）性能的瓶颈</p>
<p>由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量。</p>
<ol>
<li>HDFS Federation架构设计</li>
</ol>
<p>能不能有多个NameNode</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">NameNode</th>
<th style="text-align:center">NameNode</th>
<th style="text-align:center">NameNode</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">元数据</td>
<td style="text-align:center">元数据</td>
<td style="text-align:center">元数据</td>
</tr>
<tr>
<td style="text-align:center">Log</td>
<td style="text-align:center">machine</td>
<td style="text-align:center">电商数据/话单数据</td>
</tr>
</tbody>
</table>
</div>
<img src="/2020/07/27/Hadoop-HDFS%E8%AF%A6%E8%A7%A3/HDFS-Federation%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1.png" class="" title="HDFS-Federation架构设计">
<ol>
<li>HDFS Federation应用思考</li>
</ol>
<p>不同应用可以使用不同NameNode进行数据管理：图片业务、爬虫业务、日志审计业务</p>
<p>Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2020/07/27/Hadoop-HDFS详解/">http://hibiscidai.com/2020/07/27/Hadoop-HDFS详解/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/08/21/%E6%B7%B1%E6%BE%9C%E8%AE%A4%E8%AF%81%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B0%E8%B7%AF%E7%94%B1%E5%99%A8%E8%AE%A4%E8%AF%81/"><i class="fa fa-chevron-left">  </i><span>校园网深澜客户端实现路由器认证</span></a></div><div class="next-post pull-right"><a href="/2020/07/26/Hadoop%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"><span>Hadoop源码编译</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://www.paofu.cloud/auth/register?code=j4I7">好用、实惠、稳定的梯子,点击这里<img src="https://pic.imgdb.cn/item/65572abac458853aefef30cd.png" width="1000" height="124" object-fit="cover" ></a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2024 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>