<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="CGAN-TensorFlow"><meta name="keywords" content="GAN,人工智能,深度学习"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>CGAN-TensorFlow | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CGAN-TensorFlow"><span class="toc-number">1.</span> <span class="toc-text">CGAN-TensorFlow</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CGAN-Formulation-and-Architecture"><span class="toc-number">2.</span> <span class="toc-text">CGAN: Formulation and Architecture</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CGAN-Implementation-in-TensorFlow"><span class="toc-number">3.</span> <span class="toc-text">CGAN: Implementation in TensorFlow</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CGAN-Results"><span class="toc-number">4.</span> <span class="toc-text">CGAN: Results</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">240</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">87</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">33</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">CGAN-TensorFlow</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2024-07-10</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">4.6k</span><span class="post-meta__separator">|</span><span>阅读时长: 23 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2024/07/10/CGAN-TensorFlow/CGAN-TensorFlow.png" class="" title="CGAN-TensorFlow">
<p>CGAN-TensorFlow</p>
<span id="more"></span>
<p>[TOC]</p>
<h1 id="CGAN-TensorFlow"><a href="#CGAN-TensorFlow" class="headerlink" title="CGAN-TensorFlow"></a>CGAN-TensorFlow</h1><p>原文链接：<a target="_blank" rel="noopener" href="https://agustinus.kristia.de/techblog/2016/12/24/conditional-gan-tensorflow/">Conditional Generative Adversarial Nets in TensorFlow</a></p>
<p>We have seen the Generative Adversarial Nets (GAN) model in the previous post. We have also seen the arch nemesis of GAN, the VAE and its conditional variation: Conditional VAE (CVAE). Hence, it is only proper for us to study conditional variation of GAN, called Conditional GAN or CGAN for short.<br>我们在上一篇文章中看到了生成对抗网络（GAN）模型。我们还看到了 GAN 的死对头，VAE 及其条件变体：条件 VAE（CVAE）。因此，我们研究 GAN 的条件变体（称为条件 GAN 或简称 CGAN）是再合适不过的了。</p>
<h1 id="CGAN-Formulation-and-Architecture"><a href="#CGAN-Formulation-and-Architecture" class="headerlink" title="CGAN: Formulation and Architecture"></a>CGAN: Formulation and Architecture</h1><p>Recall, in GAN, we have two neural nets: the generator <script type="math/tex">G(z)</script>  and the discriminator <script type="math/tex">D(X)</script>.<br>Now, as we want to condition those networks with some vector <script type="math/tex">y</script>, the easiest way to do it is to feed <script type="math/tex">y</script> into both networks. Hence, our generator and discriminator are now <script type="math/tex">G(z,y)</script>  and  <script type="math/tex">D(X,y)</script> respectively.<br>回想一下，在 GAN 中，我们有两个神经网络：生成器 <script type="math/tex">G(z)</script> 和鉴别器 <script type="math/tex">D(X)</script>。<br>现在，由于我们想用某个向量 <script type="math/tex">y</script> 来调节这些网络，最简单的方法是将 <script type="math/tex">y</script> 输入两个网络。因此，我们的生成器和鉴别器现在分别是 <script type="math/tex">G(z,y)</script> 和 <script type="math/tex">D(X,y)</script>。</p>
<p>We can see it with a probabilistic point of view. <script type="math/tex">G(z,y)</script> is modeling the distribution of our data, given  <script type="math/tex">z</script> and  <script type="math/tex">y</script>, that is, our data is generated with this scheme <script type="math/tex">X~G(X|z，y)</script>.<br>我们可以用概率的角度来看待它。给定<script type="math/tex">z</script>和<script type="math/tex">y</script>，<script type="math/tex">G(z,y)</script>正在对我们的数据分布进行建模，也就是说，我们的数据是用这个方案<script type="math/tex">X \sim G(X|z，y)</script>生成的。</p>
<p>Likewise for the discriminator, now it tries to find discriminating label for <script type="math/tex">X</script> and <script type="math/tex">X_G</script>, that are modeled with <script type="math/tex">d \sim D(d|X,y)</script>.<br>同样对于鉴别器来说，现在它试图为 <script type="math/tex">X</script> 和 <script type="math/tex">X_G</script> 找到鉴别标签，用 <script type="math/tex">d \sim D(d|X,y)</script> 建模。</p>
<p>Hence, we could see that both <script type="math/tex">D</script> and <script type="math/tex">G</script> is jointly conditioned to two variables <script type="math/tex">z</script> or <script type="math/tex">X</script> and <script type="math/tex">y</script>.<br>因此，我们可以看到 <script type="math/tex">D</script> 和 <script type="math/tex">G</script> 都与两个变量 <script type="math/tex">z</script> 或 <script type="math/tex">X</script> 和 <script type="math/tex">y</script> 联合相关。</p>
<p>Now, the objective function is given by:<br>现在，目标函数如下：</p>
<script type="math/tex; mode=display">\min_G\max_DV(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x,y)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z,y),y))]</script><p>If we compare the above loss to GAN loss, the difference only lies in the additional parameter <script type="math/tex">y</script> in both <script type="math/tex">D</script> and <script type="math/tex">G</script>.<br>如果我们将上述损失与 GAN 损失进行比较，差异仅在于 <script type="math/tex">D</script> 和 <script type="math/tex">G</script> 中的附加参数 <script type="math/tex">y</script>。</p>
<p>The architecture of CGAN is now as follows (taken from [1]):<br>CGAN 的架构如下（取自[1]）：</p>
<img src="/2024/07/10/CGAN-TensorFlow/CGAN%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" class="" title="CGAN网络结构">
<p>In contrast with the architecture of GAN, we now has an additional input layer in both discriminator net and generator net.<br>与 GAN 的架构相比，我们现在在鉴别器网络和生成器网络中都增加了一个输入层。</p>
<h1 id="CGAN-Implementation-in-TensorFlow"><a href="#CGAN-Implementation-in-TensorFlow" class="headerlink" title="CGAN: Implementation in TensorFlow"></a>CGAN: Implementation in TensorFlow</h1><p>I’d like to direct the reader to the previous post about GAN, particularly for the implementation in TensorFlow. Implementing CGAN is so simple that we just need to add a handful of lines to the original GAN implementation. So, here we will only look at those modifications.<br>我想引导读者阅读上一篇关于 GAN 的文章，特别是关于在 TensorFlow 中的实现。实现 CGAN 非常简单，我们只需要在原始 GAN 实现中添加几行代码即可。因此，我们在这里只讨论这些修改。</p>
<p><a target="_blank" rel="noopener" href="https://agustinus.kristia.de/techblog/2016/09/17/gan-tensorflow/">Generative Adversarial Nets in TensorFlow</a></p>
<p>The first additional code for CGAN is here:<br>CGAN 的第一个附加代码在这里：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, y_dim])</span><br></pre></td></tr></table></figure>
<p>We are adding new input to hold our variable we are conditioning our CGAN to.<br>我们正在添加新输入来保存我们用来调节 CGAN 的变量。</p>
<p>Next, we add it to both our generator net and discriminator net:<br>接下来，我们将其添加到我们的生成器网络和鉴别器网络中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generator</span>(<span class="params">z, y</span>):</span><br><span class="line">    <span class="comment"># Concatenate z and y</span></span><br><span class="line">    inputs = tf.concat(concat_dim=<span class="number">1</span>, values=[z, y])</span><br><span class="line"></span><br><span class="line">    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)</span><br><span class="line">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class="line">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="comment"># Concatenate x and y</span></span><br><span class="line">    inputs = tf.concat(concat_dim=<span class="number">1</span>, values=[x, y])</span><br><span class="line"></span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)</span><br><span class="line">    D_logit = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    D_prob = tf.nn.sigmoid(D_logit)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D_prob, D_logit</span><br></pre></td></tr></table></figure>
<p>The problem we have here is how to incorporate the new variable <script type="math/tex">y</script> into <script type="math/tex">D(x)</script> and <script type="math/tex">G(z)</script>. As we are trying to model the joint conditional, the simplest way to do it is to just concatenate both variables. Hence, in <script type="math/tex">G(z,y)</script>, we are concatenating <script type="math/tex">z</script> and <script type="math/tex">y</script> before we feed it into the networks. The same procedure is applied to <script type="math/tex">D(X,y)</script>.<br>我们这里的问题是如何将新变量 <script type="math/tex">y</script> 合并到 <script type="math/tex">D(x)</script> 和 <script type="math/tex">G(z)</script> 中。由于我们试图对联合条件进行建模，最简单的方法就是将两个变量连接起来。因此，在 <script type="math/tex">G(z,y)</script> 中，我们在将其输入网络之前将 <script type="math/tex">z</script> 和 <script type="math/tex">y</script> 连接起来。同样的程序也适用于 <script type="math/tex">D(X,y)</script>。</p>
<p>Of course, as our inputs for <script type="math/tex">D(X,y)</script> and <script type="math/tex">G(z,y)</script> is now different than the original GAN, we need to modify our weights:<br>当然，由于我们对 <script type="math/tex">D(X,y)</script> 和 <script type="math/tex">G(z,y)</script> 的输入现在与原始 GAN 不同，我们需要修改我们的权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Modify input to hidden weights for discriminator</span></span><br><span class="line"><span class="comment"># 修改鉴别器隐藏权重的输入</span></span><br><span class="line">D_W1 = tf.Variable(shape=[X_dim + y_dim, h_dim]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Modify input to hidden weights for generator</span></span><br><span class="line"><span class="comment"># 修改生成器隐藏权重的输入</span></span><br><span class="line">G_W1 = tf.Variable(shape=[Z_dim + y_dim, h_dim]))</span><br></pre></td></tr></table></figure>
<p>That is, we just adjust the dimensionality of our weights.<br>也就是说，我们只是调整权重的维数。</p>
<p>Next, we just use our new networks:<br>接下来，我们只需使用我们的新网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add additional parameter y into all networks</span></span><br><span class="line"><span class="comment"># 在所有网络中添加附加参数 y</span></span><br><span class="line">G_sample = generator(Z, y)</span><br><span class="line">D_real, D_logit_real = discriminator(X, y)</span><br><span class="line">D_fake, D_logit_fake = discriminator(G_sample, y)</span><br></pre></td></tr></table></figure>
<p>And finally, when training, we also feed the value of <script type="math/tex">y</script> into the networks:<br>最后，在训练时，我们还将 <script type="math/tex">y</script> 的值输入网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_mb, y_mb = mnist.train.next_batch(mb_size)</span><br><span class="line"></span><br><span class="line">Z_sample = sample_Z(mb_size, Z_dim)</span><br><span class="line">_, D_loss_curr = sess.run([D_solver, D_loss], feed_dict=&#123;X: X_mb, Z: Z_sample, y:y_mb&#125;)</span><br><span class="line">_, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=&#123;Z: Z_sample, y:y_mb&#125;)</span><br></pre></td></tr></table></figure>
<p>As an example above, we are training our GAN with MNIST data, and the conditional variable <script type="math/tex">y</script> is the labels.<br>如上例，我们使用 MNIST 数据训练我们的 GAN，条件变量 <script type="math/tex">y</script> 是标签。</p>
<h1 id="CGAN-Results"><a href="#CGAN-Results" class="headerlink" title="CGAN: Results"></a>CGAN: Results</h1><p>At test time, we want to generate new data samples with certain label. For example, we set the label to be 5, i.e. we want to generate digit “5”:<br>在测试时，我们希望生成具有特定标签的新数据样本。例如，我们将标签设置为 5，即我们希望生成数字“5”：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_sample = <span class="number">16</span></span><br><span class="line">Z_sample = sample_Z(n_sample, Z_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create conditional one-hot vector, with index 5 = 1</span></span><br><span class="line"><span class="comment"># 创建条件独热向量，索引 5 = 1</span></span><br><span class="line">y_sample = np.zeros(shape=[n_sample, y_dim])</span><br><span class="line">y_sample[:, <span class="number">5</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">samples = sess.run(G_sample, feed_dict=&#123;Z: Z_sample, y:y_sample&#125;)</span><br></pre></td></tr></table></figure>
<p>Above, we just sample <script type="math/tex">z</script>, and then construct the conditional variables. In our example case, the conditional variables is a collection of one-hot vectors with value 1 in the 5th index. The last thing we need to is to run the network with those variables as inputs.<br>上面，我们只是对 <script type="math/tex">z</script> 进行采样，然后构造条件变量。在我们的示例中，条件变量是第 5 个索引中值为 1 的独热向量集合。我们需要做的最后一件事是使用这些变量作为输入来运行网络。</p>
<p>Here is the results:<br>结果如下：</p>
<img src="/2024/07/10/CGAN-TensorFlow/CGAN%E7%BB%93%E6%9E%9C1.png" class="" title="CGAN结果1">
<p>Looks pretty much like digit 5, right?<br>看起来很像数字 5，对吧？</p>
<p>If we set our one-hot vectors to have value of 1 in the 7th index:<br>如果我们将独热向量设置为第 7 个索引中的值为 1：</p>
<img src="/2024/07/10/CGAN-TensorFlow/CGAN%E7%BB%93%E6%9E%9C2.png" class="" title="CGAN结果2">
<p>Those results confirmed that have successfully trained our CGAN.<br>这些结果证实了我们的 CGAN 训练成功。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/wiseodd/generative-models">完整代码</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cgan_pytorch.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;../../MNIST_data&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">mb_size = <span class="number">64</span></span><br><span class="line">Z_dim = <span class="number">100</span></span><br><span class="line">X_dim = mnist.train.images.shape[<span class="number">1</span>]</span><br><span class="line">y_dim = mnist.train.labels.shape[<span class="number">1</span>]</span><br><span class="line">h_dim = <span class="number">128</span></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">lr = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_init</span>(<span class="params">size</span>):</span><br><span class="line">    in_dim = size[<span class="number">0</span>]</span><br><span class="line">    xavier_stddev = <span class="number">1.</span> / np.sqrt(in_dim / <span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> Variable(torch.randn(*size) * xavier_stddev, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ==================== GENERATOR ======================== &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])</span><br><span class="line">bzh = Variable(torch.zeros(h_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Whx = xavier_init(size=[h_dim, X_dim])</span><br><span class="line">bhx = Variable(torch.zeros(X_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">G</span>(<span class="params">z, c</span>):</span><br><span class="line">    inputs = torch.cat([z, c], <span class="number">1</span>)</span><br><span class="line">    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ==================== DISCRIMINATOR ======================== &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">Wxh = xavier_init(size=[X_dim + y_dim, h_dim])</span><br><span class="line">bxh = Variable(torch.zeros(h_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Why = xavier_init(size=[h_dim, <span class="number">1</span>])</span><br><span class="line">bhy = Variable(torch.zeros(<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">D</span>(<span class="params">X, c</span>):</span><br><span class="line">    inputs = torch.cat([X, c], <span class="number">1</span>)</span><br><span class="line">    h = nn.relu(inputs @ Wxh + bxh.repeat(inputs.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G_params = [Wzh, bzh, Whx, bhx]</span><br><span class="line">D_params = [Wxh, bxh, Why, bhy]</span><br><span class="line">params = G_params + D_params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ===================== TRAINING ======================== &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reset_grad</span>():</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data = p.grad.data</span><br><span class="line">            p.grad = Variable(data.new().resize_as_(data).zero_())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G_solver = optim.Adam(G_params, lr=<span class="number">1e-3</span>)</span><br><span class="line">D_solver = optim.Adam(D_params, lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line">ones_label = Variable(torch.ones(mb_size, <span class="number">1</span>))</span><br><span class="line">zeros_label = Variable(torch.zeros(mb_size, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100000</span>):</span><br><span class="line">    <span class="comment"># Sample data</span></span><br><span class="line">    z = Variable(torch.randn(mb_size, Z_dim))</span><br><span class="line">    X, c = mnist.train.next_batch(mb_size)</span><br><span class="line">    X = Variable(torch.from_numpy(X))</span><br><span class="line">    c = Variable(torch.from_numpy(c.astype(<span class="string">&#x27;float32&#x27;</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Dicriminator forward-loss-backward-update</span></span><br><span class="line">    G_sample = G(z, c)</span><br><span class="line">    D_real = D(X, c)</span><br><span class="line">    D_fake = D(G_sample, c)</span><br><span class="line"></span><br><span class="line">    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)</span><br><span class="line">    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)</span><br><span class="line">    D_loss = D_loss_real + D_loss_fake</span><br><span class="line"></span><br><span class="line">    D_loss.backward()</span><br><span class="line">    D_solver.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Housekeeping - reset gradient</span></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generator forward-loss-backward-update</span></span><br><span class="line">    z = Variable(torch.randn(mb_size, Z_dim))</span><br><span class="line">    G_sample = G(z, c)</span><br><span class="line">    D_fake = D(G_sample, c)</span><br><span class="line"></span><br><span class="line">    G_loss = nn.binary_cross_entropy(D_fake, ones_label)</span><br><span class="line"></span><br><span class="line">    G_loss.backward()</span><br><span class="line">    G_solver.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Housekeeping - reset gradient</span></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print and plot every now and then</span></span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Iter-&#123;&#125;; D_loss: &#123;&#125;; G_loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(it, D_loss.data.numpy(), G_loss.data.numpy()))</span><br><span class="line"></span><br><span class="line">        c = np.zeros(shape=[mb_size, y_dim], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        c[:, np.random.randint(<span class="number">0</span>, <span class="number">10</span>)] = <span class="number">1.</span></span><br><span class="line">        c = Variable(torch.from_numpy(c))</span><br><span class="line">        samples = G(z, c).data.numpy()[:<span class="number">16</span>]</span><br><span class="line"></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">        gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">        gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">            ax = plt.subplot(gs[i])</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">            ax.set_xticklabels([])</span><br><span class="line">            ax.set_yticklabels([])</span><br><span class="line">            ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">            plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&#x27;Greys_r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;out/&#x27;</span>):</span><br><span class="line">            os.makedirs(<span class="string">&#x27;out/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        plt.savefig(<span class="string">&#x27;out/&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(cnt).zfill(<span class="number">3</span>)), bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        plt.close(fig)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;../../MNIST_data&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">mb_size = <span class="number">64</span></span><br><span class="line">Z_dim = <span class="number">100</span></span><br><span class="line">X_dim = mnist.train.images.shape[<span class="number">1</span>]</span><br><span class="line">y_dim = mnist.train.labels.shape[<span class="number">1</span>]</span><br><span class="line">h_dim = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_init</span>(<span class="params">size</span>):</span><br><span class="line">    in_dim = size[<span class="number">0</span>]</span><br><span class="line">    xavier_stddev = <span class="number">1.</span> / tf.sqrt(in_dim / <span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; Discriminator Net model &quot;&quot;&quot;</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, y_dim])</span><br><span class="line"></span><br><span class="line">D_W1 = tf.Variable(xavier_init([X_dim + y_dim, h_dim]))</span><br><span class="line">D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class="line"></span><br><span class="line">D_W2 = tf.Variable(xavier_init([h_dim, <span class="number">1</span>]))</span><br><span class="line">D_b2 = tf.Variable(tf.zeros(shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator</span>(<span class="params">x, y</span>):</span><br><span class="line">    inputs = tf.concat(axis=<span class="number">1</span>, values=[x, y])</span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)</span><br><span class="line">    D_logit = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    D_prob = tf.nn.sigmoid(D_logit)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D_prob, D_logit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; Generator Net model &quot;&quot;&quot;</span></span><br><span class="line">Z = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, Z_dim])</span><br><span class="line"></span><br><span class="line">G_W1 = tf.Variable(xavier_init([Z_dim + y_dim, h_dim]))</span><br><span class="line">G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class="line"></span><br><span class="line">G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))</span><br><span class="line">G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))</span><br><span class="line"></span><br><span class="line">theta_G = [G_W1, G_W2, G_b1, G_b2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator</span>(<span class="params">z, y</span>):</span><br><span class="line">    inputs = tf.concat(axis=<span class="number">1</span>, values=[z, y])</span><br><span class="line">    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)</span><br><span class="line">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class="line">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_Z</span>(<span class="params">m, n</span>):</span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(-<span class="number">1.</span>, <span class="number">1.</span>, size=[m, n])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">samples</span>):</span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">    gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">        ax = plt.subplot(gs[i])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        ax.set_xticklabels([])</span><br><span class="line">        ax.set_yticklabels([])</span><br><span class="line">        ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">        plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&#x27;Greys_r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G_sample = generator(Z, y)</span><br><span class="line">D_real, D_logit_real = discriminator(X, y)</span><br><span class="line">D_fake, D_logit_fake = discriminator(G_sample, y)</span><br><span class="line"></span><br><span class="line">D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))</span><br><span class="line">D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))</span><br><span class="line">D_loss = D_loss_real + D_loss_fake</span><br><span class="line">G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))</span><br><span class="line"></span><br><span class="line">D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)</span><br><span class="line">G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;out/&#x27;</span>):</span><br><span class="line">    os.makedirs(<span class="string">&#x27;out/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        n_sample = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">        Z_sample = sample_Z(n_sample, Z_dim)</span><br><span class="line">        y_sample = np.zeros(shape=[n_sample, y_dim])</span><br><span class="line">        y_sample[:, <span class="number">7</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        samples = sess.run(G_sample, feed_dict=&#123;Z: Z_sample, y:y_sample&#125;)</span><br><span class="line"></span><br><span class="line">        fig = plot(samples)</span><br><span class="line">        plt.savefig(<span class="string">&#x27;out/&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(i).zfill(<span class="number">3</span>)), bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        plt.close(fig)</span><br><span class="line"></span><br><span class="line">    X_mb, y_mb = mnist.train.next_batch(mb_size)</span><br><span class="line"></span><br><span class="line">    Z_sample = sample_Z(mb_size, Z_dim)</span><br><span class="line">    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict=&#123;X: X_mb, Z: Z_sample, y:y_mb&#125;)</span><br><span class="line">    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=&#123;Z: Z_sample, y:y_mb&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Iter: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(it))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;D loss: &#123;:.4&#125;&#x27;</span>. <span class="built_in">format</span>(D_loss_curr))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;G_loss: &#123;:.4&#125;&#x27;</span>.<span class="built_in">format</span>(G_loss_curr))</span><br><span class="line">        <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure>
<p>修改后代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要库</span></span><br><span class="line"><span class="comment"># PyTorch用于深度学习</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># NumPy用于数值计算</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Matplotlib用于绘图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="comment"># os用于文件操作</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># TensorFlow用于加载MNIST数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#from tensorflow.examples.tutorials.mnist import input_data</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图像数据标准化到 [0, 1] 范围</span></span><br><span class="line">x_train = x_train.reshape(x_train.shape[<span class="number">0</span>], <span class="number">784</span>).astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255.</span></span><br><span class="line">x_test = x_test.reshape(x_test.shape[<span class="number">0</span>], <span class="number">784</span>).astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转换为 one-hot 编码</span></span><br><span class="line">y_train = tf.keras.utils.to_categorical(y_train, <span class="number">10</span>)</span><br><span class="line">y_test = tf.keras.utils.to_categorical(y_test, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不再使用原来的mnist.train.next_batch()方法，我们需要创建一个函数来模拟这个功能：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">next_batch</span>(<span class="params">num, data, labels</span>):</span><br><span class="line">    idx = np.arange(<span class="number">0</span> , <span class="built_in">len</span>(data))</span><br><span class="line">    np.random.shuffle(idx)</span><br><span class="line">    idx = idx[:num]</span><br><span class="line">    data_shuffle = [data[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    labels_shuffle = [labels[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    <span class="keyword">return</span> np.asarray(data_shuffle), np.asarray(labels_shuffle)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># mnist = input_data.read_data_sets(&#x27;../../MNIST_data&#x27;, one_hot=True) # 加载MNIST数据集，旧的TensorFlow版本</span></span><br><span class="line">mb_size = <span class="number">64</span>    <span class="comment">#设置mini-batch大小为64</span></span><br><span class="line">Z_dim = <span class="number">100</span> <span class="comment">#设置随机向量维度为100</span></span><br><span class="line">X_dim = x_train.shape[<span class="number">1</span>]</span><br><span class="line">y_dim = y_train.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#X_dim = mnist.train.images.shape[1] # 设置输入图像维度(784,因为MNIST图像是28x28=784像素)</span></span><br><span class="line"><span class="comment">#y_dim = mnist.train.labels.shape[1] # 设置标签维度(10,因为MNIST有10个类别)</span></span><br><span class="line">h_dim = <span class="number">128</span> <span class="comment">#设置隐藏层维度为128</span></span><br><span class="line">c = <span class="number">0</span>   <span class="comment">#初始化计数器c</span></span><br><span class="line">lr = <span class="number">1e-3</span>   <span class="comment">#学习率lr</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Xavier初始化方法,用于初始化网络权重，它有助于解决深度网络中的梯度消失问题</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_init</span>(<span class="params">size</span>):</span><br><span class="line">    in_dim = size[<span class="number">0</span>]</span><br><span class="line">    xavier_stddev = <span class="number">1.</span> / np.sqrt(in_dim / <span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> Variable(torch.randn(*size) * xavier_stddev, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ==================== GENERATOR ======================== &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 初始化了生成器的权重和偏置。Wzh和bzh是第一层的权重和偏置,Whx和bhx是第二层的。</span></span><br><span class="line">Wzh = xavier_init(size=[Z_dim, h_dim])</span><br><span class="line">bzh = Variable(torch.zeros(h_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Whx = xavier_init(size=[h_dim, X_dim])</span><br><span class="line">bhx = Variable(torch.zeros(X_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成器函数。它接受噪声z作为输入,通过两层网络生成假图像。第一层使用ReLU激活函数,第二层使用Sigmoid函数。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">G</span>(<span class="params">z</span>):</span><br><span class="line">    h = nn.relu(z @ Wzh + bzh.repeat(z.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ==================== DISCRIMINATOR ======================== &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 初始化判别器的权重和偏置。Wzh和bzh是第一层的权重和偏置,Whx和bhx是第二层的。</span></span><br><span class="line">Wxh = xavier_init(size=[X_dim, h_dim])</span><br><span class="line">bxh = Variable(torch.zeros(h_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Why = xavier_init(size=[h_dim, <span class="number">1</span>])</span><br><span class="line">bhy = Variable(torch.zeros(<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判别器函数。它接受图像X作为输入,输出一个0到1之间的数,表示图像是真实的概率。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">D</span>(<span class="params">X</span>):</span><br><span class="line">    h = nn.relu(X @ Wxh + bxh.repeat(X.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将生成器和判别器的参数分别组织起来,方便后续优化。</span></span><br><span class="line">G_params = [Wzh, bzh, Whx, bhx]</span><br><span class="line">D_params = [Wxh, bxh, Why, bhy]</span><br><span class="line">params = G_params + D_params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; ===================== TRAINING ======================== &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重置所有参数的梯度。在每次更新参数之后调用,以准备下一次反向传播。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reset_grad</span>():</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data = p.grad.data</span><br><span class="line">            p.grad = Variable(data.new().resize_as_(data).zero_())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为生成器和判别器分别创建了Adam优化器。</span></span><br><span class="line">G_solver = optim.Adam(G_params, lr=<span class="number">1e-3</span>)</span><br><span class="line">D_solver = optim.Adam(D_params, lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表示&quot;真&quot;和&quot;假&quot;的标签,用于计算损失。</span></span><br><span class="line">ones_label = Variable(torch.ones(mb_size, <span class="number">1</span>))</span><br><span class="line">zeros_label = Variable(torch.zeros(mb_size, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练循环外创建两个列表来存储loss值</span></span><br><span class="line">D_losses = []</span><br><span class="line">G_losses = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 曲线平滑方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">moving_average</span>(<span class="params">data, window_size</span>):</span><br><span class="line">    cumsum = np.cumsum(np.insert(data, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> (cumsum[window_size:] - cumsum[:-window_size]) / window_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存loss曲线</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;out_gan_pytorch_1/&#x27;</span>):</span><br><span class="line">    os.makedirs(<span class="string">&#x27;out_gan_pytorch_1/&#x27;</span>)</span><br><span class="line">csv_file = <span class="built_in">open</span>(<span class="string">&#x27;out_gan_pytorch_1/loss_data.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">csv_writer = csv.writer(csv_file)</span><br><span class="line">csv_writer.writerow([<span class="string">&#x27;Iteration&#x27;</span>, <span class="string">&#x27;D_loss&#x27;</span>, <span class="string">&#x27;G_loss&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主训练循环,总共进行100000次迭代。</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100000</span>):</span><br><span class="line">    <span class="comment"># Sample data</span></span><br><span class="line">    z = Variable(torch.randn(mb_size, Z_dim))   <span class="comment">#生成一个随机噪声批次z作为生成器的输入。</span></span><br><span class="line">    <span class="comment"># X, _ = mnist.train.next_batch(mb_size)  #从MNIST数据集中获取一批真实图像X。</span></span><br><span class="line">    X, _ = next_batch(mb_size, x_train, y_train)</span><br><span class="line">    X = Variable(torch.from_numpy(X))   <span class="comment">#将X转换为PyTorch的Variable。</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; ===================== 判别器训练 ======================== &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Dicriminator forward-loss-backward-update</span></span><br><span class="line">    G_sample = G(z) <span class="comment">#使用生成器G生成一批假图像G_sample</span></span><br><span class="line">    D_real = D(X)   <span class="comment">#判别器D对真实图像X进行评估,得到D_real</span></span><br><span class="line">    D_fake = D(G_sample)    <span class="comment">#判别器D对生成的假图像G_sample进行评估,得到D_fake</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算判别器的损失</span></span><br><span class="line">    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)   <span class="comment">#D_loss_real是真实图像的损失, 目标是使D_real接近1。</span></span><br><span class="line">    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)  <span class="comment">#D_loss_fake是假图像的损失, 目标是使D_fake接近0。</span></span><br><span class="line">    D_loss = D_loss_real + D_loss_fake  <span class="comment">#总损失D_loss是这两部分之和。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对判别器进行反向传播和参数更新。</span></span><br><span class="line">    D_loss.backward()</span><br><span class="line">    D_solver.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Housekeeping - reset gradient</span></span><br><span class="line">    <span class="comment"># 重置所有参数的梯度,为下一步做准备。</span></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; ===================== 生成器训练 ======================== &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Generator forward-loss-backward-update</span></span><br><span class="line">    <span class="comment"># 为训练生成器,我们再次生成假图像并用判别器评估</span></span><br><span class="line">    z = Variable(torch.randn(mb_size, Z_dim))</span><br><span class="line">    G_sample = G(z)</span><br><span class="line">    D_fake = D(G_sample)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算生成器的损失。注意这里的目标是让D_fake接近1, 即欺骗判别器</span></span><br><span class="line">    G_loss = nn.binary_cross_entropy(D_fake, ones_label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对生成器进行反向传播和参数更新。</span></span><br><span class="line">    G_loss.backward()</span><br><span class="line">    G_solver.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Housekeeping - reset gradient</span></span><br><span class="line">    <span class="comment"># 再次重置梯度。</span></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每次迭代都记录loss值</span></span><br><span class="line">    D_losses.append(D_loss.item())</span><br><span class="line">    G_losses.append(G_loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print and plot every now and then</span></span><br><span class="line">    <span class="comment"># 每1000次迭代打印一次当前的损失值。</span></span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Iter-&#123;&#125;; D_loss: &#123;&#125;; G_loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(it, D_loss.data.numpy(), G_loss.data.numpy()))</span><br><span class="line"></span><br><span class="line">        samples = G(z).data.numpy()[:<span class="number">16</span>]    <span class="comment"># 生成16个样本。</span></span><br><span class="line"></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))    <span class="comment"># 创建一个4x4的图像网格。</span></span><br><span class="line">        gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)    <span class="comment"># 将每个生成的样本绘制到网格中。</span></span><br><span class="line">        gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">            ax = plt.subplot(gs[i])</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">            ax.set_xticklabels([])</span><br><span class="line">            ax.set_yticklabels([])</span><br><span class="line">            ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">            plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&#x27;Greys_r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;out_gan_pytorch_1/&#x27;</span>):</span><br><span class="line">            os.makedirs(<span class="string">&#x27;out_gan_pytorch_1/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        plt.savefig(<span class="string">&#x27;out_gan_pytorch_1/&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(c).zfill(<span class="number">3</span>)), bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">        c += <span class="number">1</span></span><br><span class="line">        plt.close(fig)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存当前的loss数据</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;out_gan_pytorch_1/loss_data_ongoing.csv&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">            writer = csv.writer(file)</span><br><span class="line">            <span class="keyword">if</span> it == <span class="number">0</span>:  <span class="comment"># 如果是第一次写入，添加表头</span></span><br><span class="line">                writer.writerow([<span class="string">&quot;Iteration&quot;</span>, <span class="string">&quot;G_loss&quot;</span>, <span class="string">&quot;D_loss&quot;</span>])</span><br><span class="line">            writer.writerow([it, G_loss.item(), D_loss.item()])</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Loss data at iteration <span class="subst">&#123;it&#125;</span> has been appended to &#x27;out_gan_pytorch_1/loss_data_ongoing.csv&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算移动平均-曲线平滑</span></span><br><span class="line">window_size = <span class="number">1000</span></span><br><span class="line">G_losses_avg = moving_average(G_losses, window_size)</span><br><span class="line">D_losses_avg = moving_average(D_losses, window_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制loss曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">plt.title(<span class="string">&quot;Generator and Discriminator Loss During Training&quot;</span>)</span><br><span class="line">plt.plot(G_losses, label=<span class="string">&quot;G&quot;</span>)</span><br><span class="line">plt.plot(D_losses, label=<span class="string">&quot;D&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iterations&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&#x27;out_gan_pytorch_1/loss_curve.png&#x27;</span>)</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存loss数据到CSV文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;out_gan_pytorch_1/loss_data.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    writer.writerow([<span class="string">&quot;Iteration&quot;</span>, <span class="string">&quot;G_loss&quot;</span>, <span class="string">&quot;D_loss&quot;</span>])  <span class="comment"># 写入表头</span></span><br><span class="line">    <span class="keyword">for</span> i, (g_loss, d_loss) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(G_losses, D_losses)):</span><br><span class="line">        writer.writerow([i, g_loss, d_loss])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loss data has been saved to &#x27;out_gan_pytorch_1/loss_data.csv&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算移动平均</span></span><br><span class="line">window_size = <span class="number">1000</span></span><br><span class="line">G_losses_avg = moving_average(G_losses, window_size)</span><br><span class="line">D_losses_avg = moving_average(D_losses, window_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存移动平均后的loss数据到CSV文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;out_gan_pytorch_1/loss_data_avg.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    writer.writerow([<span class="string">&quot;Iteration&quot;</span>, <span class="string">&quot;G_loss_avg&quot;</span>, <span class="string">&quot;D_loss_avg&quot;</span>])  <span class="comment"># 写入表头</span></span><br><span class="line">    <span class="keyword">for</span> i, (g_loss, d_loss) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(G_losses_avg, D_losses_avg)):</span><br><span class="line">        writer.writerow([i+window_size, g_loss, d_loss])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Averaged loss data has been saved to &#x27;out_gan_pytorch_1/loss_data_avg.csv&#x27;&quot;</span>)</span><br><span class="line"><span class="comment"># loss_data.csv：包含所有迭代的原始loss数据</span></span><br><span class="line"><span class="comment"># loss_data_avg.csv：包含移动平均后的loss数据</span></span><br><span class="line"><span class="comment"># loss_data_ongoing.csv：在训练过程中定期保存的loss数据</span></span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2024/07/10/CGAN-TensorFlow/">http://hibiscidai.com/2024/07/10/CGAN-TensorFlow/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GAN/">GAN</a><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2024/08/06/Cesium-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"><i class="fa fa-chevron-left">  </i><span>Cesium-使用指南</span></a></div><div class="next-post pull-right"><a href="/2024/07/03/%E5%B0%8F%E4%B8%91%E5%9C%A8%E6%AE%BF%E5%A0%82/"><span>小丑在殿堂</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://xn--mesr8b36x.agency/#/register?code=R5RS1JHy">好用、实惠、稳定的梯子,点击这里<img src="https://pic.imgdb.cn/item/65572abac458853aefef30cd.png" width="1000" height="124" object-fit="cover" ></a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2024 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>