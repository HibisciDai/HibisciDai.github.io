<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算"><meta name="keywords" content="学习笔记,Tensorflow,TensorflowMOOC"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97"><span class="toc-number">1.</span> <span class="toc-text">人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%AC%E8%AE%B2%E7%9B%AE%E6%A0%87%EF%BC%9A%E5%AD%A6%E4%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%9F%BA%E4%BA%8ETF2%E5%8E%9F%E7%94%9F"><span class="toc-number">2.</span> <span class="toc-text">本讲目标：学会神经网络计算过程，使用基于TF2原生</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%89%E5%AD%A6%E6%B4%BE"><span class="toc-number">3.</span> <span class="toc-text">人工智能三学派</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%E8%BF%87%E7%A8%8B"><span class="toc-number">4.</span> <span class="toc-text">神经网络设计过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB"><span class="toc-number">4.1.</span> <span class="toc-text">用神经网络实现鸢尾花分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%89%B9%E5%BE%81"><span class="toc-number">4.1.1.</span> <span class="toc-text">准备数据&#x2F;数据集特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C"><span class="toc-number">4.1.2.</span> <span class="toc-text">搭建网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.1.3.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.4.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.1.5.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.1.6.</span> <span class="toc-text">反向传播</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%94%9F%E6%88%90"><span class="toc-number">5.</span> <span class="toc-text">张量生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">5.1.</span> <span class="toc-text">数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%A0%E9%87%8F"><span class="toc-number">5.2.</span> <span class="toc-text">创建一个张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AATensor"><span class="toc-number">5.3.</span> <span class="toc-text">创建一个Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86numpy%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BATensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">5.3.1.</span> <span class="toc-text">将numpy的数据类型转换为Tensor数据类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%85%A8%E4%B8%BA0%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-number">5.3.2.</span> <span class="toc-text">创建全为0的张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%85%A8%E4%B8%BA1%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-number">5.3.3.</span> <span class="toc-text">创建全为1的张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%85%A8%E4%B8%BA%E6%8C%87%E5%AE%9A%E5%80%BC%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-number">5.3.4.</span> <span class="toc-text">创建全为指定值的张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%95%B0%EF%BC%8C%E9%BB%98%E8%AE%A4%E5%9D%87%E5%80%BC%E4%B8%BA0%EF%BC%8C%E6%A0%87%E5%87%86%E5%B7%AE%E4%B8%BA1"><span class="toc-number">5.3.5.</span> <span class="toc-text">生成正态分布的随机数，默认均值为0，标准差为1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%88%AA%E6%96%AD%E5%BC%8F%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%95%B0"><span class="toc-number">5.3.6.</span> <span class="toc-text">生成截断式正态分布的随机数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83%E9%9A%8F%E6%9C%BA%E6%95%B0-minval-maxval"><span class="toc-number">5.3.7.</span> <span class="toc-text">生成均匀分布随机数[minval, maxval)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">6.</span> <span class="toc-text">常用函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%88%B6tensor%E8%BD%AC%E6%8D%A2%E4%B8%BA%E8%AF%A5%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">6.1.</span> <span class="toc-text">强制tensor转换为该数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%BC%A0%E9%87%8F%E7%BB%B4%E5%BA%A6%E4%B8%8A%E5%85%83%E7%B4%A0%E7%9A%84%E6%9C%80%E5%B0%8F%E5%80%BC"><span class="toc-number">6.2.</span> <span class="toc-text">计算张量维度上元素的最小值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%BC%A0%E9%87%8F%E7%BB%B4%E5%BA%A6%E4%B8%8A%E5%85%83%E7%B4%A0%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC"><span class="toc-number">6.3.</span> <span class="toc-text">计算张量维度上元素的最大值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#axis"><span class="toc-number">6.4.</span> <span class="toc-text">axis</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%BC%A0%E9%87%8F%E6%B2%BF%E7%9D%80%E6%8C%87%E5%AE%9A%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%B9%B3%E5%9D%87%E5%80%BC"><span class="toc-number">6.4.1.</span> <span class="toc-text">计算张量沿着指定维度的平均值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%BC%A0%E9%87%8F%E6%B2%BF%E7%9D%80%E6%8C%87%E5%AE%9A%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%92%8C"><span class="toc-number">6.4.2.</span> <span class="toc-text">计算张量沿着指定维度的和</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-Variable"><span class="toc-number">6.5.</span> <span class="toc-text">tf.Variable</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number">6.6.</span> <span class="toc-text">TensorFlow中的数学运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%BA%94%E5%85%83%E7%B4%A0%E7%9A%84%E5%9B%9B%E5%88%99%E8%BF%90%E7%AE%97"><span class="toc-number">6.6.1.</span> <span class="toc-text">对应元素的四则运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E6%96%B9%E3%80%81%E6%AC%A1%E6%96%B9%E4%B8%8E%E5%BC%80%E6%96%B9"><span class="toc-number">6.6.2.</span> <span class="toc-text">平方、次方与开方</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98-tf-matmul"><span class="toc-number">6.6.3.</span> <span class="toc-text">矩阵乘 tf.matmul</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-data-Dataset-from-tensor-slices"><span class="toc-number">6.7.</span> <span class="toc-text">tf.data.Dataset.from_tensor_slices</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%87%E5%88%86%E4%BC%A0%E5%85%A5%E5%BC%A0%E9%87%8F%E7%9A%84%E7%AC%AC%E4%B8%80%E7%BB%B4%E5%BA%A6%EF%BC%8C%E7%94%9F%E6%88%90%E8%BE%93%E5%85%A5%E7%89%B9%E5%BE%81-%E6%A0%87%E7%AD%BE%E5%AF%B9%EF%BC%8C%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.7.1.</span> <span class="toc-text">切分传入张量的第一维度，生成输入特征&#x2F;标签对，构建数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E6%A2%AF%E5%BA%A6%E6%B1%82%E5%AF%BCtf-GradientTape"><span class="toc-number">6.8.</span> <span class="toc-text">张量梯度求导tf.GradientTape</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%9A%E4%B8%BEenumerate"><span class="toc-number">6.9.</span> <span class="toc-text">枚举enumerate</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81tf-one-hot"><span class="toc-number">6.10.</span> <span class="toc-text">独热编码tf.one_hot</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83tf-nn-softmax"><span class="toc-number">6.11.</span> <span class="toc-text">正态分布tf.nn.softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%87%AA%E6%9B%B4%E6%96%B0assign-sub"><span class="toc-number">6.12.</span> <span class="toc-text">参数自更新assign_sub</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E6%9C%80%E5%A4%A7%E7%B4%A2%E5%BC%95tf-argmax"><span class="toc-number">6.13.</span> <span class="toc-text">返回最大索引tf.argmax</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%BB%E5%85%A5"><span class="toc-number">7.</span> <span class="toc-text">鸢尾花数据集读入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">7.1.</span> <span class="toc-text">数据集介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">7.2.</span> <span class="toc-text">读入数据集</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB"><span class="toc-number">8.</span> <span class="toc-text">神经网络实现鸢尾花分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-number">8.1.</span> <span class="toc-text">准备数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%BB%E5%85%A5"><span class="toc-number">8.1.1.</span> <span class="toc-text">数据集读入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B9%B1%E5%BA%8F"><span class="toc-number">8.1.2.</span> <span class="toc-text">数据集乱序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%EF%BC%88%E5%8D%B3-x-train-y-train-x-test-y-test%EF%BC%89"><span class="toc-number">8.1.3.</span> <span class="toc-text">生成训练集和测试集（即 x_train &#x2F; y_train , x_test &#x2F; y_test）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E6%88%90-%EF%BC%88%E8%BE%93%E5%85%A5%E7%89%B9%E5%BE%81%EF%BC%8C%E6%A0%87%E7%AD%BE%EF%BC%89-%E5%AF%B9%EF%BC%8C%E6%AF%8F%E6%AC%A1%E8%AF%BB%E5%85%A5%E4%B8%80%E5%B0%8F%E6%92%AE%EF%BC%88batch%EF%BC%89"><span class="toc-number">8.1.4.</span> <span class="toc-text">配成 （输入特征，标签） 对，每次读入一小撮（batch）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C-1"><span class="toc-number">8.2.</span> <span class="toc-text">搭建网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF%E4%B8%AD%E6%89%80%E6%9C%89%E5%8F%AF%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="toc-number">8.2.1.</span> <span class="toc-text">定义神经网路中所有可训练参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-number">8.3.</span> <span class="toc-text">参数优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%A5%97%E5%BE%AA%E7%8E%AF%E8%BF%AD%E4%BB%A3%EF%BC%8C-with%E7%BB%93%E6%9E%84%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%EF%BC%8C%E6%98%BE%E7%A4%BA%E5%BD%93%E5%89%8Dloss"><span class="toc-number">8.3.1.</span> <span class="toc-text">嵌套循环迭代， with结构更新参数，显示当前loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E6%95%88%E6%9E%9C"><span class="toc-number">8.4.</span> <span class="toc-text">测试效果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%BD%93%E5%89%8D%E5%8F%82%E6%95%B0%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%90%8E%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%8C%E6%98%BE%E7%A4%BA%E5%BD%93%E5%89%8Dacc"><span class="toc-number">8.4.1.</span> <span class="toc-text">计算当前参数前向传播后的准确率，显示当前acc</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#acc-loss%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">8.5.</span> <span class="toc-text">acc &#x2F; loss可视化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85tensorflow"><span class="toc-number">9.</span> <span class="toc-text">安装tensorflow</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">243</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">88</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">33</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-02-16</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">6.9k</span><span class="post-meta__separator">|</span><span>阅读时长: 31 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97.png" class="" title="人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算">
<p>人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算</p>
<span id="more"></span>
<p>[TOC]</p>
<h1 id="人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算"><a href="#人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算" class="headerlink" title="人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算"></a>人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算</h1><h1 id="本讲目标：学会神经网络计算过程，使用基于TF2原生"><a href="#本讲目标：学会神经网络计算过程，使用基于TF2原生" class="headerlink" title="本讲目标：学会神经网络计算过程，使用基于TF2原生"></a>本讲目标：学会神经网络计算过程，使用基于TF2原生</h1><p>代码搭建你的第一个的神经网络训练模型</p>
<ul>
<li>当今人工智能主流方向——连接主义</li>
<li>前向传播</li>
<li>损失函数（初体会）</li>
<li>梯度下降（初体会）</li>
<li>学习率（初体会）</li>
<li>反向传播更新参数</li>
<li>Tensorflow 2 常用函数</li>
</ul>
<p>人工智能：让机器具备人的思维和意识。</p>
<h1 id="人工智能三学派"><a href="#人工智能三学派" class="headerlink" title="人工智能三学派"></a>人工智能三学派</h1><p>我们常说的人工智能，就是让机器具备人的思维和意识。 人工智能主要有三个学派，即<code>行为主义</code>、<code>符号主义</code>和<code>连接主义</code>。</p>
<ul>
<li><p>行为主义： 基于控制论，构建感知-动作控制系统。 （控制论，如平衡、行走、避障等自适应控制系统）<br>是基于控制论的，是在构建感知、动作的控制系统。单脚站立是行为主义一个典型例子， 通过感知要摔倒的方向，控制两只手的动作，保持身体的平衡。这就构建了一个感知、动作的控制系统，是典型的行为主义。</p>
</li>
<li><p>符号主义： 基于算数逻辑表达式，求解问题时先把问题描述为表达式，再求解表达式。 （可用公式描述、实现理性思维，如专家系统）<br>基于算数逻辑表达式。即在求解问题时，先把问题描述为表达式，再求解表达式。 例如在求解某个问题时， 利用 <code>if case</code> 等条件语句和若干计算公式描述出来， 即使用了符号主义的方法， 如专家系统。符号主义是能用公式描述的人工智能，它让计算机具备了理性思维。</p>
</li>
<li><p>连接主义： 仿生学，模仿神经元连接关系。 （仿脑神经元连接，实现感性思维，如神经网络）<br>仿造人脑内的神经元连接关系，使人类不仅具备理性思维， 还具备无法用公式描述的感性思维，如对某些知识产生记忆。</p>
</li>
</ul>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E4%BA%BA%E8%84%91%E4%B8%AD%E7%9A%84%E4%B8%80%E6%A0%B9%E7%A5%9E%E7%BB%8F%E5%85%83.png" class="" title="人脑中的一根神经元">
<p>人脑中的一根神经元，其中紫色部分为树突，其作为神经元的输入。黄色部分为轴突，其作为神经元的输出。人脑就是由 860 亿个这样的神经元首尾相接组成的网络。</p>
<p>基于连接主义的神经网络模仿上图的神经元，使计算机具有感性思维。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E4%BA%BA%E8%84%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%98%E5%8C%96%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="人脑神经网络变化示意图">
<p>随着我们的成长，大量的数据通过视觉、听觉涌入大脑，使我们的神经网络连接，也就是这些神经元连接线上的权重发生了变化，有些线上的权重增强了，有些线上的权重减弱了。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%8F%98%E5%8C%96%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="神经网络权重变化示意图">
<h1 id="神经网络设计过程"><a href="#神经网络设计过程" class="headerlink" title="神经网络设计过程"></a>神经网络设计过程</h1><p>计算机具备感性思维的几个步骤</p>
<ul>
<li><p>准备数据<br>准备 数据-标签，数量越多越好。</p>
</li>
<li><p>搭建网络</p>
</li>
<li><p>优化参数<br><code>反向传播</code>，优化权重直到准确率达到要求。</p>
</li>
<li><p>应用网络<br>新数据<code>向前传播</code>，输出概率值最大的一个。</p>
</li>
</ul>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="搭建与使用神经网络示意图">
<h2 id="用神经网络实现鸢尾花分类"><a href="#用神经网络实现鸢尾花分类" class="headerlink" title="用神经网络实现鸢尾花分类"></a>用神经网络实现鸢尾花分类</h2><h3 id="准备数据-数据集特征"><a href="#准备数据-数据集特征" class="headerlink" title="准备数据/数据集特征"></a>准备数据/数据集特征</h3><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/0%E7%8B%97%E5%B0%BE%E8%8D%89%E8%8E%BA%E5%B0%BE.png" class="" title="0狗尾草莺尾">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/1%E6%9D%82%E8%89%B2%E8%8E%BA%E5%B0%BE.png" class="" title="1杂色莺尾">
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/2%E5%BC%97%E5%90%89%E5%B0%BC%E4%BA%9A%E9%B8%A2%E5%B0%BE.png" class="" title="2弗吉尼亚鸢尾">
<p>人们通过经验总结出了规律：通过测量花的花萼长、花萼宽、花瓣长、花瓣宽，可以得出鸢尾花的类别。<br>例如：花萼长&gt;花萼宽 且 花瓣长/花瓣宽&gt;2 则为 1杂色鸢尾</p>
<blockquote>
<p>if语句 case语句 —— 专家系统 把专家的经验告知计算机，计算机执行逻辑判别（理性计算） ，给出分类。</p>
</blockquote>
<p>神经网络算法：采用搭建神经网络的办法对其进行分类，即将鸢尾花花萼长、花萼宽、花瓣长、花瓣宽四个输入属性喂入搭建好的神经网络，网络优化参数得到模型，输出分类结果。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="用神经网络实现鸢尾花分类示意图">
<p>粉色小球是神经元，1943年已经提出神经元的计算模型——MP模型。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/MP%E6%A8%A1%E5%9E%8B.png" class="" title="MP模型">
<h3 id="搭建网络"><a href="#搭建网络" class="headerlink" title="搭建网络"></a>搭建网络</h3><p>为本次求解简单，将MP模型中的非线性函数去掉，得到简化的MP模型。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E7%AE%80%E5%8C%96%E7%9A%84MP%E6%A8%A1%E5%9E%8B.png" class="" title="简化的MP模型">
<p>x是1行4列的输入特征<br>w是4行3列的权重<br>b是偏置项，3个<br>y是1行3列的输出(三种鸢尾花的可能大小)</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%95%E5%BC%80%E6%A8%A1%E5%9E%8B.png" class="" title="鸢尾花神经网络展开模型">
<blockquote>
<p>输入特征为标签0狗尾鸢尾</p>
</blockquote>
<p>y0、y1、y2和x0、x1、x2、x3都有连接，称为全连接网络。</p>
<p>网络搭建完成后进行参数初始化，为随机数。</p>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><script type="math/tex; mode=display">y = x * w +b</script><script type="math/tex; mode=display">\begin{bmatrix}5.8 & 4.0 & 0.2\end{bmatrix} \tag{2} $$ * $$ \begin{bmatrix}-0.8 & -0.34 & -1.4\\ 0.6 & 1.3 & 0.25 \\ 0.5 & 1.45 & 0.9 \\ 0.65 & 0.7 & -1.2 \end{bmatrix} \tag{2} $$ + $$ \begin{bmatrix} 2.52 & -3.1 & 5.62 \end{bmatrix} \tag{2} $$ = $$ \begin{bmatrix} 1.01 & 2.01 & -0.66\end{bmatrix} \tag{2}</script><p>这个结果发现0类的鸢尾得分不是最高的。</p>
<p>因为初始权重值是随机给的。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><strong>损失函数(loss function) = 预测值（y）与标准答案（y_）的差距。</strong></p>
<p>损失函数可以定量判断参数w、偏置项b的优劣，当损失函数输出最小时，参数w、偏置项b会出现最优值。</p>
<p>均方误差：<script type="math/tex">MSE(y, y_{\_}) =  \frac{\sum_{k=0}^{n} (y-y_{\_})^{2}}{n}</script></p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>目的：想找到一组参数w和b，使得损失函数最小。</p>
<p>梯度：函数对各参数求偏导后的向量。函数梯度下降方向是函数减小方向。</p>
<p>梯度下降法：沿损失函数梯度下降的方向，寻找损失函数的最小值，得到最优参数的方法。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="梯度下降示意图">
<script type="math/tex; mode=display">w_{t+1} = w_{t} - lr * \frac{\partial loss}{\partial w_{t}}</script><script type="math/tex; mode=display">b_{t+1} = b - lr * \frac{\partial loss}{\partial b_{t}}</script><script type="math/tex; mode=display">w_{t+1} * x + b_{t+1} → y</script><p><strong>学习率（learning rate，lr）</strong>：当学习率设置过小，收敛过程将变得很缓慢。当学习率设置过大，梯度可能会在最小值附近来回震荡，甚至可能无法收敛。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><script type="math/tex; mode=display">w_{t+1} = w_{t} - lr * \frac{\partial loss}{\partial w_{t}}</script><p><strong>反向传播</strong>：从后向前，逐层求损失函数对每层神经元参数的偏导数，迭代更新所有参数。</p>
<p>例如：损失函数 $ loss = (w+1)^{2} $， $ \frac{\partial loss}{\partial w} = 2w+2 $</p>
<p>参数w初始化为5，学习率为0.2，则：<br>1次    参数w：5    5 - 0.2 <em> ( 2 </em> 5 + 2 ) = 2.6<br>2次    参数w：2.6    2.6 - 0.2 <em> ( 2 </em> 2.6 +2 ) = 1.16<br>3次    参数w：1.16    1.16 - 0.2 <em> ( 2 </em> 1.16 + 2 ) = 0.296<br>4次    参数w：0.296<br>…</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%84%8F%E5%9B%BE.png" class="" title="反向传播意图">
<p>目的是找到反向w=-1，损失函数最低的点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))</span><br><span class="line">lr = <span class="number">0.2</span></span><br><span class="line">epoch = <span class="number">40</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构到grads框起了梯度的计算过程。</span></span><br><span class="line">        loss = tf.square(w + <span class="number">1</span>)</span><br><span class="line">    grads = tape.gradient(loss, w)  <span class="comment"># .gradient函数告知谁对谁求导</span></span><br><span class="line"></span><br><span class="line">    w.assign_sub(lr * grads)  <span class="comment"># .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After %s epoch,w is %f,loss is %f&quot;</span> % (epoch, w.numpy(), loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程</span></span><br><span class="line"><span class="comment"># 最终目的：找到 loss 最小 即 w = -1 的最优参数w</span></span><br></pre></td></tr></table></figure>
<h1 id="张量生成"><a href="#张量生成" class="headerlink" title="张量生成"></a>张量生成</h1><p><strong>张量(Tensor)：多维数组（列表）</strong></p>
<p><strong>阶：张量的维数</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">维数</th>
<th style="text-align:center">阶</th>
<th style="text-align:center">名字</th>
<th style="text-align:center">例子</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0-D</td>
<td style="text-align:center">0</td>
<td style="text-align:center">标量 scalar</td>
<td style="text-align:center">s = 123</td>
</tr>
<tr>
<td style="text-align:center">1-D</td>
<td style="text-align:center">1</td>
<td style="text-align:center">向量 vector</td>
<td style="text-align:center">v = [1, 2, 3]</td>
</tr>
<tr>
<td style="text-align:center">2-D</td>
<td style="text-align:center">2</td>
<td style="text-align:center">矩阵 matrix</td>
<td style="text-align:center">m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</td>
</tr>
<tr>
<td style="text-align:center">n-D</td>
<td style="text-align:center">n</td>
<td style="text-align:center">张量 tensor</td>
<td style="text-align:center">t = [[[.. n个</td>
</tr>
</tbody>
</table>
</div>
<p>张量可以表示0阶到n阶数组（列表）</p>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p><code>tf.int, tf.float ......</code></p>
<p>tf.int 32, tf.float 32, tf.float 64</p>
<p><code>tf.bool</code></p>
<p>tf.constant([True, False])</p>
<p><code>tf.string</code></p>
<p>tf.constant(“Hello, world!”)</p>
<h2 id="创建一个张量"><a href="#创建一个张量" class="headerlink" title="创建一个张量"></a>创建一个张量</h2><p><code>tf.constant(张量内容, dtype = 数据类型(可选))</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">5</span>], dtype = tf.int64)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.dtype)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br></pre></td></tr></table></figure>
<p>运行结果：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor([1,5], shape=(2 ,), dtype = int64)</span><br><span class="line">&lt;dtype:&#x27;int64&#x27;&gt;</span><br><span class="line">(2,)</span><br></pre></td></tr></table></figure></p>
<p>shape隔开1个数组说明是1纬，值为2，说明有两数字。</p>
<h2 id="创建一个Tensor"><a href="#创建一个Tensor" class="headerlink" title="创建一个Tensor"></a>创建一个Tensor</h2><h3 id="将numpy的数据类型转换为Tensor数据类型"><a href="#将numpy的数据类型转换为Tensor数据类型" class="headerlink" title="将numpy的数据类型转换为Tensor数据类型"></a>将numpy的数据类型转换为Tensor数据类型</h3><p><code>tf.convert_to_tensor(数据名, dtype = 数据类型(可选))</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.arange(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">b = tf.convert_to_tensor(a, dtype = tf.int64)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure>
<p>运行结果：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[0 1 2 3 4]</span><br><span class="line">tf.Tensor([0 1 2 3 4], shape=(5,), dtype = int64)</span><br></pre></td></tr></table></figure></p>
<p>纬度：<br>一维 直接写个数<br>二维 用[行, 列]<br>多维 用[n, m, j, k ……]</p>
<h3 id="创建全为0的张量"><a href="#创建全为0的张量" class="headerlink" title="创建全为0的张量"></a>创建全为0的张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.zero(维度)</span><br></pre></td></tr></table></figure>
<h3 id="创建全为1的张量"><a href="#创建全为1的张量" class="headerlink" title="创建全为1的张量"></a>创建全为1的张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.ones(纬度)</span><br></pre></td></tr></table></figure>
<h3 id="创建全为指定值的张量"><a href="#创建全为指定值的张量" class="headerlink" title="创建全为指定值的张量"></a>创建全为指定值的张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.fill(纬度, 指定值)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tf.zeros([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.ones(<span class="number">4</span>)</span><br><span class="line">c = tf.fill([<span class="number">2</span>, <span class="number">2</span>], <span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([[0. 0. 0.][0. 0. 0.]], shape = (2, 3), dtype = float32)</span><br><span class="line">tf.Tensor([1. 1. 1. 1.], shape = (4, ), dtype = float32)</span><br><span class="line">tf.Tensor([[9 9][9 9]], shape = (2, 2), dtype = int32)</span><br></pre></td></tr></table></figure>
<h3 id="生成正态分布的随机数，默认均值为0，标准差为1"><a href="#生成正态分布的随机数，默认均值为0，标准差为1" class="headerlink" title="生成正态分布的随机数，默认均值为0，标准差为1"></a>生成正态分布的随机数，默认均值为0，标准差为1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random.normal(纬度, mean=均值，stddev=标准差)</span><br></pre></td></tr></table></figure>
<h3 id="生成截断式正态分布的随机数"><a href="#生成截断式正态分布的随机数" class="headerlink" title="生成截断式正态分布的随机数"></a>生成截断式正态分布的随机数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random.truncated_normal(纬度, mean=均值, stddev=标准差)</span><br></pre></td></tr></table></figure>
<p>在<code>tf.turncated_normal</code>中如果随机生成数据的取值在$ (\mu - 2 \sigma, \mu +2 \sigma) $之外则重新进行生成，保证了生成值在均值附近。</p>
<p>$ \mu $：均值<br>$ \sigma $：标准差</p>
<p>标准差计算公式：<script type="math/tex">\sigma = \sqrt{\frac{ \sum_{i=1}^{n}  (x_{i} - \bar{x})^{2}} {n}}</script></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d = tf.random.normal ([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.5</span>, stddev=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line">e = tf.random.truncated_normal ([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.5</span>, stddev=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[0.7925745 0.643315 ]</span><br><span class="line">[1.4752257 0.2533372]], shape=(2, 2), dtype=float32)</span><br><span class="line"></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ 1.3688478 1.0125661 ]</span><br><span class="line">[ 0.17475659 -0.02224463]], shape=(2, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="生成均匀分布随机数-minval-maxval"><a href="#生成均匀分布随机数-minval-maxval" class="headerlink" title="生成均匀分布随机数[minval, maxval)"></a>生成均匀分布随机数[minval, maxval)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random.uniform(维度, minval=最小值, maxval=最大值)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f = tf.random.uniform([<span class="number">2</span>, <span class="number">2</span>], minval=<span class="number">0</span>, maxval=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[0.28219545 0.15581512]</span><br><span class="line">[0.77972126 0.47817433]], shape=(2, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<h1 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h1><h2 id="强制tensor转换为该数据类型"><a href="#强制tensor转换为该数据类型" class="headerlink" title="强制tensor转换为该数据类型"></a>强制tensor转换为该数据类型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.cast(张量名, dtype=数据类型)</span><br></pre></td></tr></table></figure>
<h2 id="计算张量维度上元素的最小值"><a href="#计算张量维度上元素的最小值" class="headerlink" title="计算张量维度上元素的最小值"></a>计算张量维度上元素的最小值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_min(张量名)</span><br></pre></td></tr></table></figure>
<h2 id="计算张量维度上元素的最大值"><a href="#计算张量维度上元素的最大值" class="headerlink" title="计算张量维度上元素的最大值"></a>计算张量维度上元素的最大值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_max(张量名)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x1 = tf.constant([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], dtype=tf.float64)</span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"></span><br><span class="line">x2 = tf.cast(x1, tf.int32)</span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="built_in">print</span>(tf.reduce_min(x2), tf.reduce_max(x2))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)</span><br><span class="line"></span><br><span class="line">tf.Tensor([1 2 3], shape=(3,), dtype=int32)</span><br><span class="line"></span><br><span class="line">tf.Tensor(1, shape=(), dtype=int32)</span><br><span class="line"></span><br><span class="line">tf.Tensor(3, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>
<h2 id="axis"><a href="#axis" class="headerlink" title="axis"></a>axis</h2><p>在一个二维张量或数组中，可以通过调整 axis 等于0或1控制执行维度。</p>
<ul>
<li>axis=0代表跨行（经度， down)，而axis=1代表跨列（纬度， across)</li>
<li>如果不指定axis，则所有元素参与计算。</li>
</ul>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/axis%E7%A4%BA%E6%84%8F%E5%9B%BE.png" class="" title="axis示意图">
<h3 id="计算张量沿着指定维度的平均值"><a href="#计算张量沿着指定维度的平均值" class="headerlink" title="计算张量沿着指定维度的平均值"></a>计算张量沿着指定维度的平均值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_mean(张量名, axis=操作轴)</span><br></pre></td></tr></table></figure>
<h3 id="计算张量沿着指定维度的和"><a href="#计算张量沿着指定维度的和" class="headerlink" title="计算张量沿着指定维度的和"></a>计算张量沿着指定维度的和</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_sum(张量名, axis=操作轴)</span><br></pre></td></tr></table></figure>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(tf.reduce_mean(x))</span><br><span class="line"><span class="built_in">print</span>(tf.reduce_sum(x, axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[1 2 3]</span><br><span class="line">[2 2 3]], shape=(2, 3), dtype=int32)</span><br><span class="line"></span><br><span class="line">tf.Tensor(2, shape=(), dtype=int32)</span><br><span class="line"></span><br><span class="line">tf.Tensor([6 7], shape=(2,), dtype=int32)</span><br></pre></td></tr></table></figure>
<h2 id="tf-Variable"><a href="#tf-Variable" class="headerlink" title="tf.Variable"></a>tf.Variable</h2><p><code>tf.Variable()</code> 将变量标记为“可训练” ，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(初始值)</span><br><span class="line">w = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="TensorFlow中的数学运算"><a href="#TensorFlow中的数学运算" class="headerlink" title="TensorFlow中的数学运算"></a>TensorFlow中的数学运算</h2><ul>
<li>对应元素的四则运算： <code>tf.add</code>, <code>tf.subtract</code>, <code>tf.multiply</code>, <code>tf.divide</code></li>
<li>平方、次方与开方： <code>tf.square</code>, <code>tf.pow</code>, <code>tf.sqrt</code></li>
<li>矩阵乘： <code>tf.matmul</code></li>
</ul>
<h3 id="对应元素的四则运算"><a href="#对应元素的四则运算" class="headerlink" title="对应元素的四则运算"></a>对应元素的四则运算</h3><ul>
<li>实现两个张量的对应元素相加</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.add(张量<span class="number">1</span>, 张量<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>实现两个张量的对应元素相减</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.subtract(张量<span class="number">1</span>, 张量<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>实现两个张量的对应元素相乘</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.multiply(张量<span class="number">1</span>, 张量<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>实现两个张量的对应元素相除</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.divide(张量<span class="number">1</span>, 张量<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>只有维度相同的张量才可以做四则运算</p>
</blockquote>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.fill([<span class="number">1</span>, <span class="number">3</span>], <span class="number">3.</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(tf.add(a,b))</span><br><span class="line"><span class="built_in">print</span>(tf.subtract(a,b))</span><br><span class="line"><span class="built_in">print</span>(tf.multiply(a,b))</span><br><span class="line"><span class="built_in">print</span>(tf.divide(b,a))</span><br></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)</span><br><span class="line"></span><br><span class="line">tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32</span><br><span class="line"></span><br><span class="line">tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)</span><br><span class="line"></span><br><span class="line">tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)</span><br><span class="line"></span><br><span class="line">tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)</span><br><span class="line"></span><br><span class="line">tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="平方、次方与开方"><a href="#平方、次方与开方" class="headerlink" title="平方、次方与开方"></a>平方、次方与开方</h3><ul>
<li>计算某个张量的平方</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.square(张量名)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算某个张量的n次方</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="built_in">pow</span>(张量名, n次方数)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算某个张量的开方</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.sqrt(张量名）</span><br></pre></td></tr></table></figure>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.fill([<span class="number">1</span>, <span class="number">2</span>], <span class="number">3.</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(tf.<span class="built_in">pow</span>(a, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(tf.square(a))</span><br><span class="line"><span class="built_in">print</span>(tf.sqrt(a))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)</span><br><span class="line">tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)</span><br><span class="line">tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)</span><br><span class="line">tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="矩阵乘-tf-matmul"><a href="#矩阵乘-tf-matmul" class="headerlink" title="矩阵乘 tf.matmul"></a>矩阵乘 tf.matmul</h3><ul>
<li>实现两个矩阵的相乘</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.matmul(矩阵<span class="number">1</span>, 矩阵<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.fill([<span class="number">2</span>, <span class="number">3</span>], <span class="number">3.</span>)</span><br><span class="line"><span class="built_in">print</span>(tf.matmul(a, b))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<p>[3, 2] * [2 ,3] = [3, 3]<br>1 1  3 3 3<br>1 1  3 3 3<br>1 1</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[6. 6. 6.]</span><br><span class="line">[6. 6. 6.]</span><br><span class="line">[6. 6. 6.]], shape=(3, 3), dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="tf-data-Dataset-from-tensor-slices"><a href="#tf-data-Dataset-from-tensor-slices" class="headerlink" title="tf.data.Dataset.from_tensor_slices"></a>tf.data.Dataset.from_tensor_slices</h2><p>神经网络在训练时，是将输入特征和标签配对后喂入网络的。</p>
<h3 id="切分传入张量的第一维度，生成输入特征-标签对，构建数据集"><a href="#切分传入张量的第一维度，生成输入特征-标签对，构建数据集" class="headerlink" title="切分传入张量的第一维度，生成输入特征/标签对，构建数据集"></a>切分传入张量的第一维度，生成输入特征/标签对，构建数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = tf.data.Dataset.from_tensor_slices((输入特征, 标签))</span><br></pre></td></tr></table></figure>
<p>（Numpy和Tensor格式都可用该语句读入数据）</p>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 收集特征</span></span><br><span class="line">features = tf.constant([<span class="number">12</span>,<span class="number">23</span>,<span class="number">10</span>,<span class="number">17</span>])</span><br><span class="line"><span class="comment"># 对应标签</span></span><br><span class="line">labels = tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 打特征和标签</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((features, labels))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> dataset:</span><br><span class="line">	<span class="built_in">print</span>(element)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">(特征, 标签) 配对</span></span><br><span class="line">&lt;TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32))&gt;</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: id=9, shape=(), dtype=int32, numpy=12&gt;, &lt;tf.Tensor: id=10, shape=(), dtype=int32, numpy=0&gt;)</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: id=11, shape=(), dtype=int32, numpy=23&gt;, &lt;tf.Tensor: id=12, shape=(), dtype=int32, numpy=1&gt;)</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: id=13, shape=(), dtype=int32, numpy=10&gt;, &lt;tf.Tensor: id=14, shape=(), dtype=int32, numpy=1&gt;)</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: id=15, shape=(), dtype=int32, numpy=17&gt;, &lt;tf.Tensor: id=16, shape=(), dtype=int32, numpy=0&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="张量梯度求导tf-GradientTape"><a href="#张量梯度求导tf-GradientTape" class="headerlink" title="张量梯度求导tf.GradientTape"></a>张量梯度求导tf.GradientTape</h2><p>with 结构中使用来实现对指定参数的求导运算<br>with 结构记录计算过程， gradient求出张量的梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape( ) <span class="keyword">as</span> tape:</span><br><span class="line">	若干个计算过程</span><br><span class="line">grad=tape.gradient(函数, 对谁求导)</span><br></pre></td></tr></table></figure>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape( ) <span class="keyword">as</span> tape:</span><br><span class="line">	w = tf.Variable(tf.constant(<span class="number">3.0</span>))</span><br><span class="line">	loss = tf.<span class="built_in">pow</span>(w,<span class="number">2</span>) <span class="comment">#loss=w^2 loss’=2w</span></span><br><span class="line">grad = tape.gradient(loss,w)</span><br><span class="line"><span class="built_in">print</span>(grad)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">\frac{ \sigma w^{2} } { \sigma w } = 2 w = 2 * 3.0 = 6.0</script><p>运行结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(6.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="枚举enumerate"><a href="#枚举enumerate" class="headerlink" title="枚举enumerate"></a>枚举enumerate</h2><p><code>enumerate</code>是python的内建函数，它可遍历每个元素(如列表、元组或字符串)， 组合为：索引 元素，常在for循环中使用。</p>
<p>可以在元素前配上对应的索引号</p>
<p><code>enumerate(列表名)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seq = [<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, element <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">	<span class="built_in">print</span>(i, element)</span><br></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0 one</span><br><span class="line">1 two</span><br><span class="line">2 three</span><br></pre></td></tr></table></figure>
<h2 id="独热编码tf-one-hot"><a href="#独热编码tf-one-hot" class="headerlink" title="独热编码tf.one_hot"></a>独热编码tf.one_hot</h2><p><strong>独热编码（one-hot encoding</strong>：在分类问题中，常用独热码做标签，标记类别： 1表示是， 0表示非。</p>
<p>例如：0狗尾草鸢尾 1杂色鸢尾 2弗吉尼亚鸢尾 三种<br>对于标签1，对应独热码是(0. 1. 0.)</p>
<p>即标签1，0%概率是0狗尾草鸢尾，100%概率是1杂色鸢尾，0%概率是2弗吉尼亚鸢尾</p>
<p><code>tf.one_hot()</code>函数将待转换数据，转换为one-hot形式的数据输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.one_hot(待转换数据, depth=几分类)</span><br></pre></td></tr></table></figure>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">classes = <span class="number">3</span></span><br><span class="line">labels = tf.constant([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]) <span class="comment"># 输入的元素值最小为0, 最大为2</span></span><br><span class="line">output = tf.one_hot(labels, depth=classes)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[0. 1. 0.]</span><br><span class="line">[1. 0. 0.]</span><br><span class="line">[0. 0. 1.]], shape=(3, 3), dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="正态分布tf-nn-softmax"><a href="#正态分布tf-nn-softmax" class="headerlink" title="正态分布tf.nn.softmax"></a>正态分布tf.nn.softmax</h2><img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%8C%E6%88%90%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%ADsoftmax.png" class="" title="神经网络完成前向传播softmax">
<p>神经网络完成前向传播，计算出了每一种类型的可能性大小，数字在符合概率分布后才可以和独热码的标签作比较。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%8C%E6%88%90%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%ADsoftmax2.png" class="" title="神经网络完成前向传播softmax2">
<script type="math/tex; mode=display">Softmax(y_{i}) = \frac{e^{y_{i}}}{\sum_{j=0}^{n} e^{y_{i}}}</script><p><code>tf.nn.softmax(x)</code> 使输出符合概率分布</p>
<p>当n分类的n个输出</p>
<script type="math/tex; mode=display">( y_{ 0 }, y_{1},  ……, y_{ n  - 1})</script><p>通过 <code>softmax()</code> 函数，便符合概率分布了。</p>
<p>即每个输出值变为0~1之间的概率值。</p>
<script type="math/tex; mode=display">\forall x P(X = x) \in [0, 1] $$ 且 $$ \sum_{x} P(X = x) = 1</script><ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = tf.constant ( [<span class="number">1.01</span>, <span class="number">2.01</span>, -<span class="number">0.66</span>] )</span><br><span class="line">y_pro = tf.nn.softmax(y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After softmax, y_pro is:&quot;</span>, y_pro)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">After softmax, y_pro <span class="keyword">is</span>: tf.Tensor([<span class="number">0.25598174</span> <span class="number">0.69583046</span> <span class="number">0.0481878</span>], shape=(<span class="number">3</span>, ), dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="参数自更新assign-sub"><a href="#参数自更新assign-sub" class="headerlink" title="参数自更新assign_sub"></a>参数自更新assign_sub</h2><ul>
<li>赋值操作，更新参数的值并返回。</li>
<li>调用<code>assign_sub</code>前，先用<code>tf.Variable</code>定义变量 w 为可训练（可自更新）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w.assign_sub(w要自减的内容)</span><br></pre></td></tr></table></figure>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># w -= 1 即 w = w - 1</span></span><br><span class="line">w.assign_sub(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(w)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable &#x27;Variable:0&#x27; shape=() dtype=int32, numpy=3&gt;</span><br></pre></td></tr></table></figure>
<h2 id="返回最大索引tf-argmax"><a href="#返回最大索引tf-argmax" class="headerlink" title="返回最大索引tf.argmax"></a>返回最大索引tf.argmax</h2><p>返回张量沿指定维度最大值的索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># axis=0经度，纵向列</span></span><br><span class="line"><span class="comment"># axis=1纬度，横向行</span></span><br><span class="line">tf.argmax(张量名, axis=操作轴)</span><br></pre></td></tr></table></figure>
<ul>
<li>案例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">test = np.array(</span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], </span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], </span><br><span class="line">[<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>], </span><br><span class="line">[<span class="number">8</span>, <span class="number">7</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test)</span><br><span class="line"><span class="built_in">print</span>(tf.argmax(test, axis=<span class="number">0</span>)) <span class="comment">#返回每一列（经度）最大值的索引</span></span><br><span class="line"><span class="built_in">print</span>(tf.argmax(test, axis=<span class="number">1</span>)) <span class="comment">#返回每一行（纬度）最大值的索引</span></span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[1 2 3]</span><br><span class="line">[2 3 4]</span><br><span class="line">[5 4 3]</span><br><span class="line">[8 7 2]]</span><br><span class="line"></span><br><span class="line">tf.Tensor([3 3 1], shape=(3, ), dtype=int64)</span><br><span class="line">tf.Tensor([2 2 0 0], shape=(4, ), dtype=int64)</span><br></pre></td></tr></table></figure>
<h1 id="鸢尾花数据集读入"><a href="#鸢尾花数据集读入" class="headerlink" title="鸢尾花数据集读入"></a>鸢尾花数据集读入</h1><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>共有数据150组，每组包括花萼长、花萼宽、花瓣长、花瓣宽4个输入特征。<br>同时给出了，这一组特征对应的鸢尾花类别。<br>类别包括Setosa Iris（狗尾草鸢尾）， Versicolour Iris（杂色鸢尾）， Virginica Iris（弗吉尼亚鸢尾）三类，分别用数字0， 1， 2表示。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D.png" class="" title="鸢尾花数据集介绍">
<h2 id="读入数据集"><a href="#读入数据集" class="headerlink" title="读入数据集"></a>读入数据集</h2><p>从<code>sklearn</code>包<code>datasets</code>读入数据集，语法为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">x_data = datasets.load_iris().data <span class="comment"># 返回iris数据集所有输入特征</span></span><br><span class="line">y_data = datasets.load_iris().target <span class="comment"># 返回iris数据集所有标签</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">x_data = datasets.load_iris().data  <span class="comment"># .data返回iris数据集所有输入特征</span></span><br><span class="line">y_data = datasets.load_iris().target  <span class="comment"># .target返回iris数据集所有标签</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data from datasets: \n&quot;</span>, x_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_data from datasets: \n&quot;</span>, y_data)</span><br><span class="line"></span><br><span class="line">x_data = DataFrame(x_data, columns=[<span class="string">&#x27;花萼长度&#x27;</span>, <span class="string">&#x27;花萼宽度&#x27;</span>, <span class="string">&#x27;花瓣长度&#x27;</span>, <span class="string">&#x27;花瓣宽度&#x27;</span>]) <span class="comment"># 为表格增加行索引（左侧）和列标签（上方）</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.unicode.east_asian_width&#x27;</span>, <span class="literal">True</span>)  <span class="comment"># 设置列名对齐</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data add index: \n&quot;</span>, x_data)</span><br><span class="line"></span><br><span class="line">x_data[<span class="string">&#x27;类别&#x27;</span>] = y_data  <span class="comment"># 新加一列，列标签为‘类别’，数据为y_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data add a column: \n&quot;</span>, x_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#类型维度不确定时，建议用print函数打印出来确认效果</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line">x_data from datasets: </span><br><span class="line"> [[5.1 3.5 1.4 0.2]</span><br><span class="line"> [4.9 3.  1.4 0.2]</span><br><span class="line"> [4.7 3.2 1.3 0.2]</span><br><span class="line"> [4.6 3.1 1.5 0.2]</span><br><span class="line"> [5.  3.6 1.4 0.2]</span><br><span class="line"> [5.4 3.9 1.7 0.4]</span><br><span class="line"> [4.6 3.4 1.4 0.3]</span><br><span class="line"> [5.  3.4 1.5 0.2]</span><br><span class="line"> [4.4 2.9 1.4 0.2]</span><br><span class="line"> [4.9 3.1 1.5 0.1]</span><br><span class="line"> [5.4 3.7 1.5 0.2]</span><br><span class="line"> [4.8 3.4 1.6 0.2]</span><br><span class="line"> [4.8 3.  1.4 0.1]</span><br><span class="line"> [4.3 3.  1.1 0.1]</span><br><span class="line"> [5.8 4.  1.2 0.2]</span><br><span class="line"> [5.7 4.4 1.5 0.4]</span><br><span class="line"> [5.4 3.9 1.3 0.4]</span><br><span class="line"> [5.1 3.5 1.4 0.3]</span><br><span class="line"> [5.7 3.8 1.7 0.3]</span><br><span class="line"> [5.1 3.8 1.5 0.3]</span><br><span class="line"> [5.4 3.4 1.7 0.2]</span><br><span class="line"> [5.1 3.7 1.5 0.4]</span><br><span class="line"> [4.6 3.6 1.  0.2]</span><br><span class="line"> [5.1 3.3 1.7 0.5]</span><br><span class="line"> [4.8 3.4 1.9 0.2]</span><br><span class="line"> [5.  3.  1.6 0.2]</span><br><span class="line"> [5.  3.4 1.6 0.4]</span><br><span class="line"> [5.2 3.5 1.5 0.2]</span><br><span class="line"> [5.2 3.4 1.4 0.2]</span><br><span class="line"> [4.7 3.2 1.6 0.2]</span><br><span class="line"> [4.8 3.1 1.6 0.2]</span><br><span class="line"> [5.4 3.4 1.5 0.4]</span><br><span class="line"> [5.2 4.1 1.5 0.1]</span><br><span class="line"> [5.5 4.2 1.4 0.2]</span><br><span class="line"> [4.9 3.1 1.5 0.2]</span><br><span class="line"> [5.  3.2 1.2 0.2]</span><br><span class="line"> [5.5 3.5 1.3 0.2]</span><br><span class="line"> [4.9 3.6 1.4 0.1]</span><br><span class="line"> [4.4 3.  1.3 0.2]</span><br><span class="line"> [5.1 3.4 1.5 0.2]</span><br><span class="line"> [5.  3.5 1.3 0.3]</span><br><span class="line"> [4.5 2.3 1.3 0.3]</span><br><span class="line"> [4.4 3.2 1.3 0.2]</span><br><span class="line"> [5.  3.5 1.6 0.6]</span><br><span class="line"> [5.1 3.8 1.9 0.4]</span><br><span class="line"> [4.8 3.  1.4 0.3]</span><br><span class="line"> [5.1 3.8 1.6 0.2]</span><br><span class="line"> [4.6 3.2 1.4 0.2]</span><br><span class="line"> [5.3 3.7 1.5 0.2]</span><br><span class="line"> [5.  3.3 1.4 0.2]</span><br><span class="line"> [7.  3.2 4.7 1.4]</span><br><span class="line"> [6.4 3.2 4.5 1.5]</span><br><span class="line"> [6.9 3.1 4.9 1.5]</span><br><span class="line"> [5.5 2.3 4.  1.3]</span><br><span class="line"> [6.5 2.8 4.6 1.5]</span><br><span class="line"> [5.7 2.8 4.5 1.3]</span><br><span class="line"> [6.3 3.3 4.7 1.6]</span><br><span class="line"> [4.9 2.4 3.3 1. ]</span><br><span class="line"> [6.6 2.9 4.6 1.3]</span><br><span class="line"> [5.2 2.7 3.9 1.4]</span><br><span class="line"> [5.  2.  3.5 1. ]</span><br><span class="line"> [5.9 3.  4.2 1.5]</span><br><span class="line"> [6.  2.2 4.  1. ]</span><br><span class="line"> [6.1 2.9 4.7 1.4]</span><br><span class="line"> [5.6 2.9 3.6 1.3]</span><br><span class="line"> [6.7 3.1 4.4 1.4]</span><br><span class="line"> [5.6 3.  4.5 1.5]</span><br><span class="line"> [5.8 2.7 4.1 1. ]</span><br><span class="line"> [6.2 2.2 4.5 1.5]</span><br><span class="line"> [5.6 2.5 3.9 1.1]</span><br><span class="line"> [5.9 3.2 4.8 1.8]</span><br><span class="line"> [6.1 2.8 4.  1.3]</span><br><span class="line"> [6.3 2.5 4.9 1.5]</span><br><span class="line"> [6.1 2.8 4.7 1.2]</span><br><span class="line"> [6.4 2.9 4.3 1.3]</span><br><span class="line"> [6.6 3.  4.4 1.4]</span><br><span class="line"> [6.8 2.8 4.8 1.4]</span><br><span class="line"> [6.7 3.  5.  1.7]</span><br><span class="line"> [6.  2.9 4.5 1.5]</span><br><span class="line"> [5.7 2.6 3.5 1. ]</span><br><span class="line"> [5.5 2.4 3.8 1.1]</span><br><span class="line"> [5.5 2.4 3.7 1. ]</span><br><span class="line"> [5.8 2.7 3.9 1.2]</span><br><span class="line"> [6.  2.7 5.1 1.6]</span><br><span class="line"> [5.4 3.  4.5 1.5]</span><br><span class="line"> [6.  3.4 4.5 1.6]</span><br><span class="line"> [6.7 3.1 4.7 1.5]</span><br><span class="line"> [6.3 2.3 4.4 1.3]</span><br><span class="line"> [5.6 3.  4.1 1.3]</span><br><span class="line"> [5.5 2.5 4.  1.3]</span><br><span class="line"> [5.5 2.6 4.4 1.2]</span><br><span class="line"> [6.1 3.  4.6 1.4]</span><br><span class="line"> [5.8 2.6 4.  1.2]</span><br><span class="line"> [5.  2.3 3.3 1. ]</span><br><span class="line"> [5.6 2.7 4.2 1.3]</span><br><span class="line"> [5.7 3.  4.2 1.2]</span><br><span class="line"> [5.7 2.9 4.2 1.3]</span><br><span class="line"> [6.2 2.9 4.3 1.3]</span><br><span class="line"> [5.1 2.5 3.  1.1]</span><br><span class="line"> [5.7 2.8 4.1 1.3]</span><br><span class="line"> [6.3 3.3 6.  2.5]</span><br><span class="line"> [5.8 2.7 5.1 1.9]</span><br><span class="line"> [7.1 3.  5.9 2.1]</span><br><span class="line"> [6.3 2.9 5.6 1.8]</span><br><span class="line"> [6.5 3.  5.8 2.2]</span><br><span class="line"> [7.6 3.  6.6 2.1]</span><br><span class="line"> [4.9 2.5 4.5 1.7]</span><br><span class="line"> [7.3 2.9 6.3 1.8]</span><br><span class="line"> [6.7 2.5 5.8 1.8]</span><br><span class="line"> [7.2 3.6 6.1 2.5]</span><br><span class="line"> [6.5 3.2 5.1 2. ]</span><br><span class="line"> [6.4 2.7 5.3 1.9]</span><br><span class="line"> [6.8 3.  5.5 2.1]</span><br><span class="line"> [5.7 2.5 5.  2. ]</span><br><span class="line"> [5.8 2.8 5.1 2.4]</span><br><span class="line"> [6.4 3.2 5.3 2.3]</span><br><span class="line"> [6.5 3.  5.5 1.8]</span><br><span class="line"> [7.7 3.8 6.7 2.2]</span><br><span class="line"> [7.7 2.6 6.9 2.3]</span><br><span class="line"> [6.  2.2 5.  1.5]</span><br><span class="line"> [6.9 3.2 5.7 2.3]</span><br><span class="line"> [5.6 2.8 4.9 2. ]</span><br><span class="line"> [7.7 2.8 6.7 2. ]</span><br><span class="line"> [6.3 2.7 4.9 1.8]</span><br><span class="line"> [6.7 3.3 5.7 2.1]</span><br><span class="line"> [7.2 3.2 6.  1.8]</span><br><span class="line"> [6.2 2.8 4.8 1.8]</span><br><span class="line"> [6.1 3.  4.9 1.8]</span><br><span class="line"> [6.4 2.8 5.6 2.1]</span><br><span class="line"> [7.2 3.  5.8 1.6]</span><br><span class="line"> [7.4 2.8 6.1 1.9]</span><br><span class="line"> [7.9 3.8 6.4 2. ]</span><br><span class="line"> [6.4 2.8 5.6 2.2]</span><br><span class="line"> [6.3 2.8 5.1 1.5]</span><br><span class="line"> [6.1 2.6 5.6 1.4]</span><br><span class="line"> [7.7 3.  6.1 2.3]</span><br><span class="line"> [6.3 3.4 5.6 2.4]</span><br><span class="line"> [6.4 3.1 5.5 1.8]</span><br><span class="line"> [6.  3.  4.8 1.8]</span><br><span class="line"> [6.9 3.1 5.4 2.1]</span><br><span class="line"> [6.7 3.1 5.6 2.4]</span><br><span class="line"> [6.9 3.1 5.1 2.3]</span><br><span class="line"> [5.8 2.7 5.1 1.9]</span><br><span class="line"> [6.8 3.2 5.9 2.3]</span><br><span class="line"> [6.7 3.3 5.7 2.5]</span><br><span class="line"> [6.7 3.  5.2 2.3]</span><br><span class="line"> [6.3 2.5 5.  1.9]</span><br><span class="line"> [6.5 3.  5.2 2. ]</span><br><span class="line"> [6.2 3.4 5.4 2.3]</span><br><span class="line"> [5.9 3.  5.1 1.8]]</span><br><span class="line">y_data from datasets: </span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span><br><span class="line"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2</span><br><span class="line"> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span><br><span class="line"> 2 2]</span><br><span class="line">x_data add index: </span><br><span class="line">      花萼长度  花萼宽度  花瓣长度  花瓣宽度</span><br><span class="line">0         5.1       3.5       1.4       0.2</span><br><span class="line">1         4.9       3.0       1.4       0.2</span><br><span class="line">2         4.7       3.2       1.3       0.2</span><br><span class="line">3         4.6       3.1       1.5       0.2</span><br><span class="line">4         5.0       3.6       1.4       0.2</span><br><span class="line">..        ...       ...       ...       ...</span><br><span class="line">145       6.7       3.0       5.2       2.3</span><br><span class="line">146       6.3       2.5       5.0       1.9</span><br><span class="line">147       6.5       3.0       5.2       2.0</span><br><span class="line">148       6.2       3.4       5.4       2.3</span><br><span class="line">149       5.9       3.0       5.1       1.8</span><br><span class="line">[150 rows x 4 columns]</span><br><span class="line">x_data add a column: </span><br><span class="line">      花萼长度  花萼宽度  花瓣长度  花瓣宽度  类别</span><br><span class="line">0         5.1       3.5       1.4       0.2     0</span><br><span class="line">1         4.9       3.0       1.4       0.2     0</span><br><span class="line">2         4.7       3.2       1.3       0.2     0</span><br><span class="line">3         4.6       3.1       1.5       0.2     0</span><br><span class="line">4         5.0       3.6       1.4       0.2     0</span><br><span class="line">..        ...       ...       ...       ...   ...</span><br><span class="line">145       6.7       3.0       5.2       2.3     2</span><br><span class="line">146       6.3       2.5       5.0       1.9     2</span><br><span class="line">147       6.5       3.0       5.2       2.0     2</span><br><span class="line">148       6.2       3.4       5.4       2.3     2</span><br><span class="line">149       5.9       3.0       5.1       1.8     2</span><br><span class="line">[150 rows x 5 columns]</span><br></pre></td></tr></table></figure>
<h1 id="神经网络实现鸢尾花分类"><a href="#神经网络实现鸢尾花分类" class="headerlink" title="神经网络实现鸢尾花分类"></a>神经网络实现鸢尾花分类</h1><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><h3 id="数据集读入"><a href="#数据集读入" class="headerlink" title="数据集读入"></a>数据集读入</h3><ul>
<li>从sklearn包datasets 读入数据集：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> datasets</span><br><span class="line">x_data = datasets.load_iris().data <span class="comment"># 返回iris数据集所有输入特征</span></span><br><span class="line">y_data = datasets.load_iris().target <span class="comment"># 返回iris数据集所有标签</span></span><br></pre></td></tr></table></figure>
<h3 id="数据集乱序"><a href="#数据集乱序" class="headerlink" title="数据集乱序"></a>数据集乱序</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">116</span>) <span class="comment"># 使用相同的seed，使输入特征/标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br></pre></td></tr></table></figure>
<h3 id="生成训练集和测试集（即-x-train-y-train-x-test-y-test）"><a href="#生成训练集和测试集（即-x-train-y-train-x-test-y-test）" class="headerlink" title="生成训练集和测试集（即 x_train / y_train , x_test / y_test）"></a>生成训练集和测试集（即 x_train / y_train , x_test / y_test）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = x_data[:-<span class="number">30</span>]</span><br><span class="line">y_train = y_data[:-<span class="number">30</span>]</span><br><span class="line">x_test = x_data[-<span class="number">30</span>:]</span><br><span class="line">y_test = y_data[-<span class="number">30</span>:]</span><br></pre></td></tr></table></figure>
<h3 id="配成-（输入特征，标签）-对，每次读入一小撮（batch）"><a href="#配成-（输入特征，标签）-对，每次读入一小撮（batch）" class="headerlink" title="配成 （输入特征，标签） 对，每次读入一小撮（batch）"></a>配成 （输入特征，标签） 对，每次读入一小撮（batch）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<h2 id="搭建网络-1"><a href="#搭建网络-1" class="headerlink" title="搭建网络"></a>搭建网络</h2><h3 id="定义神经网路中所有可训练参数"><a href="#定义神经网路中所有可训练参数" class="headerlink" title="定义神经网路中所有可训练参数"></a>定义神经网路中所有可训练参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.Variable(tf.random.truncated_normal([ <span class="number">4</span>, <span class="number">3</span> ], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([ <span class="number">3</span> ], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="参数优化"><a href="#参数优化" class="headerlink" title="参数优化"></a>参数优化</h2><h3 id="嵌套循环迭代，-with结构更新参数，显示当前loss"><a href="#嵌套循环迭代，-with结构更新参数，显示当前loss" class="headerlink" title="嵌套循环迭代， with结构更新参数，显示当前loss"></a>嵌套循环迭代， with结构更新参数，显示当前loss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch): <span class="comment">#数据集级别迭代</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span> (train_db): <span class="comment">#batch级别迭代</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment"># 记录梯度信息</span></span><br><span class="line">        <span class="comment"># 前向传播过程计算y</span></span><br><span class="line">        <span class="comment"># 计算总loss</span></span><br><span class="line">        grads = tape.gradient(loss, [ w1, b1 ])</span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>]) <span class="comment">#参数自更新</span></span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_all/<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<h2 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h2><h3 id="计算当前参数前向传播后的准确率，显示当前acc"><a href="#计算当前参数前向传播后的准确率，显示当前acc" class="headerlink" title="计算当前参数前向传播后的准确率，显示当前acc"></a>计算当前参数前向传播后的准确率，显示当前acc</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">    y = tf.matmul(h, w) + b <span class="comment"># y为预测结果</span></span><br><span class="line">    y = tf.nn.softmax(y) <span class="comment"># y符合概率分布</span></span><br><span class="line">    pred = tf.argmax(y, axis=<span class="number">1</span>) <span class="comment"># 返回y中最大值的索引，即预测的分类</span></span><br><span class="line">    pred = tf.cast(pred, dtype=y_test.dtype) <span class="comment">#调整数据类型与标签一致</span></span><br><span class="line">    correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">    correct = tf.reduce_sum (correct) <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">    total_correct += <span class="built_in">int</span> (correct) <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">    total_number += x_test.shape [<span class="number">0</span>]</span><br><span class="line">acc = total_correct / total_number</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;test_acc:&quot;</span>, acc)</span><br></pre></td></tr></table></figure>
<h2 id="acc-loss可视化"><a href="#acc-loss可视化" class="headerlink" title="acc / loss可视化"></a>acc / loss可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>) <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>) <span class="comment"># x轴名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>) <span class="comment"># y轴名称</span></span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy$&quot;</span>) <span class="comment"># 逐点画出test_acc值并连线</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="安装tensorflow"><a href="#安装tensorflow" class="headerlink" title="安装tensorflow"></a>安装tensorflow</h1><p>1、安装Anaconda（Python3.7版本）<br>2、安装Pycharm<br>3、打开Anaconda Powershell Prompt<br>4、安装软件包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">conda create -n TF2.1 python=3.7</span><br><span class="line"></span><br><span class="line">conda activate TF2.1</span><br><span class="line">conda install cudatoolkit=10.1</span><br><span class="line"></span><br><span class="line">conda install cudnn=7.6</span><br><span class="line"></span><br><span class="line">pip install tensorflow==2.1</span><br><span class="line"></span><br><span class="line">python</span><br><span class="line">import tensorflow as tf</span><br><span class="line">tf.__version__</span><br></pre></td></tr></table></figure>
<p>5、配置Pycharm</p>
<p>6、使用GPU</p>
<p>前提在本机安装nvidia驱动和cuda版本，CUDA版本向下兼容，可以安装最新的。</p>
<img src="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97/tensorflowGUP%E7%89%88%E6%9C%AC.png" class="" title="tensorflowGUP版本">
<p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/install/source_windows?hl=zh-cn">tensorflow-gpu对应python,cuda,cudnn版本查看链接</a></p>
<p>在conda虚拟环境中安装CUDA驱动库。<br>python3.7<br>tensorflow2.1<br>cudatoolkit10.1<br>cudnn7.6</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda install cudatoolkit=10.1</span><br><span class="line">conda install cudnn=7.6</span><br><span class="line">conda install tensorflow-gpu==2.1</span><br><span class="line"></span><br><span class="line">pip install --upgrade keras</span><br></pre></td></tr></table></figure>
<p>保险起见，可以先定cuda号然后让conda搜索</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda search tensorflow-gpu</span><br><span class="line">conda install tensorflow-gpu==?.?.?</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或者直接让conda解决</span></span><br><span class="line">conda install tensorflow-gpu</span><br></pre></td></tr></table></figure>
<p>运行测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;tf version: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tf.__version__))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;keras version: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tf.keras.__version__))</span><br><span class="line">    <span class="comment"># print(&#x27;GPU: &#123;&#125;&#x27;.format(tf.test.is_gpu_available())) # 1.x版本的TF使用此行</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;GPU: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tf.config.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>))) <span class="comment"># 2.x版本的TF使用</span></span><br></pre></td></tr></table></figure></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2023/02/16/人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算/">http://hibiscidai.com/2023/02/16/人工智能实践-Tensorflow笔记-MOOC-第一讲神经网络计算/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/TensorflowMOOC/">TensorflowMOOC</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2023/10/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D-%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6/"><i class="fa fa-chevron-left">  </i><span>梯度下降-最速下降-共轭梯度</span></a></div><div class="next-post pull-right"><a href="/2023/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5-Tensorflow%E7%AC%94%E8%AE%B0-MOOC-%E7%AC%AC%E4%B8%89%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1/"><span>人工智能实践-Tensorflow笔记-MOOC-第三讲神经网络八股</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://www.paofu.cloud/auth/register?code=j4I7">好用、实惠、稳定的梯子,点击这里<img src="https://pic.imgdb.cn/item/65572abac458853aefef30cd.png" width="1000" height="124" object-fit="cover" ></a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2024 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>