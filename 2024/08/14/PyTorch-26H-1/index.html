<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="PyTorch-26H-1"><meta name="keywords" content="学习笔记,PyTorch"><meta name="author" content="HibisciDai"><meta name="copyright" content="HibisciDai"><title>PyTorch-26H-1 | HibisciDai</title><link rel="shortcut icon" href="/img/avatar.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="HibisciDai" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-26H-1"><span class="toc-number">1.</span> <span class="toc-text">PyTorch-26H-1</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter-0-%E2%80%93-PyTorch-Fundamentals"><span class="toc-number">2.</span> <span class="toc-text">Chapter 0 – PyTorch Fundamentals</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-deep-learning"><span class="toc-number">2.1.</span> <span class="toc-text">what is deep learning?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-use-machine-deep-learning"><span class="toc-number">2.2.</span> <span class="toc-text">Why use machine&#x2F;deep learning?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-number-one-rule-of-ML"><span class="toc-number">2.3.</span> <span class="toc-text">The number one rule of ML</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Machine-learning-vs-deep-learning"><span class="toc-number">2.4.</span> <span class="toc-text">Machine learning vs deep learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Anatomy-of-neural-networks"><span class="toc-number">2.5.</span> <span class="toc-text">Anatomy of neural networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Different-learning-paradigms"><span class="toc-number">2.6.</span> <span class="toc-text">Different learning paradigms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-can-deep-learning-be-used-for"><span class="toc-number">2.7.</span> <span class="toc-text">What can deep learning be used for?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-is-why-PyTorch"><span class="toc-number">2.8.</span> <span class="toc-text">What is&#x2F;why PyTorch?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-are-tensors"><span class="toc-number">2.9.</span> <span class="toc-text">What are tensors?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Outline"><span class="toc-number">2.10.</span> <span class="toc-text">Outline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-to-and-how-not-to-approach-this-course"><span class="toc-number">2.11.</span> <span class="toc-text">How to (and how not to) approach this course</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Important-resources"><span class="toc-number">2.12.</span> <span class="toc-text">Important resources</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Getting-setup"><span class="toc-number">2.13.</span> <span class="toc-text">Getting setup</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction-to-tensors"><span class="toc-number">2.14.</span> <span class="toc-text">Introduction to tensors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scalar%E2%86%92%E6%A0%87%E9%87%8F"><span class="toc-number">2.14.1.</span> <span class="toc-text">scalar→标量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vecotr%E2%86%92%E5%90%91%E9%87%8F"><span class="toc-number">2.14.2.</span> <span class="toc-text">vecotr→向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MATRIX%E2%86%92%E7%9F%A9%E9%98%B5"><span class="toc-number">2.14.3.</span> <span class="toc-text">MATRIX→矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TENSOR%E2%86%92%E5%BC%A0%E9%87%8F"><span class="toc-number">2.14.4.</span> <span class="toc-text">TENSOR→张量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Creating-tensors"><span class="toc-number">2.15.</span> <span class="toc-text">Creating tensors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Zeros-and-ones"><span class="toc-number">2.15.1.</span> <span class="toc-text">Zeros and ones</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-a-range-of-tensors-and-tensors-like"><span class="toc-number">2.15.2.</span> <span class="toc-text">Creating a range of tensors and tensors-like</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor-datatypes"><span class="toc-number">2.16.</span> <span class="toc-text">Tensor datatypes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor-attributes-information-about-tensors"><span class="toc-number">2.17.</span> <span class="toc-text">Tensor attributes (information about tensors)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Manipulating-tensors"><span class="toc-number">2.18.</span> <span class="toc-text">Manipulating tensors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#basic-operations"><span class="toc-number">2.18.1.</span> <span class="toc-text">basic operations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Matrix-multiplication"><span class="toc-number">2.18.2.</span> <span class="toc-text">Matrix multiplication</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%9C%80%E5%B8%B8%E8%A7%81%E7%9A%84%E9%94%99%E8%AF%AF%E4%B9%8B%E4%B8%80%EF%BC%88%E5%BD%A2%E7%8A%B6%E9%94%99%E8%AF%AF%EF%BC%89"><span class="toc-number">2.18.3.</span> <span class="toc-text">深度学习中最常见的错误之一（形状错误）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%83%8F%E8%BF%99%E6%A0%B7%E7%9A%84%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E4%B9%9F%E7%A7%B0%E4%B8%BA%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9A%84%E7%82%B9%E7%A7%AF%E3%80%82"><span class="toc-number">2.18.4.</span> <span class="toc-text">像这样的矩阵乘法也称为两个矩阵的点积。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Finding-the-min-max-mean-amp-sum"><span class="toc-number">2.19.</span> <span class="toc-text">Finding the min, max, mean &amp; sum</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E6%9C%80%E5%A4%A7%E5%B9%B3%E5%9D%87%E6%B1%82%E5%92%8C"><span class="toc-number">2.19.1.</span> <span class="toc-text">最小最大平均求和</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E6%89%BE%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E7%B4%A2%E5%BC%95%E4%BD%8D%E7%BD%AE"><span class="toc-number">2.19.2.</span> <span class="toc-text">查找最大最小索引位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E6%94%B9%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.19.3.</span> <span class="toc-text">更改数据类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E5%A1%91%E3%80%81%E8%A7%86%E5%9B%BE%E3%80%81%E5%A0%86%E5%8F%A0%E3%80%81%E5%8E%8B%E7%BC%A9%E3%80%81%E8%A7%A3%E5%8E%8B%E3%80%81%E7%BD%AE%E6%8D%A2"><span class="toc-number">2.20.</span> <span class="toc-text">重塑、视图、堆叠、压缩、解压、置换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reshaping-%E9%87%8D%E5%A1%91"><span class="toc-number">2.20.1.</span> <span class="toc-text">Reshaping-重塑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#View-%E8%A7%86%E5%9B%BE"><span class="toc-number">2.20.2.</span> <span class="toc-text">View-视图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stacking-%E5%A0%86%E5%8F%A0"><span class="toc-number">2.20.3.</span> <span class="toc-text">Stacking-堆叠</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-stack-%E5%92%8C-torch-vstack-%E5%92%8C-torch-hstack"><span class="toc-number">2.20.3.1.</span> <span class="toc-text">torch.stack 和 torch.vstack 和 torch.hstack</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Squeeze-%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.20.4.</span> <span class="toc-text">Squeeze-压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unsqueeze-%E8%A7%A3%E5%8E%8B"><span class="toc-number">2.20.5.</span> <span class="toc-text">Unsqueeze-解压</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Permute-%E7%BD%AE%E6%8D%A2"><span class="toc-number">2.20.6.</span> <span class="toc-text">Permute-置换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Selecting-data-indexing"><span class="toc-number">2.21.</span> <span class="toc-text">Selecting data (indexing)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-tensors-and-NumPy"><span class="toc-number">2.22.</span> <span class="toc-text">PyTorch tensors and NumPy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NumPy-array-to-tensor"><span class="toc-number">2.22.1.</span> <span class="toc-text">NumPy array to tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B9%E5%8F%98%E6%95%B0%E7%BB%84%EF%BC%8C%E5%BC%A0%E9%87%8F%E4%B8%8D%E5%8F%98"><span class="toc-number">2.22.2.</span> <span class="toc-text">改变数组，张量不变</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor-to-NumPy-array"><span class="toc-number">2.22.3.</span> <span class="toc-text">Tensor to NumPy array</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B9%E5%8F%98%E5%BC%A0%E9%87%8F%EF%BC%8C%E6%95%B0%E7%BB%84%E4%B8%8D%E5%8F%98"><span class="toc-number">2.22.4.</span> <span class="toc-text">改变张量，数组不变</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reproducibility"><span class="toc-number">2.23.</span> <span class="toc-text">Reproducibility</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8-GPU-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%BC%A0%E9%87%8F%EF%BC%88%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%9B%B4%E5%BF%AB%E7%9A%84%E8%AE%A1%E7%AE%97%EF%BC%89"><span class="toc-number">2.24.</span> <span class="toc-text">在 GPU 上运行张量（并进行更快的计算）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96GPU"><span class="toc-number">2.24.1.</span> <span class="toc-text">获取GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A9-PyTorch-%E5%9C%A8-GPU-%E4%B8%8A%E8%BF%90%E8%A1%8C"><span class="toc-number">2.24.2.</span> <span class="toc-text">让 PyTorch 在 GPU 上运行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5cuda%E5%8C%85"><span class="toc-number">2.24.2.1.</span> <span class="toc-text">检查cuda包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AEdevice"><span class="toc-number">2.24.2.2.</span> <span class="toc-text">设置device</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E4%B8%AAGPU%E8%AE%A1%E7%AE%97"><span class="toc-number">2.24.2.3.</span> <span class="toc-text">多个GPU计算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E5%BC%A0%E9%87%8F%EF%BC%88%E5%92%8C%E6%A8%A1%E5%9E%8B%EF%BC%89%E6%94%BE%E5%9C%A8-GPU-%E4%B8%8A"><span class="toc-number">2.24.3.</span> <span class="toc-text">将张量（和模型）放在 GPU 上</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%A0%E9%87%8F%E5%B9%B6%E5%B0%86%E5%85%B6%E6%94%BE%E5%9C%A8-GPU-%E4%B8%8A"><span class="toc-number">2.24.3.1.</span> <span class="toc-text">创建一个张量并将其放在 GPU 上</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E5%BC%A0%E9%87%8F%E7%A7%BB%E5%9B%9E-CPU"><span class="toc-number">2.24.4.</span> <span class="toc-text">将张量移回 CPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPU%E4%B8%8A%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90"><span class="toc-number">2.24.5.</span> <span class="toc-text">GPU上的随机种子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-number">2.25.</span> <span class="toc-text">练习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEpytroch%E7%8E%AF%E5%A2%83"><span class="toc-number">2.26.</span> <span class="toc-text">配置pytroch环境</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/Avatar.png"></div><div class="author-info__name text-center">HibisciDai</div><div class="author-info__description text-center">HibisciDai'Blog</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">244</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">88</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">33</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://github.com/HibisciDai/hexo-theme-melody">HexoTheme-github</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com/">molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/banner2.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HibisciDai</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/about">关于我</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">PyTorch-26H-1</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2024-08-14</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/PyTorch/">PyTorch</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">8.6k</span><span class="post-meta__separator">|</span><span>阅读时长: 40 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1.png" class="" title="PyTorch-26H-1">
<p>PyTorch-26H-1</p>
<span id="more"></span>
<h1 id="PyTorch-26H-1"><a href="#PyTorch-26H-1" class="headerlink" title="PyTorch-26H-1"></a>PyTorch-26H-1</h1><p>主页：<a target="_blank" rel="noopener" href="https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/">https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/</a></p>
<p>youtub：<a target="_blank" rel="noopener" href="https://youtu.be/V_xro1bcAuA">https://youtu.be/V_xro1bcAuA</a></p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning">https://github.com/mrdbourke/pytorch-deep-learning</a></p>
<p>Learn PyTorch for Deep Learning: Zero to Mastery book：<a target="_blank" rel="noopener" href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p>
<p>PyTorch documentation：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>
<h1 id="Chapter-0-–-PyTorch-Fundamentals"><a href="#Chapter-0-–-PyTorch-Fundamentals" class="headerlink" title="Chapter 0 – PyTorch Fundamentals"></a>Chapter 0 – PyTorch Fundamentals</h1><h2 id="what-is-deep-learning"><a href="#what-is-deep-learning" class="headerlink" title="what is deep learning?"></a>what is deep learning?</h2><p>Machine learning is turning things (data) into numbers and finding patterns in those numbers.</p>
<p>Deep Learning ∈ Machine Learning ∈ Aritfical Intelligence</p>
<p>传统程序：输入+规则→输出<br>机器学些：输入+输出→规则</p>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-1.png" class="" title="PyTorch-26H-1-1">
<h2 id="Why-use-machine-deep-learning"><a href="#Why-use-machine-deep-learning" class="headerlink" title="Why use machine/deep learning?"></a>Why use machine/deep learning?</h2><p>对于复杂的问题，无法找到所有的规则。</p>
<h2 id="The-number-one-rule-of-ML"><a href="#The-number-one-rule-of-ML" class="headerlink" title="The number one rule of ML"></a>The number one rule of ML</h2><p>“If you can build a simple rule-based system that doesn’t require machine learning, do that.”</p>
<ul>
<li><p>A wise software engineer… (actually rule 1 of Google’s Machine Learning Handbook)</p>
</li>
<li><p>What deep learning is good for</p>
</li>
</ul>
<p>Problems with long lists of rules- when the traditional approach fails, machine learning/deep learning may help.<br>规则列表过长的问题——当传统方法失败时，机器学习/深度学习可能会有所帮助。</p>
<p>Continually changing environments- deep learning can adapt (learn’) to new scenarios.<br>不断变化的环境——深度学习可以适应（学习）新场景。</p>
<p>Discovering insights within large collections of data- can you imagine trying to hand-craft rules for what 101 different kinds of food look like?<br>在大量数据中发现见解——你能想象尝试手工制定 101 种不同食物的规则吗？</p>
<ul>
<li>What deep learning is not good for</li>
</ul>
<p>When you need explainability- -the patterns learned by a deep learning model are typically uninterpretable by a human.<br>当你需要可解释性时——深度学习模型学习到的模式通常无法被人类解释。</p>
<p>When the traditional approach is a better option一if you can accomplish what you need with a simple rule-based system.<br>当传统方法是更好的选择时——如果你可以使用简单的基于规则的系统完成所需的工作。</p>
<p>When errors are unacceptable一since the outputs of deep learning model aren’t always predictable.<br>当错误不可接受时——因为深度学习模型的输出并不总是可预测的。</p>
<p>When you don’t have much data一deep learning models usually require a fairly large amount of data to produce great results.<br>当你没有太多数据时——深度学习模型通常需要相当大量的数据才能产生很好的结果。</p>
<h2 id="Machine-learning-vs-deep-learning"><a href="#Machine-learning-vs-deep-learning" class="headerlink" title="Machine learning vs deep learning"></a>Machine learning vs deep learning</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-2.png" class="" title="PyTorch-26H-1-2">
<ul>
<li>Machine learning</li>
</ul>
<p>适合处理结构化数据</p>
<p>常见算法：<br>Random forest 随机森林<br>Gradient boosted models 梯度提升模型<br>Naive Bayes 朴素贝叶斯<br>Nearest neighbour 最近邻<br>Support vector machine 支持向量机</p>
<ul>
<li>Deep learning</li>
</ul>
<p>适合处理非结构化数据</p>
<p>常见算法：<br>Neural networks 神经网络<br>Fully connected neural network 全连接神经网络<br>Convolutional neural network 卷积神经网络<br>Recurrent neuralnetwork 循环神经网络<br>Transformer</p>
<h2 id="Anatomy-of-neural-networks"><a href="#Anatomy-of-neural-networks" class="headerlink" title="Anatomy of neural networks"></a>Anatomy of neural networks</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-3.png" class="" title="PyTorch-26H-1-3">
<p>数据→数字→神经网络→权重→输出</p>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-4.png" class="" title="PyTorch-26H-1-4">
<h2 id="Different-learning-paradigms"><a href="#Different-learning-paradigms" class="headerlink" title="Different learning paradigms"></a>Different learning paradigms</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-5.png" class="" title="PyTorch-26H-1-5">
<p>监督学习：大量已知数据标注。</p>
<p>无监督学习：自动分析数据。</p>
<p>迁移学习：将学习到的模式嵌入到新的模型中。</p>
<p>强化学习reinforcement learning：奖励想要的结果。</p>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-6.png" class="" title="PyTorch-26H-1-6">
<h2 id="What-can-deep-learning-be-used-for"><a href="#What-can-deep-learning-be-used-for" class="headerlink" title="What can deep learning be used for?"></a>What can deep learning be used for?</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-7.png" class="" title="PyTorch-26H-1-7">
<p>CV</p>
<p>NATURAL LANGUAGE PROGRESS</p>
<p>SEQUENCE IN AND SEQUENCE OUT</p>
<h2 id="What-is-why-PyTorch"><a href="#What-is-why-PyTorch" class="headerlink" title="What is/why PyTorch?"></a>What is/why PyTorch?</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-8.png" class="" title="PyTorch-26H-1-8">
<p>Most popular research deep learning framework<br>最流行的深度学习研究框架</p>
<p>Write fast deep learning code in Python (able to run on a GPU/many GPUs)<br>用 Python 编写快速的深度学习代码（可在 GPU/多个 GPU 上运行）</p>
<p>Able to access many pre-built deep learning models (Torch Hub/torchvision.models)<br>能够访问许多预构建的深度学习模型（Torch Hub/torchvision.models）</p>
<p>Whole stack: preprocess data, model data, deploy model in your application/cloud<br>整个堆栈：预处理数据、建立数据模型、在应用程序/云中部署模型</p>
<p>Originally designed and used in-house by Facebook/Meta (now opensource and used by companies such as Tesla, Microsoft, OpenAI)<br>最初由 Facebook/Meta 内部设计和使用（现已开源，并被特斯拉、微软、OpenAI 等公司使用）</p>
<p><a href="www.paperswithcode.com/trends">paperswithcode</a></p>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-9.png" class="" title="PyTorch-26H-1-9">
<p>GPU(Graphics Processing Unit)</p>
<p>TPU(Tensor Processing Unit)</p>
<h2 id="What-are-tensors"><a href="#What-are-tensors" class="headerlink" title="What are tensors?"></a>What are tensors?</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-10.png" class="" title="PyTorch-26H-1-10">
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-11.png" class="" title="PyTorch-26H-1-11">
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>Now:</p>
<ul>
<li>PyTorch basics &amp; fundamentals (dealing with tensors and tensor operations)</li>
<li>PyTorch 基础和基本原理（处理张量和张量运算）</li>
</ul>
<p>Later:</p>
<ul>
<li>Preprocessing data (getting it into tensors)</li>
<li>预处理数据（将数据转化为张量）</li>
<li>Building and using pretrained deep learning models</li>
<li>构建和使用预训练的深度学习模型</li>
<li>Fitting a model to the data (learning patterns)</li>
<li>根据数据拟合模型（学习模式）</li>
<li>Making predictions with a model (using patterns)</li>
<li>使用模型进行预测（使用模式）</li>
<li>Evaluating model predictions</li>
<li>评估模型预测</li>
<li>Saving and loading models</li>
<li>保存和加载模型</li>
<li>Using a trained model to make predictions on custom data</li>
<li>使用训练好的模型对自定义数据进行预测</li>
</ul>
<p>一点科学，一点艺术。</p>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-12.png" class="" title="PyTorch-26H-1-12">
<h2 id="How-to-and-how-not-to-approach-this-course"><a href="#How-to-and-how-not-to-approach-this-course" class="headerlink" title="How to (and how not to) approach this course"></a>How to (and how not to) approach this course</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-13.png" class="" title="PyTorch-26H-1-13">
<p>1、code alone</p>
<p>2、explore and experiment</p>
<p>3、visuallize what you don’t understand</p>
<p>4、ask questions</p>
<p>5、do the exercises</p>
<p>6、share your work</p>
<h2 id="Important-resources"><a href="#Important-resources" class="headerlink" title="Important resources"></a>Important resources</h2><img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-14.png" class="" title="PyTorch-26H-1-14">
<p>课程资料：<br><a target="_blank" rel="noopener" href="https://www.github.com/mrdbourke/pytorch-deep-learning">Course materials</a></p>
<p>问答：<br><a target="_blank" rel="noopener" href="https://www.github.com/mrdbourke/pytorch-deep-learning/discussions">Course Q&amp;A</a></p>
<p>书籍：<br><a target="_blank" rel="noopener" href="https://learnpytorch.io">Course online book</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/">pytorch_org</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a></p>
<p>出问题去讨论区</p>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-15.png" class="" title="PyTorch-26H-1-15">
<h2 id="Getting-setup"><a href="#Getting-setup" class="headerlink" title="Getting setup"></a>Getting setup</h2><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/">Google colab</a></p>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-16.png" class="" title="PyTorch-26H-1-16">
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-17.png" class="" title="PyTorch-26H-1-17">
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-18.png" class="" title="PyTorch-26H-1-18">
<p><code>00_pytorch_fundamentals_vedio.ipynb</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello I&#x27;m excited to learn PyTorch!&quot;</span>)</span><br><span class="line"></span><br><span class="line">&gt;Hello I<span class="string">&#x27;m excited to learn PyTorch!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">!nvidia-smi</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Mon Aug 26 11:28:50 2024       </span></span><br><span class="line"><span class="string">+---------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="string">| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |</span></span><br><span class="line"><span class="string">|-----------------------------------------+----------------------+----------------------+</span></span><br><span class="line"><span class="string">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span></span><br><span class="line"><span class="string">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span></span><br><span class="line"><span class="string">|                                         |                      |               MIG M. |</span></span><br><span class="line"><span class="string">|=========================================+======================+======================|</span></span><br><span class="line"><span class="string">|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |</span></span><br><span class="line"><span class="string">| N/A   49C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |</span></span><br><span class="line"><span class="string">|                                         |                      |                  N/A |</span></span><br><span class="line"><span class="string">+-----------------------------------------+----------------------+----------------------+</span></span><br><span class="line"><span class="string">                                                                                         </span></span><br><span class="line"><span class="string">+---------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="string">| Processes:                                                                            |</span></span><br><span class="line"><span class="string">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span></span><br><span class="line"><span class="string">|        ID   ID                                                             Usage      |</span></span><br><span class="line"><span class="string">|=======================================================================================|</span></span><br><span class="line"><span class="string">|  No running processes found                                                           |</span></span><br><span class="line"><span class="string">+---------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">import torch</span></span><br><span class="line"><span class="string">import pandas as pd</span></span><br><span class="line"><span class="string">import numpy as np</span></span><br><span class="line"><span class="string">import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="string">print(torch.__version__)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&gt;2.3.1+cu121</span></span><br></pre></td></tr></table></figure>
<h2 id="Introduction-to-tensors"><a href="#Introduction-to-tensors" class="headerlink" title="Introduction to tensors"></a>Introduction to tensors</h2><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">torch.Tensor</a></p>
<p>A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.<br>张量是包含单一数据类型元素的多维矩阵。</p>
<h3 id="scalar→标量"><a href="#scalar→标量" class="headerlink" title="scalar→标量"></a><strong>scalar→标量</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scalar</span></span><br><span class="line">scalar = torch.tensor(<span class="number">7</span>)</span><br><span class="line">scalar</span><br><span class="line"><span class="comment">#&gt;tensor(7)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看张量的维度</span></span><br><span class="line">scalar.ndim</span><br><span class="line"><span class="comment">#&gt;0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#取回张量→int</span></span><br><span class="line">scalar.item()</span><br><span class="line"><span class="comment">#&gt;7</span></span><br></pre></td></tr></table></figure>
<h3 id="vecotr→向量"><a href="#vecotr→向量" class="headerlink" title="vecotr→向量"></a><strong>vecotr→向量</strong></h3><p>拥有大小和方向</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Vector</span></span><br><span class="line">vector = torch.tensor([<span class="number">7</span>,<span class="number">7</span>])</span><br><span class="line">vector</span><br><span class="line"><span class="comment">#&gt;tensor([7, 7])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看向量的维度</span></span><br><span class="line">vector.ndim</span><br><span class="line"><span class="comment">#&gt;1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看向量的形状</span></span><br><span class="line">vector.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([2])</span></span><br></pre></td></tr></table></figure>
<h3 id="MATRIX→矩阵"><a href="#MATRIX→矩阵" class="headerlink" title="MATRIX→矩阵"></a><strong>MATRIX→矩阵</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">MATRIX = torch.tensor([[<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">						[<span class="number">9</span>,<span class="number">10</span>]])</span><br><span class="line">MATRIX</span><br><span class="line"></span><br><span class="line"><span class="comment">#&gt;tensor([[ 7,  8],</span></span><br><span class="line"><span class="comment">#&gt;        [ 9, 10]])</span></span><br><span class="line"></span><br><span class="line">MATRIX.ndim</span><br><span class="line"><span class="comment">#&gt;2</span></span><br><span class="line"></span><br><span class="line">MATRIX[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([7, 8])</span></span><br><span class="line"></span><br><span class="line">MATRIX[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([ 9, 10])</span></span><br><span class="line"></span><br><span class="line">MATRIX.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([2, 2])</span></span><br></pre></td></tr></table></figure>
<h3 id="TENSOR→张量"><a href="#TENSOR→张量" class="headerlink" title="TENSOR→张量"></a><strong>TENSOR→张量</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">TENSOR = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">						[<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>],</span><br><span class="line">						[<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]]])</span><br><span class="line">TENSOR</span><br><span class="line"><span class="comment">#&gt;tensor([[[1, 2, 3],</span></span><br><span class="line"><span class="comment">#&gt;         [3, 6, 9],</span></span><br><span class="line"><span class="comment">#&gt;         [2, 4, 6]]])</span></span><br><span class="line"></span><br><span class="line">TENSOR.ndim</span><br><span class="line"><span class="comment">#&gt;3</span></span><br><span class="line"></span><br><span class="line">TENSOR.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([1, 3, 3])</span></span><br><span class="line"><span class="comment">## 一维 3x3形状的张量</span></span><br><span class="line"></span><br><span class="line">TENSOR[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#&gt;        [3, 6, 9],</span></span><br><span class="line"><span class="comment">#&gt;        [2, 4, 6]])</span></span><br></pre></td></tr></table></figure>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-19.png" class="" title="PyTorch-26H-1-19">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TENSOR = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">						[<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">						[<span class="number">2</span>, <span class="number">4</span>]]])</span><br><span class="line">TENSOR.ndim</span><br><span class="line"><span class="comment">#&gt;3</span></span><br><span class="line"></span><br><span class="line">TENSOR.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([1, 3, 2])</span></span><br><span class="line"></span><br><span class="line">TENSOR[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([[1, 2],</span></span><br><span class="line"><span class="comment">#&gt;        [3, 6],</span></span><br><span class="line"><span class="comment">#&gt;        [2, 4]])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">TENSOR = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">						[<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">						[<span class="number">2</span>, <span class="number">4</span>]],</span><br><span class="line">					   [[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">					    [<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">					    [<span class="number">2</span>, <span class="number">4</span>]]])</span><br><span class="line">TENSOR.ndim</span><br><span class="line"><span class="comment">#&gt;3</span></span><br><span class="line"></span><br><span class="line">TENSOR.shape</span><br><span class="line"><span class="comment">#&gt;torch.Size([2, 3, 2])</span></span><br><span class="line"></span><br><span class="line">TENSOR[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([[1, 2],</span></span><br><span class="line"><span class="comment">#&gt;        [3, 6],</span></span><br><span class="line"><span class="comment">#&gt;        [2, 4]])</span></span><br><span class="line"></span><br><span class="line">TENSOR[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#&gt;tensor([[1, 2],</span></span><br><span class="line"><span class="comment">#&gt;        [3, 6],</span></span><br><span class="line"><span class="comment">#&gt;        [2, 4]])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">TENSOR = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">						[<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">						[<span class="number">2</span>, <span class="number">4</span>]],</span><br><span class="line">					   [[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">					    [<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">					    [<span class="number">2</span>, <span class="number">4</span>],</span><br><span class="line">					    [<span class="number">2</span>, <span class="number">4</span>]]])</span><br><span class="line"><span class="comment"># ValueError: expected sequence of length 3 at dim 1 (got 4)</span></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">解释</th>
<th style="text-align:center">维度</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">scalar标量</td>
<td style="text-align:center">一个数字</td>
<td style="text-align:center">0</td>
<td style="text-align:center">Lower(a)</td>
</tr>
<tr>
<td style="text-align:center">vector向量</td>
<td style="text-align:center">带有方向的数字（例如带有方向的风速），但也可以有许多其他数字</td>
<td style="text-align:center">1</td>
<td style="text-align:center">Lower(y)</td>
</tr>
<tr>
<td style="text-align:center">matrix矩阵</td>
<td style="text-align:center">二维数字数组</td>
<td style="text-align:center">2</td>
<td style="text-align:center">Upper(Q)</td>
</tr>
<tr>
<td style="text-align:center">tensor张量</td>
<td style="text-align:center">n 维数字数组</td>
<td style="text-align:center">n,0 维张量是标量，1 维张量是矢量</td>
<td style="text-align:center">Upper(X)</td>
</tr>
</tbody>
</table>
</div>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-20.png" class="" title="PyTorch-26H-1-20">
<h2 id="Creating-tensors"><a href="#Creating-tensors" class="headerlink" title="Creating tensors"></a>Creating tensors</h2><p>Why random tensors?<br>Random tensors are important because the way many neural networks learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data .<br>随机张量很重要，因为许多神经网络的学习方式是从充满随机数的张量开始，然后调整这些随机数以更好地表示数据。</p>
<p>start with random numbersy → look at data → update random numbers → look at data → update random numbers</p>
<ul>
<li>创建随机张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a random tensor of size (3, 4)</span></span><br><span class="line">random_tensor = torch.rand(size=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">random_tensor, random_tensor.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0.6541, 0.4807, 0.2162, 0.6168],</span><br><span class="line">         [0.4428, 0.6608, 0.6194, 0.8620],</span><br><span class="line">         [0.2795, 0.6055, 0.4958, 0.5483]]),</span><br><span class="line"> torch.float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>创建随机图像张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a random tensor with similar shape to an image tensor</span></span><br><span class="line">random_image_size_tensor = torch.rand(size=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)) <span class="comment"># height, width, colour channels (R, G, B)</span></span><br><span class="line">random_image_size_tensor.shape, random_image_size_tensor.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([224, 224, 3]), 3)</span><br></pre></td></tr></table></figure>
<p>图像表示为具有形状的张量，[3, 224, 224]这意味着[colour_channels, height, width]，图像具有3颜色通道（红色、绿色、蓝色）、像素高度224和像素宽度224。</p>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-21.png" class="" title="PyTorch-26H-1-21">
<h3 id="Zeros-and-ones"><a href="#Zeros-and-ones" class="headerlink" title="Zeros and ones"></a>Zeros and ones</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor of all zeros</span></span><br><span class="line">zeros = torch.zeros(size=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">zeros</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0.]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor of all ones</span></span><br><span class="line">ones = torch.ones(size=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">ones</span><br><span class="line">ones.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.]])</span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure>
<h3 id="Creating-a-range-of-tensors-and-tensors-like"><a href="#Creating-a-range-of-tensors-and-tensors-like" class="headerlink" title="Creating a range of tensors and tensors-like"></a>Creating a range of tensors and tensors-like</h3><ul>
<li>数字范围</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use torch.range() and get deprecated message, use torch.arange()</span></span><br><span class="line">one_to_ten = torch.arange(start=<span class="number">1</span>, end=<span class="number">11</span>, step=<span class="number">1</span>)</span><br><span class="line">one_to_ten</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])</span><br></pre></td></tr></table></figure>
<ul>
<li>相同形状填充零或一的张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating tensors like</span></span><br><span class="line">ten_zeros = torch.zeros_like(<span class="built_in">input</span>=one_to_ten)</span><br><span class="line">ten_zeros</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating tensors like</span></span><br><span class="line">ten_zeros = torch.oness_like(<span class="built_in">input</span>=one_to_ten)</span><br><span class="line">ten_zeros</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</span><br></pre></td></tr></table></figure>
<h2 id="Tensor-datatypes"><a href="#Tensor-datatypes" class="headerlink" title="Tensor datatypes"></a>Tensor datatypes</h2><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#data-types">torch.Tensor-Data tpyes</a></p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Precision_(computer_science">Precision in computing</a>#:~:text=In%20computer%20science%2C%20the%20precision,used%20to%20express%20a%20value)</p>
<p>张量数据类型是使用 PyTorch 和深度学习时会遇到的 3 个大错误之一：</p>
<ol>
<li>张量数据类型不正确datatype</li>
<li>张量形状不正确shape</li>
<li>张量不在正确的设备上device</li>
</ol>
<ul>
<li>默认为float32</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Float 32 tensor</span></span><br><span class="line">float_32_tensor = torch.tensor([<span class="number">3.0</span>, <span class="number">6.0</span>, <span class="number">9.0</span>],</span><br><span class="line">                               dtype=<span class="literal">None</span>)</span><br><span class="line">float_32_tensor</span><br><span class="line">float_32_tensor.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([3., 6., 9.])</span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure>
<ul>
<li>修改默认数据类型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Float 32 tensor</span></span><br><span class="line">float_32_tensor = torch.tensor([<span class="number">3.0</span>, <span class="number">6.0</span>, <span class="number">9.0</span>],</span><br><span class="line">                               dtype=torch.float16)</span><br><span class="line">float_32_tensor</span><br><span class="line">float_32_tensor.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([3., 6., 9.]), dtype=torch.float16</span><br><span class="line">torch.float16</span><br></pre></td></tr></table></figure>
<ul>
<li>设备和记录操作</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Float 32 tensor</span></span><br><span class="line">float_32_tensor = torch.tensor([<span class="number">3.0</span>, <span class="number">6.0</span>, <span class="number">9.0</span>],</span><br><span class="line">                               dtype=<span class="literal">None</span>,	<span class="comment">#张量数据类型</span></span><br><span class="line">                               device=<span class="literal">None</span>,	<span class="comment">#cpu or cuda</span></span><br><span class="line">                               requires_grad=<span class="literal">False</span>)	<span class="comment">#是否使用此张量操作跟踪</span></span><br><span class="line">float_32_tensor</span><br><span class="line">float_32_tensor.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([3., 6., 9.])</span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure>
<ul>
<li>张量转换32 → 16</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">float_16_tensor = float_32_tensor.<span class="built_in">type</span>(torch.float16)</span><br><span class="line">float_16_tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([3., 6., 9.], dtype=torch.float16)</span><br></pre></td></tr></table></figure>
<ul>
<li>不同数据类型张量相乘</li>
</ul>
<p>进行了自动类型转换，向上转换</p>
<p>float16 × float32 → float32</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test = float_16_tensor * float_32_tensor</span><br><span class="line">test, test.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 9., 36., 81.]), torch.float32)</span><br></pre></td></tr></table></figure>
<p>int64 × float32 → float32</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int_32_tensor = torch.tensor([<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>], dtype=torch.int64)</span><br><span class="line">int_32_tensor, int_32_tensor.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([3, 6, 9]), torch.int64)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test = float_32_tensor * int_32_tensor</span><br><span class="line">test, test.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 9., 36., 81.]), torch.float32)</span><br></pre></td></tr></table></figure>
<p>long × float32 → float32</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int_32_tensor = torch.tensor([<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>], dtype=torch.long)</span><br><span class="line">int_32_tensor, int_32_tensor.dtpye</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([3, 6, 9]), torch.int64)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test = float_32_tensor * int_32_tensor</span><br><span class="line">test, test.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 9., 36., 81.]), torch.float32)</span><br></pre></td></tr></table></figure>
<h2 id="Tensor-attributes-information-about-tensors"><a href="#Tensor-attributes-information-about-tensors" class="headerlink" title="Tensor attributes (information about tensors)"></a>Tensor attributes (information about tensors)</h2><ul>
<li>shape/size()：形状</li>
<li>dtpye：数据类型</li>
<li>device：设备</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor</span></span><br><span class="line">some_tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find out details about it</span></span><br><span class="line"><span class="built_in">print</span>(some_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;some_tensor.shape&#125;</span>&quot;</span>)	<span class="comment">#some_tensor.shape=some_tensor.size()</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;some_tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;some_tensor.device&#125;</span>&quot;</span>) <span class="comment"># default to CPU</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4688, 0.0055, 0.8551, 0.0646],</span><br><span class="line">        [0.6538, 0.5157, 0.4071, 0.2109],</span><br><span class="line">        [0.9960, 0.3061, 0.9369, 0.7008]])</span><br><span class="line">Shape of tensor: torch.Size([3, 4])</span><br><span class="line">Datatype of tensor: torch.float32</span><br><span class="line">Device tensor is stored on: cpu</span><br></pre></td></tr></table></figure>
<h2 id="Manipulating-tensors"><a href="#Manipulating-tensors" class="headerlink" title="Manipulating tensors"></a>Manipulating tensors</h2><ul>
<li>Addition：加法</li>
<li>Substraction：减法</li>
<li>Multiplication (element-wise)：乘法</li>
<li>Division：除法</li>
<li>Matrix multiplication：矩阵乘法</li>
</ul>
<h3 id="basic-operations"><a href="#basic-operations" class="headerlink" title="basic operations"></a>basic operations</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor of values and add a number to it</span></span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor + <span class="number">10</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([11, 12, 13])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Multiply it by 10</span></span><br><span class="line">tensor * <span class="number">10</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([10, 20, 30])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Tensors don&#x27;t change unless reassigned</span><br><span class="line">tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Subtract and reassign</span></span><br><span class="line">tensor = tensor - <span class="number">10</span></span><br><span class="line">tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-9, -8, -7])</span><br></pre></td></tr></table></figure>
<p>PyTorch也有一些内置函数，如<code>torch.mul()</code>(乘法的缩写)和<code>torch.add()</code>来执行基本操作。</p>
<h3 id="Matrix-multiplication"><a href="#Matrix-multiplication" class="headerlink" title="Matrix multiplication"></a>Matrix multiplication</h3><p><a target="_blank" rel="noopener" href="https://www.mathsisfun.com/algebra/matrix-multiplying.html">How to Multiply Matrices</a></p>
<p>矩阵乘法需要记住的两个主要规则是：</p>
<ol>
<li><p><strong>内部尺寸必须匹配</strong>：<br>(3, 2) @ (3, 2)不起作用<br>(2, 3) @ (3, 2)会起作用<br>(3, 2) @ (2, 3)会起作用</p>
</li>
<li><p><strong>得到的矩阵具有外部尺寸的形状</strong>：<br>(2, 3) @ (3, 2)-&gt;(2, 2)<br>(3, 2) @ (2, 3)-&gt;(3, 3)</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3])</span><br></pre></td></tr></table></figure>
<ul>
<li>元素乘法：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor * tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 4, 9])</span><br></pre></td></tr></table></figure>
<ul>
<li>矩阵乘法：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.matmul(tensor, tensor)</span><br><span class="line"><span class="comment"># 1 * 1 + 2 * 2 + 3 * 3</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(14)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(tensor, tensor)</span><br><span class="line"><span class="comment"># 1 * 1 + 2 * 2 + 3 * 3</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(14)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor @ tensor</span><br><span class="line"><span class="comment"># 1 * 1 + 2 * 2 + 3 * 3</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(14)</span><br></pre></td></tr></table></figure>
<ul>
<li>内置<code>torch.matmul()</code>方法更快。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"><span class="comment"># Matrix multiplication by hand </span></span><br><span class="line"><span class="comment"># (avoid doing operations with for loops at all cost, they are computationally expensive)</span></span><br><span class="line">value = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tensor)):</span><br><span class="line">  value += tensor[i] * tensor[i]</span><br><span class="line">value</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CPU times: user 773 µs, sys: 0 ns, total: 773 µs</span><br><span class="line">Wall time: 499 µs</span><br><span class="line">tensor(14)</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">%time</span></span><br><span class="line">torch.matmul(tensor, tensor)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CPU times: user 146 µs, sys: 83 µs, total: 229 µs</span><br><span class="line">Wall time: 171 µs</span><br><span class="line">tensor(14)</span><br></pre></td></tr></table></figure>
<h3 id="深度学习中最常见的错误之一（形状错误）"><a href="#深度学习中最常见的错误之一（形状错误）" class="headerlink" title="深度学习中最常见的错误之一（形状错误）"></a>深度学习中最常见的错误之一（形状错误）</h3><p><a target="_blank" rel="noopener" href="http://matrixmultiplication.xyz/">matrixmultiplication矩阵可视化</a></p>
<p>由于深度学习的大部分内容是对矩阵进行乘法和执行运算，并且矩阵对于可以组合的形状和大小有严格的规则，因此在深度学习中遇到的最常见错误之一就是形状不匹配。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shapes need to be in the right way  </span></span><br><span class="line">tensor_A = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                         [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">                         [<span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">tensor_B = torch.tensor([[<span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">                         [<span class="number">8</span>, <span class="number">11</span>], </span><br><span class="line">                         [<span class="number">9</span>, <span class="number">12</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">torch.matmul(tensor_A, tensor_B) <span class="comment"># (this will error)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">RuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)</span><br></pre></td></tr></table></figure>
<p>通过矩阵的内部维度匹配来使矩阵乘法在tensor_A和之间进行tensor_B。</p>
<p><strong>使用转置（切换给定张量的维度）。</strong></p>
<p>您可以使用以下任一方式在 PyTorch 中执行转置：</p>
<ul>
<li><code>torch.transpose(input, dim0, dim1)</code>， 其中 <code>input</code> 是需要转置的张量， 和 <code>dim0</code> 是 <code>dim1</code> 需要交换的维度。</li>
<li><code>tensor.T</code>，<code>tensor</code> 是需要转置的张量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tensor([[1., 2.],</span></span><br><span class="line"><span class="comment">#        [3., 4.],</span></span><br><span class="line"><span class="comment">#        [5., 6.]])</span></span><br><span class="line"><span class="comment">#tensor([[ 7., 10.],</span></span><br><span class="line"><span class="comment">#        [ 8., 11.],</span></span><br><span class="line"><span class="comment">#        [ 9., 12.]])</span></span><br><span class="line"><span class="built_in">print</span>(tensor_A)</span><br><span class="line"><span class="built_in">print</span>(tensor_B.T)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.],</span><br><span class="line">        [5., 6.]])</span><br><span class="line">tensor([[ 7.,  8.,  9.],</span><br><span class="line">        [10., 11., 12.]])</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">The operation works when tensor_B is transposed</span></span><br><span class="line">print(f&quot;Original shapes: tensor_A = &#123;tensor_A.shape&#125;, tensor_B = &#123;tensor_B.shape&#125;\n&quot;)</span><br><span class="line">print(f&quot;New shapes: tensor_A = &#123;tensor_A.shape&#125; (same as above), tensor_B.T = &#123;tensor_B.T.shape&#125;\n&quot;)</span><br><span class="line">print(f&quot;Multiplying: &#123;tensor_A.shape&#125; * &#123;tensor_B.T.shape&#125; &lt;- inner dimensions match\n&quot;)</span><br><span class="line">print(&quot;Output:\n&quot;)</span><br><span class="line">output = torch.matmul(tensor_A, tensor_B.T)</span><br><span class="line">print(output) </span><br><span class="line">print(f&quot;\nOutput shape: &#123;output.shape&#125;&quot;)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])</span><br><span class="line"></span><br><span class="line">New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])</span><br><span class="line"></span><br><span class="line">Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"></span><br><span class="line">tensor([[ 27.,  30.,  33.],</span><br><span class="line">        [ 61.,  68.,  75.],</span><br><span class="line">        [ 95., 106., 117.]])</span><br><span class="line"></span><br><span class="line">Output shape: torch.Size([3, 3])</span><br></pre></td></tr></table></figure>
<img src="/2024/08/14/PyTorch-26H-1/PyTorch-26H-1-22.gif" class="" title="PyTorch-26H-1-22">
<h3 id="像这样的矩阵乘法也称为两个矩阵的点积。"><a href="#像这样的矩阵乘法也称为两个矩阵的点积。" class="headerlink" title="像这样的矩阵乘法也称为两个矩阵的点积。"></a>像这样的矩阵乘法也称为两个矩阵的点积。</h3><p>神经网络充满了矩阵乘法和点积。</p>
<p>该<code>torch.nn.Linear()</code>模块（我们稍后会看到它的实际作用），也称为前馈层或全连接层，实现输入<code>x</code>和权重矩阵之间的矩阵乘法<code>A</code>。</p>
<script type="math/tex; mode=display">y = x\cdot{A^T} + b</script><ul>
<li>x是该层的输入（深度学习是将多个层<code>torch.nn.Linear()</code>和其他层堆叠在一起）。</li>
<li>A是由该层创建的权重矩阵，它开始是随机数，随着神经网络学习更好地表示数据中的模式，这些随机数会进行调整（请注意“ T”，这是因为权重矩阵被转置了）。</li>
<li>注意：您可能还经常看到<code>W</code>或另一个字母，<code>X</code>用于展示权重矩阵。</li>
<li><code>b</code>是用于稍微偏移权重和输入的偏差项。</li>
<li><code>y</code>是输出（对输入进行操作以期发现其中的模式）。</li>
</ul>
<p>这是一个线性函数（你可能在高中或其他地方见过类似 $y = mx+b$ 的函数），可以用来画一条直线！</p>
<p>让我们尝试一下线性层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Since the linear layer starts with a random weights matrix, let&#x27;s make it reproducible (more on this later)由于线性层以随机权重矩阵开始，因此让我们使其可重现</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># This uses matrix multiplication使用矩阵乘法</span></span><br><span class="line">linear = torch.nn.Linear(in_features=<span class="number">2</span>, <span class="comment"># in_features = matches inner dimension of input 匹配输入的内维度</span></span><br><span class="line">                         out_features=<span class="number">6</span>) <span class="comment"># out_features = describes outer value 描述外部值</span></span><br><span class="line">x = tensor_A</span><br><span class="line">output = linear(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Input shape: <span class="subst">&#123;x.shape&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output:\n<span class="subst">&#123;output&#125;</span>\n\nOutput shape: <span class="subst">&#123;output.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Input shape: torch.Size([3, 2])</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],</span><br><span class="line">        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],</span><br><span class="line">        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],</span><br><span class="line">       grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Output shape: torch.Size([3, 6])</span><br></pre></td></tr></table></figure>
<h2 id="Finding-the-min-max-mean-amp-sum"><a href="#Finding-the-min-max-mean-amp-sum" class="headerlink" title="Finding the min, max, mean &amp; sum"></a>Finding the min, max, mean &amp; sum</h2><h3 id="最小最大平均求和"><a href="#最小最大平均求和" class="headerlink" title="最小最大平均求和"></a>最小最大平均求和</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor</span></span><br><span class="line">x = torch.arange(<span class="number">0</span>, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Minimum: <span class="subst">&#123;x.<span class="built_in">min</span>()&#125;</span>&quot;</span>)	<span class="comment">#torch.min(x)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Maximum: <span class="subst">&#123;x.<span class="built_in">max</span>()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(f&quot;Mean: &#123;x.mean()&#125;&quot;) # this will error</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Mean: <span class="subst">&#123;x.<span class="built_in">type</span>(torch.float32).mean()&#125;</span>&quot;</span>) <span class="comment"># won&#x27;t work without float datatype</span></span><br><span class="line"><span class="comment"># torch.mean(x.type(torch.float32))</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Sum: <span class="subst">&#123;x.<span class="built_in">sum</span>()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Minimum: 0</span><br><span class="line">Maximum: 90</span><br><span class="line">Mean: 45.0</span><br><span class="line">Sum: 450</span><br></pre></td></tr></table></figure>
<h3 id="查找最大最小索引位置"><a href="#查找最大最小索引位置" class="headerlink" title="查找最大最小索引位置"></a>查找最大最小索引位置</h3><p><code>torch.argmax()</code>，<code>torch.argmin()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor</span></span><br><span class="line">tensor = torch.arange(<span class="number">10</span>, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor: <span class="subst">&#123;tensor&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Returns index of max and min values</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Index where max value occurs: <span class="subst">&#123;tensor.argmax()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Index where min value occurs: <span class="subst">&#123;tensor.argmin()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])</span><br><span class="line">Index where max value occurs: 8</span><br><span class="line">Index where min value occurs: 0</span><br></pre></td></tr></table></figure>
<h3 id="更改数据类型"><a href="#更改数据类型" class="headerlink" title="更改数据类型"></a>更改数据类型</h3><p><code>torch.Tensor.type(dtype=None)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor and check its datatype</span></span><br><span class="line">tensor = torch.arange(<span class="number">10.</span>, <span class="number">100.</span>, <span class="number">10.</span>)</span><br><span class="line">tensor.dtype</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.float32</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a float16 tensor</span></span><br><span class="line">tensor_float16 = tensor.<span class="built_in">type</span>(torch.float16)</span><br><span class="line">tensor_float16</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create an int8 tensor</span></span><br><span class="line">tensor_int8 = tensor.<span class="built_in">type</span>(torch.int8)</span><br><span class="line">tensor_int8</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>数字越小（例如 32、16、8），计算机存储的值就越不精确。存储量越少，通常计算速度越快，整体模型就越小。基于移动端的神经网络通常使用 8 位整数，与 float32 相比，它们更小、运行速度更快，但准确度较低。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">torch.Tensor-doc</a></p>
<h2 id="重塑、视图、堆叠、压缩、解压、置换"><a href="#重塑、视图、堆叠、压缩、解压、置换" class="headerlink" title="重塑、视图、堆叠、压缩、解压、置换"></a>重塑、视图、堆叠、压缩、解压、置换</h2><ul>
<li>Reshaping - reshapes an input tensor to a defined shape</li>
<li>View - return a view of a input tensor of certain shape but keep the same memory as the original tensor</li>
<li>Stacking - combine multiple tnesors on top of each other (vstack) or side by side(hstack)</li>
<li>Squeeze - removes all 1 dimensions from a tensor</li>
<li>Unsqueeze - add a 1 dimensions to a target tensor</li>
<li><p>Permute - Return a view of the input with dimensions permuted(swapped) in a certain way</p>
</li>
<li><p>重塑 - 将输入张量重塑为定义的形状</p>
</li>
<li>视图 - 返回特定形状的输入张量的视图，但保留与原始张量相同的内存</li>
<li>堆叠 - 将多个张量组合在一起（vstack）或并排组合（hstack）</li>
<li>压缩 - 从张量中删除所有 1 维</li>
<li>解压 - 向目标张量添加 1 维</li>
<li>置换 - 返回以特定方式置换（交换）维度的输入视图</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">torch.reshape(input, shape)</td>
<td style="text-align:center">重塑input为shape（如果兼容），也可以使用torch.Tensor.reshape()。</td>
</tr>
<tr>
<td style="text-align:center">Tensor.view(shape)</td>
<td style="text-align:center">shape返回与原始张量不同的但共享相同数据的原始张量的视图。</td>
</tr>
<tr>
<td style="text-align:center">torch.stack(tensors, dim=0)</td>
<td style="text-align:center">沿新维度（dim）连接一系列张量（tensor），所有tensors必须大小相同。</td>
</tr>
<tr>
<td style="text-align:center">torch.squeeze(input)</td>
<td style="text-align:center">挤压input以删除所有具有值的维度1。</td>
</tr>
<tr>
<td style="text-align:center">torch.unsqueeze(input, dim)</td>
<td style="text-align:center">返回在 处添加input的维度值。1dim</td>
</tr>
<tr>
<td style="text-align:center">torch.permute(input, dims)</td>
<td style="text-align:center">返回原始输入的视图，其尺寸已置换（重新排列）为 dims。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Reshaping-重塑"><a href="#Reshaping-重塑" class="headerlink" title="Reshaping-重塑"></a>Reshaping-重塑</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">1.</span>, <span class="number">8.</span>)</span><br><span class="line">x, x.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add an extra dimension</span></span><br><span class="line">x_reshaped = x.reshape(<span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">x_reshaped, x_reshaped.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</span><br></pre></td></tr></table></figure>
<p><code>x.reshape(1, 8)</code> → error，因为元素数和原来数组不相同。</p>
<p><code>x.reshape(2, 7)</code> → error，因为在不增加元素数量的情况情况下将元素数量增加一倍。</p>
<p><code>x.reshape(7, 1)</code> → 可以运行，行列转换。</p>
<h3 id="View-视图"><a href="#View-视图" class="headerlink" title="View-视图"></a>View-视图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Change view (keeps same data as original but changes view)</span></span><br><span class="line"><span class="comment"># See more: https://stackoverflow.com/a/54507446/7900723</span></span><br><span class="line">z = x.view(<span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">z, z.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</span><br></pre></td></tr></table></figure>
<p>改变张量的视图实际上只会创建同<code>torch.view()</code>一张量的新视图。</p>
<p>因此<strong>改变视图也会改变原始张量。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Changing z changes x</span></span><br><span class="line">z[:, <span class="number">0</span>] = <span class="number">5</span></span><br><span class="line">z, x</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))</span><br></pre></td></tr></table></figure>
<h3 id="Stacking-堆叠"><a href="#Stacking-堆叠" class="headerlink" title="Stacking-堆叠"></a>Stacking-堆叠</h3><p>新的张量堆叠在其自身之上五次，<code>torch.stack()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stack tensors on top of each other</span></span><br><span class="line">x_stacked = torch.stack([x, x, x, x], dim=<span class="number">0</span>)</span><br><span class="line">x_stacked</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5., 2., 3., 4., 5., 6., 7.],</span><br><span class="line">        [5., 2., 3., 4., 5., 6., 7.],</span><br><span class="line">        [5., 2., 3., 4., 5., 6., 7.],</span><br><span class="line">        [5., 2., 3., 4., 5., 6., 7.]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stack tensors on top of each other</span></span><br><span class="line">x_stacked = torch.stack([x, x, x, x], dim=<span class="number">1</span>)</span><br><span class="line">x_stacked</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5., 5., 5., 5.],</span><br><span class="line">        [2., 2., 2., 2.],</span><br><span class="line">        [3., 3., 3., 3.],</span><br><span class="line">        [4., 4., 4., 4.],</span><br><span class="line">        [5., 5., 5., 5.],</span><br><span class="line">        [6., 6., 6., 6.],</span><br><span class="line">        [7., 7., 7., 7.]])</span><br></pre></td></tr></table></figure>
<p><code>x_stacked = torch.stack([x, x, x, x], dim=2)</code> 维度为2不行，因为原来的形状与二维不兼容。</p>
<h4 id="torch-stack-和-torch-vstack-和-torch-hstack"><a href="#torch-stack-和-torch-vstack-和-torch-hstack" class="headerlink" title="torch.stack 和 torch.vstack 和 torch.hstack"></a>torch.stack 和 torch.vstack 和 torch.hstack</h4><ul>
<li>stack</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/main/generated/torch.stack.html">torch.stack</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack(tensors, dim=0, *, out=None) → Tensor</span><br></pre></td></tr></table></figure>
<p>沿新维度连接一系列张量。</p>
<p>所有张量都需要具有相同的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">x, x.size()</span><br><span class="line"></span><br><span class="line">x1 = torch.stack((x, x)) <span class="comment"># same as torch.stack((x, x), dim=0)</span></span><br><span class="line">x1, x1.size()</span><br><span class="line"></span><br><span class="line">x2 = torch.stack((x, x), dim=<span class="number">1</span>)</span><br><span class="line">x2, x2.size()</span><br><span class="line"></span><br><span class="line">x3 = torch.stack((x, x), dim=<span class="number">2</span>)</span><br><span class="line">x3, x3.size()</span><br><span class="line"></span><br><span class="line">x4 = torch.stack((x, x), dim=-<span class="number">1</span>)</span><br><span class="line">x4, x4.size()</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.1801,  0.1566, -2.0349],</span><br><span class="line">         [ 0.0183, -0.0088, -0.6409]]),</span><br><span class="line"> torch.Size([2, 3]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(tensor([[[ 1.3646, -0.4994, -0.6540],</span><br><span class="line">          [-0.3804, -0.2373,  1.8952]],</span><br><span class="line"> </span><br><span class="line">         [[ 1.3646, -0.4994, -0.6540],</span><br><span class="line">          [-0.3804, -0.2373,  1.8952]]]),</span><br><span class="line"> torch.Size([2, 2, 3]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(tensor([[[ 1.3646, -0.4994, -0.6540],</span><br><span class="line">          [ 1.3646, -0.4994, -0.6540]],</span><br><span class="line"> </span><br><span class="line">         [[-0.3804, -0.2373,  1.8952],</span><br><span class="line">          [-0.3804, -0.2373,  1.8952]]]),</span><br><span class="line"> torch.Size([2, 2, 3]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(tensor([[[ 1.3646,  1.3646],</span><br><span class="line">          [-0.4994, -0.4994],</span><br><span class="line">          [-0.6540, -0.6540]],</span><br><span class="line"> </span><br><span class="line">         [[-0.3804, -0.3804],</span><br><span class="line">          [-0.2373, -0.2373],</span><br><span class="line">          [ 1.8952,  1.8952]]]),</span><br><span class="line"> torch.Size([2, 3, 2]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(tensor([[[ 1.3646,  1.3646],</span><br><span class="line">          [-0.4994, -0.4994],</span><br><span class="line">          [-0.6540, -0.6540]],</span><br><span class="line"> </span><br><span class="line">         [[-0.3804, -0.3804],</span><br><span class="line">          [-0.2373, -0.2373],</span><br><span class="line">          [ 1.8952,  1.8952]]]),</span><br><span class="line"> torch.Size([2, 3, 2]))</span><br></pre></td></tr></table></figure>
<ul>
<li>vstack</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.vstack.html">torch.vstack</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.vstack(tensors, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure>
<p>按垂直顺序（按行）堆叠张量。</p>
<p>这相当于在所有一维张量被重塑后沿第一个轴进行连接<code>torch.atleast_2d()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">torch.vstack((a,b))</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])</span><br><span class="line">torch.vstack((a,b))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6]])</span><br><span class="line"></span><br><span class="line">tensor([[1],</span><br><span class="line">        [2],</span><br><span class="line">        [3],</span><br><span class="line">        [4],</span><br><span class="line">        [5],</span><br><span class="line">        [6]])</span><br></pre></td></tr></table></figure>
<ul>
<li>hstack</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.hstack.html">torch.hstack</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.hstack(tensors, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure>
<p>按水平顺序（按列）堆叠张量。</p>
<p>这相当于沿第一个轴对一维张量进行连接，并沿第二个轴对所有其他张量进行连接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">torch.hstack((a,b))</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])</span><br><span class="line">torch.hstack((a,b))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3, 4, 5, 6])</span><br><span class="line"></span><br><span class="line">tensor([[1, 4],</span><br><span class="line">        [2, 5],</span><br><span class="line">        [3, 6]])</span><br></pre></td></tr></table></figure>
<h3 id="Squeeze-压缩"><a href="#Squeeze-压缩" class="headerlink" title="Squeeze-压缩"></a>Squeeze-压缩</h3><p>从张量中删除所有单一维度（将张量压缩为仅具有超过 1 的维度）</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/main/generated/torch.squeeze.html">torch.squeeze</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.squeeze(<span class="built_in">input</span>: Tensor, dim: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">int</span>, <span class="type">List</span>[<span class="built_in">int</span>]]]) → Tensor</span><br></pre></td></tr></table></figure>
<p>返回已删除所有指定尺寸为input1的张量。</p>
<p>input：(A × 1 × B × C × 1 × D)<br>method：input.squeeze()<br>output：(A × B × C × D)</p>
<p>input：(A × 1 × B)<br>method：squeeze(input, 0)<br>output：(A × 1 × B)</p>
<p>input：(A × 1 × B)<br>method：squeeze(input, 1)<br>output：(A × B)</p>
<blockquote>
<p>返回的张量与输入张量共享存储，因此改变一个张量的内容也会改变另一个张量的内容。<br>果张量的批处理维度为 1，则squeeze(input) 也会删除批处理维度，这可能会导致意外错误。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous tensor: <span class="subst">&#123;x_reshaped&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous shape: <span class="subst">&#123;x_reshaped.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove extra dimension from x_reshaped</span></span><br><span class="line">x_squeezed = x_reshaped.squeeze()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nNew tensor: <span class="subst">&#123;x_squeezed&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New shape: <span class="subst">&#123;x_squeezed.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])</span><br><span class="line">Previous shape: torch.Size([1, 7])</span><br><span class="line"></span><br><span class="line">New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])</span><br><span class="line">New shape: torch.Size([7])</span><br></pre></td></tr></table></figure>
<h3 id="Unsqueeze-解压"><a href="#Unsqueeze-解压" class="headerlink" title="Unsqueeze-解压"></a>Unsqueeze-解压</h3><p>在特定索引处添加维度值 1</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/main/generated/torch.unsqueeze.html">torch.unsqueeze</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim) → Tensor</span><br></pre></td></tr></table></figure>
<p>返回在指定位置插入一个维度为一的新张量。</p>
<p>返回的张量与该张量共享相同的底层数据。</p>
<p>可以使用dim范围内的值。负数将对应于= 处的应用。[-input.dim() - 1, input.dim() + 1)dimunsqueeze()dimdim + input.dim() + 1</p>
<p>可以使用 <code>[-input.dim() - 1, input.dim() + 1)</code> 范围内的 dim 值。负 dim 将对应于在 <code>dim = dim + input.dim() + 1</code> 处应用的 unsqueeze()。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous tensor: <span class="subst">&#123;x_squeezed&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous shape: <span class="subst">&#123;x_squeezed.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Add an extra dimension with unsqueeze</span></span><br><span class="line">x_unsqueezed = x_squeezed.unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nNew tensor: <span class="subst">&#123;x_unsqueezed&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New shape: <span class="subst">&#123;x_unsqueezed.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_unsqueezed = x_squeezed.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nNew tensor: <span class="subst">&#123;x_unsqueezed&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New shape: <span class="subst">&#123;x_unsqueezed.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])</span><br><span class="line">Previous shape: torch.Size([7])</span><br><span class="line"></span><br><span class="line">New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])</span><br><span class="line">New shape: torch.Size([1, 7])</span><br><span class="line"></span><br><span class="line">New tensor: tensor([[5.],</span><br><span class="line">					[2.],</span><br><span class="line">                    [3.],</span><br><span class="line">                    [4.],</span><br><span class="line">                    [5.],</span><br><span class="line">                    [6.],</span><br><span class="line">                    [7.]])</span><br><span class="line">New shape: torch.Size([7, 1])</span><br></pre></td></tr></table></figure>
<h3 id="Permute-置换"><a href="#Permute-置换" class="headerlink" title="Permute-置换"></a>Permute-置换</h3><p>重新排列轴值的顺序</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.permute.html">torch.permute</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.permute(<span class="built_in">input</span>, dims) → Tensor</span><br></pre></td></tr></table></figure>
<p>返回维度已排列的原始张量输入的视图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.permute(x, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).size()</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create tensor with specific shape</span></span><br><span class="line">x_original = torch.rand(size=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Permute the original tensor to rearrange the axis order</span></span><br><span class="line">x_permuted = x_original.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment"># shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Previous shape: <span class="subst">&#123;x_original.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New shape: <span class="subst">&#123;x_permuted.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Previous shape: torch.Size([224, 224, 3])</span><br><span class="line">New shape: torch.Size([3, 224, 224])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>因为排列返回一个视图（与原始共享相同的数据），所以排列张量中的值将与原始张量相同，如果更改视图中的值，它将更改原始的值。</p>
</blockquote>
<h2 id="Selecting-data-indexing"><a href="#Selecting-data-indexing" class="headerlink" title="Selecting data (indexing)"></a>Selecting data (indexing)</h2><p>从张量中选择特定数据（例如，仅第一列或第二行）。可以使用索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor </span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">10</span>).reshape(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x, x.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[[1, 2, 3],</span><br><span class="line">          [4, 5, 6],</span><br><span class="line">          [7, 8, 9]]]),</span><br><span class="line"> torch.Size([1, 3, 3]))</span><br></pre></td></tr></table></figure>
<p>索引值从外部维度 -&gt; 内部维度（检查方括号）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let&#x27;s index bracket by bracket</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First square bracket:\n<span class="subst">&#123;x[<span class="number">0</span>]&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Second square bracket: <span class="subst">&#123;x[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Third square bracket: <span class="subst">&#123;x[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">First square bracket:</span><br><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6],</span><br><span class="line">        [7, 8, 9]])</span><br><span class="line">Second square bracket: tensor([1, 2, 3])</span><br><span class="line">Third square bracket: 1</span><br></pre></td></tr></table></figure>
<p><code>:</code>指定“此维度中的所有值”，然后使用逗号 ( <code>,</code>) 添加另一个维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get all values of 0th dimension and the 0 index of 1st dimension, 获取第 0 维的所有值以及第 1 维的 0 索引</span></span><br><span class="line">x[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 2, 3]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get all values of 0th &amp; 1st dimensions but only index 1 of 2nd dimension, 获取第 0 维和第 1 维的所有值，但仅获取第 2 维的索引 1</span></span><br><span class="line">x[:, :, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[2, 5, 8]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension,获取 0 维的所有值，但仅获取第 1 维和第 2 维的 1 索引值</span></span><br><span class="line">x[:, <span class="number">1</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([5])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get index 0 of 0th and 1st dimension and all values of 2nd dimension,获取第 0 维和第 1 维的索引 0 以及第 2 维的所有值</span></span><br><span class="line">x[<span class="number">0</span>, <span class="number">0</span>, :] <span class="comment"># same as x[0][0]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Index on x to return 9</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>][<span class="number">2</span>][<span class="number">2</span>])</span><br><span class="line"><span class="comment">#x[0, 2, 2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Index on x to return 3, 6, 9</span></span><br><span class="line"><span class="built_in">print</span>(x[:, :, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># x[0, :, 2]</span></span><br></pre></td></tr></table></figure>
<h2 id="PyTorch-tensors-and-NumPy"><a href="#PyTorch-tensors-and-NumPy" class="headerlink" title="PyTorch tensors and NumPy"></a>PyTorch tensors and NumPy</h2><p>NumPy 是一个流行的 Python 数值计算库，因此 PyTorch 具有与其良好交互的功能。</p>
<p>从 NumPy 到 PyTorch（以及返回）需要使用的两种主要方法是：</p>
<ul>
<li><code>torch.from_numpy(ndarray)</code> ： NumPy 数组 -&gt; PyTorch 张量。</li>
<li><code>torch.Tensor.numpy()</code> ： PyTorch 张量 -&gt; NumPy 数组。</li>
</ul>
<h3 id="NumPy-array-to-tensor"><a href="#NumPy-array-to-tensor" class="headerlink" title="NumPy array to tensor"></a>NumPy array to tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NumPy array to tensor</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">array = np.arange(<span class="number">1.0</span>, <span class="number">8.0</span>)</span><br><span class="line">tensor = torch.from_numpy(array)</span><br><span class="line"></span><br><span class="line">array, tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([1., 2., 3., 4., 5., 6., 7.]),</span><br><span class="line"> tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</span><br></pre></td></tr></table></figure>
<p>默认情况下，NumPy 数组是使用数据类型创建的float64，如果将其转换为 PyTorch 张量，将保留相同的数据类型（如上）。</p>
<p>许多 PyTorch 计算默认使用float32。</p>
<p>转换 NumPy 数组 (float64) -&gt; PyTorch 张量 (float64) -&gt; PyTorch 张量 (float32)，使用<code>tensor = torch.from_numpy(array).type(torch.float32)</code>。</p>
<h3 id="改变数组，张量不变"><a href="#改变数组，张量不变" class="headerlink" title="改变数组，张量不变"></a>改变数组，张量不变</h3><p>tensor上面重新分配了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改变数组，保留张量数组</span></span><br><span class="line"><span class="comment"># Change the array, keep the tensor</span></span><br><span class="line">array = array + <span class="number">1</span></span><br><span class="line">array, tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([2., 3., 4., 5., 6., 7., 8.]),</span><br><span class="line"> tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</span><br></pre></td></tr></table></figure>
<h3 id="Tensor-to-NumPy-array"><a href="#Tensor-to-NumPy-array" class="headerlink" title="Tensor to NumPy array"></a>Tensor to NumPy array</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tensor to NumPy array</span></span><br><span class="line">tensor = torch.ones(<span class="number">7</span>) <span class="comment"># create a tensor of ones with dtype=float32</span></span><br><span class="line">numpy_tensor = tensor.numpy() <span class="comment"># will be dtype=float32 unless changed</span></span><br><span class="line">tensor, numpy_tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1., 1., 1., 1., 1., 1., 1.]),</span><br><span class="line"> array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</span><br></pre></td></tr></table></figure>
<h3 id="改变张量，数组不变"><a href="#改变张量，数组不变" class="headerlink" title="改变张量，数组不变"></a>改变张量，数组不变</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Change the tensor, keep the array the same</span></span><br><span class="line">tensor = tensor + <span class="number">1</span></span><br><span class="line">tensor, numpy_tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([2., 2., 2., 2., 2., 2., 2.]),</span><br><span class="line"> array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</span><br></pre></td></tr></table></figure>
<h2 id="Reproducibility"><a href="#Reproducibility" class="headerlink" title="Reproducibility"></a>Reproducibility</h2><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch-Reproducibility</a></p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Random_seed">Random seed-WiKi</a></p>
<p>伪随机性。</p>
<p>计算机的设计从根本上来说就是确定性的（每个步骤都是可预测的），所以它们产生的随机性是模拟随机性。</p>
<p>神经网络从随机数开始描述数据中的模式（这些数字是糟糕的描述），并尝试使用张量运算（以及一些我们尚未讨论的其他内容）来改进这些随机数，以更好地描述数据中的模式。</p>
<p>流程：start with random numbers -&gt; tensor operations -&gt; try to make better (again and again and again)</p>
<p>尽管随机性很好而且很强大，但有时还是希望随机性少一点，因此可以进行可重复的实验。</p>
<p>A计算机上运行与B计算机上运行相同的代码是否可以获得相同（或非常相似）的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create two random tensors</span></span><br><span class="line">random_tensor_A = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">random_tensor_B = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor A:\n<span class="subst">&#123;random_tensor_A&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor B:\n<span class="subst">&#123;random_tensor_B&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does Tensor A equal Tensor B? (anywhere)&quot;</span>)</span><br><span class="line">random_tensor_A == random_tensor_B</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Tensor A:</span><br><span class="line">tensor([[0.8016, 0.3649, 0.6286, 0.9663],</span><br><span class="line">        [0.7687, 0.4566, 0.5745, 0.9200],</span><br><span class="line">        [0.3230, 0.8613, 0.0919, 0.3102]])</span><br><span class="line"></span><br><span class="line">Tensor B:</span><br><span class="line">tensor([[0.9536, 0.6002, 0.0351, 0.6826],</span><br><span class="line">        [0.3743, 0.5220, 0.1336, 0.9666],</span><br><span class="line">        [0.9754, 0.8474, 0.8988, 0.1105]])</span><br><span class="line"></span><br><span class="line">Does Tensor A equal Tensor B? (anywhere)</span><br><span class="line">tensor([[False, False, False, False],</span><br><span class="line">        [False, False, False, False],</span><br><span class="line">        [False, False, False, False]])</span><br></pre></td></tr></table></figure>
<p>创建两个具有相同值的随机张量。</p>
<p>张量仍然包含随机值，但它们具有相同的特性。</p>
<p>这就是<code>torch.manual_seed(seed)</code>出现的位置，其中seed是一个整数（类似于42但可以是任何东西），它决定了随机性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Set the random seed</span></span><br><span class="line">RANDOM_SEED=<span class="number">42</span> <span class="comment"># try changing this to different values and see what happens to the numbers below</span></span><br><span class="line">torch.manual_seed(seed=RANDOM_SEED) </span><br><span class="line">random_tensor_C = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Have to reset the seed every time a new rand() is called </span></span><br><span class="line"><span class="comment"># Without this, tensor_D would be different to tensor_C </span></span><br><span class="line">torch.random.manual_seed(seed=RANDOM_SEED) <span class="comment"># try commenting this line out and seeing what happens</span></span><br><span class="line">random_tensor_D = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor C:\n<span class="subst">&#123;random_tensor_C&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor D:\n<span class="subst">&#123;random_tensor_D&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does Tensor C equal Tensor D? (anywhere)&quot;</span>)</span><br><span class="line">random_tensor_C == random_tensor_D</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Tensor C:</span><br><span class="line">tensor([[0.8823, 0.9150, 0.3829, 0.9593],</span><br><span class="line">        [0.3904, 0.6009, 0.2566, 0.7936],</span><br><span class="line">        [0.9408, 0.1332, 0.9346, 0.5936]])</span><br><span class="line"></span><br><span class="line">Tensor D:</span><br><span class="line">tensor([[0.8823, 0.9150, 0.3829, 0.9593],</span><br><span class="line">        [0.3904, 0.6009, 0.2566, 0.7936],</span><br><span class="line">        [0.9408, 0.1332, 0.9346, 0.5936]])</span><br><span class="line"></span><br><span class="line">Does Tensor C equal Tensor D? (anywhere)</span><br><span class="line">tensor([[True, True, True, True],</span><br><span class="line">        [True, True, True, True],</span><br><span class="line">        [True, True, True, True]])</span><br></pre></td></tr></table></figure>
<h2 id="在-GPU-上运行张量（并进行更快的计算）"><a href="#在-GPU-上运行张量（并进行更快的计算）" class="headerlink" title="在 GPU 上运行张量（并进行更快的计算）"></a>在 GPU 上运行张量（并进行更快的计算）</h2><p>深度学习算法需要大量的数值运算。</p>
<p>默认情况下这些操作通常在 CPU（计算机处理单元）上完成。</p>
<p>然而，还有另一种常见的硬件，称为 GPU（图形处理单元），它在执行神经网络所需的特定类型的操作（矩阵乘法）时通常比 CPU 快得多。</p>
<h3 id="获取GPU"><a href="#获取GPU" class="headerlink" title="获取GPU"></a>获取GPU</h3><p><a target="_blank" rel="noopener" href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/">Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>Method</strong></th>
<th style="text-align:center"><strong>Difficulty to setup</strong></th>
<th style="text-align:center"><strong>Pros</strong></th>
<th style="text-align:center"><strong>Cons</strong></th>
<th style="text-align:center"><strong>How to setup</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Google Colab</td>
<td style="text-align:center">Easy</td>
<td style="text-align:center">Free to use, almost zero setup required, can share work with others as easy as a link</td>
<td style="text-align:center">Doesn’t save your data outputs, limited compute, subject to timeouts</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://colab.research.google.com/notebooks/gpu.ipynb">Follow the Google Colab Guide</a></td>
</tr>
<tr>
<td style="text-align:center">Use your own</td>
<td style="text-align:center">Medium</td>
<td style="text-align:center">Run everything locally on your own machine</td>
<td style="text-align:center">GPUs aren’t free, require upfront cost</td>
<td style="text-align:center">Follow the <a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">PyTorch   installation guidelines</a></td>
</tr>
<tr>
<td style="text-align:center">Cloud computing (AWS, GCP, Azure)</td>
<td style="text-align:center">Medium-Hard</td>
<td style="text-align:center">Small upfront cost, access to almost infinite compute</td>
<td style="text-align:center">Can get expensive if running continually, takes some time to setup right</td>
<td style="text-align:center">Follow the <a target="_blank" rel="noopener" href="https://pytorch.org/get-started/cloud-partners/">PyTorch installation guidelines</a></td>
</tr>
</tbody>
</table>
</div>
<p>检查本机GPU</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\windows11&gt;nvidia-smi</span><br><span class="line">Fri Oct 11 20:12:11 2024</span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 551.68                 Driver Version: 551.68         CUDA Version: 12.4     |</span><br><span class="line">|-----------------------------------------+------------------------+----------------------+</span><br><span class="line">| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                        |               MIG M. |</span><br><span class="line">|=========================================+========================+======================|</span><br><span class="line">|   0  NVIDIA GeForce RTX 3090      WDDM  |   00000000:73:00.0  On |                  N/A |</span><br><span class="line">| 30%   44C    P8             30W /  350W |    2290MiB /  24576MiB |      3%      Default |</span><br><span class="line">|                                         |                        |                  N/A |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br></pre></td></tr></table></figure>
<h3 id="让-PyTorch-在-GPU-上运行"><a href="#让-PyTorch-在-GPU-上运行" class="headerlink" title="让 PyTorch 在 GPU 上运行"></a>让 PyTorch 在 GPU 上运行</h3><h4 id="检查cuda包"><a href="#检查cuda包" class="headerlink" title="检查cuda包"></a>检查cuda包</h4><p>一旦您准备好访问 GPU，下一步就是使用 PyTorch 来存储数据（张量）和计算数据（对张量执行操作）。</p>
<p>为此，您可以使用该<code>torch.cuda</code>包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check for GPU</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Fri Oct 11 12:55:41 2024       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |</span><br><span class="line">| N/A   37C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<p>如果上述输出为True，则 PyTorch 可以看到并使用 GPU；如果输出False，则它看不到 GPU，在这种情况下，您必须返回安装步骤。</p>
<h4 id="设置device"><a href="#设置device" class="headerlink" title="设置device"></a>设置device</h4><p>让我们创建一个device变量来存储可用的设备类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set device type</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">device</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;cuda&#x27;</span><br></pre></td></tr></table></figure>
<p>如果输出上述内容，”cuda”则意味着我们可以将所有 PyTorch 代码设置为使用可用的 CUDA 设备（GPU），如果输出”cpu”，则我们的 PyTorch 代码将坚持使用 CPU。</p>
<blockquote>
<p>在 PyTorch 中，最佳做法是编写与设备无关的代码。这意味着代码将在 CPU（始终可用）或 GPU（如果可用）上运行。</p>
</blockquote>
<p>best-practices：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/main/notes/cuda.html">PyTroch CUDA semantics</a></p>
<h4 id="多个GPU计算"><a href="#多个GPU计算" class="headerlink" title="多个GPU计算"></a>多个GPU计算</h4><p>如果您想要进行更快的计算，则可以使用 GPU，但如果您想进行更快的计算，则可以使用多个 GPU。</p>
<p>您可以计算 PyTorch 可以使用的 GPU 数量<code>torch.cuda.device_count()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Count number of devices</span></span><br><span class="line">torch.cuda.device_count()</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure>
<p>了解 PyTorch 可以访问的 GPU 数量很有帮助，以防您想在一个 GPU 上运行特定进程，而在另一个 GPU 上运行另一个进程（PyTorch 还具有允许您在所有GPU 上运行进程的功能）。</p>
<h3 id="将张量（和模型）放在-GPU-上"><a href="#将张量（和模型）放在-GPU-上" class="headerlink" title="将张量（和模型）放在 GPU 上"></a>将张量（和模型）放在 GPU 上</h3><p>您可以通过对张量（和模型，我们稍后会看到）调用 <code>to(device)</code> 来将其放置在特定设备上。其中 <code>device</code> 是您希望张量（或模型）转到的目标设备。</p>
<p>为什么要这么做？</p>
<p>GPU 提供的数值计算速度比 CPU 快得多，并且如果 GPU 不可用，由于我们的设备无关代码（参见上文），它将在 CPU 上运行。</p>
<p>使用 <code>to(device)</code> 将张量放在 GPU 上（例如 <code>some_tensor.to(device)</code>）将返回该张量的副本，例如，相同的张量将存在于 CPU 和 GPU 上。要覆盖张量，请重新分配它们：<code>some_tensor = some_tensor.to(device)</code></p>
<h4 id="创建一个张量并将其放在-GPU-上"><a href="#创建一个张量并将其放在-GPU-上" class="headerlink" title="创建一个张量并将其放在 GPU 上"></a>创建一个张量并将其放在 GPU 上</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create tensor (default on CPU)</span></span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensor not on GPU</span></span><br><span class="line"><span class="built_in">print</span>(tensor, tensor.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move tensor to GPU (if available)</span></span><br><span class="line">tensor_on_gpu = tensor.to(device)</span><br><span class="line">tensor_on_gpu</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3]) cpu</span><br><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<p>二个张量有device=’cuda:0’，这意味着它存储在第 0 个可用的 GPU 上（GPU 的索引为 0，如果有两个可用的 GPU，则它们分别为’cuda:0’和’cuda:1’，最多为’cuda:n’）。</p>
<h3 id="将张量移回-CPU"><a href="#将张量移回-CPU" class="headerlink" title="将张量移回 CPU"></a>将张量移回 CPU</h3><p>使用 NumPy 与张量进行交互（NumPy 不利用 GPU），您需要执行此操作。</p>
<p><code>torch.Tensor.numpy()</code>使用方法<code>tensor_on_gpu</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If tensor is on GPU, can&#x27;t transform it to NumPy (this will error)</span></span><br><span class="line">tensor_on_gpu.numpy()</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: can&#x27;t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.</span><br></pre></td></tr></table></figure>
<p>相反，为了将张量返回到 CPU 并可供 NumPy 使用，使用<code>Tensor.cpu()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instead, copy the tensor back to cpu</span></span><br><span class="line">tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()</span><br><span class="line">tensor_back_on_cpu</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 2, 3])</span><br></pre></td></tr></table></figure>
<p>上面返回了 CPU 内存中 GPU 张量的副本，原始张量仍然在 GPU 上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor_on_gpu</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<h3 id="GPU上的随机种子"><a href="#GPU上的随机种子" class="headerlink" title="GPU上的随机种子"></a>GPU上的随机种子</h3><p>在cpu上的随机种子，使用<code>torch.manual_seed()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)  <span class="comment"># 设置CPU种子</span></span><br><span class="line">torch.cuda.manual_seed(<span class="number">42</span>)  <span class="comment"># 设置当前GPU的种子</span></span><br><span class="line">torch.cuda.manual_seed_all(<span class="number">42</span>)  <span class="comment"># 如果使用多个GPU，则为所有GPU设置种子</span></span><br><span class="line"></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span>	<span class="comment">#确保卷积操作的确定性，强制使用确定性的算法。</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span>	<span class="comment">#禁用动态算法选择，这样算法的选择不会因为输入数据的变化而改变。</span></span><br></pre></td></tr></table></figure>
<p>通过上述设置，可以确保在 CPU 和 GPU 上运行的 PyTorch 代码具有可重复性。</p>
<h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><p><code>pytorch-deep-learning/extras/exercises</code></p>
<p>…\pytorch-deep-learning\extras\exercises\00_pytorch_fundamentals_exercises.ipynb<br>…\pytorch-deep-learning\extras\solutions\00_pytorch_fundamentals_exercise_solutions.ipynb</p>
<h2 id="配置pytroch环境"><a href="#配置pytroch环境" class="headerlink" title="配置pytroch环境"></a>配置pytroch环境</h2><p><a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/#windows-anaconda">PyTorch Get Started</a></p>
<p>pytorch 2.4.1<br>windows<br>conda<br>python<br>CUDA12.4</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pytorch241 python=3.8</span><br><span class="line"></span><br><span class="line">conda activate pytorch241</span><br><span class="line"></span><br><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia</span><br><span class="line"></span><br><span class="line">conda env remove -n pytorch241</span><br><span class="line">conda clean -i</span><br></pre></td></tr></table></figure></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HibisciDai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://hibiscidai.com/2024/08/14/PyTorch-26H-1/">http://hibiscidai.com/2024/08/14/PyTorch-26H-1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://hibiscidai.com">HibisciDai</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="social-share pull-right" data-disabled="linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2024/08/15/PyTorch-26H-2/"><i class="fa fa-chevron-left">  </i><span>PyTorch-26H-2</span></a></div><div class="next-post pull-right"><a href="/2024/08/13/Linux%E7%99%BB%E5%BD%95%E6%8F%90%E7%A4%BA%E8%AF%AD/"><span>Linux登录提示语</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://www.paofu.cloud/auth/register?code=j4I7">好用、实惠、稳定的梯子,点击这里<img src="https://pic.imgdb.cn/item/65572abac458853aefef30cd.png" width="1000" height="124" object-fit="cover" ></a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTQ2NC8xMjAwMA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(/img/banner2.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2025 By HibisciDai</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>